================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2024-08-24T21:41:26.135Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
alembic/
  versions/
    02a7f45559cf_initial_migration.py
    da95aeecb8f1_refactored_user_and_repo_models.py
    f2797c3367b1_added_language_and_repo_size_to_repo.py
  env.py
  README
  script.py.mako
notebooks/
  recommend.ipynb
src/
  auth/
    models.py
    permissions.py
    service.py
    sm.py
    views.py
  aws/
    sm.py
  database/
    core.py
    manage.py
  extensions/
    __init__.py
    sentry.py
  health/
    views.py
  queue/
    core.py
    models.py
    permissions.py
    service.py
    views.py
  repo/
    extensions.py
    models.py
    repository.py
    service.py
    views.py
  scripts/
    drop_db.py
    neuter_repo.py
    show_tables.py
  config.py
  exceptions.py
  logger.py
  models.py
  utils.py
.cursorignore
.gitignore
.gitmodules
main.py
pyproject.toml
repopack.config.json
requirements.txt
test.py
uvicorn.yaml

================================================================
Repository Files
================================================================

================
File: alembic/versions/02a7f45559cf_initial_migration.py
================
"""Initial migration

Revision ID: 02a7f45559cf
Revises: 
Create Date: 2024-08-22 13:51:57.184148

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '02a7f45559cf'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('repo_config',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('repo_name', sa.String(), nullable=True),
    sa.Column('url', sa.String(), nullable=True),
    sa.Column('remote', sa.String(), nullable=True),
    sa.Column('main', sa.String(), nullable=True),
    sa.Column('language', sa.String(), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('user',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('email', sa.String(), nullable=True),
    sa.Column('password', sa.LargeBinary(), nullable=False),
    sa.Column('last_mfa_time', sa.DateTime(), nullable=True),
    sa.Column('experimental_features', sa.Boolean(), nullable=True),
    sa.Column('admin', sa.Boolean(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('updated_at', sa.DateTime(), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('email')
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('user')
    op.drop_table('repo_config')
    # ### end Alembic commands ###

================
File: alembic/versions/da95aeecb8f1_refactored_user_and_repo_models.py
================
"""refactored user and repo models

Revision ID: da95aeecb8f1
Revises: 02a7f45559cf
Create Date: 2024-08-22 15:19:51.032770

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'da95aeecb8f1'
down_revision: Union[str, None] = '02a7f45559cf'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('repos',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('repo_name', sa.String(), nullable=True),
    sa.Column('url', sa.String(), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    op.drop_table('repo_config')
    op.add_column('user', sa.Column('repo_user_id', sa.Integer(), nullable=True))
    op.create_foreign_key(None, 'user', 'repos', ['repo_user_id'], ['id'])
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(None, 'user', type_='foreignkey')
    op.drop_column('user', 'repo_user_id')
    op.create_table('repo_config',
    sa.Column('id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('repo_name', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('url', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('remote', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('main', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.Column('language', sa.VARCHAR(), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('id', name='repo_config_pkey')
    )
    op.drop_table('repos')
    # ### end Alembic commands ###

================
File: alembic/versions/f2797c3367b1_added_language_and_repo_size_to_repo.py
================
"""added language and repo_size to Repo

Revision ID: f2797c3367b1
Revises: da95aeecb8f1
Create Date: 2024-08-22 19:46:45.724533

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'f2797c3367b1'
down_revision: Union[str, None] = 'da95aeecb8f1'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('repos', sa.Column('language', sa.String(), nullable=True))
    op.add_column('repos', sa.Column('repo_size', sa.Integer(), nullable=True))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('repos', 'repo_size')
    op.drop_column('repos', 'language')
    # ### end Alembic commands ###

================
File: alembic/env.py
================
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

from src.database.core import Base  # Wherever your Base is defined
from src.auth.models import User
from src.repo.models import Repo

from src.config import SQLALCHEMY_DATABASE_URI


# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = SQLALCHEMY_DATABASE_URI
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def patch_sql_url(config):
    config["sqlalchemy.url"] = SQLALCHEMY_DATABASE_URI

    return config


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        patch_sql_url(config.get_section(config.config_ini_section, {})),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

================
File: alembic/README
================
Generic single-database configuration.

================
File: alembic/script.py.mako
================
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}

================
File: notebooks/recommend.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cowboy-server-good/src/test_modules/models.py:18: SAWarning: This declarative base already contains a class with the same class name and module name as src.test_modules.models.TargetCodeModel, and will be replaced in the string-lookup table.\n",
      "  class TargetCodeModel(Base):\n",
      "[autoreload of src.test_modules.models failed: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/ubuntu/cowboy-server-good/src/test_modules/models.py\", line 18, in <module>\n",
      "    class TargetCodeModel(Base):\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/sqlalchemy/orm/decl_api.py\", line 196, in __init__\n",
      "    _as_declarative(reg, cls, dict_)\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py\", line 244, in _as_declarative\n",
      "    return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/.local/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py\", line 325, in setup_mapping\n",
      "    return _ClassScanMapperConfig(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "sqlalchemy.exc.InvalidRequestError: Table 'target_code' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing code ..\n",
      "Serializing code ..\n",
      "Serializing code ..\n",
      "Serializing code ..\n",
      "LINES:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:15: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:30: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:31: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:33: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:42: SyntaxWarning: invalid escape sequence '\\/'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m tm \u001b[38;5;241m=\u001b[39m tm\u001b[38;5;241m.\u001b[39mserialize(src_repo)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tm\u001b[38;5;241m.\u001b[39mchunks:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/cowboy-server-good/cowboy_lib/test_modules/test_module.py:185\u001b[0m, in \u001b[0;36mTestModule.print_chunks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m         curr_file \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28mrepr\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28mrepr\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mrepr\u001b[39m\n",
      "File \u001b[0;32m~/cowboy-server-good/cowboy_lib/test_modules/target_code.py:40\u001b[0m, in \u001b[0;36mTargetCode.to_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLINES: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlines)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mrepr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mrepr\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mrepr\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/cowboy-server-good\")\n",
    "\n",
    "from cowboy_lib.repo import SourceRepo\n",
    "from src.test_modules.service import get_all_tms\n",
    "\n",
    "from src.database.core import engine\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "repo_path = \"/home/ubuntu/cowboy-server-good/repos/test8/mbvyvqlp\"\n",
    "src_repo = SourceRepo(Path(repo_path))\n",
    "Session = sessionmaker(bind=engine)\n",
    "db_session = Session()\n",
    "\n",
    "tms = get_all_tms(db_session=db_session, repo_id=7)\n",
    "for tm in tms:\n",
    "    tm = tm.serialize(src_repo)\n",
    "    if tm.chunks:\n",
    "        print(tm.print_chunks())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: src/auth/models.py
================
from src.config import COWBOY_JWT_ALG, COWBOY_JWT_EXP, COWBOY_JWT_SECRET, ANON_LOGIN
from src.database.core import Base
from src.models import TimeStampMixin, RTFSBase, PrimaryKey

import random
import string
import secrets
import bcrypt
from jose import jwt
from typing import Optional
from pydantic import validator, Field, BaseModel, root_validator
from pydantic.networks import EmailStr
from sqlalchemy import (
    ForeignKey,
    DateTime,
    Column,
    String,
    LargeBinary,
    Integer,
    Boolean,
)
from datetime import datetime, timedelta


def generate_token(email):
    now = datetime.utcnow()
    exp = (now + timedelta(seconds=COWBOY_JWT_EXP)).timestamp()
    data = {
        "exp": exp,
        "email": email,
    }
    return jwt.encode(data, COWBOY_JWT_SECRET, algorithm=COWBOY_JWT_ALG)


def generate_password():
    """Generates a reasonable password if none is provided."""
    alphanumeric = string.ascii_letters + string.digits
    while True:
        password = "".join(secrets.choice(alphanumeric) for i in range(10))
        if (
            any(c.islower() for c in password)
            and any(c.isupper() for c in password)  # noqa
            and sum(c.isdigit() for c in password) >= 3  # noqa
        ):
            break
    return password

def generate_email():
    return "".join([random.choice(string.ascii_letters) for _ in range(10)]) + "@hotmail.com"


def hash_password(password: str):
    """Generates a hashed version of the provided password."""
    pw = bytes(password, "utf-8")
    salt = bcrypt.gensalt()
    return bcrypt.hashpw(pw, salt)


class User(Base, TimeStampMixin):
    __tablename__ = "user"

    id = Column(Integer, primary_key=True)
    email = Column(String, unique=True)
    password = Column(LargeBinary, nullable=False)
    last_mfa_time = Column(DateTime, nullable=True)
    experimental_features = Column(Boolean, default=False)
    admin = Column(Boolean, default=False)

    repo_user_id = Column(Integer, ForeignKey('repos.id'))

    # search_vector = Column(
    #     TSVectorType("email", regconfig="pg_catalog.simple", weights={"email": "A"})
    # )

    def check_password(self, password):
        return bcrypt.checkpw(password.encode("utf-8"), self.password)

    @property
    def token(self):
        return generate_token(self.email)


class UserBase(RTFSBase):
    email: Optional[EmailStr]


class UserLogin(UserBase):
    password: str

    @validator("password")
    def password_required(cls, v):
        if not v:
            raise ValueError("Must not be empty string")
        return v


class UserRegister(UserLogin):
    password: Optional[str] = Field(None, nullable=True)

    @root_validator(pre=True)
    def validate_or_anon_auth(cls, values):
        email = values.get("email", None)
        if not ANON_LOGIN and not email:
            raise ValueError("Email must not be empty string")
        if ANON_LOGIN and not email:
            values["email"] = generate_email()
        
        password = values.get("password", None)
        if not password:
            password = generate_password()
        
        values["password"] = hash_password(password)

        return values


# shit doesnt work when returning directly ...
class UserLoginResponse(RTFSBase):
    token: Optional[str] = Field(None, nullable=True) 

class UserRead(UserBase):
    id: PrimaryKey
    role: Optional[str] = Field(None, nullable=True)
    experimental_features: Optional[bool]


class UserUpdate(RTFSBase):
    id: PrimaryKey
    password: Optional[str] = Field(None, nullable=True)

    @validator("password", pre=True)
    def hash(cls, v):
        return hash_password(str(v))


class UserCreate(RTFSBase):
    email: EmailStr
    password: Optional[str] = Field(None, nullable=True)

    @validator("password", pre=True)
    def hash(cls, v):
        return hash_password(str(v))


class UserRegisterResponse(RTFSBase):
    token: Optional[str] = Field(None, nullable=True)


class UpdateOAIKey(BaseModel):
    openai_api_key: str

================
File: src/auth/permissions.py
================
from starlette.requests import Request

import logging
from abc import ABC, abstractmethod

from fastapi import HTTPException
from starlette.requests import Request
from starlette.status import HTTP_403_FORBIDDEN, HTTP_404_NOT_FOUND

from .service import get_current_user


class BasePermission(ABC):
    """
    Abstract permission that all other Permissions must be inherited from.

    Defines basic error message, status & error codes.

    Upon initialization, calls abstract method  `has_required_permissions`
    which will be specific to concrete implementation of Permission class.

    You would write your permissions like this:

    .. code-block:: python

        class TeapotUserAgentPermission(BasePermission):

            def has_required_permissions(self, request: Request) -> bool:
                return request.headers.get('User-Agent') == "Teapot v1.0"

    """

    user_error_msg = [{"msg": "User not found"}]
    user_error_code = HTTP_404_NOT_FOUND

    role = None

    # @abstractmethod
    # def has_required_permissions(self, request: Request) -> bool: ...

    def __init__(self, request: Request):
        user = get_current_user(request=request)
        if not user:
            raise HTTPException(
                status_code=self.user_error_code, detail=self.user_error_msg
            )

        # if not self.has_required_permissions(request):
        #     raise HTTPException(
        #         status_code=self.user_role_error_code, detail=self.user_role_error_msg
        #     )


class PermissionsDependency(object):
    """
    Permission dependency that is used to define and check all the permission
    classes from one place inside route definition.

    Use it as an argument to FastAPI's `Depends` as follows:

    .. code-block:: python

        app = FastAPI()

        @app.get(
            "/teapot/",
            dependencies=[Depends(
                PermissionsDependency([TeapotUserAgentPermission]))]
        )
        async def teapot() -> dict:
            return {"teapot": True}
    """

    def __init__(self, permissions_classes: list):
        self.permissions_classes = permissions_classes

    def __call__(self, request: Request):
        for permission_class in self.permissions_classes:
            permission_class(request=request)


# class InternalClientPermissions(BasePermission):
#     """
#     Permissions used by our internal client to get ahold of user JWT tokens
#     for testing and other purposes
#     """

#     def __init__(self, request: Request):
#         user = get_current_user(request=request)
#         if not user:
#             raise HTTPException(
#                 status_code=self.user_error_code, detail=self.user_error_msg
#             )

#         if not user.is_admin:
#             raise HTTPException(
#                 status_code=HTTP_403_FORBIDDEN, detail="User is not an admin"
#             )

================
File: src/auth/service.py
================
import logging

from typing import Optional

from fastapi import HTTPException
from fastapi.security.utils import get_authorization_scheme_param
from jose import JWTError, jwt
from jose.exceptions import JWKError
from starlette.requests import Request
from starlette.status import HTTP_401_UNAUTHORIZED

from src.config import COWBOY_JWT_SECRET
from src.database.core import DBNotSetException, get_db

from .models import generate_token


log = logging.getLogger(__name__)

from .sm import SecretManager
from .models import User, UserRegister, UserCreate

InvalidCredentialException = HTTPException(
    status_code=HTTP_401_UNAUTHORIZED,
    detail=[{"msg": "Could not validate credentials"}],
)


def get(*, db_session, user_id: int) -> Optional[User]:
    """Returns a user based on the given user id."""
    return db_session.query(User).filter(User.id == user_id).one_or_none()


def get_by_email(*, db_session, email: str) -> Optional[User]:
    """Returns a user object based on user email."""
    return db_session.query(User).filter(User.email == email).one_or_none()


def create(*, db_session, user_in: UserRegister | UserCreate) -> User:
    """Creates a new dispatch user."""
    # pydantic forces a string password, but we really want bytes
    password = bytes(user_in.password, "utf-8")

    # create the user
    user = User(
        **user_in.dict(exclude={"password", "openai_api_key"}),
        password=password,
    )
    db_session.add(user)
    db_session.commit()

    print("Token: ", user.token)
    # create the credentials
    # store_oai_key(user_in.openai_api_key, user.id)

    return user


def get_user_token(*, db_session, user_id):
    user = get(db_session=db_session, user_id=user_id)
    return generate_token(user.email)


def extract_user_email_jwt(request: Request, **kwargs):
    try:
        authorization: str = request.headers.get("Authorization")
        scheme, param = get_authorization_scheme_param(authorization)
        if not authorization or scheme.lower() != "bearer":
            log.exception(
                f"Malformed authorization header. Scheme: {scheme} Param: {param} Authorization: {authorization}"
            )
            return

        token = authorization.split()[1]
        data = jwt.decode(token, COWBOY_JWT_SECRET)
    except (JWKError, JWTError, IndexError, KeyError):
        raise HTTPException(
            status_code=HTTP_401_UNAUTHORIZED,
            detail=[{"msg": "Could not validate credentials"}],
        ) from None
    return data["email"]


# def get_current_user(request: Request) -> CowboyUser:
#     user_email = extract_user_email_jwt(request=request)

#     if not user_email:
#         log.exception(f"Failed to extract user email")
#         raise InvalidCredentialException

#     # kinda of strange ... if user exists, we generate a random password
#     # for the user here ...
#     return get_or_create(
#         db_session=request.state.db,
#         user_in=UserRegister(email=user_email),
#     )


def get_current_user(request: Request) -> User:
    user_email = extract_user_email_jwt(request=request)

    if not user_email:
        log.exception(f"Failed to extract user email")
        raise InvalidCredentialException

    # kinda of strange ... if user exists, we generate a random password
    # for the user here ...
    try:
        user = get_by_email(
            db_session=get_db(request),
            email=user_email,
        )
    # this is special case for requests polling the /task/get endpoint
    # where we are not passed a db session, and we want to proceed with the rest
    # of endpoint logic
    except DBNotSetException:
        return None

    # generic case for user not existing
    if not user:
        raise HTTPException(
            status_code=HTTP_401_UNAUTHORIZED, detail=[{"msg": "User not found"}]
        )

    return user


def store_oai_key(api_key, user_id):
    sm = SecretManager()
    sm.store_parameter("OAI_KEY_" + str(user_id), api_key)


def retrieve_oai_key(user_id):
    sm = SecretManager()
    return sm.retrieve_parameter("OAI_KEY_" + str(user_id))

================
File: src/auth/sm.py
================
import boto3
from botocore.exceptions import ClientError

from src.config import AWS_REGION


class SecretManager:
    def __init__(self, region_name=AWS_REGION):
        """Initialize the ParameterManager with a specific AWS region."""
        self.client = boto3.client("ssm", region_name=region_name)

    def store_parameter(self, name, value, description="", key_id=None):
        """
        Store a parameter in AWS Parameter Store.

        :param name: Name of the parameter.
        :param value: Value of the parameter.
        :param description: Description of the parameter.
        :param key_id: The ID of the KMS key to use for encryption. If None, the default KMS key is used.
        :return: Response from the put_parameter call.
        """
        try:
            params = {
                "Name": name,
                "Value": value,
                "Type": "SecureString" if key_id else "String",
                "Description": description,
                "Overwrite": True,
            }
            if key_id:
                params["KeyId"] = key_id

            response = self.client.put_parameter(**params)
            return response
        except ClientError as e:
            print(f"An error occurred: {e}")
            return None

    def retrieve_parameter(self, name, with_decryption=True):
        """
        Retrieve a parameter from AWS Parameter Store.

        :param name: Name of the parameter.
        :param with_decryption: Whether to decrypt the parameter value.
        :return: The parameter value.
        """
        try:
            response = self.client.get_parameter(
                Name=name, WithDecryption=with_decryption
            )
            return response["Parameter"]["Value"]
        except ClientError as e:
            print(f"An error occurred: {e}")
            return None


# Example usage:
if __name__ == "__main__":
    region = "us-east-2"
    param_manager = SecretManager(region_name=region)

    # Store a parameter
    param_name = "MySecret"
    param_value = "SuperSecretValue"
    response = param_manager.store_parameter(
        name=param_name, value=param_value, description="A test secret parameter"
    )
    if response:
        print(f"Parameter {param_name} stored successfully.")

    # Retrieve a parameter
    retrieved_value = param_manager.retrieve_parameter(name=param_name)
    if retrieved_value:
        print(f"Retrieved value: {retrieved_value}")

================
File: src/auth/views.py
================
from src.database.core import get_db
from src.auth.models import User
from src.models import HTTPSuccess

from src.queue.core import get_queue, TaskQueue

from .service import get_current_user, get, get_by_email, create, store_oai_key
from .models import UserLoginResponse, UserRegister, UpdateOAIKey

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
# import logfire

auth_router = APIRouter()


@auth_router.post("/user/register", response_model=UserLoginResponse)
def register_user(
    user_in: UserRegister,
    db_session: Session = Depends(get_db),
):
    user = get_by_email(db_session=db_session, email=user_in.email)
    if user:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="A user with this email already exists.",
        )

    user = create(db_session=db_session, user_in=user_in)
    res = UserLoginResponse(token=user.token)
    
    return res


@auth_router.get("/user/delete")
async def delete_user(
    curr_user: User = Depends(get_current_user),
    db_session: Session = Depends(get_db),
    task_queue: TaskQueue = Depends(get_queue),
):
    user = get(db_session=db_session, user_id=curr_user.id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found",
        )
    
    db_session.delete(user)
    db_session.commit()

    return HTTPSuccess()


@auth_router.post("/user/update/openai-key")
def update_oai_key(
    request: UpdateOAIKey,
    curr_user: User = Depends(get_current_user),
    db_session: Session = Depends(get_db),
):
    user = get(db_session=db_session, user_id=curr_user.id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found",
        )

    store_oai_key(user_id=user.id, api_key=request.openai_api_key)

    return HTTPSuccess()

================
File: src/aws/sm.py
================
import boto3
from botocore.exceptions import ClientError

from src.config import AWS_REGION


class SecretManager:
    def __init__(self, region_name=AWS_REGION):
        """Initialize the ParameterManager with a specific AWS region."""
        self.client = boto3.client("ssm", region_name=region_name)

    def store_parameter(self, name, value, description="", key_id=None):
        """
        Store a parameter in AWS Parameter Store.

        :param name: Name of the parameter.
        :param value: Value of the parameter.
        :param description: Description of the parameter.
        :param key_id: The ID of the KMS key to use for encryption. If None, the default KMS key is used.
        :return: Response from the put_parameter call.
        """
        try:
            params = {
                "Name": name,
                "Value": value,
                "Type": "SecureString" if key_id else "String",
                "Description": description,
                "Overwrite": True,
            }
            if key_id:
                params["KeyId"] = key_id

            response = self.client.put_parameter(**params)
            return response
        except ClientError as e:
            print(f"An error occurred: {e}")
            return None

    def retrieve_parameter(self, name, with_decryption=True):
        """
        Retrieve a parameter from AWS Parameter Store.

        :param name: Name of the parameter.
        :param with_decryption: Whether to decrypt the parameter value.
        :return: The parameter value.
        """
        try:
            response = self.client.get_parameter(
                Name=name, WithDecryption=with_decryption
            )
            return response["Parameter"]["Value"]
        except ClientError as e:
            print(f"An error occurred: {e}")
            return None


# Example usage:
if __name__ == "__main__":
    region = "us-east-2"
    param_manager = SecretManager(region_name=region)

    # Store a parameter
    param_name = "MySecret"
    param_value = "SuperSecretValue"
    response = param_manager.store_parameter(
        name=param_name, value=param_value, description="A test secret parameter"
    )
    if response:
        print(f"Parameter {param_name} stored successfully.")

    # Retrieve a parameter
    retrieved_value = param_manager.retrieve_parameter(name=param_name)
    if retrieved_value:
        print(f"Retrieved value: {retrieved_value}")

================
File: src/database/core.py
================
import functools
import re

from sqlalchemy import create_engine, inspect
from sqlalchemy.ext.declarative import declarative_base
from starlette.requests import Request

import src.config as config

engine = create_engine(
    config.SQLALCHEMY_DATABASE_URI,
    # pool_size=config.SQLALCHEMY_ENGINE_POOL_SIZE,
    # max_overflow=config.DATABASE_ENGINE_MAX_OVERFLOW,
    # pool_pre_ping=config.DATABASE_ENGINE_POOL_PING,
)


def resolve_table_name(name):
    """Resolves table names to their mapped names."""
    names = re.split("(?=[A-Z])", name)
    return "_".join([x.lower() for x in names if x])


# nested level get() function
def resolve_attr(obj, attr, default=None):
    """Attempts to access attr via dotted notation, returns none if attr does not exist."""
    try:
        return functools.reduce(getattr, attr.split("."), obj)
    except AttributeError:
        return default


class CustomBase:
    __repr_attrs__ = []
    __repr_max_length__ = 15

    # @declared_attr
    # def __tablename__(self):
    #     return resolve_table_name(self.__name__)

    def dict(self):
        """Returns a dict representation of a model."""
        return {c.name: getattr(self, c.name) for c in self.__table__.columns}

    def update(self, obj):
        """Updates a model with values from another model."""
        for key, value in obj.dict().items():
            if key in self.dict():
                setattr(self, key, value)

    @property
    def _id_str(self):
        ids = inspect(self).identity
        if ids:
            return "-".join([str(x) for x in ids]) if len(ids) > 1 else str(ids[0])
        else:
            return "None"

    @property
    def _repr_attrs_str(self):
        max_length = self.__repr_max_length__

        values = []
        single = len(self.__repr_attrs__) == 1
        for key in self.__repr_attrs__:
            if not hasattr(self, key):
                raise KeyError(
                    "{} has incorrect attribute '{}' in "
                    "__repr__attrs__".format(self.__class__, key)
                )
            value = getattr(self, key)
            wrap_in_quote = isinstance(value, str)

            value = str(value)
            if len(value) > max_length:
                value = value[:max_length] + "..."

            if wrap_in_quote:
                value = "'{}'".format(value)
            values.append(value if single else "{}:{}".format(key, value))

        return " ".join(values)

    def __repr__(self):
        # get id like '#123'
        id_str = ("#" + self._id_str) if self._id_str else ""
        # join class name, id and repr_attrs
        return "<{} {}{}>".format(
            self.__class__.__name__,
            id_str,
            " " + self._repr_attrs_str if self._repr_attrs_str else "",
        )


Base = declarative_base(cls=CustomBase)


class DBNotSetException(Exception):
    pass


def get_db(request: Request):
    try:
        return request.state.db
    except AttributeError:
        raise DBNotSetException("Database not set on request.")


# Triggers initial response field validation error
# DbSession = Annotated[Session, Depends(get_db)]

================
File: src/database/manage.py
================
import os
import logging

from alembic import command as alembic_command
from alembic.config import Config as AlembicConfig

from sqlalchemy import text
from sqlalchemy.schema import CreateSchema
from sqlalchemy_utils import create_database, database_exists

import src.config as config

from .core import Base, sessionmaker


log = logging.getLogger(__file__)


def version_schema(script_location: str):
    """Applies alembic versioning to schema."""

    # add it to alembic table
    alembic_cfg = AlembicConfig(config.ALEMBIC_INI_PATH)
    alembic_cfg.set_main_option("script_location", script_location)
    alembic_command.stamp(alembic_cfg, "head")


def get_core_tables():
    """Fetches tables that belong to the 'dispatch_core' schema."""
    core_tables = []
    for _, table in Base.metadata.tables.items():
        if table.schema == "dispatch_core":
            core_tables.append(table)
    return core_tables


def get_tenant_tables():
    """Fetches tables that belong to their own tenant tables."""
    tenant_tables = []
    for _, table in Base.metadata.tables.items():
        if not table.schema:
            tenant_tables.append(table)
    return tenant_tables


def init_database(engine):
    """Initializes the database."""

    print(engine.__dict__)

    if not database_exists(str(config.SQLALCHEMY_DATABASE_URI)):
        create_database(str(config.SQLALCHEMY_DATABASE_URI))

    # schema_name = "dispatch_core"
    # if not engine.dialect.has_schema(engine, schema_name):
    #     with engine.connect() as connection:
    #         connection.execute(CreateSchema(schema_name))

    tables = get_core_tables()

    print(tables)

    # Base.metadata.create_all(engine, tables=tables)

    # version_schema(script_location=config.ALEMBIC_CORE_REVISION_PATH)
    # setup_fulltext_search(engine, tables)

    # # setup an required database functions
    # session = sessionmaker(bind=engine)
    # db_session = session()

    # # we create the default organization if it doesn't exist
    # organization = (
    #     db_session.query(Organization)
    #     .filter(Organization.name == "default")
    #     .one_or_none()
    # )
    # if not organization:
    #     print("Creating default organization...")
    #     organization = Organization(
    #         name="default",
    #         slug="default",
    #         default=True,
    #         description="Default Dispatch organization.",
    #     )

    #     db_session.add(organization)
    #     db_session.commit()

    # # we initialize the database schema
    # init_schema(engine=engine, organization=organization)

    # # we install all plugins
    # from dispatch.common.utils.cli import install_plugins
    # from dispatch.plugins.base import plugins

    # install_plugins()

    # for p in plugins.all():
    #     plugin = Plugin(
    #         title=p.title,
    #         slug=p.slug,
    #         type=p.type,
    #         version=p.version,
    #         author=p.author,
    #         author_url=p.author_url,
    #         multiple=p.multiple,
    #         description=p.description,
    #     )
    #     db_session.add(plugin)
    # db_session.commit()

    # # we create the default project if it doesn't exist
    # project = db_session.query(Project).filter(Project.name == "default").one_or_none()
    # if not project:
    #     print("Creating default project...")
    #     project = Project(
    #         name="default",
    #         default=True,
    #         description="Default Dispatch project.",
    #         organization=organization,
    #     )
    #     db_session.add(project)
    #     db_session.commit()

    #     # we initialize the project with defaults
    #     from dispatch.project import flows as project_flows

    #     print("Initializing default project...")
    #     project_flows.project_init_flow(
    #         project_id=project.id,
    #         organization_slug=organization.slug,
    #         db_session=db_session,
    #     )


# def init_schema(*, engine, organization: Organization):
#     """Initializes a new schema."""
#     schema_name = f"test_schema"

#     if not engine.dialect.has_schema(engine, schema_name):
#         with engine.connect() as connection:
#             connection.execute(CreateSchema(schema_name))

#     # set the schema for table creation
#     tables = get_tenant_tables()

#     schema_engine = engine.execution_options(
#         schema_translate_map={
#             None: schema_name,
#         }
#     )

#     Base.metadata.create_all(schema_engine, tables=tables)

#     # put schema under version control
#     version_schema(script_location=config.ALEMBIC_TENANT_REVISION_PATH)

#     with engine.connect() as connection:
#         # we need to map this for full text search as it uses sql literal strings
#         # and schema translate map does not apply
#         for t in tables:
#             t.schema = schema_name

#         setup_fulltext_search(connection, tables)

#     session = sessionmaker(bind=schema_engine)
#     db_session = session()

#     organization = db_session.merge(organization)
#     db_session.add(organization)
#     db_session.commit()
#     return organization


# def setup_fulltext_search(connection, tables):
#     """Syncs any required fulltext table triggers and functions."""
#     # parsing functions
#     function_path = os.path.join(
#         os.path.dirname(os.path.abspath(fulltext.__file__)), "expressions.sql"
#     )
#     connection.execute(text(open(function_path).read()))

#     for table in tables:
#         table_triggers = []
#         for column in table.columns:
#             if column.name.endswith("search_vector"):
#                 if hasattr(column.type, "columns"):
#                     table_triggers.append(
#                         {
#                             "conn": connection,
#                             "table": table,
#                             "tsvector_column": "search_vector",
#                             "indexed_columns": column.type.columns,
#                         }
#                     )
#                 else:
#                     log.warning(
#                         f"Column search_vector defined but no index columns found. Table: {table.name}"
#                     )

#         for trigger in table_triggers:
#             sync_trigger(**trigger)

================
File: src/extensions/__init__.py
================
from .sentry import init_sentry

================
File: src/extensions/sentry.py
================
import sentry_sdk


def init_sentry():
    sentry_sdk.init(
        dsn="https://0de14048c02d4d10a00dafb70966c33c@o4507195649294336.ingest.us.sentry.io/4507195650736128",
        # Set traces_sample_rate to 1.0 to capture 100%
        # of transactions for performance monitoring.
        traces_sample_rate=1.0,
        # Set profiles_sample_rate to 1.0 to profile 100%
        # of sampled transactions.
        # We recommend adjusting this value in production.
        profiles_sample_rate=1.0,
    )

================
File: src/health/views.py
================
from fastapi import APIRouter, Depends
from src.models import HTTPSuccess
from src.auth.service import get_current_user
from src.auth.models import User


health_router = APIRouter()


@health_router.get("/health")
async def health():
    return HTTPSuccess()

================
File: src/queue/core.py
================
from cowboy_lib.api.runner.shared import Task, TaskStatus

from fastapi import Request
from threading import Lock
from collections import defaultdict
from typing import List, Dict
from asyncio import Event, wait_for


class TaskEvent:
    def __init__(self, task: Task):
        self.event = Event()
        self.task = task
        self.result = None

    async def wait(self, timeout: float = None):
        try:
            if timeout:
                await wait_for(self.event.wait(), timeout)
            else:
                await self.event.wait()
        except TimeoutError:
            return None

        return self.result

    def complete(self, result):
        """
        Complete with result and signal event to wake up
        """
        self.result = result
        self.event.set()

    @property
    def task_id(self):
        return self.task.task_id

    def __eq__(self, other):
        return self.task_id == other.task_id

    # def __hash__(self):
    #     return sum([ord(c) for c in self.task_id])


class TaskQueue:
    """
    A set of queues separated by user_id
    """

    _instance = None

    def __new__(cls, *args, **kwargs):
        if not isinstance(cls._instance, cls):
            print("Creating new TaskQueue instance")
            cls._instance = super(TaskQueue, cls).__new__(cls, *args, **kwargs)
            cls._instance._initialized = False

        return cls._instance

    def __init__(self):
        if not self._initialized:
            # Initialize instance variables only once
            self.queue: Dict[str, List[TaskEvent]] = defaultdict(list)
            self.locks = defaultdict(list)
            self._initialized = True  # Mark as initialized

    def _acquire_lock(self, user_id: int):
        if self.locks.get(user_id, None) is None:
            self.locks[user_id] = Lock()
        return self.locks.get(user_id)

    def put(self, user_id: int, task: str) -> TaskEvent:
        with self._acquire_lock(user_id):
            t = TaskEvent(task)
            self.queue[user_id].append(t)

            return t

    def complete(self, user_id: int, task_id: str, res):
        with self._acquire_lock(user_id):
            for i in range(len(self.queue[user_id])):
                if self.queue[user_id][i].task_id == task_id:
                    t = self.queue[user_id].pop(i)
                    t.complete(res)
                    break

    # def get(self, user_id: int) -> Task:
    #     """
    #     Returns the first PENDING task and changes its status to STARTED
    #     """
    #     with self._acquire_lock(user_id):
    #         if len(self.queue[user_id]) == 0:
    #             return None

    #         return self.queue[user_id].pop()

    def get_all(self, user_id: int) -> List[Task]:
        with self._acquire_lock(user_id):
            if len(self.queue[user_id]) == 0:
                return []

            tasks = []
            for t in filter(
                lambda t: t.task.status == TaskStatus.PENDING.value, self.queue[user_id]
            ):
                t.task.status = TaskStatus.STARTED.value
                tasks.append(t.task)

            return tasks

    def peak(self, user_id: int, n: int) -> List[Task]:
        """
        Get the first n tasks in queue without removing
        """
        with self._acquire_lock(user_id):
            if len(self.queue[user_id]) == 0:
                return []

            return [t.task for t in self.queue[user_id][:n]]


def get_queue(request: Request):
    return request.state.task_queue


def get_token_registry(request: Request):
    from main import token_registry

    return token_registry


def get_token(request: Request):
    """
    Returns the user id
    """
    token = request.headers.get("x-task-auth", None)
    # need this or else we end up converting None to "None" **shakes fist @ python moment"
    return str(token) if token else None

================
File: src/queue/models.py
================
from cowboy_lib.api.runner.shared import Task


class CompleteTaskRequest(Task):
    pass


class GetTaskResponse(Task):
    pass

================
File: src/queue/permissions.py
================
from src.auth.permissions import BasePermission
from src.auth.service import get_current_user

from fastapi import HTTPException

from starlette.requests import Request
from starlette.responses import Response


class TaskGetPermissions(BasePermission):
    def __init__(self, request: Request):
        try:
            user = get_current_user(request=request)
            if not user:
                raise HTTPException(
                    status_code=self.user_error_code, detail=self.user_error_msg
                )

        # this happens when the db is not set
        except AttributeError:
            pass

================
File: src/queue/service.py
================
from cowboy_lib.api.runner.shared import Task

from typing import Optional, List, Dict

from src.queue.core import TaskQueue


def list_tasks(*, task_queue: TaskQueue, user_id: int, n: int) -> Optional[List[Task]]:
    """List all tasks in the queue."""

    return task_queue.peak(user_id, n)


def dequeue_task(*, task_queue: TaskQueue, user_id: int) -> Optional[List[Task]]:
    """Dequeue the first task in the queue: retrieve and delete it."""

    return task_queue.get_all(user_id)


def complete_task(
    *, task_queue: TaskQueue, user_id: int, task_id: str, result: Dict
) -> None:
    """Mark a task as completed."""
    task_queue.complete(user_id, task_id, result)


def enqueue_task_and_wait(*, task_queue: TaskQueue, task: Task, user_id: int):
    """Enqueue a task to the specified queue."""

    f = task_queue.put(user_id, task)
    return f

================
File: src/queue/views.py
================
from cowboy_lib.api.runner.shared import Task

from .service import list_tasks, dequeue_task, complete_task
from .models import CompleteTaskRequest
from .core import TaskQueue, get_queue, get_token_registry, get_token

from fastapi import APIRouter, Depends, HTTPException, Response

from src.database.core import get_db
from src.auth.service import get_current_user
from src.auth.models import User

from typing import List, Set

task_queue_router = APIRouter()


@task_queue_router.get("/task/list", response_model=List[Task])
def list(
    task_queue: TaskQueue = Depends(get_queue),
    curr_user: User = Depends(get_current_user),
):
    tasks = list_tasks(task_queue=task_queue, user_id=curr_user.id, n=3)
    return tasks


# incredibly hacky, basically, to prevent db connections from being used up
# we exclude db connections for this endpoint, we do the following:
# 1. First request actually does get a db sess, which we use to auth the user
# 2. Grab user id and add it into a in-mem token_registry list
# 3. Return user id as "set-x-task-auth" header
# 4. When the client puts user id into x-task-auth header
# 5. Our DBMiddleware will check the header, and if token is in registry, will not
# add a db session to the request
@task_queue_router.get("/task/get", response_model=List[Task])
def get(
    response: Response,
    task_queue: TaskQueue = Depends(get_queue),
    # don't try to do anything with curr_user because most of the time
    # we only have user_token to work with
    curr_user: User = Depends(get_current_user),
    token_registry: Set[str] = Depends(get_token_registry),
    user_token: str = Depends(get_token),
    # perms: str = Depends(PermissionsDependency([TaskGetPermissions])),
):
    # at this point we have passed db user auth; test
    # catches if user sets random token
    if user_token and user_token not in token_registry:
        raise HTTPException(
            status_code=401,
            detail="Token not in registry, cannot proceed. \
            Are you sure you are logged in on the client?",
        )
    # issue token if it does not exist
    elif not user_token:
        print("Setting new token ..")
        response.headers["set-x-task-auth"] = str(curr_user.id)
        token_registry.add(str(curr_user.id))

    tasks = dequeue_task(
        task_queue=task_queue, user_id=curr_user.id if curr_user else int(user_token)
    )
    return tasks


@task_queue_router.post("/task/complete", response_model=CompleteTaskRequest)
def complete(
    task: CompleteTaskRequest,
    task_queue: TaskQueue = Depends(get_queue),
    curr_user: User = Depends(get_current_user),
):

    task_queue = complete_task(
        task_queue=task_queue,
        user_id=curr_user.id,
        task_id=task.task_id,
        result=task.result,
    )
    return task

================
File: src/repo/extensions.py
================
EXTENSIONS = {
  ".py": "Python",
  ".js": "JavaScript",
  ".ts": "TypeScript",
  ".html": "HTML",
  ".css": "CSS",
  ".java": "Java",
  ".c": "C",
  ".cpp": "C++",
  ".cs": "C#",
  ".php": "PHP",
  ".rb": "Ruby",
  ".go": "Go",
  ".rs": "Rust",
  ".swift": "Swift",
  ".kt": "Kotlin",
  ".scala": "Scala",
  ".m": "Objective-C",
  ".mm": "Objective-C++",
  ".pl": "Perl",
  ".sh": "Shell",
  ".bash": "Bash",
  ".ps1": "PowerShell",
  ".sql": "SQL",
  ".r": "R",
  ".lua": "Lua",
  ".groovy": "Groovy",
  ".dart": "Dart",
  ".f": "Fortran",
  ".f90": "Fortran",
  ".hs": "Haskell",
  ".ml": "OCaml",
  ".clj": "Clojure",
  ".coffee": "CoffeeScript",
  ".ts": "TypeScript",
  ".vb": "Visual Basic",
  ".elm": "Elm",
  ".ex": "Elixir",
  ".erl": "Erlang",
  ".fs": "F#",
  ".jl": "Julia",
  ".nim": "Nim",
  ".rkt": "Racket",
  ".v": "Verilog",
  ".vhd": "VHDL",
  ".asm": "Assembly",
  ".s": "Assembly",
  ".lisp": "Lisp",
  ".tex": "LaTeX",
  ".md": "Markdown",
  ".json": "JSON",
  ".xml": "XML",
  ".yaml": "YAML",
  ".toml": "TOML"
}

================
File: src/repo/models.py
================
from cowboy_lib.coverage import TestCoverage

from sqlalchemy import Column, Integer, String, JSON, ForeignKey, Boolean
from sqlalchemy.orm import relationship
from pydantic import Field, root_validator

from src.models import RTFSBase
from src.database.core import Base
from src.config import Language
from src.auth.models import User

from typing import List, Any, Dict, Optional


class Repo(Base):
    """
    Stores configuration for a repository
    """

    __tablename__ = "repos"

    id = Column(Integer, primary_key=True)
    repo_name = Column(String)
    url = Column(String)
    language = Column(String)
    repo_size = Column(Integer)

    # remote = Column(String)
    # main = Column(String)
    # language = Column(String)

    users = relationship("User", uselist=False, cascade="all, delete-orphan")

    def to_dict(self):
        return {
            # "repo_name": self.repo_name,
            "url": self.url,
        }

class LangConf(RTFSBase):
    """
    Holds the language/framework specific settings
    for a repo
    """

    # currently I expect only an interpreter/compiler path that points
    # to the runtime for the targeted repo
    interp: str


class RepoBase(RTFSBase):
    url: str
    # source_folder: str
    # cloned_folders: List[str]
    # python_conf: PythonConf

    # language: Optional[Language] = Field(default="python")
    # is_experiment: Optional[bool] = Field(default=False)
    # main: Optional[str] = Field(default="main")
    # remote: Optional[str] = Field(default="origin")


class RepoGet(RepoBase):
    pass


class RepoCreate(RepoBase):
    url: str
    repo_name: Optional[str] = Field(None, nullable=True)

    @root_validator(pre=True)
    def set_repo_name(cls, values):
        url = values.get("url", None)
        parts = url.rstrip('/').split('/')
        # stip out .
        parts[1] = parts[1].split(".")[0]
        
        if len(parts) >= 2:
            values["repo_name"] = "_".join(parts)
            return values

        raise ValueError(f"Malformed GH URL: {values['url']}")
        
class RepoList(RTFSBase):
    repo_list: List[RepoBase]

class RepoRemoteCommit(RTFSBase):
    sha: str

class PrivateRepoAccess(Exception):
    pass

================
File: src/repo/repository.py
================
from cowboy_lib.utils import gen_random_name
from cowboy_lib.repo.diff import CommitDiff

import os
import tempfile
import hashlib
from pathlib import Path
from logging import getLogger
from git import Repo, GitCommandError
import shutil
import random
from typing import List, Union, Tuple, Dict
from pydantic import BaseModel
import dotenv
import re

from src.repo.extensions import EXTENSIONS

dotenv.load_dotenv()

log = getLogger(__name__)


class NoRemoteException(Exception):
    pass

class NoMainBranch(Exception):
    pass

class PrivateRepoError(Exception):
    pass

def http_to_ssh(url):
    """Convert HTTP(S) URL to SSH URL."""
    match = re.match(r'https?://(?:www\.)?github\.com/(.+)/(.+)\.git', url)
    if match:
        return f"git@github.com:{match.group(1)}/{match.group(2)}.git"
    return url  # Return original if not a GitHub HTTP(S) URL

def del_file(func, path, exc_info):
    """
    Error handler for ``shutil.rmtree``.

    If the error is due to an access error (read only file)
    it attempts to add write permission and then retries.

    If the error is for another reason it re-raises the error.

    Usage : ``shutil.rmtree(path, onerror=onerror)``
    """
    import stat

    # Is the error an access error?
    if not os.access(path, os.W_OK):
        os.chmod(path, stat.S_IWUSR)
        func(path)
    else:
        raise

class GitRepo:
    """
    Used to manage git operations on a git repo
    """

    def __init__(self, repo_path: Path, remote: str = "origin", main: str = ""):
        if not repo_path.exists():
            # test suite may be renamed or deleted
            raise Exception("GitRepo does not exist: ", repo_path)

        # used for reversing patches
        self.patched_files = {}
        self.repo_folder = repo_path
        self.repo = Repo(repo_path)
        self.head = self.repo.head

        if main:
            if not self.branch_exists(main):
                raise NoMainBranch(main)
        else:
            potential = ["master", "main"]
            for branch in potential:
                main = branch if self.branch_exists(branch) else ""
                if main:
                    break
            if not main:
                raise NoMainBranch(main)

        self.main = main
        try:
            self.origin = self.repo.remotes.__getattr__(remote)
        except AttributeError:
            raise NoRemoteException(remote)

        self.username = self.origin.url.split("/")[-2]
        self.repo_name = self.origin.url.split("/")[-1]

        self.branch_prefix = "cowboy_"

    @classmethod
    def clone_repo(cls, clone_dst: Path, url: str) -> "GitRepo":
        """
        Creates a clone of the repo locally
        """
        ssh_url = http_to_ssh(url)
        if not os.path.exists(clone_dst):
            os.makedirs(clone_dst)
            
        try:
            Repo.clone_from(ssh_url, clone_dst)

            return cls(clone_dst)
        except GitCommandError as e:
            if "Permission denied (publickey)." in str(e):
                raise PrivateRepoError
            
            raise e

    @classmethod
    def delete_repo(cls, repo_dst: Path):
        """
        Deletes a repo from the db and all its cloned folders
        """
        import platform

        if not repo_dst.exists():
            return

        if platform.system() == "Windows":
            shutil.rmtree(repo_dst, onerror=del_file)
        else:
            shutil.rmtree(repo_dst)

    def iterate_content(self) -> Dict:
        """
        Iterates through the repo and returns a json that represents
        the filepaths
        """
        content_dict = {}
        for root, dirs, files in os.walk(self.repo.working_dir):
            for file in files:
                file_path = os.path.relpath(os.path.join(root, file), self.repo.working_dir)
                with open(os.path.join(root, file), 'r', encoding="utf-8") as f:
                    try:
                        content = f.read()
                        content_dict[file_path] = content

                    # TODO: log this somewhere
                    except UnicodeDecodeError:
                        continue
                    
        return content_dict

    def get_lang_and_size(self):
        """
        Identifies language of repo
        """
        max_content_length = 0
        identified_language = None
        lang_len = {}

        content_dict = self.iterate_content()
        for file_path, content in content_dict.items():
            for ext, language in EXTENSIONS.items():
                if file_path.endswith(ext) and len(content) > max_content_length:
                    max_content_length = len(content)
                    identified_language = language
                    if not lang_len.get(language, None):
                        lang_len[language] = 0

                    lang_len[language] += len(content)

        return identified_language, lang_len[identified_language]
        
    def reset_to_commit(self, commit_sha, parent=None, head: int = 0):
        """
        Resets the index of the repository to a specific commit.
        """
        self.repo.git.reset(f"--hard", commit_sha)
        return f"Successfully reset to commit {commit_sha}"

    def reset_to_commit_head(self, head: int = 0):
        head = f"HEAD~{str(head)}" if head else ""

        self.repo.git.reset(f"--hard", head)
        return f"Successfully reset to commit {head}"

    def commit_exists(self, commit_sha: str) -> bool:
        """
        Checks if a commit exists in the repo
        """
        try:
            self.repo.commit(commit_sha)
            return True
        except Exception:
            return False

    def get_curr_commit(self):
        """
        Returns the current commit sha
        """
        return self.head.commit.hexsha

    def get_prev_commit(self, commit_sha):
        """
        Returns the previous commit of a given commit sha
        """
        return self.repo.commit(commit_sha).parents[0]

    def apply_patch(self, patch: str) -> None:
        """
        Applies a patch from a .diff file to a single file in the repository
        """
        with tempfile.NamedTemporaryFile(mode="wb", delete=False) as patch_file:
            patch_file.write(patch.encode("utf-8") + b"\n")
            patch_file.flush()

        patch_hash = hashlib.md5(patch.encode("utf-8")).hexdigest()
        self.repo.git.apply(patch_file.name, whitespace="nowarn")
        self.patched_files[patch_hash] = patch_file.name

    def reverse_patch(self, patch: str) -> None:
        """
        Reverses a patch from a .diff
        """
        patch_hash = hashlib.md5(patch.encode()).hexdigest()
        patch_file = self.patched_files[patch_hash]

        self.repo.git.apply(patch_file, reverse=True)
        self.patched_files.pop(patch_hash)

    def branch_exists(self, branch: str):
        """
        Checks if a branch exists in the repo
        """
        # print(f"Branch: {branch}")
        # print(f"Branches {[str(br) for br in self.repo.heads]} in {self.repo_folder}")
        if branch in [str(br) for br in self.repo.heads]:
            print(
                branch,
                [str(br) for br in self.repo.heads],
                branch in [str(br) for br in self.repo.heads],
            )
            return True

        return False

    def checkout(self, branch_name: str, new=False):
        """
        Checks out an existing branch in the repo
        """
        if new:
            branch = self.repo.create_head(branch_name)
        else:
            branch = self.repo.heads[branch_name]

        print(f"Checking out: {branch}")
        branch.checkout()

    def clean_branches(self, branch_prefix: str):
        """
        Deletes all branches with a specific prefix
        """
        removed = []
        for branch in self.repo.branches:
            if branch.name.startswith(branch_prefix):
                removed.append(branch.name)
                self.repo.delete_head(branch)

        return removed

    @property
    def remote_commit(self) -> str:
        """
        Gets the sha of latest commit on origin
        """
        self.repo.remotes.origin.fetch()
        remote_sha = self.repo.remotes.origin.refs.__getattr__(self.main).commit.hexsha

        return remote_sha

    @property
    def local_commit(self) -> str:
        return self.repo.head.commit.hexsha

    # TODO: move this into sync_repo
    def fetch_diffs(self) -> Tuple[CommitDiff, str]:
        """
        Diffs the remote with our local repo
        """
        try:
            if self.local_commit == self.remote_commit:
                print("No updates available.")
                return None, None
            else:
                print("Updates found!")

                # Get the diff between the old commit and the new HEAD
                diff = self.repo.git.diff(self.local_commit, self.remote_commit)
                commit_diff = CommitDiff(diff)

                return commit_diff, self.remote_commit
        except Exception as e:
            print(f"An error occurred: {e}")
            return None

    def pull(self):
        self.repo.remotes.origin.pull()

    def add_n_commit(self, files: List[str], msg: str):
        self.repo.index.add(files)
        self.repo.index.commit(msg)

    def push(self, branch_name: str = "", force: bool = False):
        if not branch_name:
            branch_name = self.main
        self.origin.push(refspec=f"{branch_name}:{branch_name}", force=True)

    def checkout_and_push(
        self,
        name: str,
        commit_message: str,
        files_to_commit: list,
    ):
        """
        Checks out a new branch, commits changes, and pushes to the remote. Returns the
        URL for the merge request of our new branch against main

        Args:
        - name: The "suggested" name
        - commit_message: The commit message to use.
        - files_to_commit: List of file paths (relative to the repo root) to commit.

        Returns:
        - None
        """
        branch_name = name
        if self.branch_exists(name):
            branch_name = self.branch_prefix + name + "_" + gen_random_name()

        # Check out a new branch
        try:
            new_branch = self.repo.create_head(branch_name)
            new_branch.checkout()

            # Add and commit changes
            self.repo.index.add(files_to_commit)
            self.repo.index.commit(commit_message)
            self.push(branch_name=branch_name)
            origin_url = self.origin.url.replace(".git", "")
        except Exception as e:
            log.error(f"Exception in {self.repo_name}: {str(e)}")
            pass
        finally:
            self.checkout(self.main)
            log.info(f"Resetting to branch {self.main}")

        # url for branch merge request
        return f"{origin_url}/compare/{self.main}...{self.username}:{self.repo_name}:{branch_name}?expand=1"


class PatchApplyExcepion(Exception):
    pass


class IncompatibleCommit(Exception):
    pass


class PatchFile(BaseModel):
    path: Path
    patch: str


class PatchFileContext:
    """
    Context manager for applying and reversing patches
    """

    def __init__(
        self, repo: GitRepo, patch: Union[str, PatchFile], revert: bool = True
    ):

        self.repo = repo
        self.patch = patch
        # assume all cases repo and patch are both specified, or neither are
        self.head_commit = self.repo.head.commit if self.patch else None
        self.failed_id = random.randint(0, 1000000)
        # for debugging
        self.revert = revert

    # def _write_broken_patch(self):
    #     with open(
    #         f"log/failed_patches/patch_{self.failed_id}.diff", "w+", encoding="utf-8"
    #     ) as f:
    #         f.write(self.patch)

    def __enter__(self):
        if not self.patch:
            return

        try:
            if isinstance(self.patch, PatchFile):
                with open(self.patch.path, "w", encoding="utf-8") as f:
                    f.write(self.patch.patch)
            elif isinstance(self.patch, str):
                self.repo.apply_patch(self.patch)

        except GitCommandError as e:
            # self._write_broken_patch()
            raise PatchApplyExcepion(e)

    def __exit__(self, exc_type, exc_value, traceback):
        if not self.patch:
            return

        try:
            if isinstance(self.patch, PatchFile) and self.revert:
                self.repo.reset_to_commit(self.head_commit)
            elif isinstance(self.patch, str) and self.revert:
                self.repo.reverse_patch(self.patch)

        except GitCommandError as e:
            log.info(f"Error reversing patch")
            raise PatchApplyExcepion(e)


class ResetLocalCommitContext:
    """
    Resets the repository to a specific commit
    """

    def __init__(self, repo: GitRepo, fd_reset: bool = False, revert: bool = True):
        self.repo = repo
        self.revert = revert
        self.fd_reset = fd_reset

    def __enter__(self):
        """
        Saves the current commit hash when entering the context.
        """
        self.original_commit = self.repo.head.commit.hexsha
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """
        Restores the repository to the original commit when exiting the context.
        """
        if exc_type is not None:
            print(f"An exception occurred: {exc_type.__name__}: {exc_value}")
            # Optionally, log the traceback here

        if self.fd_reset:
            self._add_files()

        if self.original_commit and self.revert:
            self.repo.reset_to_commit(self.original_commit)

        # reset the patched files in GitRepo
        self.repo.patched_files = {}

    def _add_files(self):
        """
        Adds all files in repo so that they are tracked by git and can be resetted
        when the context is exited
        """
        self.repo.repo.git.add(".")
        print(self.repo.repo.git.status())

    def reset_to_commit(self, commit_sha: str, parent=None):
        """
        Resets the index of the repository to a specific commit.
        """
        return self.repo.reset_to_commit(commit_sha, parent)


class ResetRemoteCommitContext:
    """
    Pushes a single commit to remote repo and then reverts it in the remote
    by pushing HEAD~1. Used for unit tests
    """

    def __init__(self, repo: GitRepo):
        self.repo = repo
        self._called = False

    def __enter__(self):
        """
        Saves the current commit hash when entering the context.
        """
        self.original_commit = self.repo.head.commit.hexsha
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """
        Restores remote repo to HEAD~1
        """
        if not self._called:
            # reset local repo state if we dont end up pushing
            self.repo.reset_to_commit(self.original_commit)
            print("Resetting to og state")
            return

        self.repo.reset_to_commit_head(head=1)
        self.repo.push()

    def add_commit_push(self, files: List[str], msg: str):
        """
        Adds a commit to the repo and pushes it to the remote
        """
        try:
            self.repo.add_n_commit(files, msg)
            self.repo.push()
        except Exception as e:
            print("Error adding commit")
            self.repo.reset_to_commit(self.original_commit)
            raise e
        finally:
            self._called = True

================
File: src/repo/service.py
================
from src.auth.models import User
from src.config import REPOS_ROOT, CHUNKS_ROOT
from src.queue.core import TaskQueue

from .repository import GitRepo, PrivateRepoError
from .models import Repo, RepoCreate, PrivateRepoAccess

from pathlib import Path
from logging import getLogger
from fastapi import HTTPException
from rtfs.rtfs.chunker import chunk

logger = getLogger(__name__)

# Return GitRepo here
def get(*, db_session, repo_name: str) -> Repo:
    """Returns a repo based on the given repo name."""
    return (
        db_session.query(Repo)
        .filter(Repo.repo_name == repo_name)
        .one_or_none()
    )

def delete(*, db_session, curr_user: User, repo_name: str) -> Repo:
    """Deletes a repo based on the given repo name."""

    repo = get(db_session=db_session, curr_user=curr_user, repo_name=repo_name)
    if repo:
        db_session.delete(repo)
        db_session.commit()

        GitRepo.delete_repo(Path(repo.source_folder))
        return repo

    return None

# def list(*, db_session, curr_user: User) -> Repo:
#     """Lists all repos for a user."""

#     return db_session.query(Repo).filter(curr_user.id in Repo.users).all()

async def create_or_find(
    *,
    db_session,
    curr_user: User,
    repo_in: RepoCreate,
    task_queue: TaskQueue,
) -> Repo:
    """Creates a new repo or returns an existing repo if we already have it downloaded"""
    repo = get(
        db_session=db_session, repo_name=repo_in.repo_name
    )
    if repo:
        return repo

    repo_dst = None
    try:
        repo_dst = Path(REPOS_ROOT) / repo_in.repo_name
        git_repo = GitRepo.clone_repo(repo_dst, repo_in.url)
        
        # chunk the repo
        chunk(repo_dst, Path(CHUNKS_ROOT) / repo_in.repo_name)
        
        lang, sz = git_repo.get_lang_and_size()
        repo = Repo(
            **repo_in.dict(),
            users=curr_user,
            language=lang,
            repo_size=sz
        )

        db_session.add(repo)
        db_session.commit()

        return repo

    except PrivateRepoError as e:
        raise PrivateRepoAccess

    except Exception as e:
        db_session.rollback()
        if repo:
            delete(db_session=db_session, repo_name=repo.repo_name)

        if repo_dst:
            GitRepo.delete_repo(repo_dst)

        logger.error(f"Failed to create repo configuration: {e}")
        raise

================
File: src/repo/views.py
================
from cowboy_lib.repo import GitRepo

from src.database.core import get_db
from src.auth.service import get_current_user, User
from src.queue.core import get_queue, TaskQueue
from src.exceptions import ClientActionException

from .service import create_or_find, get, delete, list
from .models import (
    RepoCreate,
    PrivateRepoAccess,
    RepoList,
    RepoGet,
    RepoRemoteCommit,
)

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from pathlib import Path


repo_router = APIRouter()


@repo_router.post("/repo/create", response_model=RepoCreate)
async def create_repo(
    repo_in: RepoCreate,
    db_session: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
    task_queue: TaskQueue = Depends(get_queue),
):
    try:
        repo = get(
            db_session=db_session, repo_name=repo_in.repo_name
        )
        if repo:
            return repo.to_dict()
        
        repo_config = await create_or_find(
            db_session=db_session,
            repo_in=repo_in,
            curr_user=current_user,
            task_queue=task_queue,
        )
        # need as_dict to convert cloned_folders to list
        return repo_config.to_dict()
    except PrivateRepoAccess as e:
        raise ClientActionException(message="Private repo not yet supported", ex=e)


# @repo_router.delete("/repo/delete/{repo_name}", response_model=HTTPSuccess)
# async def delete_repo(
#     repo_name: str,
#     db_session: Session = Depends(get_db),
#     current_user: CowboyUser = Depends(get_current_user),
#     task_queue: TaskQueue = Depends(get_queue),
# ):
#     deleted = delete(db_session=db_session, repo_name=repo_name, curr_user=current_user)
#     if not deleted:
#         raise HTTPException(
#             status_code=400, detail="A repo with this name does not exists."
#         )

#     # need this to shut down the client after a repo is deleted, or else
#     # it will use old cloned_folders to execute the runner
#     args = RunServiceArgs(user_id=current_user.id, task_queue=task_queue)
#     await shutdown_client(args)

#     return HTTPSuccess()


# @repo_router.delete("/repo/clean/{repo_name}", response_model=HTTPSuccess)
# def clean_repo(
#     repo_name: str,
#     db_session: Session = Depends(get_db),
#     current_user: CowboyUser = Depends(get_current_user),
# ):
#     cleaned = clean(db_session=db_session, repo_name=repo_name, curr_user=current_user)

#     if not cleaned:
#         raise HTTPException(
#             status_code=400, detail="A repo with this name does not exists."
#         )
#     return HTTPSuccess()


# @repo_router.get("/repo/get/{repo_name}", response_model=RepoGet)
# def get_repo(
#     repo_name: str,
#     db_session: Session = Depends(get_db),
#     current_user: CowboyUser = Depends(get_current_user),
# ):
#     repo = get(db_session=db_session, repo_name=repo_name, curr_user=current_user)
#     if not repo:
#         raise HTTPException(
#             status_code=400, detail="A repo with this name does not exists."
#         )
#     return repo.to_dict()


# @repo_router.get("/repo/list", response_model=RepoList)
# def list_repos(
#     db_session: Session = Depends(get_db),
#     current_user: CowboyUser = Depends(get_current_user),
# ):
#     repos = list(db_session=db_session, curr_user=current_user)
#     return RepoList(repo_list=repos)


# # TODO: this should return HEAD of repo.source_folder rather than the remote repo
# # once we finish our task refactor
# @repo_router.get("/repo/get_head/{repo_name}", response_model=RepoRemoteCommit)
# def get_head(
#     repo_name: str,
#     db_session: Session = Depends(get_db),
#     current_user: CowboyUser = Depends(get_current_user),
# ):
#     repo = get(db_session=db_session, repo_name=repo_name, curr_user=current_user)
#     if not repo:
#         raise HTTPException(
#             status_code=400, detail="A repo with this name does not exists."
#         )

#     git_repo = GitRepo(Path(repo.source_folder))

#     # return RepoRemoteCommit(sha=git_repo.local_commit)
#     return RepoRemoteCommit(sha=git_repo.remote_commit)

================
File: src/scripts/drop_db.py
================
import src.config as config
import click
from src.database.core import engine


@click.group()
def cowboy_database():
    pass


@cowboy_database.command("drop")
def drop_database():
    """Drops all data in database."""
    from sqlalchemy_utils import database_exists, drop_database

    if database_exists(str(config.SQLALCHEMY_DATABASE_URI)):
        drop_database(str(config.SQLALCHEMY_DATABASE_URI))


drop_database()

================
File: src/scripts/neuter_repo.py
================
from src.test_modules.iter_tms import iter_test_modules
from cowboy_lib.repo import SourceRepo
from cowboy_lib.test_modules import TestModule
from cowboy_lib.ast import NodeType

from typing import List

import sys
from pathlib import Path


def num_delete(tm: TestModule, to_keep: int = 1, to_delete: int = 1) -> int:
    if to_keep and to_delete:
        raise Exception("Cannot have both values > 0")

    # always leave at least one test
    if to_keep:
        num_to_del = max(0, len(tm.tests) - to_keep)
        return num_to_del
    elif to_delete:
        num_to_del = min(len(tm.tests) - 1, to_delete)
        return num_to_del
    else:
        raise Exception("Must provide either to_keep or to_delete value")


def neuter_tests(
    test_modules: List[TestModule], src_repo: SourceRepo, to_keep, to_delete=0
):
    total_deleted = 0
    failed_mod = 0
    for tm in test_modules:
        try:
            print("Deleting tm: ", tm.name)
            to_exclude = []
            # BUG: tm.tests gets modified somehow
            num_to_del = num_delete(tm, to_keep=to_keep, to_delete=to_delete)
            total_tests = len(tm.tests)

            for func in tm.tests[:num_to_del]:
                to_exclude.append((func, tm.test_file.path))
                # CARE: this operation has changes state of src_repo,
                # which is then propagated to strategy below
                src_repo.find_file(tm.path).delete(
                    func.name, node_type=NodeType.Function
                )
                # tm.test_file.delete(func.name, node_type=NodeType.Function)

                with open(src_repo.repo_path / tm.test_file.path, "w") as f:
                    # print(tm.test_file.to_code())
                    f.write(src_repo.find_file(tm.path).to_code())

                total_deleted += 1
        except Exception as e:
            failed_mod += 1

    print("Total failed:", failed_mod)


if __name__ == "__main__":
    """
    python -m neuter_repo <repo_path>
    """
    repo = Path(sys.argv[1])
    if not repo.exists():
        print("Repo does not exist")
        sys.exit()

    src_repo = SourceRepo(repo)
    test_modules = iter_test_modules(src_repo)

    neuter_tests(test_modules, src_repo, to_keep=2, to_delete=0)

================
File: src/scripts/show_tables.py
================
from cowboy_lib.repo.source_repo import SourceRepo
from src.test_modules.iter_tms import iter_test_modules
from src.test_modules.models import TestModuleModel
from pathlib import Path

from sqlalchemy.sql import text


from src.database.core import engine
from sqlalchemy.orm import sessionmaker

Session = sessionmaker(bind=engine)
query = text(
    """
    SELECT schema_name FROM information_schema.schemata
    WHERE schema_name NOT IN ('pg_catalog', 'information_schema')
    AND schema_name NOT LIKE 'pg_toast%'
    AND schema_name NOT LIKE 'pg_temp_%'
"""
)

# Execute the query
with engine.connect() as connection:
    # Query to get all tables from the public schema
    table_query = text(
        """
        SELECT table_name
        FROM information_schema.tables
        WHERE table_schema = 'public'
    """
    )

    # Execute the query to get all table names
    with engine.connect() as connection:
        tables = connection.execute(table_query).fetchall()

        # Iterate through each table and get its schema details
        for table in tables:
            table_name = table[0]
            print(f"\nSchema for table '{table_name}':")

            # Query to get schema details for each table
            schema_query = text(
                f"""
                SELECT column_name, data_type, is_nullable, column_default
                FROM information_schema.columns
                WHERE table_schema = 'public' AND table_name = :table_name
                ORDER BY ordinal_position
            """
            )

            # Fetch and print the schema details for each table
            schema_details = connection.execute(
                schema_query, {"table_name": table_name}
            ).fetchall()
            for detail in schema_details:
                print(detail)
                # print(f"Column Name: {detail['column_name']}, "
                #       f"Type: {detail['data_type']}, "
                #       f"Nullable: {detail['is_nullable']}, "
                #       f"Default: {detail['column_default']}")

================
File: src/config.py
================
from starlette.config import Config
from urllib.parse import urljoin

from enum import Enum

config = Config(".env")

ENV = config("ENV", default="dev")
PORT = int(config("PORT"))
API_ENDPOINT = urljoin(config("HOST"), str(PORT))

# JWT settings
COWBOY_JWT_SECRET = config("DISPATCH_JWT_SECRET", default="")
COWBOY_JWT_ALG = config("DISPATCH_JWT_ALG", default="HS256")
COWBOY_JWT_EXP = config("DISPATCH_JWT_EXP", cast=int, default=308790000)  # Seconds

COWBOY_OPENAI_API_KEY = config("OPENAI_API_KEY")

DB_PASS = config("DB_PASS")
SQLALCHEMY_DATABASE_URI = f"postgresql://postgres:{DB_PASS}@127.0.0.1:5432/codesearch"
SQLALCHEMY_ENGINE_POOL_SIZE = 50

ALEMBIC_INI_PATH = "."
ALEMBIC_CORE_REVISION_PATH = "alembic"

# LLM settings and test gen settings
AUGMENT_ROUNDS = 4 if ENV == "release" else 1
LLM_RETRIES = 3
AUTO_GEN_SIZE = 7
LOG_DIR = "log"

# TODO: auto-create these
REPOS_ROOT = "/home/ubuntu/repos"
CHUNKS_ROOT = "/home/ubuntu/chunks"

AWS_REGION = "us-east-2"

SSH_KEY_PATH = config("SSH_KEY_PATH")
# MAX_REPO_SIZE = 

# Anonymous user
ANON_LOGIN = True

class Language(str, Enum):
    """
    Currently supported languages
    """

    python = "python"

================
File: src/exceptions.py
================
class ClientActionException(Exception):
    """
    Exception type that needs to be displayed and handled in the client
    """
    def __init__(self, *, message, ex: Exception):
        self.message = message
        self.type = ex.__class__.__name__
        
        super().__init__()

================
File: src/logger.py
================
from src.config import LOG_DIR

import logging
import os
from datetime import datetime
import pytz
# import logfire


def converter(timestamp):
    dt = datetime.fromtimestamp(timestamp, tz=pytz.utc)
    return dt.astimezone(pytz.timezone("US/Eastern")).timetuple()


formatter = logging.Formatter(
    "%(asctime)s - %(name)s:%(levelname)s: %(filename)s:%(lineno)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
formatter.converter = converter


def get_file_handler(log_dir=LOG_DIR, file_prefix: str = ""):
    """
    Returns a file handler for logging.
    """
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y-%m-%d")
    file_name = f"{file_prefix}_{timestamp}.log"
    file_handler = logging.FileHandler(os.path.join(log_dir, file_name))
    file_handler.setFormatter(formatter)
    return file_handler


def get_console_handler():
    """
    Returns a console handler for logging.
    """
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    return console_handler


testgen_logger = logging.getLogger("testgen_logger")
testgen_logger.setLevel(logging.INFO)
testgen_logger.addHandler(get_file_handler(file_prefix="testgen"))
testgen_logger.addHandler(get_console_handler())

sync_repo = logging.getLogger("sync_repo")
sync_repo.setLevel(logging.INFO)
sync_repo.addHandler(get_file_handler(file_prefix="syncrepo"))
sync_repo.addHandler(get_console_handler())

loggers = [testgen_logger, sync_repo]


def set_log_level(level=logging.INFO):
    """
    Sets the logging level for all defined loggers.
    """
    for logger in loggers:
        logger.setLevel(level)
        for handler in logger.handlers:
            handler.setLevel(level)


def configure_uvicorn_logger():
    uvicorn_error_logger = logging.getLogger("uvicorn.error")
    uvicorn_error_logger.addHandler(get_file_handler())
    uvicorn_error_logger.addHandler(get_console_handler())


# LOGFIRE METRICS
# accepted_count = logfire.metric_counter("accepted_tests", unit="1")
# failed_count = logfire.metric_counter("failed_tests", unit="1")
# total_count = logfire.metric_counter("total_tests", unit="1")

================
File: src/models.py
================
from datetime import datetime
from sqlalchemy import Column, DateTime, event
from pydantic import BaseModel, Field
from pydantic.types import SecretStr

from typing import Annotated

PrimaryKey = Annotated[int, Field(gt=0, lt=2147483647)]
NameStr = Annotated[
    str, Field(pattern=r"^(?!\s*$).+", strip_whitespace=True, min_length=3)
]


class TimeStampMixin(object):
    """Timestamping mixin"""

    created_at = Column(DateTime, default=datetime.utcnow)
    created_at._creation_order = 9998
    updated_at = Column(DateTime, default=datetime.utcnow)
    updated_at._creation_order = 9998

    @staticmethod
    def _updated_at(mapper, connection, target):
        target.updated_at = datetime.utcnow()

    @classmethod
    def __declare_last__(cls):
        event.listen(cls, "before_update", cls._updated_at)


class RTFSBase(BaseModel):
    class Config:
        from_attributes = True
        validate_assignment = True
        arbitrary_types_allowed = True
        str_strip_whitespace = True

        json_encoders = {
            # custom output conversion for datetime
            datetime: lambda v: v.strftime("%Y-%m-%dT%H:%M:%SZ") if v else None,
            SecretStr: lambda v: v.get_secret_value() if v else None,
        }


class HTTPSuccess(BaseModel):
    msg: str = "Success"

================
File: src/utils.py
================
import functools
import random
import string
import uuid
import time
import functools
import os
from contextlib import contextmanager

from src.logger import testgen_logger

@contextmanager
def set_temp_env_var(key, value):
    old_value = os.environ.get(key)
    os.environ[key] = value
    try:
        yield
    finally:
        if old_value is None:
            del os.environ[key]
        else:
            os.environ[key] = old_value


# nested level get() function
def resolve_attr(obj, attr, default=None):
    """Attempts to access attr via dotted notation, returns none if attr does not exist."""
    try:
        return functools.reduce(getattr, attr.split("."), obj)
    except AttributeError:
        return default


def gen_random_name():
    """
    Generates a random name using ASCII, 8 characters in length
    """

    return "".join(random.choices(string.ascii_lowercase, k=8))


def generate_id():
    """
    Generates a random UUID
    """
    return str(uuid.uuid4())


def async_timed(func):
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        end_time = time.time()
        testgen_logger.info(
            f"[PARALLEL] Function {func.__name__} took {end_time - start_time:.4f} seconds"
        )
        return result

    return wrapper

================
File: .cursorignore
================
# Add directories or file patterns to ignore during indexing (e.g. foo/ or *.csv)
cowboy_lib/*

================
File: .gitignore
================
*.py[cod]

# C extensions
*.so

# Packages
*.egg
*.egg-info
build
eggs
parts
bin
var
sdist
develop-eggs
.installed.cfg
lib
lib64

# Installer logs
pip-log.txt

# Unit test / coverage reports
.coverage
.tox
nosetests.xml

# Translations
*.mo

# Mr Developer
.mr.developer.cfg
.project
.pydevproject

# Complexity
output/*.html
output/*/index.html

# Sphinx
docs/_build

.webassets-cache

# Virtualenvs
env/

# npm
/node_modules/

# webpack-built files
/auto_test/static/build/

# Configuration
.env

# Development database
*.db

# test repos
repos/*
!log/*.py
log/*.log

alembic.ini
run

================
File: .gitmodules
================
[submodule "cowboy_lib"]
	path = cowboy_lib
	url = git@github.com:JohnPeng47/cowboy-lib.git
    branch = main

================
File: main.py
================
from typing import Optional, Final
from contextvars import ContextVar
import os

from fastapi import FastAPI, status
from fastapi.responses import JSONResponse
from pydantic import ValidationError

from starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint
from starlette.requests import Request
from starlette.responses import Response, StreamingResponse

from sqlalchemy.orm import sessionmaker

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse

import uvicorn
from logging import getLogger

# from src.logger import configure_uvicorn_logger
# from src.auth.service import get_current_user

from src.queue.core import TaskQueue
from src.auth.views import auth_router
from src.repo.views import repo_router
from src.queue.views import task_queue_router
from src.health.views import health_router
from src.exceptions import ClientActionException
from src.database.core import engine

from src.extensions import init_sentry
from src.config import PORT, REPOS_ROOT

import uuid

log = getLogger(__name__)

init_sentry()


# def disable_uvicorn_logging():
#     uvicorn_error = logging.getLogger("uvicorn.error")
#     uvicorn_error.disabled = True
#     uvicorn_access = logging.getLogger("uvicorn.access")
#     uvicorn_access.disabled = True


async def not_found(request, exc):
    return JSONResponse(
        status_code=status.HTTP_404_NOT_FOUND,
        content={"detail": [{"msg": "Not Found."}]},
    )


exception_handlers = {404: not_found}


app = FastAPI(exception_handlers=exception_handlers, openapi_url="/docs/openapi.json")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# def get_path_params_from_request(request: Request) -> str:
#     path_params = {}
#     for r in api_router.routes:
#         path_regex, path_format, param_converters = compile_path(r.path)
#         path = request["path"].removeprefix(
#             "/api/v1"
#         )  # remove the /api/v1 for matching
#         match = path_regex.match(path)
#         if match:
#             path_params = match.groupdict()
#     return path_params


def get_path_template(request: Request) -> str:
    if hasattr(request, "path"):
        return ",".join(request.path.split("/")[1:])
    return ".".join(request.url.path.split("/")[1:])


REQUEST_ID_CTX_KEY: Final[str] = "request_id"
_request_id_ctx_var: ContextVar[Optional[str]] = ContextVar(
    REQUEST_ID_CTX_KEY, default=None
)


def get_request_id() -> Optional[str]:
    return _request_id_ctx_var.get()


# these paths do not require DB
NO_DB_PATHS = ["/task/get"]


class ExceptionMiddleware(BaseHTTPMiddleware):
    async def dispatch(
        self, request: Request, call_next: RequestResponseEndpoint
    ) -> StreamingResponse:
        try:
            response = await call_next(request)

        # this is an interface with client
        except ClientActionException as e:
            response = JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"error" : e.message, "type": e.type}
            )    

        except ValidationError as e:
            log.exception(e)
            response = JSONResponse(
                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                content={"detail": e.errors(), "error": True},
            )
        except ValueError as e:
            log.exception(e)
            response = JSONResponse(
                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                content={
                    "detail": [
                        {"msg": "Unknown", "loc": ["Unknown"], "type": "Unknown"}
                    ],
                    "error": True,
                },
            )
        except Exception as e:
            log.exception(e)
            response = JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "detail": [
                        {"msg": "Unknown", "loc": ["Unknown"], "type": "Unknown"}
                    ],
                    "error": True,
                },
            )

        return response


token_registry = set()


class DBMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # request_id = str(uuid1())

        # we create a per-request id such that we can ensure that our session is scoped for a particular request.
        # see: https://github.com/tiangolo/fastapi/issues/726
        # ctx_token = _request_id_ctx_var.set(request_id)
        # path_params = get_path_params_from_request(request)

        # # if this call is organization specific set the correct search path
        # organization_slug = path_params.get("organization", "default")
        # request.state.organization = organization_slug

        # # Find out more about
        # schema = f"dispatch_organization_{organization_slug}"
        # # validate slug exists
        # schema_names = inspect(engine).get_schema_names()
        # if schema in schema_names:
        #     # add correct schema mapping depending on the request
        #     schema_engine = engine.execution_options(
        #         schema_translate_map={
        #             None: schema,
        #         }
        #     )
        # else:
        #     return JSONResponse(
        #         status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        #         content={"detail": [{"msg": f"Unknown database schema name: {schema}"}]},
        #     )

        try:
            request.state.session_id = str(uuid.uuid4())
            # this is a very janky implementation to handle the fact that assigning a db session
            # to every request blows up our db connection pool
            task_auth_token = request.headers.get("x-task-auth", None)
            if not task_auth_token or not task_auth_token in token_registry:
                session = sessionmaker(bind=engine)
                request.state.db = session()
                request.state.db.id = str(uuid.uuid4())

            response = await call_next(request)
        except Exception as e:
            raise e from None
        finally:
            db = getattr(request.state, "db", None)
            if db:
                db.close()

        # _request_id_ctx_var.reset(ctx_token)
        return response


# class LogfireLogUser(BaseHTTPMiddleware):
#     async def dispatch(self, request: Request, call_next):
#         try:
#             # we have to skip requests with x-task-auth or else logfire will log an exception for this
#             # request when it tries to acces request.state.db
#             if not request.headers.get("x-task-auth", None):
#                 with logfire.span("request"):
#                     user = get_current_user(request)
#                     logfire.info("{user}", user=user.email)
#         except AttributeError as e:
#             pass
#         finally:
#             response = await call_next(request)
#             return response


task_queue = TaskQueue()


class AddTaskQueueMiddleware(BaseHTTPMiddleware):
    async def dispatch(
        self, request: Request, call_next: RequestResponseEndpoint
    ) -> Response:
        request.state.task_queue = task_queue
        response = await call_next(request)
        return response


# app.add_middleware(LogfireLogUser)
app.add_middleware(ExceptionMiddleware)
app.add_middleware(DBMiddleware)
app.add_middleware(AddTaskQueueMiddleware)

app.include_router(auth_router)
app.include_router(repo_router)
app.include_router(task_queue_router)
app.include_router(health_router)

# logfire.configure(console=False)
# logfire.instrument_fastapi(app, excluded_urls=["/task/get"])

def init():
    if not os.path.exists(REPOS_ROOT):
        os.makedirs(REPOS_ROOT)

if __name__ == "__main__":
    import argparse

    # start the repo sync thread
    # Session = sessionmaker(bind=engine)
    # db_session = Session()
    # start_sync_thread(db_session, task_queue)

    # logfire.configure()

    init()

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=PORT,
        reload=True,
        reload_excludes=["./repos"],
        # log_config=config,
    )

================
File: pyproject.toml
================
[tool.poetry]
name = "codesearch-backend"
version = "0.1.0"
description = ""
authors = ["JohnPeng47 <johnpeng47@gmail.com>"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.10"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

================
File: repopack.config.json
================
{
  "output": {
    "filePath": "repopack-output.txt",
    "style": "plain",
    "removeComments": false,
    "removeEmptyLines": false,
    "topFilesLength": 5,
    "showLineNumbers": false
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": ["rtfs", "cowboy_lib"]
  }
}

================
File: requirements.txt
================
alembic
fastapi
sqlalchemy
uvicorn
gitpython
pydantic==1.10.11
psycopg2
psycopg2-binary
python-jose
pydantic[email]
black
sentry-sdk[fastapi]anthropic==0.32.0
infomap==2.8.0
intervaltree==3.1.0
llama_index==0.10.59
networkx==3.3
openai==1.38.0
python-dotenv==1.0.1
PyYAML==6.0.1
PyYAML==6.0.1
setuptools==65.5.0
simple_parsing==0.1.5
starlette==0.38.2
tenacity==8.5.0
tiktoken==0.7.0
tree_sitter==0.22.3
tree_sitter_java==0.21.0
tree_sitter_python==0.21.0
typing_extensions==4.12.2

================
File: test.py
================
# import sys

# sys.path.append("/home/ubuntu/cowboy-server-good")

from cowboy_lib.repo import SourceRepo
from src.test_modules.service import get_tm_by_name, get_all_tms

from src.database.core import engine

from sqlalchemy.orm import sessionmaker
from pathlib import Path


repo_path = "/home/ubuntu/cowboy-server-good/repos/test2/qrjmnlxt"
src_repo = SourceRepo(Path(repo_path))
Session = sessionmaker(bind=engine)
db_session = Session()


# tm_model = get_tm_by_name(db_session=db_session, repo_id=17, tm_name="TestWoodpecker")
# tm = tm_model.serialize(src_repo)

tm_models = get_all_tms(db_session=db_session, repo_id=17)
tm_models = sorted(tm_models, key=lambda tm: tm.agg_score(src_repo), reverse=True)

for tm in tm_models:
    print(tm.name, tm.agg_score(src_repo))

================
File: uvicorn.yaml
================
version: 1
disable_existing_loggers: False
formatters:
  simple:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

handlers:
  console:
    class: logging.StreamHandler
    level: DEBUG
    formatter: simple
    stream: ext://sys.stdout  # Use sys.stderr for standard error

loggers:
  uvicorn:
    level: DEBUG
    handlers: [console]
    propagate: no
  uvicorn.error:
    level: DEBUG
    handlers: [console]
    propagate: no
  uvicorn.access:
    level: DEBUG
    handlers: [console]
    propagate: no

root:
  level: DEBUG
  handlers: [console]
