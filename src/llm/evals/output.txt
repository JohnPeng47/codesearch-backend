Configuration and Provider Management:
ell\configurator.py::1
from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)

--------------------------------------------------------------------------------

ell\configurator.py::2
class Config(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    registry: Dict[str, _Model] = Field(default_factory=dict, description="A dictionary mapping model names to their configurations.")
    verbose: bool = Field(default=False, description="If True, enables verbose logging.")
    wrapped_logging: bool = Field(default=True, description="If True, enables wrapped logging for better readability.")
    override_wrapped_logging_width: Optional[int] = Field(default=None, description="If set, overrides the default width for wrapped logging.")
    store: Optional[Store] = Field(default=None, description="An optional Store instance for persistence.")
    autocommit: bool = Field(default=False, description="If True, enables automatic committing of changes to the store.")
    lazy_versioning: bool = Field(default=True, description="If True, enables lazy versioning for improved performance.")
    default_api_params: Dict[str, Any] = Field(default_factory=dict, description="Default parameters for language models.")
    default_client: Optional[openai.Client] = Field(default=None, description="The default OpenAI client used when a specific model client is not found.")
    autocommit_model: str = Field(default="gpt-4o-mini", description="When set, changes the default autocommit model from GPT 4o mini.")
    providers: Dict[Type, Provider] = Field(default_factory=dict, description="A dictionary mapping client types to provider classes.")
    def __init__(self, **data):
        super().__init__(**data)
        self._lock = threading.Lock()
        self._local = threading.local()

--------------------------------------------------------------------------------

ell\configurator.py::3
class Config(BaseModel):


    def register_model(
        self, 
        name: str,
        default_client: Optional[Union[openai.Client, Any]] = None,
        supports_streaming: Optional[bool] = None
    ) -> None:
        """
        Register a model with its configuration.
        """
        with self._lock:
            # XXX: Will be deprecated in 0.1.0
            self.registry[name] = _Model(
                name=name,
                default_client=default_client,
                supports_streaming=supports_streaming
            )

--------------------------------------------------------------------------------

ell\configurator.py::4
class Config(BaseModel):



    @contextmanager
    def model_registry_override(self, overrides: Dict[str, _Model]):
        """
        Temporarily override the model registry with new model configurations.

        :param overrides: A dictionary of model names to ModelConfig instances to override.
        :type overrides: Dict[str, ModelConfig]
        """
        if not hasattr(self._local, 'stack'):
            self._local.stack = []

        with self._lock:
            current_registry = self._local.stack[-1] if self._local.stack else self.registry
            new_registry = current_registry.copy()
            new_registry.update(overrides)

        self._local.stack.append(new_registry)
        try:
            yield
        finally:
            self._local.stack.pop()

--------------------------------------------------------------------------------

ell\configurator.py::5
class Config(BaseModel):

    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:
        """
        Get the OpenAI client for a specific model name.

        :param model_name: The name of the model to get the client for.
        :type model_name: str
        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.
        :rtype: Tuple[Optional[openai.Client], bool]
        """
        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry
        model_config = current_registry.get(model_name)
        fallback = False
        if not model_config:
            warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."
            if self.verbose:
                from colorama import Fore, Style
                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")
            else:
                _config_logger.debug(warning_message)
            client = self.default_client
            fallback = True
        else:
            client = model_config.default_client
        return client, fallback

--------------------------------------------------------------------------------

ell\configurator.py::7
class Config(BaseModel):

    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:
        """
        Get the provider instance for a specific client instance.

        :param client: The client instance to get the provider for.
        :type client: Any
        :return: The provider instance for the specified client, or None if not found.
        :rtype: Optional[Provider]
        """

        client_type = type(client) if not isinstance(client, type) else client
        for provider_type, provider in self.providers.items():
            if issubclass(client_type, provider_type) or client_type == provider_type:
                return provider
        return None

# Single* instance
# XXX: Make a singleton
config = Config()

--------------------------------------------------------------------------------

ell\configurator.py::8
def init(
    store: Optional[Union[Store, str]] = None,
    verbose: bool = False,
    autocommit: bool = True,
    lazy_versioning: bool = True,
    default_api_params: Optional[Dict[str, Any]] = None,
    default_client: Optional[Any] = None,
    autocommit_model: str = "gpt-4o-mini"
) -> None:
    """
    Initialize the ELL configuration with various settings.

    :param verbose: Set verbosity of ELL operations.
    :type verbose: bool
    :param store: Set the store for ELL. Can be a Store instance or a string path for SQLiteStore.
    :type store: Union[Store, str], optional
    :param autocommit: Set autocommit for the store operations.
    :type autocommit: bool
    :param lazy_versioning: Enable or disable lazy versioning.
    :type lazy_versioning: bool
    :param default_api_params: Set default parameters for language models.
    :type default_api_params: Dict[str, Any], optional
    :param default_openai_client: Set the default OpenAI client.
    :type default_openai_client: openai.Client, optional
    :param autocommit_model: Set the model used for autocommitting.
    :type autocommit_model: str
    """
    # XXX: prevent double init
    config.verbose = verbose
    config.lazy_versioning = lazy_versioning

    if isinstance(store, str):
        from ell.stores.sql import SQLiteStore
        config.store = SQLiteStore(store)
    else:
        config.store = store
    config.autocommit = autocommit or config.autocommit

    if default_api_params is not None:
        config.default_api_params.update(default_api_params)

    if default_client is not None:
        config.default_client = default_client

    if autocommit_model is not None:
        config.autocommit_model = autocommit_model

--------------------------------------------------------------------------------

ell\configurator.py::9
# Existing helper functions
def get_store() -> Union[Store, None]:
    return config.store

# Will be deprecated at 0.1.0 

# You can add more helper functions here if needed
def register_provider(provider: Provider, client_type: Type[Any]) -> None:
    return config.register_provider(provider, client_type)

# Deprecated now (remove at 0.1.0)
def set_store(*args, **kwargs) -> None:
    raise DeprecationWarning("The set_store function is deprecated and will be removed in a future version. Use ell.init(store=...) instead.")

--------------------------------------------------------------------------------

models\__init__.py::1
"""
Attempts to registeres model names with their respective API client bindings. This allows for the creation of a unified interface for interacting with different LLM providers.

For example, to register an OpenAI model:
@ell.simple(model='gpt-4o-mini') -> @ell.simple(model='gpt-4o-mini', client=openai.OpenAI())

"""

import ell.models.openai
import ell.models.anthropic
import ell.models.ollama
import ell.models.groq
import ell.models.bedrock

--------------------------------------------------------------------------------

providers\__init__.py::1
import ell.providers.openai
import ell.providers.groq
import ell.providers.anthropic
import ell.providers.bedrock
# import ell.providers.mistral
# import ell.providers.cohere
# import ell.providers.gemini
# import ell.providers.elevenlabs
# import ell.providers.replicate
# import ell.providers.huggingface

--------------------------------------------------------------------------------

ell\configurator.py::6
class Config(BaseModel):

    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:
        """
        Register a provider class for a specific client type.

        :param provider_class: The provider class to register.
        :type provider_class: Type[Provider]
        """
        assert isinstance(client_type, type), "client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))"
        with self._lock:
            self.providers[client_type] = provider

--------------------------------------------------------------------------------

ell\provider.py::1
from abc import ABC, abstractmethod
from collections import defaultdict
from functools import lru_cache
import inspect
from types import MappingProxyType
from typing import (
    Any,
    Callable,
    Dict,
    FrozenSet,
    List,
    Optional,
    Set,
    Tuple,
    Type,
    TypedDict,
    Union,
)

from pydantic import BaseModel, ConfigDict, Field
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from dataclasses import dataclass
from ell.types.message import LMP

--------------------------------------------------------------------------------

ell\provider.py::2
# XXX: Might leave this internal to providers so that the complex code is simpler &
# we can literally jsut call provider.call like any openai fn.
class EllCallParams(BaseModel):
    model: str = Field(..., description="Model identifier")
    messages: List[Message] = Field(..., description="Conversation context")
    client: Any = Field(..., description="API client")
    tools: List[LMP] = Field(default_factory=list, description="Available tools")
    api_params: Dict[str, Any] = Field(
        default_factory=dict, description="API parameters"
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def get_tool_by_name(self, name: str) -> Optional[LMP]:
        """Get a tool by name."""
        return next(
            (tool for tool in (self.tools or [])  if tool.__name__ == name), None
        )


Metadata = Dict[str, Any]

--------------------------------------------------------------------------------

ell\provider.py::4
class Provider(ABC):
    def call(
        self,
        #XXX: In future refactors, we can fully enumerate the args and make ell_call's internal to the _provider implementer interface.
        # This gives us a litellm style interface for free.
        ell_call: EllCallParams,
        origin_id: Optional[str] = None,
        logger: Optional[Any] = None,
    ) -> Tuple[List[Message], Dict[str, Any], Metadata]:
        # Automatic validation of params
        assert (
            not set(ell_call.api_params.keys()).intersection(self.disallowed_api_params()) 
        ), f"Disallowed api parameters: {ell_call.api_params}"

        final_api_call_params = self.translate_to_provider(ell_call)

        call = self.provider_call_function(ell_call.client, final_api_call_params)
        assert self.dangerous_disable_validation or _validate_provider_call_params(final_api_call_params, call)


        provider_resp = call(**final_api_call_params)

        messages, metadata = self.translate_from_provider(
            provider_resp, ell_call, final_api_call_params, origin_id, logger
        )
        assert "choices" not in metadata, "choices should be in the metadata."
        assert self.dangerous_disable_validation or _validate_messages_are_tracked(messages, origin_id)

        return messages, final_api_call_params, metadata

--------------------------------------------------------------------------------

ell\provider.py::5
# handhold the the implementer, in production mode we can turn these off for speed.
@lru_cache(maxsize=None)
def _call_params(call: Callable[..., Any]) -> MappingProxyType[str, inspect.Parameter]:
    return inspect.signature(call).parameters


def _validate_provider_call_params(
    api_call_params: Dict[str, Any], call: Callable[..., Any]
):
    provider_call_params = _call_params(call)

    required_params = {
        name: param
        for name, param in provider_call_params.items()
        if param.default == param.empty and param.kind != param.VAR_KEYWORD
    }

    for param_name in required_params:
        assert (
            param_name in api_call_params
        ), f"Provider implementation error: Required parameter '{param_name}' is missing in the converted call parameters converted from ell call."

    for param_name, param_value in api_call_params.items():
        assert (
            param_name in provider_call_params
        ), f"Provider implementation error: Unexpected parameter '{param_name}' in the converted call parameters."

    return True

--------------------------------------------------------------------------------

providers\openai.py::1
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast

from pydantic import BaseModel
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from ell.configurator import _Model, config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image

try:
    # XXX: Could genericize.
    import openai
    from openai._streaming import Stream
    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam

    class OpenAIProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and (isinstance(fmt := api_call_params.get("response_format"), type)) and issubclass(fmt, BaseModel):
                return client.beta.chat.completions.parse
            else:
                return client.chat.completions.create

        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:
            final_call_params = ell_call.api_params.copy()
            final_call_params["model"] = ell_call.model
            # Stream by default for verbose logging.
            final_call_params["stream"] = True
            final_call_params["stream_options"] = {"include_usage": True}

            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.
            if ell_call.tools or final_call_params.get("response_format") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:
                final_call_params.pop("stream", None)
                final_call_params.pop("stream_options", None)
            if ell_call.tools:
                final_call_params.update(
                    tool_choice=final_call_params.get("tool_choice", "auto"),
                    tools=[  
                        dict(
                            type="function",
                            function=dict(
                                name=tool.__name__,
                                description=tool.__doc__,
                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore
                            )
                        ) for tool in ell_call.tools
                    ]
                )
            # messages
            openai_messages : List[ChatCompletionMessageParam] = []
            for message in ell_call.messages:
                if (tool_calls := message.tool_calls):
                    assert message.role == "assistant", "Tool calls must be from the assistant."
                    assert all(t.tool_call_id for t in tool_calls), "Tool calls must have tool call ids."
                    openai_messages.append(dict(
                        tool_calls=[
                            dict(
                                id=cast(str, tool_call.tool_call_id),
                                type="function",
                                function=dict(
                                    name=tool_call.tool.__name__,
                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)
                                )
                            ) for tool_call in tool_calls ],
                        role="assistant",
                        content=None,
                    ))
                elif (tool_results := message.tool_results):
                    for tool_result in tool_results:
                        assert all(cb.type == "text" for cb in tool_result.result), "Tool result does not match expected content blocks."
                        openai_messages.append(dict(
                            role="tool",
                            tool_call_id=tool_result.tool_call_id,
                            content=tool_result.text_only, 
                        ))
                else:
                    openai_messages.append(cast(ChatCompletionMessageParam, dict(
                        role=message.role,
                        content=[_content_block_to_openai_format(c) for c in message.content] 
                             if message.role != "system" 
                             else message.text_only
                    )))

            final_call_params["messages"] = openai_messages

            return final_call_params

        def translate_from_provider(
            self,
            provider_response: Union[
                ChatCompletion, 
                ParsedChatCompletion,
                Stream[ChatCompletionChunk], Any],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            metadata : Metadata = {}
            messages : List[Message] = []
            did_stream = provider_call_params.get("stream", False)


            if did_stream:
                stream = cast(Stream[ChatCompletionChunk], provider_response)
                message_streams = defaultdict(list)
                role : Optional[str] = None
                for chunk in stream:
                    metadata.update(chunk.model_dump(exclude={"choices"}))

                    for chat_compl_chunk in chunk.choices:
                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)
                        delta = chat_compl_chunk.delta
                        role = role or delta.role
                        if  chat_compl_chunk.index == 0 and logger:
                            logger(delta.content, is_refusal=hasattr(delta, "refusal") and delta.refusal)
                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):
                    text = "".join((choice.delta.content or "") for choice in message_stream)
                    messages.append(
                        Message(role=role, 
                                content=_lstr(content=text,origin_trace=origin_id)))
                    #XXX: Support streaming other types.
            else:
                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)
                metadata = chat_completion.model_dump(exclude={"choices"})
                for oai_choice in chat_completion.choices:
                    role = oai_choice.message.role
                    content_blocks = []
                    if (hasattr(message := oai_choice.message, "refusal") and (refusal := message.refusal)):
                        raise ValueError(refusal)
                    if hasattr(message, "parsed"):
                        if (parsed := message.parsed):
                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing
                            if logger: logger(parsed.model_dump_json())
                    else:
                        if (content := message.content):
                            content_blocks.append(
                                ContentBlock(
                                    text=_lstr(content=content,origin_trace=origin_id)))
                            if logger: logger(content)
                        if (tool_calls := message.tool_calls):
                            for tool_call in tool_calls:
                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)
                                assert matching_tool, "Model called tool not found in provided toolset."
                                content_blocks.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                tool_call.id, origin_trace= origin_id),
                                            params=json.loads(tool_call.function.arguments),
                                        )
                                    )
                                )
                                if logger: logger(repr(tool_call))
                    messages.append(Message(role=role, content=content_blocks))
            return messages, metadata


    # xx: singleton needed
    openai_provider = OpenAIProvider()
    register_provider(openai_provider, openai.Client)
except ImportError:
    pass

--------------------------------------------------------------------------------

providers\openai.py::2
def _content_block_to_openai_format(content_block: ContentBlock) -> Dict[str, Any]:
    if (image := content_block.image):
        image_url = dict(url=serialize_image(image.image) if image.image else image.url)
        # XXX: Solve per content params better
        if image.detail: image_url["detail"] = image.detail
        return {
            "type": "image_url",
            "image_url": image_url
        }
    elif ((text := content_block.text) is not None): return dict(type="text", text=text)
    elif (parsed := content_block.parsed): return dict(type="text", text=parsed.model_dump_json())
    else:
        raise ValueError(f"Unsupported content block type for openai: {content_block}")

--------------------------------------------------------------------------------

models\openai.py::1
"""
This module handles the registration of OpenAI models within the ell framework.

It provides functionality to register various OpenAI models with a given OpenAI client,
making them available for use throughout the system. The module also sets up a default
client behavior for unregistered models.

Key features:
1. Registration of specific OpenAI models with their respective types (system, openai, openai-internal).
2. Utilization of a default OpenAI client for any unregistered models,

The default client behavior ensures that even if a specific model is not explicitly
registered, the system can still attempt to use it with the default OpenAI client.
This fallback mechanism provides flexibility in model usage while maintaining a
structured approach to model registration.

Note: The actual model availability may depend on your OpenAI account's access and the
current offerings from OpenAI.

Additionally, due to the registration of default mdoels, the OpenAI client may be used for
anthropic, cohere, groq, etc. models if their clients are not registered or fail
to register due to an error (lack of API keys, rate limits, etc.)
"""

from ell.configurator import config
import openai

import logging
import colorama

logger = logging.getLogger(__name__)

--------------------------------------------------------------------------------

models\openai.py::2
def register(client: openai.Client):
    """
    Register OpenAI models with the provided client.

    This function takes an OpenAI client and registers various OpenAI models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (openai.Client): An instance of the OpenAI client to be used
                                for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    #XXX: Deprecation in 0.1.0
    standard_models = [
        'gpt-4-1106-preview',
        'gpt-4-32k-0314',
        'text-embedding-3-large',
        'gpt-4-0125-preview',
        'babbage-002',
        'gpt-4-turbo-preview',
        'gpt-4o',
        'gpt-4o-2024-05-13',
        'gpt-4o-mini-2024-07-18',
        'gpt-4o-mini',
        'gpt-4o-2024-08-06',
        'gpt-3.5-turbo-0301',
        'gpt-3.5-turbo-0613',
        'tts-1',
        'gpt-3.5-turbo',
        'gpt-3.5-turbo-16k',
        'davinci-002',
        'gpt-3.5-turbo-16k-0613',
        'gpt-4-turbo-2024-04-09',
        'gpt-3.5-turbo-0125',
        'gpt-4-turbo',
        'gpt-3.5-turbo-1106',
        'gpt-3.5-turbo-instruct-0914',
        'gpt-3.5-turbo-instruct',
        'gpt-4-0613',
        'gpt-4',
        'gpt-4-0314',
        'gpt-4o-audio-preview',
        'gpt-4o-realtime',
    ]
    for model_id in standard_models:
        config.register_model(model_id, client)

    #XXX: Deprecation in 0.1.0
    config.register_model('o1-preview', client, supports_streaming=False)
    config.register_model('o1-mini', client, supports_streaming=False)

default_client = None
try:
    default_client = openai.Client()
except openai.OpenAIError as e:
    pass

register(default_client)
config.default_client = default_client

--------------------------------------------------------------------------------



Language Model Programs (LMP) and Invocation:
lmp\complex.py::1
from ell.configurator import config
from ell.lmp._track import _track
from ell.provider import EllCallParams
from ell.types._lstr import _lstr
from ell.types import Message, ContentBlock
from ell.types.message import LMP, InvocableLM, LMPParams, MessageOrDict, _lstr_generic
from ell.types.studio import LMPType
from ell.util._warnings import _no_api_key_warning, _warnings
from ell.util.verbosity import compute_color, model_usage_logger_pre

from ell.util.verbosity import model_usage_logger_post_end, model_usage_logger_post_intermediate, model_usage_logger_post_start

from functools import wraps
from typing import Any, Dict, Optional, List, Callable, Tuple, Union

--------------------------------------------------------------------------------

lmp\complex.py::2
def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    default_client_from_decorator = client
    default_model_from_decorator = model
    default_api_params_from_decorator = api_params
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        _warnings(model, prompt, default_client_from_decorator)

        @wraps(prompt)
        def model_call(
            *prompt_args,
            _invocation_origin : Optional[str] = None,
            client: Optional[Any] = None,
            api_params: Optional[Dict[str, Any]] = None,
            lm_params: Optional[DeprecationWarning] = None,
            **prompt_kwargs,
        ) -> Tuple[Any, Any, Any]:
            # XXX: Deprecation in 0.1.0
            if lm_params:
                raise DeprecationWarning("lm_params is deprecated. Use api_params instead.")

            # promt -> str
            res = prompt(*prompt_args, **prompt_kwargs)
            # Convert prompt into ell messages
            messages = _get_messages(res, prompt)

            # XXX: move should log to a logger.
            should_log = not exempt_from_tracking and config.verbose
            # Cute verbose logging.
            if should_log: model_usage_logger_pre(prompt, prompt_args, prompt_kwargs, "[]", messages) #type: ignore

            # Call the model.
            # Merge API params
            merged_api_params = {**config.default_api_params, **default_api_params_from_decorator, **(api_params or {})}
            n = merged_api_params.get("n", 1)
            # Merge client overrides & client registry
            merged_client = _client_for_model(model, client or default_client_from_decorator)
            ell_call = EllCallParams(
                # XXX: Could change behaviour of overriding ell params for dyanmic tool calls.
                model=merged_api_params.pop("model", default_model_from_decorator),
                messages=messages,
                client = merged_client,
                api_params=merged_api_params,
                tools=tools or [],
            )
            # Get the provider for the model
            provider = config.get_provider_for(ell_call.client)
            assert provider is not None, f"No provider found for client {ell_call.client}."

            if should_log: model_usage_logger_post_start(n)
            with model_usage_logger_post_intermediate(n) as _logger:
                (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)
                if isinstance(result, list) and len(result) == 1:
                    result = result[0]

            result = post_callback(result) if post_callback else result
            if should_log:
                model_usage_logger_post_end()
            #
            #  These get sent to track. This is wack.           
            return result, final_api_params, metadata
        # ... other code
    # ... other code

--------------------------------------------------------------------------------

lmp\complex.py::3
def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        # ... other code



        model_call.__ell_api_params__ = default_api_params_from_decorator #type: ignore
        model_call.__ell_func__ = prompt #type: ignore
        model_call.__ell_type__ = LMPType.LM #type: ignore
        model_call.__ell_exempt_from_tracking = exempt_from_tracking #type: ignore


        if exempt_from_tracking:
            return model_call
        else:
            # XXX: Analyze decorators with AST instead.
            return _track(model_call, forced_dependencies=dict(tools=tools, response_format=api_params.get("response_format", {})))
    return parameterized_lm_decorator

--------------------------------------------------------------------------------

lmp\complex.py::4
def _get_messages(prompt_ret: Union[str, list[MessageOrDict]], prompt: LMP) -> list[Message]:
    """
    Helper function to convert the output of an LMP into a list of Messages.
    """
    if isinstance(prompt_ret, str):
        has_system_prompt = prompt.__doc__ is not None and prompt.__doc__.strip() != ""
        messages =     [Message(role="system", content=[ContentBlock(text=_lstr(prompt.__doc__ ) )])] if has_system_prompt else []
        return messages + [
            Message(role="user", content=[ContentBlock(text=prompt_ret)])
        ]
    else:
        assert isinstance(
            prompt_ret, list
        ), "Need to pass a list of Messages to the language model"
        return prompt_ret

--------------------------------------------------------------------------------

lmp\complex.py::5
def _client_for_model(
    model: str,
    client: Optional[Any] = None,
    _name: Optional[str] = None,
) -> Any:
    # XXX: Move to config to centralize api keys etc.
    if not client:
        client, was_fallback = config.get_client_for(model)

        # XXX: Wrong.
        if not client and not was_fallback:
            raise RuntimeError(_no_api_key_warning(model, _name, '', long=True, error=True))

    if client is None:
        raise ValueError(f"No client found for model '{model}'. Ensure the model is registered using 'register_model' in 'config.py' or specify a client directly using the 'client' argument in the decorator or function call.")
    return client


complex.__doc__ =
 # ... other code

--------------------------------------------------------------------------------

lmp\simple.py::1
from functools import wraps
from typing import Any, Optional

from ell.lmp.complex import complex


def simple(model: str, client: Optional[Any] = None,  exempt_from_tracking=False, **api_params):
    assert 'tools' not in api_params, "tools are not supported in lm decorator, use multimodal decorator instead"
    assert 'tool_choice' not in api_params, "tool_choice is not supported in lm decorator, use multimodal decorator instead"
    assert 'response_format' not in api_params or isinstance(api_params.get('response_format', None), dict), "response_format is not supported in lm decorator, use multimodal decorator instead"

    def convert_multimodal_response_to_lstr(response):
        return [x.content[0].text for x in response] if isinstance(response, list) else response.content[0].text
    return complex(model, client,  exempt_from_tracking=exempt_from_tracking, **api_params, post_callback=convert_multimodal_response_to_lstr)

--------------------------------------------------------------------------------

lmp\simple.py::2
simple.__doc__ = """The fundamental unit of language model programming in ell.

  This decorator simplifies the process of creating Language Model Programs (LMPs) 
  that return text-only outputs from language models, while supporting multimodal inputs.
  It wraps the more complex 'complex' decorator, providing a streamlined interface for common use cases.

  :param model: The name or identifier of the language model to use.
  :type model: str
  :param client: An optional OpenAI client instance. If not provided, a default client will be used.
  :type client: Optional[openai.Client]
  :param exempt_from_tracking: If True, the LMP usage won't be tracked. Default is False.
  :type exempt_from_tracking: bool
  :param api_params: Additional keyword arguments to pass to the underlying API call.
  :type api_params: Any

  Usage:
  The decorated function can return either a single prompt or a list of ell.Message objects:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def summarize_text(text: str) -> str:
          '''You are an expert at summarizing text.''' # System prompt
          return f"Please summarize the following text:\\n\\n{text}" # User prompt


      @ell.simple(model="gpt-4", temperature=0.7)
      def describe_image(image : PIL.Image.Image) -> List[ell.Message]:
          '''Describe the contents of an image.''' # unused because we're returning a list of Messages
          return [
              # helper function for ell.Message(text="...", role="system")
              ell.system("You are an AI trained to describe images."),
              # helper function for ell.Message(content="...", role="user")
              ell.user(["Describe this image in detail.", image]),
          ]


      image_description = describe_image(PIL.Image.open("https://example.com/image.jpg"))
      print(image_description) 
      # Output will be a string text-only description of the image

      summary = summarize_text("Long text to summarize...")
      print(summary)
      # Output will be a text-only summary

  Notes:

  - This decorator is designed for text-only model outputs, but supports multimodal inputs.
  - It simplifies complex responses from language models to text-only format, regardless of 
    the model's capability for structured outputs, function calling, or multimodal outputs.
  - For preserving complex model outputs (e.g., structured data, function calls, or multimodal 
    outputs), use the @ell.complex decorator instead. @ell.complex returns a Message object (role='assistant')
  - The decorated function can return a string or a list of ell.Message objects for more 
    complex prompts, including multimodal inputs.
  - If called with n > 1 in api_params, the wrapped LMP will return a list of strings for the n parallel outputs
    of the model instead of just one string. Otherwise, it will return a single string.
  - You can pass LM API parameters either in the decorator or when calling the decorated function.
    Parameters passed during the function call will override those set in the decorator.

  Example of passing LM API params:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def generate_story(prompt: str) -> str:
          return f"Write a short story based on this prompt: {prompt}"

      # Using default parameters
      story1 = generate_story("A day in the life of a time traveler")

      # Overriding parameters during function call
      story2 = generate_story("An AI's first day of consciousness", api_params={"temperature": 0.9, "max_tokens": 500})

  See Also:

  - :func:`ell.complex`: For LMPs that preserve full structure of model responses, including multimodal outputs.
  - :func:`ell.tool`: For defining tools that can be used within complex LMPs.
  - :mod:`ell.studio`: For visualizing and analyzing LMP executions.
    """

--------------------------------------------------------------------------------

lmp\_track.py::1
import json
import logging
import threading
from ell.types import SerializedLMP, Invocation, InvocationTrace, InvocationContents
from ell.types.studio import LMPType, utc_now
from ell.util._warnings import _autocommit_warning
import ell.util.closure
from ell.configurator import config
from ell.types._lstr import _lstr

import inspect

import secrets
import time
from datetime import datetime
from functools import wraps
from typing import Any, Callable, Dict, Iterable, Optional, OrderedDict, Tuple

from ell.util.serialization import get_immutable_vars
from ell.util.serialization import compute_state_cache_key
from ell.util.serialization import prepare_invocation_params

logger = logging.getLogger(__name__)

# Thread-local storage for the invocation stack
_invocation_stack = threading.local()

def get_current_invocation() -> Optional[str]:
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    return _invocation_stack.stack[-1] if _invocation_stack.stack else None

def push_invocation(invocation_id: str):
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    _invocation_stack.stack.append(invocation_id)

def pop_invocation():
    if hasattr(_invocation_stack, 'stack') and _invocation_stack.stack:
        _invocation_stack.stack.pop()

--------------------------------------------------------------------------------

lmp\_track.py::2
def _track(func_to_track: Callable, *, forced_dependencies: Optional[Dict[str, Any]] = None) -> Callable:

    lmp_type = getattr(func_to_track, "__ell_type__", LMPType.OTHER)


    # see if it exists
    if not hasattr(func_to_track, "_has_serialized_lmp"):
        func_to_track._has_serialized_lmp = False

    if not hasattr(func_to_track, "__ell_hash__") and not config.lazy_versioning:
        ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)


    @wraps(func_to_track)
    def tracked_func(*fn_args, _get_invocation_id=False, **fn_kwargs) -> str:
        # XXX: Cache keys and global variable binding is not thread safe.
        # Compute the invocation id and hash the inputs for serialization.
        invocation_id = "invocation-" + secrets.token_hex(16)

        state_cache_key : str = None
        if not config.store:
            return func_to_track(*fn_args, **fn_kwargs, _invocation_origin=invocation_id)[0]

        parent_invocation_id = get_current_invocation()
        try:
            push_invocation(invocation_id)

            # Convert all positional arguments to named keyword arguments
            sig = inspect.signature(func_to_track)
            # Filter out kwargs that are not in the function signature
            filtered_kwargs = {k: v for k, v in fn_kwargs.items() if k in sig.parameters}

            bound_args = sig.bind(*fn_args, **filtered_kwargs)
            bound_args.apply_defaults()
            all_kwargs = dict(bound_args.arguments)

            # Get the list of consumed lmps and clean the invocation params for serialization.
            cleaned_invocation_params, ipstr, consumes = prepare_invocation_params( all_kwargs)

            try_use_cache = hasattr(func_to_track.__wrapper__, "__ell_use_cache__")

            if  try_use_cache:
                # Todo: add nice logging if verbose for when using a cahced invocaiton. IN a different color with thar args..
                if not hasattr(func_to_track, "__ell_hash__")  and config.lazy_versioning:
                    fn_closure, _ = ell.util.closure.lexically_closured_source(func_to_track)

                # compute the state cachekey
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

                cache_store = func_to_track.__wrapper__.__ell_use_cache__
                cached_invocations = cache_store.get_cached_invocations(func_to_track.__ell_hash__, state_cache_key)


                if len(cached_invocations) > 0:
                    # XXX: Fix caching.
                    results =  [d.deserialize() for  d in cached_invocations[0].results]

                    logger.info(f"Using cached result for {func_to_track.__qualname__} with state cache key: {state_cache_key}")
                    if len(results) == 1:
                        return results[0]
                    else:
                        return results
                    # Todo: Unfiy this with the non-cached case. We should go through the same code pathway.
                else:
                    logger.info(f"Attempted to use cache on {func_to_track.__qualname__} but it was not cached, or did not exist in the store. Refreshing cache...")


            _start_time = utc_now()

            # XXX: thread saftey note, if I prevent yielding right here and get the global context I should be fine re: cache key problem

            # get the prompt
            (result, invocation_api_params, metadata) = (
                (func_to_track(*fn_args, **fn_kwargs), {}, {})
                if lmp_type == LMPType.OTHER
                else func_to_track(*fn_args, _invocation_origin=invocation_id, **fn_kwargs, )
                )
            latency_ms = (utc_now() - _start_time).total_seconds() * 1000
            usage = metadata.get("usage", {"prompt_tokens": 0, "completion_tokens": 0})
            prompt_tokens= usage.get("prompt_tokens", 0) if usage else 0
            completion_tokens= usage.get("completion_tokens", 0) if usage else 0


            #XXX: cattrs add invocation origin here recursively on all pirmitive types within a message.
            #XXX: This will allow all objects to be traced automatically irrespective origin rather than relying on the API to do it, it will of vourse be expensive but unify track.
            #XXX: No other code will need to consider tracking after this point.

            if not hasattr(func_to_track, "__ell_hash__") and config.lazy_versioning:
                ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)
            _serialize_lmp(func_to_track)

            if not state_cache_key:
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

            _write_invocation(func_to_track, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                            state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id)

            if _get_invocation_id:
                return result, invocation_id
            else:
                return result
        finally:
            pop_invocation()


    func_to_track.__wrapper__  = tracked_func
    if hasattr(func_to_track, "__ell_api_params__"):
        tracked_func.__ell_api_params__ = func_to_track.__ell_api_params__
    if hasattr(func_to_track, "__ell_params_model__"):
        tracked_func.__ell_params_model__ = func_to_track.__ell_params_model__
    tracked_func.__ell_func__ = func_to_track
    tracked_func.__ell_track = True

    return tracked_func

--------------------------------------------------------------------------------

lmp\_track.py::3
def _serialize_lmp(func):
    # Serialize deptjh first all fo the used lmps.
    for f in func.__ell_uses__:
        _serialize_lmp(f)

    if getattr(func, "_has_serialized_lmp", False):
        return
    func._has_serialized_lmp = False
    fn_closure = func.__ell_closure__
    lmp_type = func.__ell_type__
    name = func.__qualname__
    api_params = getattr(func, "__ell_api_params__", None)

    lmps = config.store.get_versions_by_fqn(fqn=name)
    version = 0
    already_in_store = any(lmp.lmp_id == func.__ell_hash__ for lmp in lmps)

    if not already_in_store:
        commit = None
        if lmps:
            latest_lmp = max(lmps, key=lambda x: x.created_at)
            version = latest_lmp.version_number + 1
            if config.autocommit:
                # XXX: Move this out to autocommit itself.
                if not _autocommit_warning():
                    from ell.util.differ import write_commit_message_for_diff
                    commit = str(write_commit_message_for_diff(
                    f"{latest_lmp.dependencies}\n\n{latest_lmp.source}", 
                        f"{fn_closure[1]}\n\n{fn_closure[0]}")[0])

        serialized_lmp = SerializedLMP(
            lmp_id=func.__ell_hash__,
            name=name,
            created_at=utc_now(),
            source=fn_closure[0],
            dependencies=fn_closure[1],
            commit_message=commit,
            initial_global_vars=get_immutable_vars(fn_closure[2]),
            initial_free_vars=get_immutable_vars(fn_closure[3]),
            lmp_type=lmp_type,
            api_params=api_params if api_params else None,
            version_number=version,
        )
        config.store.write_lmp(serialized_lmp, [f.__ell_hash__ for f in func.__ell_uses__])
    func._has_serialized_lmp = True

--------------------------------------------------------------------------------

lmp\_track.py::4
def _write_invocation(func, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                     state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id):

    invocation_contents = InvocationContents(
        invocation_id=invocation_id,
        params=cleaned_invocation_params,
        results=result,
        invocation_api_params=invocation_api_params,
        global_vars=get_immutable_vars(func.__ell_closure__[2]),
        free_vars=get_immutable_vars(func.__ell_closure__[3])
    )

    if invocation_contents.should_externalize and config.store.has_blob_storage:
        invocation_contents.is_external = True

        # Write to the blob store 
        blob_id = config.store.blob_store.store_blob(
            json.dumps(invocation_contents.model_dump(
            ), default=str, ensure_ascii=False).encode('utf-8'),
            invocation_id
        )
        invocation_contents = InvocationContents(
            invocation_id=invocation_id,
            is_external=True,
        )

    invocation = Invocation(
        id=invocation_id,
        lmp_id=func.__ell_hash__,
        created_at=utc_now(),
        latency_ms=latency_ms,
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        state_cache_key=state_cache_key,
        used_by_id=parent_invocation_id,
        contents=invocation_contents
    )

    config.store.write_invocation(invocation, consumes)

--------------------------------------------------------------------------------

util\closure.py::1
"""
This should do the following.
# prompt_consts.py
import math
def test():
    return math.sin(10)

# lol3.py
import prompt_consts

X = 7
def xD():
    print(X)
    return prompt_consts.test()

###
Our goal is to use AST & dill to get a full lexical closured source of xD, with the exception of modules that are stored in site-packages. For example.

lexical_extration(xD) returns
#closure.py
import math
def test():
    return math.sin(10)

X = 7 
def xD():
    print(X)
    return test()

"""
import collections
import ast
import hashlib
import itertools
from typing import Any, Dict, Iterable, Optional, Set, Tuple, Callable
import dill
import inspect
import types
from dill.source import getsource
import re
from collections import deque
import black

from ell.util.serialization import is_immutable_variable
from ell.util.should_import import should_import

DELIM = "$$$$$$$$$$$$$$$$$$$$$$$$$"
FORBIDDEN_NAMES = ["ell", "lstr"]

--------------------------------------------------------------------------------

util\closure.py::2
def lexical_closure(
    func: Any,
    already_closed: Set[int] = None,
    initial_call: bool = False,
    recursion_stack: list = None,
    forced_dependencies: Optional[Dict[str, Any]] = None
) -> Tuple[str, Tuple[str, str], Set[str]]:
    """
    Generate a lexical closure for a given function or callable.

    Args:
        func: The function or callable to process.
        already_closed: Set of already processed function hashes.
        initial_call: Whether this is the initial call to the function.
        recursion_stack: Stack to keep track of the recursion path.

    Returns:
        A tuple containing:
        - The full source code of the closure
        - A tuple of (function source, dependencies source)
        - A set of function hashes that this closure uses
    """
    already_closed = already_closed or set()
    uses = set()
    forced_dependencies = forced_dependencies or {}
    recursion_stack = recursion_stack or []

    if hash(func) in already_closed:
        return "", ("", ""), set()

    recursion_stack.append(getattr(func, '__qualname__', str(func)))

    outer_ell_func = func
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__

    source = getsource(func, lstrip=True)
    already_closed.add(hash(func))

    globals_and_frees = _get_globals_and_frees(func)
    dependencies, imports, modules = _process_dependencies(func, globals_and_frees, already_closed, recursion_stack, uses)
    for k,v in forced_dependencies.items():
        # Todo: dictionary not necessary
        _process_signature_dependency(v, dependencies, already_closed, recursion_stack, uses, k)

    cur_src = _build_initial_source(imports, dependencies, source)

    module_src = _process_modules(modules, cur_src, already_closed, recursion_stack, uses)

    dirty_src = _build_final_source(imports, module_src, dependencies, source)
    dirty_src_without_func = _build_final_source(imports, module_src, dependencies, "")

    CLOSURE_SOURCE[hash(func)] = dirty_src

    dsrc = _clean_src(dirty_src_without_func)

    # Format the sorce and dsrc soruce using Black
    source = _format_source(source)
    dsrc = _format_source(dsrc)

    fn_hash = _generate_function_hash(source, dsrc, func.__qualname__)

    _update_ell_func(outer_ell_func, source, dsrc, globals_and_frees['globals'], globals_and_frees['frees'], fn_hash, uses)

    return (dirty_src, (source, dsrc), ({outer_ell_func} if not initial_call and hasattr(outer_ell_func, "__ell_func__") else uses))

--------------------------------------------------------------------------------

util\closure.py::3
def _format_source(source: str) -> str:
    """Format the source code using Black."""
    try:
        return black.format_str(source, mode=black.Mode())
    except:
        # If Black formatting fails, return the original source
        return source

def _get_globals_and_frees(func: Callable) -> Dict[str, Dict]:
    """Get global and free variables for a function."""
    globals_dict = collections.OrderedDict(globalvars(func))
    frees_dict = collections.OrderedDict(dill.detect.freevars(func))

    if isinstance(func, type):
        for name, method in collections.OrderedDict(func.__dict__).items():
            if isinstance(method, (types.FunctionType, types.MethodType)):
                globals_dict.update(collections.OrderedDict(dill.detect.globalvars(method)))
                frees_dict.update(collections.OrderedDict(dill.detect.freevars(method)))

    return {'globals': globals_dict, 'frees': frees_dict}

--------------------------------------------------------------------------------

util\closure.py::4
def _process_dependencies(func, globals_and_frees, already_closed, recursion_stack, uses):
    """Process function dependencies."""
    dependencies = []
    modules = deque()
    imports = []

    if isinstance(func, (types.FunctionType, types.MethodType)):
        _process_default_kwargs(func, dependencies, already_closed, recursion_stack, uses)

    for var_name, var_value in itertools.chain(globals_and_frees['globals'].items(), globals_and_frees['frees'].items()):
        _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack, uses)

    return dependencies, imports, modules

--------------------------------------------------------------------------------

util\closure.py::5
def _process_default_kwargs(func, dependencies, already_closed, recursion_stack, uses):
    """Process default keyword arguments and annotations of a function."""
    ps = inspect.signature(func).parameters
    for name, param in ps.items():
        if param.default is not inspect.Parameter.empty:
            _process_signature_dependency(param.default, dependencies, already_closed, recursion_stack, uses, name)
        if param.annotation is not inspect.Parameter.empty:
            _process_signature_dependency(param.annotation, dependencies, already_closed, recursion_stack, uses, f"{name}_annotation")
    if func.__annotations__.get('return') is not None:
        _process_signature_dependency(func.__annotations__['return'], dependencies, already_closed, recursion_stack, uses, "return_annotation")
    # XXX: In order to properly analyze this we should walk the AST rather than inspexting the signature; e.g. Field is FieldInfo not Field.
    # I don't care about the actual default at time of execution just the symbols required to statically reproduce the prompt.


--------------------------------------------------------------------------------

util\closure.py::6
def _process_signature_dependency(val, dependencies, already_closed, recursion_stack, uses, name: Optional[str] = None):
    # Todo: Build general cattr like utility for unstructuring python objects with hooks that keep track of state variables.
    # Todo: break up closure into types and functions.
    # XXX: This is not exhaustive, we should determine should import on all dependencies

    if name not in FORBIDDEN_NAMES:
        try:
            dep = None
            _uses = None
            if isinstance(val, (types.FunctionType, types.MethodType)):
                dep, _, _uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            elif isinstance(val, (list, tuple, set)):
                for item in val:
                    _process_signature_dependency(item, dependencies, already_closed, recursion_stack, uses)
            else:
                val_class = val if isinstance(val, type) else val.__class__
                try:
                    is_builtin = (val_class.__module__ == "builtins" or val_class.__module__ == "__builtins__")
                except:
                    is_builtin = False

                if not is_builtin:
                    if should_import(val_class.__module__):
                        dependencies.append(dill.source.getimport(val_class, alias=val_class.__name__))
                    else:
                        dep, _, _uses = lexical_closure(val_class, already_closed=already_closed, recursion_stack=recursion_stack.copy())

            if dep: dependencies.append(dep)
            if _uses: uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of parameter or annotation {name}", e, recursion_stack)

--------------------------------------------------------------------------------

util\closure.py::7
def _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack , uses):
    """Process a single variable."""
    try:
        name = inspect.getmodule(var_value).__name__
        if should_import(name):
            imports.append(dill.source.getimport(var_value, alias=var_name))
            return
    except:
        pass

    if isinstance(var_value, (types.FunctionType, type, types.MethodType)):
        _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses)
    elif isinstance(var_value, types.ModuleType):
        _process_module(var_name, var_value, modules, imports, uses)
    elif isinstance(var_value, types.BuiltinFunctionType):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        _process_other_variable(var_name, var_value, dependencies, uses)

--------------------------------------------------------------------------------

util\closure.py::8
def _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses):
    """Process a callable (function, method, or class)."""
    try:
        module_is_ell = 'ell' in inspect.getmodule(var_value).__name__
    except:
        module_is_ell = False

    if var_name not in FORBIDDEN_NAMES and not module_is_ell:
        try:
            dep, _, _uses = lexical_closure(var_value, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            dependencies.append(dep)
            uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of global or free variable {var_name}", e, recursion_stack)

--------------------------------------------------------------------------------

util\closure.py::9
def _process_module(var_name, var_value, modules, imports, uses):
    """Process a module."""
    if should_import(var_value.__name__):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        modules.append((var_name, var_value))

def _process_other_variable(var_name, var_value, dependencies, uses):
    """Process variables that are not callables or modules."""
    if isinstance(var_value, str) and '\n' in var_value:
        dependencies.append(f"{var_name} = '''{var_value}'''")
    elif is_immutable_variable(var_value):
        dependencies.append(f"#<BV>\n{var_name} = {repr(var_value)}\n#</BV>")
    else:
        dependencies.append(f"#<BmV>\n{var_name} = <{type(var_value).__name__} object>\n#</BmV>")

--------------------------------------------------------------------------------



Message and Tool Handling:
types\message.py::1
# todo: implement tracing for structured outs. this a v2 feature.
import json
from ell.types._lstr import _lstr
from functools import cached_property
import numpy as np
import base64
from io import BytesIO
from PIL import Image as PILImage

from pydantic import BaseModel, ConfigDict, model_validator, field_serializer
from sqlmodel import Field

from concurrent.futures import ThreadPoolExecutor, as_completed

from typing import Any, Callable, Dict, List, Optional, Union

from ell.util.serialization import serialize_image
_lstr_generic = Union[_lstr, str]
InvocableTool = Callable[..., Union["ToolResult", _lstr_generic, List["ContentBlock"], ]]

# AnyContent represents any type that can be passed to Message.
AnyContent = Union["ContentBlock", str, "ToolCall", "ToolResult", "ImageContent", np.ndarray, PILImage.Image, BaseModel]

--------------------------------------------------------------------------------

types\message.py::2
class ToolResult(BaseModel):
    tool_call_id: _lstr_generic
    result: List["ContentBlock"]

    @property
    def text(self) -> str:
        return _content_to_text(self.result)

    @property
    def text_only(self) -> str:
        return _content_to_text_only(self.result)

    # # XXX: Possibly deprecate
    # def readable_repr(self) -> str:
    #     return f"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"

    def __repr__(self):
        return f"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"

--------------------------------------------------------------------------------

types\message.py::3
class ToolCall(BaseModel):
    tool : InvocableTool
    tool_call_id : Optional[_lstr_generic] = Field(default=None)
    params : BaseModel

    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):
        if not isinstance(params, BaseModel):
            params = tool.__ell_params_model__(**params) #convenience.
        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)

    def __call__(self, **kwargs):
        assert not kwargs, "Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall."

        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
        return self.tool(**self.params.model_dump())

    # XXX: Deprecate in 0.1.0
    def call_and_collect_as_message_block(self):
        raise DeprecationWarning("call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.")

    def call_and_collect_as_content_block(self):
        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)
        return ContentBlock(tool_result=res)

    def call_and_collect_as_message(self):
        return Message(role="user", content=[self.call_and_collect_as_message_block()])

    def __repr__(self):
        return f"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id='{self.tool_call_id}')"

--------------------------------------------------------------------------------

types\message.py::4
class ImageContent(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    image: Optional[PILImage.Image] = Field(default=None)
    url: Optional[str] = Field(default=None)
    detail: Optional[str] = Field(default=None)

    @model_validator(mode='after')
    def check_image_or_url(self):
        if self.image is not None and self.url is not None:
            raise ValueError("Both 'image' and 'url' cannot be set simultaneously.")
        if self.image is None and self.url is None:
            raise ValueError("Either 'image' or 'url' must be set.")
        return self

--------------------------------------------------------------------------------

types\message.py::5
class ImageContent(BaseModel):

    @classmethod
    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, "ImageContent"]):
        if isinstance(value, cls):
            return value

        if isinstance(value, str):
            if value.startswith('http://') or value.startswith('https://'):
                return cls(url=value)
            try:
                img_data = base64.b64decode(value)
                img = PILImage.open(BytesIO(img_data))
                if img.mode not in ('L', 'RGB', 'RGBA'):
                    return cls(image=img.convert('RGB'))
            except:
                raise ValueError("Invalid base64 string or URL for image")

        if isinstance(value, np.ndarray):
            if value.ndim == 3 and value.shape[2] in (3, 4):
                mode = 'RGB' if value.shape[2] == 3 else 'RGBA'
                return cls(image=PILImage.fromarray(value, mode=mode))
            else:
                raise ValueError(f"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.")

        if isinstance(value, PILImage.Image):
            if value.mode not in ('L', 'RGB', 'RGBA'):
                value = value.convert('RGB')
            return cls(image=value)

        raise ValueError(f"Invalid image type: {type(value)}")

    @field_serializer('image')
    def serialize_image(self, image: Optional[PILImage.Image], _info):
        if image is None:
            return None
        return serialize_image(image)

--------------------------------------------------------------------------------

types\message.py::6
class ContentBlock(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    text: Optional[_lstr_generic] = Field(default=None)
    image: Optional[ImageContent] = Field(default=None)
    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)
    tool_call: Optional[ToolCall] = Field(default=None)
    parsed: Optional[BaseModel] = Field(default=None)
    tool_result: Optional[ToolResult] = Field(default=None)
    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.
    # This breaks us maintaing parity with the openai python client in some sen but so does image.

    def __init__(self, *args, **kwargs):
        if "image" in kwargs and not isinstance(kwargs["image"], ImageContent):
            im = kwargs["image"] = ImageContent.coerce(kwargs["image"])
            # XXX: Backwards compatibility, Deprecate.
            if (d := kwargs.get("image_detail", None)): im.detail = d

        super().__init__(*args, **kwargs)


    @model_validator(mode='after')
    def check_single_non_null(self):
        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]
        if len(non_null_fields) > 1:
            raise ValueError(f"Only one field can be non-null. Found: {', '.join(non_null_fields)}")
        return self

    def __str__(self):
        return repr(self)

    def __repr__(self):
        non_null_fields = [f"{field}={value}" for field, value in self.__dict__.items() if value is not None]
        return f"ContentBlock({', '.join(non_null_fields)})"

    @property
    def type(self):
        if self.text is not None:
            return "text"
        if self.image is not None:
            return "image"
        if self.audio is not None:
            return "audio"
        if self.tool_call is not None:
            return "tool_call"
        if self.parsed is not None:
            return "parsed"
        if self.tool_result is not None:
            return "tool_result"
        return None

    @property
    def content(self):
        return getattr(self, self.type)

--------------------------------------------------------------------------------

types\message.py::7
class ContentBlock(BaseModel):

    @classmethod
    def coerce(cls, content: AnyContent) -> "ContentBlock":
        """
        Coerce various types of content into a ContentBlock.

        This method provides a flexible way to create ContentBlock instances from different types of input.

        Args:
        content: The content to be coerced into a ContentBlock. Can be one of the following types:
        - str: Will be converted to a text ContentBlock.
        - ToolCall: Will be converted to a tool_call ContentBlock.
        - ToolResult: Will be converted to a tool_result ContentBlock.
        - BaseModel: Will be converted to a parsed ContentBlock.
        - ContentBlock: Will be returned as-is.
        - Image: Will be converted to an image ContentBlock.
        - np.ndarray: Will be converted to an image ContentBlock.
        - PILImage.Image: Will be converted to an image ContentBlock.

        Returns:
        ContentBlock: A new ContentBlock instance containing the coerced content.

        Raises:
        ValueError: If the content cannot be coerced into a valid ContentBlock.

        Examples:
        >>> ContentBlock.coerce("Hello, world!")
        ContentBlock(text="Hello, world!")

        >>> tool_call = ToolCall(...)
        >>> ContentBlock.coerce(tool_call)
        ContentBlock(tool_call=tool_call)

        >>> tool_result = ToolResult(...)
        >>> ContentBlock.coerce(tool_result)
        ContentBlock(tool_result=tool_result)

        >>> class MyModel(BaseModel):
        ...     field: str
        >>> model_instance = MyModel(field="value")
        >>> ContentBlock.coerce(model_instance)
        ContentBlock(parsed=model_instance)

        >>> from PIL import Image as PILImage
        >>> img = PILImage.new('RGB', (100, 100))
        >>> ContentBlock.coerce(img)
        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))

        >>> import numpy as np
        >>> arr = np.random.rand(100, 100, 3)
        >>> ContentBlock.coerce(arr)
        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))

        >>> image = Image(url="https://example.com/image.jpg")
        >>> ContentBlock.coerce(image)
        ContentBlock(image=ImageContent(url="https://example.com/image.jpg"))

        Notes:
        - This method is particularly useful when working with heterogeneous content types
          and you want to ensure they are all properly encapsulated in ContentBlock instances.
        - The method performs type checking and appropriate conversions to ensure the resulting
          ContentBlock is valid according to the model's constraints.
        - For image content, Image objects, PIL Image objects, and numpy arrays are supported,
          with automatic conversion to the appropriate format.
        - As a last resort, the method will attempt to create an image from the input before
          raising a ValueError.
        """
        if isinstance(content, ContentBlock):
            return content
        if isinstance(content, str):
            return cls(text=content)
        if isinstance(content, ToolCall):
            return cls(tool_call=content)
        if isinstance(content, ToolResult):
            return cls(tool_result=content)
        if isinstance(content, (ImageContent, np.ndarray, PILImage.Image)):
            return cls(image=ImageContent.coerce(content))
        if isinstance(content, BaseModel):
            return cls(parsed=content)

        raise ValueError(f"Invalid content type: {type(content)}")

    @field_serializer('parsed')
    def serialize_parsed(self, value: Optional[BaseModel], _info):
        if value is None:
            return None
        return value.model_dump(exclude_none=True, exclude_unset=True)

--------------------------------------------------------------------------------

types\message.py::8
def to_content_blocks(
    content: Optional[Union[AnyContent, List[AnyContent]]] = None,
    **content_block_kwargs
) -> List[ContentBlock]:
    """
    Coerce a variety of input types into a list of ContentBlock objects.

    Args:
    content: The content to be coerced. Can be a single item or a list of items.
             Supported types include str, ContentBlock, ToolCall, ToolResult, BaseModel, Image, np.ndarray, and PILImage.Image.
    **content_block_kwargs: Additional keyword arguments to pass to ContentBlock creation if content is None.

    Returns:
    List[ContentBlock]: A list of ContentBlock objects created from the input content.

    Examples:
    >>> coerce_content_list("Hello")
    [ContentBlock(text="Hello")]

    >>> coerce_content_list([ContentBlock(text="Hello"), "World"])
    [ContentBlock(text="Hello"), ContentBlock(text="World")]

    >>> from PIL import Image as PILImage
    >>> pil_image = PILImage.new('RGB', (100, 100))
    >>> coerce_content_list(pil_image)
    [ContentBlock(image=Image(image=<PIL.Image.Image object>))]

    >>> coerce_content_list(Image(url="https://example.com/image.jpg"))
    [ContentBlock(image=Image(url="https://example.com/image.jpg"))]

    >>> coerce_content_list(None, text="Default text")
    [ContentBlock(text="Default text")]
    """
    if content is None:
        return [ContentBlock(**content_block_kwargs)]

    if not isinstance(content, list):
        content = [content]

    return [ContentBlock.model_validate(ContentBlock.coerce(c)) for c in content]

--------------------------------------------------------------------------------

types\message.py::9
class Message(BaseModel):
    role: str
    content: List[ContentBlock]


    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):
        content_blocks = to_content_blocks(content, **content_block_kwargs)

        super().__init__(role=role, content=content_blocks)

    # XXX: This choice of naming is unfortunate, but it is what it is.
    @property
    def text(self) -> str:
        """Returns all text content, replacing non-text content with their representations.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text
            'Hello\\n<PilImage>\\nWorld'
        """
        return _content_to_text(self.content)

--------------------------------------------------------------------------------

types\message.py::10
class Message(BaseModel):

    @property
    def images(self) -> List[ImageContent]:
        """Returns a list of all image content.

        Example:
            >>> from PIL import Image as PILImage
            >>> image1 = Image(url="https://example.com/image.jpg")
            >>> image2 = Image(image=PILImage.new('RGB', (200, 200)))
            >>> message = Message(role="user", content=["Text", image1, "More text", image2])
            >>> len(message.images)
            2
            >>> isinstance(message.images[0], Image)
            True
            >>> message.images[0].url
            'https://example.com/image.jpg'
            >>> isinstance(message.images[1].image, PILImage.Image)
            True
        """
        return [c.image for c in self.content if c.image]

--------------------------------------------------------------------------------

types\message.py::11
class Message(BaseModel):

    @property
    def audios(self) -> List[Union[np.ndarray, List[float]]]:
        """Returns a list of all audio content.

        Example:
            >>> audio1 = np.array([0.1, 0.2, 0.3])
            >>> audio2 = np.array([0.4, 0.5, 0.6])
            >>> message = Message(role="user", content=["Text", audio1, "More text", audio2])
            >>> len(message.audios)
            2
        """
        return [c.audio for c in self.content if c.audio]

--------------------------------------------------------------------------------

types\message.py::12
class Message(BaseModel):

    @property
    def text_only(self) -> str:
        """Returns only the text content, ignoring non-text content.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text_only
            'Hello\\nWorld'
        """
        return _content_to_text_only(self.content)

    @cached_property
    def tool_calls(self) -> List[ToolCall]:
        """Returns a list of all tool calls.

        Example:
            >>> tool_call = ToolCall(tool=lambda x: x, params=BaseModel())
            >>> message = Message(role="user", content=["Text", tool_call])
            >>> len(message.tool_calls)
            1
        """
        return [c.tool_call for c in self.content if c.tool_call is not None]

    @property
    def tool_results(self) -> List[ToolResult]:
        """Returns a list of all tool results.

        Example:
            >>> tool_result = ToolResult(tool_call_id="123", result=[ContentBlock(text="Result")])
            >>> message = Message(role="user", content=["Text", tool_result])
            >>> len(message.tool_results)
            1
        """
        return [c.tool_result for c in self.content if c.tool_result is not None]

--------------------------------------------------------------------------------

types\message.py::13
class Message(BaseModel):

    @property
    def parsed(self) -> Union[BaseModel, List[BaseModel]]:
        """Returns a list of all parsed content.

        Example:
            >>> class CustomModel(BaseModel):
            ...     value: int
            >>> parsed_content = CustomModel(value=42)
            >>> message = Message(role="user", content=["Text", ContentBlock(parsed=parsed_content)])
            >>> len(message.parsed)
            1
        """
        parsed_content = [c.parsed for c in self.content if c.parsed is not None]
        return parsed_content[0] if len(parsed_content) == 1 else parsed_content

--------------------------------------------------------------------------------

types\message.py::14
class Message(BaseModel):

    def call_tools_and_collect_as_message(self, parallel=False, max_workers=None):
        if parallel:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(c.tool_call.call_and_collect_as_content_block) for c in self.content if c.tool_call]
                content = [future.result() for future in as_completed(futures)]
        else:
            content = [c.tool_call.call_and_collect_as_content_block() for c in self.content if c.tool_call]
        return Message(role="user", content=content)

--------------------------------------------------------------------------------

lmp\tool.py::1
from functools import wraps
import json
from typing import Any, Callable, Optional

from pydantic import Field, create_model
from pydantic.fields import FieldInfo
from ell.lmp._track import _track
# from ell.types import ToolFunction, InvocableTool, ToolParams
# from ell.util.verbosity import compute_color, tool_usage_logger_pre
from ell.configurator import config
from ell.types._lstr import _lstr
from ell.types.studio import LMPType
import inspect

from ell.types.message import ContentBlock, InvocableTool, ToolResult, to_content_blocks

--------------------------------------------------------------------------------

lmp\tool.py::2
def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        _under_fn = fn

        @wraps(fn)
        def wrapper(
            *fn_args,
            _invocation_origin: str = None,
            _tool_call_id: str = None,
            **fn_kwargs
        ):
            #XXX: Post release, we need to wrap all tool arguments in type primitives for tracking I guess or change that tool makes the tool function inoperable.
            #XXX: Most people are not going to manually try and call the tool without a type primitive and if they do it will most likely be wrapped with l strs.

            if config.verbose and not exempt_from_tracking:
                pass
                # tool_usage_logger_pre(fn, fn_args, fn_kwargs, name, color)

            result = fn(*fn_args, **fn_kwargs)

            _invocation_api_params = dict(tool_kwargs=tool_kwargs)

            # Here you might want to add logic for tracking the tool usage
            # Similar to how it's done in the lm decorator # Use _invocation_origin

            if isinstance(result, str) and _invocation_origin:
                result = _lstr(result,origin_trace=_invocation_origin)

            #XXX: This _tool_call_id thing is a hack. Tracking should happen via params in the api
            # So if you call wiuth a _tool_callId
            if _tool_call_id:
                # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
                try:
                    if isinstance(result, ContentBlock):
                        content_results = [result]
                    elif isinstance(result, list) and all(isinstance(c, ContentBlock) for c in result):
                        content_results = result
                    else:
                        content_results = [ContentBlock(text=_lstr(json.dumps(result, ensure_ascii=False),origin_trace=_invocation_origin))]
                except TypeError as e:
                    raise TypeError(f"Failed to convert tool use result to ContentBlock: {e}. Tools must return json serializable objects. or a list of ContentBlocks.")
                # XXX: Need to support images and other content types somehow. We should look for images inside of the the result and then go from there.
                # try:
                #     content_results = coerce_content_list(result)
                # except ValueError as e:

                # TODO: poolymorphic validation here is important (cant have tool_call or formatted_response in the result)
                # XXX: Should we put this coercion here or in the tool call/result area.
                for c in content_results:
                    assert not c.tool_call, "Tool call in tool result"
                    # assert not c.formatted_response, "Formatted response in tool result"
                    if c.parsed:
                        # Warning: Formatted response in tool result will be converted to text
                        # TODO: Logging needs to produce not print.
                        print(f"Warning: Formatted response in tool result will be converted to text. Original: {c.parsed}")
                        c.text = _lstr(c.parsed.model_dump_json(),origin_trace=_invocation_origin)
                        c.parsed = None
                    assert not c.audio, "Audio in tool result"
                return ToolResult(tool_call_id=_tool_call_id, result=content_results), _invocation_api_params, {}
            else:
                return result, _invocation_api_params, {}
        # ... other code
    # ... other code

--------------------------------------------------------------------------------

lmp\tool.py::3
def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        # ... other code


        wrapper.__ell_tool_kwargs__ = tool_kwargs
        wrapper.__ell_func__ = _under_fn
        wrapper.__ell_type__ = LMPType.TOOL
        wrapper.__ell_exempt_from_tracking = exempt_from_tracking

        # Construct the pydantic mdoel for the _under_fn's function signature parameters.
        # 1. Get the function signature.

        sig = inspect.signature(fn)

        # 2. Create a dictionary of field definitions for the Pydantic model
        fields = {}
        for param_name, param in sig.parameters.items():
            # Skip *args and **kwargs
            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                continue

            # Determine the type annotation
            if param.annotation == inspect.Parameter.empty:
                raise ValueError(f"Parameter {param_name} has no type annotation, and cannot be converted into a tool schema for OpenAI and other provisders. Should OpenAI produce a string or an integer, etc, for this parameter?")
            annotation = param.annotation

            # Determine the default value
            default = param.default

            # Check if the parameter has a Field with description
            if isinstance(param.default, FieldInfo):
                field = param.default
                fields[param_name] = (annotation, field)
            elif param.default != inspect.Parameter.empty:
                fields[param_name] = (annotation, param.default)
            else:
                # If no default value, use Field without default
                fields[param_name] = (annotation, Field(...))

        # 3. Create the Pydantic model
        model_name = f"{fn.__name__}"
        ParamsModel = create_model(model_name, **fields)

        # Attach the Pydantic model to the wrapper function
        wrapper.__ell_params_model__ = ParamsModel

        # handle tracking last.
        if exempt_from_tracking:
            ret = wrapper
        else:
            ret=  _track(wrapper)

        # Helper function to get the Pydantic model for the tool
        def get_params_model():
            return wrapper.__ell_params_model__

        # Attach the helper function to the wrapper
        wrapper.get_params_model = get_params_model
        ret.get_params_model = get_params_model
        return ret

    return tool_decorator

--------------------------------------------------------------------------------

lmp\tool.py::4
tool.__doc__ = """Defines a tool for use in language model programs (LMPs) that support tool use.

This decorator wraps a function, adding metadata and handling for tool invocations.
It automatically extracts the tool's description and parameters from the function's
docstring and type annotations, creating a structured representation for LMs to use.

:param exempt_from_tracking: If True, the tool usage won't be tracked. Default is False.
:type exempt_from_tracking: bool
:param tool_kwargs: Additional keyword arguments for tool configuration.
:return: A wrapped version of the original function, usable as a tool by LMs.
:rtype: Callable

Requirements:

- Function must have fully typed arguments (Pydantic-serializable).
- Return value must be one of: str, JSON-serializable object, Pydantic model, or List[ContentBlock].
- All parameters must have type annotations.
- Complex types should be Pydantic models.
- Function should have a descriptive docstring.
- Can only be used in LMPs with @ell.complex decorators

Functionality:

1. Metadata Extraction:
    - Uses function docstring as tool description.
    - Extracts parameter info from type annotations and docstring.
    - Creates a Pydantic model for parameter validation and schema generation.

2. Integration with LMs:
    - Can be passed to @ell.complex decorators.
    - Provides structured tool information to LMs.

3. Invocation Handling:
    - Manages tracking, logging, and result processing.
    - Wraps results in appropriate types (e.g., _lstr) for tracking.

Usage Modes:

1. Normal Function Call:
    - Behaves like a regular Python function.
    - Example: result = my_tool(arg1="value", arg2=123)

2. LMP Tool Call:
    - Used within LMPs or with explicit _tool_call_id.
    - Returns a ToolResult object.
    - Example: result = my_tool(arg1="value", arg2=123, _tool_call_id="unique_id")

Result Coercion:

- String → ContentBlock(text=result)
- Pydantic BaseModel → ContentBlock(parsed=result)
- List[ContentBlock] → Used as-is
- Other types → ContentBlock(text=json.dumps(result))

Example::

    @ell.tool()
    def create_claim_draft(
        claim_details: str,
        claim_type: str,
        claim_amount: float,
        claim_date: str = Field(description="Date format: YYYY-MM-DD")
    ) -> str:
        '''Create a claim draft. Returns the created claim ID.'''
        return "12345"

    # For use in a complex LMP:
    @ell.complex(model="gpt-4", tools=[create_claim_draft], temperature=0.1)
    def insurance_chatbot(message_history: List[Message]) -> List[Message]:
        # Chatbot implementation...

    x = insurance_chatbot([
        ell.user("I crashed my car into a tree."),
        ell.assistant("I'm sorry to hear that. Can you provide more details?"),
        ell.user("The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000")
    ]) 
    print(x)
    '''ell.Message(content=[
        ContentBlock(tool_call(
            tool_call_id="asdas4e",
            tool_fn=create_claim_draft,
            input=create_claim_draftParams({
                claim_details="The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000",
                claim_type="car",
                claim_amount=5000,
                claim_date="2024-08-01"
            })
        ))
    ], role='assistant')'''
    
    if x.tool_calls:
        next_user_message = response_message.call_tools_and_collect_as_message()
        # This actually calls create_claim_draft
        print(next_user_message)
        '''
        ell.Message(content=[
            ContentBlock(tool_result=ToolResult(
                tool_call_id="asdas4e",
                result=[ContentBlock(text="12345")]
            ))
        ], role='user')
        '''
        y = insurance_chatbot(message_history + [x, next_user_message])
        print(y)
        '''
        ell.Message("I've filed that for you!", role='assistant')
        '''

Note:
- Tools are integrated into LMP calls via the 'tools' parameter in @ell.complex.
- LMs receive structured tool information, enabling understanding and usage within the conversation context.
    """

--------------------------------------------------------------------------------

util\serialization.py::1
# Global converter
import base64
import hashlib
from io import BytesIO
import json
import cattrs
import numpy as np
from pydantic import BaseModel
import PIL
from ell.types._lstr import _lstr


pydantic_ltype_aware_cattr = cattrs.Converter()

def serialize_image(img):
    buffer = BytesIO()
    img.save(buffer, format="PNG")
    return "data:image/png;base64," + base64.b64encode(buffer.getvalue()).decode()


# Register hooks for complex types
pydantic_ltype_aware_cattr.register_unstructure_hook(
    np.ndarray,
    lambda arr: {
        "content": serialize_image(PIL.Image.fromarray(arr)),
        "__limage": True
    } if arr.ndim == 3 else (
        {
            "content": base64.b64encode(arr.tobytes()).decode(),
            "dtype": str(arr.dtype),
            "shape": arr.shape,
            "__lndarray": True
        }
    )
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    set,
    lambda s: list(sorted(s))
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    frozenset,
    lambda s: list(sorted(s))
)


pydantic_ltype_aware_cattr.register_unstructure_hook(
    PIL.Image.Image,
    lambda obj: {
        "content": serialize_image(obj),
        "__limage": True
    }
)

def unstructure_lstr(obj):
    return dict(content=str(obj), **obj.__dict__, __lstr=True)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    _lstr,
    unstructure_lstr
)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    BaseModel,
    lambda obj: obj.model_dump(exclude_none=True, exclude_unset=True)
)

--------------------------------------------------------------------------------

util\serialization.py::2
def get_immutable_vars(vars_dict):
    converter = cattrs.Converter()

    def handle_complex_types(obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj]
        elif isinstance(obj, dict):
            return {k: handle_complex_types(v) if not isinstance(v, (int, float, str, bool, type(None))) else v for k, v in obj.items()}
        elif isinstance(obj, (set, frozenset)):
            return list(sorted(handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj))
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return f"<Object of type {type(obj).__name__}>"

    converter.register_unstructure_hook(object, handle_complex_types)
    x = converter.unstructure(vars_dict)
    return x

--------------------------------------------------------------------------------



Utilities and Closure Handling:
util\closure.py::10
def _build_initial_source(imports, dependencies, source):
    """Build the initial source code."""
    return f"{DELIM}\n" + f"\n{DELIM}\n".join(imports + dependencies + [source]) + f"\n{DELIM}\n"

def _process_modules(modules, cur_src, already_closed, recursion_stack, uses):
    """Process module dependencies."""
    reverse_module_src = deque()
    while modules:
        mname, mval = modules.popleft()
        mdeps = []
        attrs_to_extract = get_referenced_names(cur_src.replace(DELIM, ""), mname)
        for attr in attrs_to_extract:
            _process_module_attribute(mname, mval, attr, mdeps, modules, already_closed, recursion_stack, uses)

        mdeps.insert(0, f"# Extracted from module: {mname}")
        reverse_module_src.appendleft("\n".join(mdeps))

        cur_src = _dereference_module_names(cur_src, mname, attrs_to_extract)

    return list(reverse_module_src)

--------------------------------------------------------------------------------

util\closure.py::11
def _process_module_attribute(mname, mval, attr, mdeps, modules, already_closed, recursion_stack, uses):
    """Process a single attribute of a module."""
    val = getattr(mval, attr)
    if isinstance(val, (types.FunctionType, type, types.MethodType)):
        try:
            dep, _, dep_uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            mdeps.append(dep)
            uses.update(dep_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of {mname}.{attr}", e, recursion_stack)
    elif isinstance(val, types.ModuleType):
        modules.append((attr, val))
    else:
        mdeps.append(f"{attr} = {repr(val)}")

--------------------------------------------------------------------------------

util\closure.py::12
def _dereference_module_names(cur_src, mname, attrs_to_extract):
    """Dereference module names in the source code."""
    for attr in attrs_to_extract:
        cur_src = cur_src.replace(f"{mname}.{attr}", attr)
    return cur_src

def _build_final_source(imports, module_src, dependencies, source):
    """Build the final source code."""
    seperated_dependencies = sorted(imports) + sorted(module_src) + sorted(dependencies) + ([source] if source else [])
    seperated_dependencies = list(dict.fromkeys(seperated_dependencies))
    return DELIM + "\n" + f"\n{DELIM}\n".join(seperated_dependencies) + "\n" + DELIM + "\n"

def _generate_function_hash(source, dsrc, qualname):
    """Generate a hash for the function."""
    return "lmp-" + hashlib.md5("\n".join((source, dsrc, qualname)).encode()).hexdigest()

--------------------------------------------------------------------------------

util\closure.py::13
def _update_ell_func(outer_ell_func, source, dsrc, globals_dict, frees_dict, fn_hash, uses):
    """Update the ell function attributes."""
    formatted_source = _format_source(source)
    formatted_dsrc = _format_source(dsrc)

    if hasattr(outer_ell_func, "__ell_func__"):

        outer_ell_func.__ell_closure__ = (formatted_source, formatted_dsrc, globals_dict, frees_dict)
        outer_ell_func.__ell_hash__ = fn_hash
        outer_ell_func.__ell_uses__ = uses

def _raise_error(message, exception, recursion_stack):
    """Raise an error with detailed information."""
    error_msg = f"{message}. Error: {str(exception)}\n"
    error_msg += f"Recursion stack: {' -> '.join(recursion_stack)}"
    # print(error_msg)
    raise Exception(error_msg)

--------------------------------------------------------------------------------

util\closure.py::14
def get_referenced_names(code: str, module_name: str):
    """
    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)
    and walks through the tree to find all instances where an attribute of the module is referenced in the code.

    Parameters:
    code (str): The block of code to be parsed.
    module_name (str): The name of the module to look for in the code.

    Returns:
    list: A list of all attributes of the module that are referenced in the code.
    """
    # Remove content between #<BV> and #</BV> tags
    code = re.sub(r'#<BV>\n.*?\n#</BV>', '', code, flags=re.DOTALL)

    # Remove content between #<BmV> and #</BmV> tags
    code = re.sub(r'#<BmV>\n.*?\n#</BmV>', '', code, flags=re.DOTALL)

    tree = ast.parse(code)
    referenced_names = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Attribute):
            if isinstance(node.value, ast.Name) and node.value.id == module_name:
                referenced_names.append(node.attr)

    return referenced_names

CLOSURE_SOURCE: Dict[str, str] = {}

--------------------------------------------------------------------------------

util\closure.py::15
def lexically_closured_source(func, forced_dependencies: Optional[Dict[str, Any]] = None):
    """
    Generate a lexically closured source for a given function.

    This function takes a callable object (function, method, or class) and generates
    a lexically closured source code. It captures all the dependencies, including
    global variables, free variables, and nested functions, to create a self-contained
    version of the function that can be executed independently.

    Args:
        func (Callable): The function or callable object to process.
        forced_dependencies (Optional[Dict[str, Any]]): A dictionary of additional
            dependencies to include in the closure. Keys are variable names, and
            values are the corresponding objects.

    Returns:
        Tuple[str, Set[Any]]: A tuple containing two elements:
            1. The lexically closured source code as a string.
            2. A set of function objects that this closure uses.

    Raises:
        ValueError: If the input is not a callable object.

    Example:
        def outer(x):
            y = 10
            def inner():
                return x + y
            return inner

        closured_source, uses = lexically_closured_source(outer)
        print(closured_source)
        # Output will include the source for both outer and inner functions,
        # along with any necessary imports and variable definitions.

    Note:
        This function relies on the `lexical_closure` function to perform the
        actual closure generation. It also uses the `__ell_closure__` attribute
        of the function, which is expected to be set by the `lexical_closure` function.
    """
    if not callable(func):
        raise ValueError("Input must be a callable object (function, method, or class).")
    _, fnclosure, uses = lexical_closure(func, initial_call=True, recursion_stack=[], forced_dependencies=forced_dependencies)
    return func.__ell_closure__, uses

--------------------------------------------------------------------------------

util\closure.py::16
import ast

def _clean_src(dirty_src):
    # Now remove all duplicates and preserve order
    split_by_setion = filter(lambda x: len(x.strip()) > 0, dirty_src.split(DELIM))

    # Now we need to remove all the duplicates
    split_by_setion = list(dict.fromkeys(split_by_setion))

    # Now we need to concat all together
    all_imports = []
    final_src = "\n".join(split_by_setion)
    out_final_src = final_src[:]
    for line in final_src.split("\n"):
        if line.startswith("import") or line.startswith("from"):
            all_imports.append(line)
            out_final_src = out_final_src.replace(line, "")

    all_imports = "\n".join(sorted(all_imports))
    final_src = all_imports + "\n" + out_final_src

    # now replace all "\n\n\n" or longer with "\n\n"
    final_src = re.sub(r"\n{3,}", "\n\n", final_src)

    return final_src

--------------------------------------------------------------------------------

util\closure.py::17
def is_function_called(func_name, source_code):
    """
    Check if a function is called in the given source code.

    Parameters:
    func_name (str): The name of the function to check.
    source_code (str): The source code to check.

    Returns:
    bool: True if the function is called, False otherwise.
    """
    # Parse the source code into an AST
    tree = ast.parse(source_code)

    # Walk through all the nodes in the AST
    for node in ast.walk(tree):
        # If the node is a function call
        if isinstance(node, ast.Call):
            # If the function being called is the function we're looking for
            if isinstance(node.func, ast.Name) and node.func.id == func_name:
                return True

    # If we've gone through all the nodes and haven't found a call to the function, it's not called
    return False

#!/usr/bin/env python
#
from dill.detect import nestedglobals
import inspect

--------------------------------------------------------------------------------

util\differ.py::1
from ell.configurator import config
from ell.lmp.simple import simple
import difflib

# Todo: update this for single change stuff so that it doesn't summarize small chage but says it specifically.
@simple(config.autocommit_model, temperature=0.2, exempt_from_tracking=True, max_tokens=500)
def write_commit_message_for_diff(old : str, new : str) -> str:
    """You are an expert programmer whose goal is to write commit messages based on diffs.
You will be given two version of source code and their unified diff.
You will be expected to write a commit message that describes the changes between the two versions.

Follow these guidelines:
1. Your commit message should be at most one sentence and highly specific to the changes made. Don't just discuss the functions changed but how they were specifically changed.
2. Your commit message cannot be more than 10 words so use sentence fragments and be concise.
3. The @ell.simple decorator turns a function into a call to a language model program: the function's docstring is the system prompt and the string returned is the user prompt. 
4. It is extremely important that if the system prompt or user prompt changes, your commit message must say what specifically changed, rather than vaguely saying they were updated or changed.
5. It is extremely important that you never refer to a @ell.simple docstring as a docstring: it is a system prompt. 
6. Do NOT say why a change was done, say what specifically changed.
7. Consider all changes ot the program including the globals and free variables

Example response:
'''
Update model temperature and refine system prompt wording:
* Changed temperature from 0.5 to 0.7.
* Updated "with specificity, brevity, and good grammar" to "clearly and concisely" in system prompt.
* The `questions` param was assigned type List[Question]
'''
Response format:
<Short commit message summarizing all the changes with specificity>:
* <Bulleted list of each specific change>.
"""
    clean_program_of_all_bv_tags = lambda program : program.replace("#<BV>", "").replace("#</BV>", "").replace("#<BmV>", "").replace("#</BmV>", "")
    old_clean = clean_program_of_all_bv_tags(old)
    new_clean = clean_program_of_all_bv_tags(new)

    diff = "\n".join(difflib.unified_diff(old_clean.splitlines(), new_clean.splitlines(), lineterm=''))

    return f"""Write a commit message succinctly and specifically describing the changes between these two versions of a program.
Old version:
```
{old_clean}
```

New version:
```
{new_clean}
```

Unified diff:
{diff}
"""

--------------------------------------------------------------------------------

util\differ.py::2
if __name__ == "__main__":

    from ell.configurator import config
    config.verbose = True

    test_version_1 = '''import ell
import numpy as np

@ell.simple(model="gpt-4o-mini")
def come_up_with_a_premise_for_a_joke_about(topic : str):
    """You are an incredibly funny comedian. Come up with a premise for a joke about topic"""
    return f"come up with a premise for a joke about {topic}"

def get_random_length():
    return int(np.random.beta(2, 5) * 300)

@ell.simple(model="gpt-4o-mini")
def joke(topic : str):
    """You are a funny comedian. You respond in scripts for a standup comedy skit."""
    return f"Act out a full joke. Make your script {get_random_length()} words long. Here's the premise: {come_up_with_a_premise_for_a_joke_about(topic)}"'''

    test_version_2 = '''import ell
import numpy as np

@ell.simple(model="gpt-4o-mini")
def come_up_with_a_premise_for_a_joke_about(topic : str):
    """You are an incredibly funny comedian. Come up with a premise for a joke about topic"""
    return f"come up with a premise for a joke about {topic}"

def get_random_length():
    return int(np.random.beta(2, 5) * 300)

@ell.simple(model="gpt-4o-mini")
def joke(topic : str):
    """You are a funny comedian. You respond in scripts for skits."""
    return f"Act out a full joke. Make your script {get_random_length()} words long. Here's the premise: {come_up_with_a_premise_for_a_joke_about(topic)}"'''

    (write_commit_message_for_diff(test_version_1, test_version_2))

--------------------------------------------------------------------------------

util\_warnings.py::1
from typing import Any, Optional
from colorama import Fore, Style

from ell.configurator import config
import logging
logger = logging.getLogger(__name__)


def _no_api_key_warning(model, client_to_use : Optional[Any], name = None,  long=False, error=False):
    color = Fore.RED if error else Fore.LIGHTYELLOW_EX
    prefix = "ERROR" if error else "WARNING"
    # openai default
    client_to_use_name = client_to_use.__class__.__name__ if (client_to_use) else "OpenAI"
    client_to_use_module = client_to_use.__class__.__module__ if (client_to_use) else "openai"
    lmp_name = f"used by LMP `{name}` " if name else ""
    return f"""{color}{prefix}: No API key found for model `{model}` {lmp_name}using client `{client_to_use_name}`""" + (f""".

To fix this:
* Set your API key in the appropriate environment variable for your chosen provider
* Or, specify a client explicitly in the decorator:
    ```
    import ell
    from {client_to_use_module} import {client_to_use_name}
                                
    @ell.simple(model="{model}", client={client_to_use_name}(api_key=your_api_key))
    def your_lmp_name(...):
        ...
    ```
* Or explicitly specify the client when calling the LMP:

    ```
    your_lmp_name(..., client={client_to_use_name}(api_key=your_api_key))
    ```
""" if long else " at time of definition. Can be okay if custom client specified later! https://docs.ell.so/core_concepts/models_and_api_clients.html ") + f"{Style.RESET_ALL}"

--------------------------------------------------------------------------------

util\_warnings.py::2
def _warnings(model, fn, default_client_from_decorator):

        if not default_client_from_decorator:
            # Check to see if the model is registered and warn the user we're gonna defualt to OpenAI.

            if model not in config.registry:
                logger.warning(f"""{Fore.LIGHTYELLOW_EX}WARNING: Model `{model}` is used by LMP `{fn.__name__}` but no client could be found that supports `{model}`. Defaulting to use the OpenAI client `{config.default_client}` for `{model}`. This is likely because you've spelled the model name incorrectly or are using a newer model from a provider added after this ell version was released. 
                            
* If this is a mistake either specify a client explicitly in the decorator:
```python
import ell
ell.simple(model, client=my_client)
def {fn.__name__}(...):
    ...
```
or explicitly specify the client when the calling the LMP:

```python
ell.simple(model, client=my_client)(...)
```
{Style.RESET_ALL}""")
            elif (client_to_use := config.registry[model].default_client) is None or not client_to_use.api_key:
                logger.warning(_no_api_key_warning(model, fn.__name__, client_to_use, long=False))


def _autocommit_warning():
    if (config.get_client_for("gpt-4o-mini")[0] is None):
        logger.warning(f"{Fore.LIGHTYELLOW_EX}WARNING: Autocommit is enabled but no OpenAI client found for autocommit model 'gpt-4o-mini' (set your OpenAI API key). Commit messages will not be written.{Style.RESET_ALL}")
        return True
    return False

--------------------------------------------------------------------------------

util\verbosity.py::1
import sys
import hashlib
import shutil
import textwrap
from typing import Dict, Tuple, List, Any, Optional
from colorama import Fore, Style, init
from ell.types import Message
from ell.configurator import config
import logging
from functools import lru_cache
import threading
from ell.types.message import LMP, ContentBlock, _content_to_text
import requests

from ell.util.plot_ascii import plot_ascii

# Initialize colorama
init(autoreset=True)

# Define colors and styles
ELL_COLORS = {k: v for k, v in vars(Fore).items() if k not in ['RESET', 'BLACK', 'LIGHTBLACK_EX']}
BOLD = Style.BRIGHT
UNDERLINE = '\033[4m'
RESET = Style.RESET_ALL
SYSTEM_COLOR = Fore.CYAN
USER_COLOR = Fore.GREEN
ASSISTANT_COLOR = Fore.YELLOW
PIPE_COLOR = Fore.BLUE

# Set up logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

has_logged_version_statement = False

_version_check_lock = threading.Lock()
_has_logged_version_statement = False

--------------------------------------------------------------------------------

util\verbosity.py::2
def check_version_and_log():
    global _has_logged_version_statement
    if _version_check_lock.acquire(blocking=False):
        try:
            if not _has_logged_version_statement:

                import ell

                try:
                    response = requests.get("https://version.ell.so/ell-ai/pypi", timeout=0.15)
                    if response.status_code == 200:
                        latest_version = response.text.strip()
                        if latest_version != ell.__version__:
                            print(f"{Fore.YELLOW}╔═════════════════════════════════════════════════════════════════╗")
                            print(f"{Fore.YELLOW}║ {Fore.GREEN}A new version of ELL is available: {Fore.CYAN}{latest_version:<29}{Fore.YELLOW}║")
                            print(f"{Fore.YELLOW}║ {Fore.GREEN}You can update by running:{Fore.YELLOW}                                      ║")
                            print(f"{Fore.YELLOW}║ {Fore.CYAN}pip install --upgrade ell-ai{Fore.YELLOW}                                    ║")
                            print(f"{Fore.YELLOW}╚═════════════════════════════════════════════════════════════════╝{Style.RESET_ALL}")
                except requests.RequestException:
                    pass  # Silently handle any network-related errors
                _has_logged_version_statement = True
        finally:
            _version_check_lock.release()

--------------------------------------------------------------------------------

util\verbosity.py::3
@lru_cache(maxsize=128)
def compute_color(invoking_lmp: LMP) -> str:
    """Compute and cache a consistent color for a given LMP."""
    name_hash = hashlib.md5(invoking_lmp.__name__.encode()).hexdigest()
    color_index = int(name_hash, 16) % len(ELL_COLORS)
    return list(ELL_COLORS.values())[color_index]

def format_arg(arg: Any, max_length: int = 8) -> str:
    """Format an argument for display with customizable max length."""
    str_arg = str(arg)
    return f"{Fore.MAGENTA}{str_arg[:max_length]}..{Style.RESET_ALL}" if len(str_arg) > max_length else f"{Fore.MAGENTA}{str_arg}{Style.RESET_ALL}"

def format_kwarg(key: str, value: Any, max_length: int = 8) -> str:
    """Format a keyword argument for display with customizable max length."""
    return f"{Style.DIM}{key}{Style.RESET_ALL}={Fore.MAGENTA}{str(value)[:max_length]}..{Style.RESET_ALL}"

def get_terminal_width() -> int:
    """Get the terminal width, defaulting to 80 if it can't be determined."""
    try:
        return shutil.get_terminal_size((80, 20)).columns
    except Exception:
        logger.warning("Unable to determine terminal size. Defaulting to 80 columns.")
        return 80

--------------------------------------------------------------------------------

util\verbosity.py::4
def wrap_text_with_prefix(message, width: int, prefix: str, subsequent_prefix: str, text_color: str) -> List[str]:
    """Wrap text while preserving the prefix and color for each line."""
    result = []
    for i, content in enumerate(message.content):
        wrapped_lines = []
        if content.image and content.image.image:
            wrapped_lines = plot_ascii(content.image.image, min(80, width - len(prefix)))
        else:
            if content.tool_result:
                contnets_to_wrap = [ContentBlock(text=f"ToolResult(tool_call_id={content.tool_result.tool_call_id}):"), *content.tool_result.result]
            else:
                contnets_to_wrap = [content]

            wrapped_lines = []
            for c in contnets_to_wrap:
                if c.image and c.image.image:
                    block_wrapped_lines = plot_ascii(c.image.image, min(80, width - len(prefix)))
                else:
                    text = _content_to_text([c])
                    paragraphs = text.split('\n')
                    wrapped_paragraphs = [textwrap.wrap(p, width=width - len(prefix)) for p in paragraphs]
                    block_wrapped_lines = [line for paragraph in wrapped_paragraphs for line in paragraph]
                wrapped_lines.extend(block_wrapped_lines)
        if i == 0:
            if wrapped_lines:
                result.append(f"{prefix}{text_color}{wrapped_lines[0]}{RESET}")
            else:
                result.append(f"{prefix}{text_color}{RESET}")
        else:
            result.append(f"{subsequent_prefix}{text_color}{wrapped_lines[0]}{RESET}")
        result.extend([f"{subsequent_prefix}{text_color}{line}{RESET}" for line in wrapped_lines[1:]])
    return result

--------------------------------------------------------------------------------

util\verbosity.py::5
def print_wrapped_messages(messages: List[Message], max_role_length: int, color: str, wrap_width: Optional[int] = None):
    """Print wrapped messages with proper indentation, customizable wrap width, and consistent ASCII piping."""
    terminal_width = get_terminal_width()
    prefix = f"{PIPE_COLOR}│   "
    role_prefix = ' ' * (max_role_length + 2)
    subsequent_prefix = f"{PIPE_COLOR}│   {role_prefix}"
    wrapping_width = wrap_width or (terminal_width - len(prefix))

    for i, message in enumerate(messages):
        role = message.role
        m = message.content[0]


        role_color = SYSTEM_COLOR if role == "system" else USER_COLOR if role == "user" else ASSISTANT_COLOR

        role_line = f"{prefix}{role_color}{role.rjust(max_role_length)}: {RESET}"
        wrapped_lines = wrap_text_with_prefix(message, wrapping_width - len(role_prefix), '', subsequent_prefix, role_color)

        print(f"{role_line}{wrapped_lines[0]}")
        for line in wrapped_lines[1:]:
            print(line)

        if i < len(messages) - 1:
            print(f"{PIPE_COLOR}│{RESET}")

--------------------------------------------------------------------------------

util\verbosity.py::6
def model_usage_logger_pre(
    invoking_lmp: LMP,
    lmp_args: Tuple,
    lmp_kwargs: Dict,
    lmp_hash: str,
    messages: List[Message],
    arg_max_length: int = 8
):
    """Log model usage before execution with customizable argument display length and ASCII box."""
    color =  compute_color(invoking_lmp)
    formatted_args = [format_arg(arg, arg_max_length) for arg in lmp_args]
    formatted_kwargs = [format_kwarg(key, lmp_kwargs[key], arg_max_length) for key in lmp_kwargs]
    formatted_params = ', '.join(formatted_args + formatted_kwargs)

    check_version_and_log()

    terminal_width = get_terminal_width()

    logger.info(f"Invoking LMP: {invoking_lmp.__name__} (hash: {lmp_hash[:8]})")

    print(f"{PIPE_COLOR}╔{'═' * (terminal_width - 2)}╗{RESET}")
    print(f"{PIPE_COLOR}║ {color}{BOLD}{UNDERLINE}{invoking_lmp.__name__}{RESET}{color}({formatted_params}){RESET}")
    print(f"{PIPE_COLOR}╠{'═' * (terminal_width - 2)}╣{RESET}")
    print(f"{PIPE_COLOR}║ {BOLD}Prompt:{RESET}")
    print(f"{PIPE_COLOR}╟{'─' * (terminal_width - 2)}╢{RESET}")

    max_role_length = max(len("assistant"), max(len(message.role) for message in messages))
    print_wrapped_messages(messages, max_role_length, color)

--------------------------------------------------------------------------------

util\verbosity.py::7
def model_usage_logger_post_start(color: str = "", n: int = 1):
    """Log the start of model output with ASCII box."""
    terminal_width = get_terminal_width()
    print(f"{PIPE_COLOR}╟{'─' * (terminal_width - 2)}╢{RESET}")
    print(f"{PIPE_COLOR}║ {BOLD}Output{f'[0 of {n}]' if n > 1 else ''}:{RESET}")
    print(f"{PIPE_COLOR}╟{'─' * (terminal_width - 2)}╢{RESET}")
    print(f"{PIPE_COLOR}│   {ASSISTANT_COLOR}assistant: {RESET}", end='')
    sys.stdout.flush()

from contextlib import contextmanager

--------------------------------------------------------------------------------

util\verbosity.py::8
@contextmanager
def model_usage_logger_post_intermediate( n: int = 1):
    """Context manager to log intermediate model output without wrapping, only indenting if necessary."""
    terminal_width = get_terminal_width()
    prefix = f"{PIPE_COLOR}│   "
    subsequent_prefix = f"{PIPE_COLOR}│   {' ' * (len('assistant: '))}"
    chars_printed = len(subsequent_prefix)

    def log_stream_chunk(stream_chunk: str , is_refusal: bool = False):
        nonlocal chars_printed
        if stream_chunk:
            lines = stream_chunk.split('\n')
            for i, line in enumerate(lines):
                if chars_printed + len(line) > terminal_width - 6:
                    print()
                    if i == 0:
                        print(subsequent_prefix, end='')
                        chars_printed = len(prefix)
                    else:
                        print(subsequent_prefix, end='')
                        chars_printed = len(subsequent_prefix)
                    print(line.lstrip(), end='')
                else:
                    print(line, end='')
                chars_printed += len(line)

                if i < len(lines) - 1:
                    print()
                    print(subsequent_prefix, end='')
                    chars_printed = len(subsequent_prefix)  # Reset for new line
            sys.stdout.flush()

    try:
        yield log_stream_chunk
    finally:
        pass

--------------------------------------------------------------------------------



Configuration Management and Initialization:
src\conf.py::1
# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

project = 'ell'
copyright = '2024, William Guss'
author = 'William Guss'

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.napoleon', 'sphinxawesome_theme', 'sphinxcontrib.autodoc_pydantic']

templates_path = ['_templates']
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

html_theme = "sphinxawesome_theme"


# Favicon configuration
html_favicon = '_static/favicon.ico'

# Configure syntax highlighting for Awesome Sphinx Theme
pygments_style = "default"
pygments_style_dark = "dracula"

# Additional theme configuration


--------------------------------------------------------------------------------

ell\configurator.py::1
from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)

--------------------------------------------------------------------------------

ell\configurator.py::2
class Config(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    registry: Dict[str, _Model] = Field(default_factory=dict, description="A dictionary mapping model names to their configurations.")
    verbose: bool = Field(default=False, description="If True, enables verbose logging.")
    wrapped_logging: bool = Field(default=True, description="If True, enables wrapped logging for better readability.")
    override_wrapped_logging_width: Optional[int] = Field(default=None, description="If set, overrides the default width for wrapped logging.")
    store: Optional[Store] = Field(default=None, description="An optional Store instance for persistence.")
    autocommit: bool = Field(default=False, description="If True, enables automatic committing of changes to the store.")
    lazy_versioning: bool = Field(default=True, description="If True, enables lazy versioning for improved performance.")
    default_api_params: Dict[str, Any] = Field(default_factory=dict, description="Default parameters for language models.")
    default_client: Optional[openai.Client] = Field(default=None, description="The default OpenAI client used when a specific model client is not found.")
    autocommit_model: str = Field(default="gpt-4o-mini", description="When set, changes the default autocommit model from GPT 4o mini.")
    providers: Dict[Type, Provider] = Field(default_factory=dict, description="A dictionary mapping client types to provider classes.")
    def __init__(self, **data):
        super().__init__(**data)
        self._lock = threading.Lock()
        self._local = threading.local()

--------------------------------------------------------------------------------

ell\configurator.py::3
class Config(BaseModel):


    def register_model(
        self, 
        name: str,
        default_client: Optional[Union[openai.Client, Any]] = None,
        supports_streaming: Optional[bool] = None
    ) -> None:
        """
        Register a model with its configuration.
        """
        with self._lock:
            # XXX: Will be deprecated in 0.1.0
            self.registry[name] = _Model(
                name=name,
                default_client=default_client,
                supports_streaming=supports_streaming
            )

--------------------------------------------------------------------------------

ell\configurator.py::4
class Config(BaseModel):



    @contextmanager
    def model_registry_override(self, overrides: Dict[str, _Model]):
        """
        Temporarily override the model registry with new model configurations.

        :param overrides: A dictionary of model names to ModelConfig instances to override.
        :type overrides: Dict[str, ModelConfig]
        """
        if not hasattr(self._local, 'stack'):
            self._local.stack = []

        with self._lock:
            current_registry = self._local.stack[-1] if self._local.stack else self.registry
            new_registry = current_registry.copy()
            new_registry.update(overrides)

        self._local.stack.append(new_registry)
        try:
            yield
        finally:
            self._local.stack.pop()

--------------------------------------------------------------------------------

ell\configurator.py::5
class Config(BaseModel):

    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:
        """
        Get the OpenAI client for a specific model name.

        :param model_name: The name of the model to get the client for.
        :type model_name: str
        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.
        :rtype: Tuple[Optional[openai.Client], bool]
        """
        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry
        model_config = current_registry.get(model_name)
        fallback = False
        if not model_config:
            warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."
            if self.verbose:
                from colorama import Fore, Style
                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")
            else:
                _config_logger.debug(warning_message)
            client = self.default_client
            fallback = True
        else:
            client = model_config.default_client
        return client, fallback

--------------------------------------------------------------------------------

ell\configurator.py::6
class Config(BaseModel):

    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:
        """
        Register a provider class for a specific client type.

        :param provider_class: The provider class to register.
        :type provider_class: Type[Provider]
        """
        assert isinstance(client_type, type), "client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))"
        with self._lock:
            self.providers[client_type] = provider

--------------------------------------------------------------------------------

ell\configurator.py::7
class Config(BaseModel):

    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:
        """
        Get the provider instance for a specific client instance.

        :param client: The client instance to get the provider for.
        :type client: Any
        :return: The provider instance for the specified client, or None if not found.
        :rtype: Optional[Provider]
        """

        client_type = type(client) if not isinstance(client, type) else client
        for provider_type, provider in self.providers.items():
            if issubclass(client_type, provider_type) or client_type == provider_type:
                return provider
        return None

# Single* instance
# XXX: Make a singleton
config = Config()

--------------------------------------------------------------------------------

ell\configurator.py::8
def init(
    store: Optional[Union[Store, str]] = None,
    verbose: bool = False,
    autocommit: bool = True,
    lazy_versioning: bool = True,
    default_api_params: Optional[Dict[str, Any]] = None,
    default_client: Optional[Any] = None,
    autocommit_model: str = "gpt-4o-mini"
) -> None:
    """
    Initialize the ELL configuration with various settings.

    :param verbose: Set verbosity of ELL operations.
    :type verbose: bool
    :param store: Set the store for ELL. Can be a Store instance or a string path for SQLiteStore.
    :type store: Union[Store, str], optional
    :param autocommit: Set autocommit for the store operations.
    :type autocommit: bool
    :param lazy_versioning: Enable or disable lazy versioning.
    :type lazy_versioning: bool
    :param default_api_params: Set default parameters for language models.
    :type default_api_params: Dict[str, Any], optional
    :param default_openai_client: Set the default OpenAI client.
    :type default_openai_client: openai.Client, optional
    :param autocommit_model: Set the model used for autocommitting.
    :type autocommit_model: str
    """
    # XXX: prevent double init
    config.verbose = verbose
    config.lazy_versioning = lazy_versioning

    if isinstance(store, str):
        from ell.stores.sql import SQLiteStore
        config.store = SQLiteStore(store)
    else:
        config.store = store
    config.autocommit = autocommit or config.autocommit

    if default_api_params is not None:
        config.default_api_params.update(default_api_params)

    if default_client is not None:
        config.default_client = default_client

    if autocommit_model is not None:
        config.autocommit_model = autocommit_model

--------------------------------------------------------------------------------



Language Model Programming Interface:
lmp\complex.py::1
from ell.configurator import config
from ell.lmp._track import _track
from ell.provider import EllCallParams
from ell.types._lstr import _lstr
from ell.types import Message, ContentBlock
from ell.types.message import LMP, InvocableLM, LMPParams, MessageOrDict, _lstr_generic
from ell.types.studio import LMPType
from ell.util._warnings import _no_api_key_warning, _warnings
from ell.util.verbosity import compute_color, model_usage_logger_pre

from ell.util.verbosity import model_usage_logger_post_end, model_usage_logger_post_intermediate, model_usage_logger_post_start

from functools import wraps
from typing import Any, Dict, Optional, List, Callable, Tuple, Union

--------------------------------------------------------------------------------

lmp\complex.py::2
def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    default_client_from_decorator = client
    default_model_from_decorator = model
    default_api_params_from_decorator = api_params
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        _warnings(model, prompt, default_client_from_decorator)

        @wraps(prompt)
        def model_call(
            *prompt_args,
            _invocation_origin : Optional[str] = None,
            client: Optional[Any] = None,
            api_params: Optional[Dict[str, Any]] = None,
            lm_params: Optional[DeprecationWarning] = None,
            **prompt_kwargs,
        ) -> Tuple[Any, Any, Any]:
            # XXX: Deprecation in 0.1.0
            if lm_params:
                raise DeprecationWarning("lm_params is deprecated. Use api_params instead.")

            # promt -> str
            res = prompt(*prompt_args, **prompt_kwargs)
            # Convert prompt into ell messages
            messages = _get_messages(res, prompt)

            # XXX: move should log to a logger.
            should_log = not exempt_from_tracking and config.verbose
            # Cute verbose logging.
            if should_log: model_usage_logger_pre(prompt, prompt_args, prompt_kwargs, "[]", messages) #type: ignore

            # Call the model.
            # Merge API params
            merged_api_params = {**config.default_api_params, **default_api_params_from_decorator, **(api_params or {})}
            n = merged_api_params.get("n", 1)
            # Merge client overrides & client registry
            merged_client = _client_for_model(model, client or default_client_from_decorator)
            ell_call = EllCallParams(
                # XXX: Could change behaviour of overriding ell params for dyanmic tool calls.
                model=merged_api_params.pop("model", default_model_from_decorator),
                messages=messages,
                client = merged_client,
                api_params=merged_api_params,
                tools=tools or [],
            )
            # Get the provider for the model
            provider = config.get_provider_for(ell_call.client)
            assert provider is not None, f"No provider found for client {ell_call.client}."

            if should_log: model_usage_logger_post_start(n)
            with model_usage_logger_post_intermediate(n) as _logger:
                (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)
                if isinstance(result, list) and len(result) == 1:
                    result = result[0]

            result = post_callback(result) if post_callback else result
            if should_log:
                model_usage_logger_post_end()
            #
            #  These get sent to track. This is wack.           
            return result, final_api_params, metadata
        # ... other code
    # ... other code

--------------------------------------------------------------------------------

lmp\complex.py::3
def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        # ... other code



        model_call.__ell_api_params__ = default_api_params_from_decorator #type: ignore
        model_call.__ell_func__ = prompt #type: ignore
        model_call.__ell_type__ = LMPType.LM #type: ignore
        model_call.__ell_exempt_from_tracking = exempt_from_tracking #type: ignore


        if exempt_from_tracking:
            return model_call
        else:
            # XXX: Analyze decorators with AST instead.
            return _track(model_call, forced_dependencies=dict(tools=tools, response_format=api_params.get("response_format", {})))
    return parameterized_lm_decorator

--------------------------------------------------------------------------------

lmp\complex.py::4
def _get_messages(prompt_ret: Union[str, list[MessageOrDict]], prompt: LMP) -> list[Message]:
    """
    Helper function to convert the output of an LMP into a list of Messages.
    """
    if isinstance(prompt_ret, str):
        has_system_prompt = prompt.__doc__ is not None and prompt.__doc__.strip() != ""
        messages =     [Message(role="system", content=[ContentBlock(text=_lstr(prompt.__doc__ ) )])] if has_system_prompt else []
        return messages + [
            Message(role="user", content=[ContentBlock(text=prompt_ret)])
        ]
    else:
        assert isinstance(
            prompt_ret, list
        ), "Need to pass a list of Messages to the language model"
        return prompt_ret

--------------------------------------------------------------------------------

lmp\simple.py::1
from functools import wraps
from typing import Any, Optional

from ell.lmp.complex import complex


def simple(model: str, client: Optional[Any] = None,  exempt_from_tracking=False, **api_params):
    assert 'tools' not in api_params, "tools are not supported in lm decorator, use multimodal decorator instead"
    assert 'tool_choice' not in api_params, "tool_choice is not supported in lm decorator, use multimodal decorator instead"
    assert 'response_format' not in api_params or isinstance(api_params.get('response_format', None), dict), "response_format is not supported in lm decorator, use multimodal decorator instead"

    def convert_multimodal_response_to_lstr(response):
        return [x.content[0].text for x in response] if isinstance(response, list) else response.content[0].text
    return complex(model, client,  exempt_from_tracking=exempt_from_tracking, **api_params, post_callback=convert_multimodal_response_to_lstr)

--------------------------------------------------------------------------------

lmp\simple.py::2
simple.__doc__ = """The fundamental unit of language model programming in ell.

  This decorator simplifies the process of creating Language Model Programs (LMPs) 
  that return text-only outputs from language models, while supporting multimodal inputs.
  It wraps the more complex 'complex' decorator, providing a streamlined interface for common use cases.

  :param model: The name or identifier of the language model to use.
  :type model: str
  :param client: An optional OpenAI client instance. If not provided, a default client will be used.
  :type client: Optional[openai.Client]
  :param exempt_from_tracking: If True, the LMP usage won't be tracked. Default is False.
  :type exempt_from_tracking: bool
  :param api_params: Additional keyword arguments to pass to the underlying API call.
  :type api_params: Any

  Usage:
  The decorated function can return either a single prompt or a list of ell.Message objects:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def summarize_text(text: str) -> str:
          '''You are an expert at summarizing text.''' # System prompt
          return f"Please summarize the following text:\\n\\n{text}" # User prompt


      @ell.simple(model="gpt-4", temperature=0.7)
      def describe_image(image : PIL.Image.Image) -> List[ell.Message]:
          '''Describe the contents of an image.''' # unused because we're returning a list of Messages
          return [
              # helper function for ell.Message(text="...", role="system")
              ell.system("You are an AI trained to describe images."),
              # helper function for ell.Message(content="...", role="user")
              ell.user(["Describe this image in detail.", image]),
          ]


      image_description = describe_image(PIL.Image.open("https://example.com/image.jpg"))
      print(image_description) 
      # Output will be a string text-only description of the image

      summary = summarize_text("Long text to summarize...")
      print(summary)
      # Output will be a text-only summary

  Notes:

  - This decorator is designed for text-only model outputs, but supports multimodal inputs.
  - It simplifies complex responses from language models to text-only format, regardless of 
    the model's capability for structured outputs, function calling, or multimodal outputs.
  - For preserving complex model outputs (e.g., structured data, function calls, or multimodal 
    outputs), use the @ell.complex decorator instead. @ell.complex returns a Message object (role='assistant')
  - The decorated function can return a string or a list of ell.Message objects for more 
    complex prompts, including multimodal inputs.
  - If called with n > 1 in api_params, the wrapped LMP will return a list of strings for the n parallel outputs
    of the model instead of just one string. Otherwise, it will return a single string.
  - You can pass LM API parameters either in the decorator or when calling the decorated function.
    Parameters passed during the function call will override those set in the decorator.

  Example of passing LM API params:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def generate_story(prompt: str) -> str:
          return f"Write a short story based on this prompt: {prompt}"

      # Using default parameters
      story1 = generate_story("A day in the life of a time traveler")

      # Overriding parameters during function call
      story2 = generate_story("An AI's first day of consciousness", api_params={"temperature": 0.9, "max_tokens": 500})

  See Also:

  - :func:`ell.complex`: For LMPs that preserve full structure of model responses, including multimodal outputs.
  - :func:`ell.tool`: For defining tools that can be used within complex LMPs.
  - :mod:`ell.studio`: For visualizing and analyzing LMP executions.
    """

--------------------------------------------------------------------------------

lmp\tool.py::1
from functools import wraps
import json
from typing import Any, Callable, Optional

from pydantic import Field, create_model
from pydantic.fields import FieldInfo
from ell.lmp._track import _track
# from ell.types import ToolFunction, InvocableTool, ToolParams
# from ell.util.verbosity import compute_color, tool_usage_logger_pre
from ell.configurator import config
from ell.types._lstr import _lstr
from ell.types.studio import LMPType
import inspect

from ell.types.message import ContentBlock, InvocableTool, ToolResult, to_content_blocks

--------------------------------------------------------------------------------

lmp\tool.py::2
def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        _under_fn = fn

        @wraps(fn)
        def wrapper(
            *fn_args,
            _invocation_origin: str = None,
            _tool_call_id: str = None,
            **fn_kwargs
        ):
            #XXX: Post release, we need to wrap all tool arguments in type primitives for tracking I guess or change that tool makes the tool function inoperable.
            #XXX: Most people are not going to manually try and call the tool without a type primitive and if they do it will most likely be wrapped with l strs.

            if config.verbose and not exempt_from_tracking:
                pass
                # tool_usage_logger_pre(fn, fn_args, fn_kwargs, name, color)

            result = fn(*fn_args, **fn_kwargs)

            _invocation_api_params = dict(tool_kwargs=tool_kwargs)

            # Here you might want to add logic for tracking the tool usage
            # Similar to how it's done in the lm decorator # Use _invocation_origin

            if isinstance(result, str) and _invocation_origin:
                result = _lstr(result,origin_trace=_invocation_origin)

            #XXX: This _tool_call_id thing is a hack. Tracking should happen via params in the api
            # So if you call wiuth a _tool_callId
            if _tool_call_id:
                # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
                try:
                    if isinstance(result, ContentBlock):
                        content_results = [result]
                    elif isinstance(result, list) and all(isinstance(c, ContentBlock) for c in result):
                        content_results = result
                    else:
                        content_results = [ContentBlock(text=_lstr(json.dumps(result, ensure_ascii=False),origin_trace=_invocation_origin))]
                except TypeError as e:
                    raise TypeError(f"Failed to convert tool use result to ContentBlock: {e}. Tools must return json serializable objects. or a list of ContentBlocks.")
                # XXX: Need to support images and other content types somehow. We should look for images inside of the the result and then go from there.
                # try:
                #     content_results = coerce_content_list(result)
                # except ValueError as e:

                # TODO: poolymorphic validation here is important (cant have tool_call or formatted_response in the result)
                # XXX: Should we put this coercion here or in the tool call/result area.
                for c in content_results:
                    assert not c.tool_call, "Tool call in tool result"
                    # assert not c.formatted_response, "Formatted response in tool result"
                    if c.parsed:
                        # Warning: Formatted response in tool result will be converted to text
                        # TODO: Logging needs to produce not print.
                        print(f"Warning: Formatted response in tool result will be converted to text. Original: {c.parsed}")
                        c.text = _lstr(c.parsed.model_dump_json(),origin_trace=_invocation_origin)
                        c.parsed = None
                    assert not c.audio, "Audio in tool result"
                return ToolResult(tool_call_id=_tool_call_id, result=content_results), _invocation_api_params, {}
            else:
                return result, _invocation_api_params, {}
        # ... other code
    # ... other code

--------------------------------------------------------------------------------

lmp\tool.py::3
def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        # ... other code


        wrapper.__ell_tool_kwargs__ = tool_kwargs
        wrapper.__ell_func__ = _under_fn
        wrapper.__ell_type__ = LMPType.TOOL
        wrapper.__ell_exempt_from_tracking = exempt_from_tracking

        # Construct the pydantic mdoel for the _under_fn's function signature parameters.
        # 1. Get the function signature.

        sig = inspect.signature(fn)

        # 2. Create a dictionary of field definitions for the Pydantic model
        fields = {}
        for param_name, param in sig.parameters.items():
            # Skip *args and **kwargs
            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                continue

            # Determine the type annotation
            if param.annotation == inspect.Parameter.empty:
                raise ValueError(f"Parameter {param_name} has no type annotation, and cannot be converted into a tool schema for OpenAI and other provisders. Should OpenAI produce a string or an integer, etc, for this parameter?")
            annotation = param.annotation

            # Determine the default value
            default = param.default

            # Check if the parameter has a Field with description
            if isinstance(param.default, FieldInfo):
                field = param.default
                fields[param_name] = (annotation, field)
            elif param.default != inspect.Parameter.empty:
                fields[param_name] = (annotation, param.default)
            else:
                # If no default value, use Field without default
                fields[param_name] = (annotation, Field(...))

        # 3. Create the Pydantic model
        model_name = f"{fn.__name__}"
        ParamsModel = create_model(model_name, **fields)

        # Attach the Pydantic model to the wrapper function
        wrapper.__ell_params_model__ = ParamsModel

        # handle tracking last.
        if exempt_from_tracking:
            ret = wrapper
        else:
            ret=  _track(wrapper)

        # Helper function to get the Pydantic model for the tool
        def get_params_model():
            return wrapper.__ell_params_model__

        # Attach the helper function to the wrapper
        wrapper.get_params_model = get_params_model
        ret.get_params_model = get_params_model
        return ret

    return tool_decorator

--------------------------------------------------------------------------------

lmp\tool.py::4
tool.__doc__ = """Defines a tool for use in language model programs (LMPs) that support tool use.

This decorator wraps a function, adding metadata and handling for tool invocations.
It automatically extracts the tool's description and parameters from the function's
docstring and type annotations, creating a structured representation for LMs to use.

:param exempt_from_tracking: If True, the tool usage won't be tracked. Default is False.
:type exempt_from_tracking: bool
:param tool_kwargs: Additional keyword arguments for tool configuration.
:return: A wrapped version of the original function, usable as a tool by LMs.
:rtype: Callable

Requirements:

- Function must have fully typed arguments (Pydantic-serializable).
- Return value must be one of: str, JSON-serializable object, Pydantic model, or List[ContentBlock].
- All parameters must have type annotations.
- Complex types should be Pydantic models.
- Function should have a descriptive docstring.
- Can only be used in LMPs with @ell.complex decorators

Functionality:

1. Metadata Extraction:
    - Uses function docstring as tool description.
    - Extracts parameter info from type annotations and docstring.
    - Creates a Pydantic model for parameter validation and schema generation.

2. Integration with LMs:
    - Can be passed to @ell.complex decorators.
    - Provides structured tool information to LMs.

3. Invocation Handling:
    - Manages tracking, logging, and result processing.
    - Wraps results in appropriate types (e.g., _lstr) for tracking.

Usage Modes:

1. Normal Function Call:
    - Behaves like a regular Python function.
    - Example: result = my_tool(arg1="value", arg2=123)

2. LMP Tool Call:
    - Used within LMPs or with explicit _tool_call_id.
    - Returns a ToolResult object.
    - Example: result = my_tool(arg1="value", arg2=123, _tool_call_id="unique_id")

Result Coercion:

- String → ContentBlock(text=result)
- Pydantic BaseModel → ContentBlock(parsed=result)
- List[ContentBlock] → Used as-is
- Other types → ContentBlock(text=json.dumps(result))

Example::

    @ell.tool()
    def create_claim_draft(
        claim_details: str,
        claim_type: str,
        claim_amount: float,
        claim_date: str = Field(description="Date format: YYYY-MM-DD")
    ) -> str:
        '''Create a claim draft. Returns the created claim ID.'''
        return "12345"

    # For use in a complex LMP:
    @ell.complex(model="gpt-4", tools=[create_claim_draft], temperature=0.1)
    def insurance_chatbot(message_history: List[Message]) -> List[Message]:
        # Chatbot implementation...

    x = insurance_chatbot([
        ell.user("I crashed my car into a tree."),
        ell.assistant("I'm sorry to hear that. Can you provide more details?"),
        ell.user("The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000")
    ]) 
    print(x)
    '''ell.Message(content=[
        ContentBlock(tool_call(
            tool_call_id="asdas4e",
            tool_fn=create_claim_draft,
            input=create_claim_draftParams({
                claim_details="The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000",
                claim_type="car",
                claim_amount=5000,
                claim_date="2024-08-01"
            })
        ))
    ], role='assistant')'''
    
    if x.tool_calls:
        next_user_message = response_message.call_tools_and_collect_as_message()
        # This actually calls create_claim_draft
        print(next_user_message)
        '''
        ell.Message(content=[
            ContentBlock(tool_result=ToolResult(
                tool_call_id="asdas4e",
                result=[ContentBlock(text="12345")]
            ))
        ], role='user')
        '''
        y = insurance_chatbot(message_history + [x, next_user_message])
        print(y)
        '''
        ell.Message("I've filed that for you!", role='assistant')
        '''

Note:
- Tools are integrated into LMP calls via the 'tools' parameter in @ell.complex.
- LMs receive structured tool information, enabling understanding and usage within the conversation context.
    """

--------------------------------------------------------------------------------



Model Registration and Management:
models\openai.py::2
def register(client: openai.Client):
    """
    Register OpenAI models with the provided client.

    This function takes an OpenAI client and registers various OpenAI models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (openai.Client): An instance of the OpenAI client to be used
                                for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    #XXX: Deprecation in 0.1.0
    standard_models = [
        'gpt-4-1106-preview',
        'gpt-4-32k-0314',
        'text-embedding-3-large',
        'gpt-4-0125-preview',
        'babbage-002',
        'gpt-4-turbo-preview',
        'gpt-4o',
        'gpt-4o-2024-05-13',
        'gpt-4o-mini-2024-07-18',
        'gpt-4o-mini',
        'gpt-4o-2024-08-06',
        'gpt-3.5-turbo-0301',
        'gpt-3.5-turbo-0613',
        'tts-1',
        'gpt-3.5-turbo',
        'gpt-3.5-turbo-16k',
        'davinci-002',
        'gpt-3.5-turbo-16k-0613',
        'gpt-4-turbo-2024-04-09',
        'gpt-3.5-turbo-0125',
        'gpt-4-turbo',
        'gpt-3.5-turbo-1106',
        'gpt-3.5-turbo-instruct-0914',
        'gpt-3.5-turbo-instruct',
        'gpt-4-0613',
        'gpt-4',
        'gpt-4-0314',
        'gpt-4o-audio-preview',
        'gpt-4o-realtime',
    ]
    for model_id in standard_models:
        config.register_model(model_id, client)

    #XXX: Deprecation in 0.1.0
    config.register_model('o1-preview', client, supports_streaming=False)
    config.register_model('o1-mini', client, supports_streaming=False)

default_client = None
try:
    default_client = openai.Client()
except openai.OpenAIError as e:
    pass

register(default_client)
config.default_client = default_client

--------------------------------------------------------------------------------

models\anthropic.py::1
from ell.configurator import config
import logging

logger = logging.getLogger(__name__)


try:
    import anthropic

    def register(client: anthropic.Anthropic):
        """
        Register Anthropic models with the provided client.

        This function takes an Anthropic client and registers various Anthropic models
        with the global configuration. It allows the system to use these models
        for different AI tasks.

        Args:
            client (anthropic.Anthropic): An instance of the Anthropic client to be used
                                          for model registration.

        Note:
            The function doesn't return anything but updates the global
            configuration with the registered models.
        """
        model_data = [
            ('claude-3-opus-20240229', 'anthropic'),
            ('claude-3-sonnet-20240229', 'anthropic'),
            ('claude-3-haiku-20240307', 'anthropic'),
            ('claude-3-5-sonnet-20240620', 'anthropic'),
        ]
        for model_id, owned_by in model_data:
            config.register_model(model_id, client)

    try:
        default_client = anthropic.Anthropic()
        register(default_client)
    except Exception as e:
        # logger.warning(f"Failed to create default Anthropic client: {e}")
        pass


except ImportError:
    pass

--------------------------------------------------------------------------------

models\bedrock.py::1
from typing import Any
from ell.configurator import config
import logging

logger = logging.getLogger(__name__)


def register(client: Any):
    """
    Register Bedrock models with the provided client.

    This function takes an boto3 client and registers various Bedrock models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (boto3.client): An instance of the bedrock client to be used
                                        for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    model_data = [
        ('anthropic.claude-3-opus-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-sonnet-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-haiku-20240307-v1:0', 'bedrock'),
        ('anthropic.claude-3-5-sonnet-20240620-v1:0', 'bedrock'),

        ('mistral.mistral-7b-instruct-v0:2', 'bedrock'),
        ('mistral.mixtral-8x7b-instruct-v0:1', 'bedrock'),
        ('mistral.mistral-large-2402-v1:0', 'bedrock'),
        ('mistral.mistral-small-2402-v1:0', 'bedrock'),


        ('ai21.jamba-instruct-v1:0','bedrock'),
        ('ai21.j2-ultra-v1', 'bedrock'),
        ('ai21.j2-mid-v1', 'bedrock'),

        ('amazon.titan-embed-text-v1', 'bedrock'),
        ('amazon.titan-text-lite-v1', 'bedrock'),
        ('amazon.titan-text-express-v1', 'bedrock'),
        ('amazon.titan-image-generator-v2:0', 'bedrock'),
        ('amazon.titan-image-generator-v1', 'bedrock'),

        ('cohere.command-r-plus-v1:0', 'bedrock'),
        ('cohere.command-r-v1:0', 'bedrock'),
        ('cohere.embed-english-v3', 'bedrock'),
        ('cohere.embed-multilingual-v3', 'bedrock'),
        ('cohere.command-text-v14', 'bedrock'),

        ('meta.llama3-8b-instruct-v1:0', 'bedrock'),
        ('meta.llama3-70b-instruct-v1:0', 'bedrock'),
        ('meta.llama2-13b-chat-v1', 'bedrock'),
        ('meta.llama2-70b-chat-v1', 'bedrock'),
        ('meta.llama2-13b-v1', 'bedrock'),

    ]

    for model_id, owned_by in model_data:
        config.register_model(name=model_id, default_client=client, supports_streaming=True)

default_client = None
try:

    import boto3
    default_client = boto3.client('bedrock-runtime')
except Exception as e:
    pass

register(default_client)

--------------------------------------------------------------------------------

models\groq.py::1
from typing import Optional
from ell.configurator import config

try:
    from groq import Groq
    def register(client: Optional[Groq] = None, **client_kwargs):
        if client is None:
            client = Groq(**client_kwargs)
        for model in client.models.list().data:
            config.register_model(model.id, default_client=client, supports_streaming=True)
except ImportError:
    pass

--------------------------------------------------------------------------------

models\ollama.py::1
from ell.configurator import config
import openai
import requests
import logging

#XXX: May be deprecated soon because of the new provider framework.
logger = logging.getLogger(__name__)
client = None

def register(base_url):
    """
    Registers Ollama models with the provided base URL.

    This function sets up the Ollama client with the given base URL and
    fetches available models from the Ollama API. It then registers these
    models with the global configuration, allowing them to be used within
    the ell framework.

    Args:
        base_url (str): The base URL of the Ollama API endpoint.

    Note:
        This function updates the global client and configuration.
        It logs any errors encountered during the process.
    """
    global client
    client = openai.Client(base_url=base_url)

    try:
        response = requests.get(f"{base_url}/../api/tags")
        response.raise_for_status()
        models = response.json().get("models", [])

        for model in models:
            config.register_model(model["name"], client)
    except requests.RequestException as e:
        logger.error(f"Failed to fetch models from {base_url}: {e}")
    except Exception as e:
        logger.error(f"An error occurred: {e}")

--------------------------------------------------------------------------------

models\openai.py::1
"""
This module handles the registration of OpenAI models within the ell framework.

It provides functionality to register various OpenAI models with a given OpenAI client,
making them available for use throughout the system. The module also sets up a default
client behavior for unregistered models.

Key features:
1. Registration of specific OpenAI models with their respective types (system, openai, openai-internal).
2. Utilization of a default OpenAI client for any unregistered models,

The default client behavior ensures that even if a specific model is not explicitly
registered, the system can still attempt to use it with the default OpenAI client.
This fallback mechanism provides flexibility in model usage while maintaining a
structured approach to model registration.

Note: The actual model availability may depend on your OpenAI account's access and the
current offerings from OpenAI.

Additionally, due to the registration of default mdoels, the OpenAI client may be used for
anthropic, cohere, groq, etc. models if their clients are not registered or fail
to register due to an error (lack of API keys, rate limits, etc.)
"""

from ell.configurator import config
import openai

import logging
import colorama

logger = logging.getLogger(__name__)

--------------------------------------------------------------------------------

providers\__init__.py::1
import ell.providers.openai
import ell.providers.groq
import ell.providers.anthropic
import ell.providers.bedrock
# import ell.providers.mistral
# import ell.providers.cohere
# import ell.providers.gemini
# import ell.providers.elevenlabs
# import ell.providers.replicate
# import ell.providers.huggingface

--------------------------------------------------------------------------------

providers\openai.py::1
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast

from pydantic import BaseModel
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from ell.configurator import _Model, config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image

try:
    # XXX: Could genericize.
    import openai
    from openai._streaming import Stream
    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam

    class OpenAIProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and (isinstance(fmt := api_call_params.get("response_format"), type)) and issubclass(fmt, BaseModel):
                return client.beta.chat.completions.parse
            else:
                return client.chat.completions.create

        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:
            final_call_params = ell_call.api_params.copy()
            final_call_params["model"] = ell_call.model
            # Stream by default for verbose logging.
            final_call_params["stream"] = True
            final_call_params["stream_options"] = {"include_usage": True}

            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.
            if ell_call.tools or final_call_params.get("response_format") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:
                final_call_params.pop("stream", None)
                final_call_params.pop("stream_options", None)
            if ell_call.tools:
                final_call_params.update(
                    tool_choice=final_call_params.get("tool_choice", "auto"),
                    tools=[  
                        dict(
                            type="function",
                            function=dict(
                                name=tool.__name__,
                                description=tool.__doc__,
                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore
                            )
                        ) for tool in ell_call.tools
                    ]
                )
            # messages
            openai_messages : List[ChatCompletionMessageParam] = []
            for message in ell_call.messages:
                if (tool_calls := message.tool_calls):
                    assert message.role == "assistant", "Tool calls must be from the assistant."
                    assert all(t.tool_call_id for t in tool_calls), "Tool calls must have tool call ids."
                    openai_messages.append(dict(
                        tool_calls=[
                            dict(
                                id=cast(str, tool_call.tool_call_id),
                                type="function",
                                function=dict(
                                    name=tool_call.tool.__name__,
                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)
                                )
                            ) for tool_call in tool_calls ],
                        role="assistant",
                        content=None,
                    ))
                elif (tool_results := message.tool_results):
                    for tool_result in tool_results:
                        assert all(cb.type == "text" for cb in tool_result.result), "Tool result does not match expected content blocks."
                        openai_messages.append(dict(
                            role="tool",
                            tool_call_id=tool_result.tool_call_id,
                            content=tool_result.text_only, 
                        ))
                else:
                    openai_messages.append(cast(ChatCompletionMessageParam, dict(
                        role=message.role,
                        content=[_content_block_to_openai_format(c) for c in message.content] 
                             if message.role != "system" 
                             else message.text_only
                    )))

            final_call_params["messages"] = openai_messages

            return final_call_params

        def translate_from_provider(
            self,
            provider_response: Union[
                ChatCompletion, 
                ParsedChatCompletion,
                Stream[ChatCompletionChunk], Any],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            metadata : Metadata = {}
            messages : List[Message] = []
            did_stream = provider_call_params.get("stream", False)


            if did_stream:
                stream = cast(Stream[ChatCompletionChunk], provider_response)
                message_streams = defaultdict(list)
                role : Optional[str] = None
                for chunk in stream:
                    metadata.update(chunk.model_dump(exclude={"choices"}))

                    for chat_compl_chunk in chunk.choices:
                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)
                        delta = chat_compl_chunk.delta
                        role = role or delta.role
                        if  chat_compl_chunk.index == 0 and logger:
                            logger(delta.content, is_refusal=hasattr(delta, "refusal") and delta.refusal)
                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):
                    text = "".join((choice.delta.content or "") for choice in message_stream)
                    messages.append(
                        Message(role=role, 
                                content=_lstr(content=text,origin_trace=origin_id)))
                    #XXX: Support streaming other types.
            else:
                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)
                metadata = chat_completion.model_dump(exclude={"choices"})
                for oai_choice in chat_completion.choices:
                    role = oai_choice.message.role
                    content_blocks = []
                    if (hasattr(message := oai_choice.message, "refusal") and (refusal := message.refusal)):
                        raise ValueError(refusal)
                    if hasattr(message, "parsed"):
                        if (parsed := message.parsed):
                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing
                            if logger: logger(parsed.model_dump_json())
                    else:
                        if (content := message.content):
                            content_blocks.append(
                                ContentBlock(
                                    text=_lstr(content=content,origin_trace=origin_id)))
                            if logger: logger(content)
                        if (tool_calls := message.tool_calls):
                            for tool_call in tool_calls:
                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)
                                assert matching_tool, "Model called tool not found in provided toolset."
                                content_blocks.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                tool_call.id, origin_trace= origin_id),
                                            params=json.loads(tool_call.function.arguments),
                                        )
                                    )
                                )
                                if logger: logger(repr(tool_call))
                    messages.append(Message(role=role, content=content_blocks))
            return messages, metadata


    # xx: singleton needed
    openai_provider = OpenAIProvider()
    register_provider(openai_provider, openai.Client)
except ImportError:
    pass

--------------------------------------------------------------------------------

providers\anthropic.py::1
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent

from ell.types._lstr import _lstr
from ell.types.message import LMP
from ell.configurator import register_provider
from ell.util.serialization import serialize_image
import base64
from io import BytesIO
import json
import requests
from PIL import Image as PILImage

try:
    import anthropic
    from anthropic import Anthropic
    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent
    from anthropic.types.message_create_params import MessageCreateParamsStreaming
    from anthropic._streaming import Stream

    class AnthropicProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            return client.messages.create

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())
            # XXX: Helper, but should be depreicated due to ssot
            assert final_call_params.get("max_tokens") is not None, f"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}}))."

            dirty_msgs = [
                MessageParam(
                    role=cast(Literal["user", "assistant"], message.role), 
                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]
            role_correct_msgs   : List[MessageParam] = []
            for msg in dirty_msgs:
                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):
                    role_correct_msgs.append(msg)
                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])

            system_message = None
            if role_correct_msgs and role_correct_msgs[0]["role"] == "system":
                system_message = role_correct_msgs.pop(0)

            if system_message:
                final_call_params["system"] = system_message["content"][0]["text"]


            final_call_params['stream'] = True
            final_call_params["model"] = ell_call.model
            final_call_params["messages"] = role_correct_msgs

            if ell_call.tools:
                final_call_params["tools"] = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        name=tool.__name__,
                        description=tool.__doc__,
                        input_schema=tool.__ell_params_model__.model_json_schema(),
                    )
                    for tool in ell_call.tools
                ]

            # print(final_call_params)
            return final_call_params

        def translate_from_provider(
            self,
            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            usage = {}
            tracked_results = []
            metadata = {}

            #XXX: Support n > 0

            if provider_call_params.get("stream", False):
                content = []
                current_blocks: Dict[int, Dict[str, Any]] = {}
                message_metadata = {}

                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:
                    for chunk in stream:
                        if chunk.type == "message_start":
                            message_metadata = chunk.message.model_dump()
                            message_metadata.pop("content", None)  # Remove content as we'll build it separately

                        elif chunk.type == "content_block_start":
                            block = chunk.content_block.model_dump()
                            current_blocks[chunk.index] = block
                            if block["type"] == "tool_use":
                                if logger: logger(f" <tool_use: {block['name']}(")
                                block["input"] = "" # force it to be a string, XXX: can implement partially parsed json later.
                        elif chunk.type == "content_block_delta":
                            if chunk.index in current_blocks:
                                block = current_blocks[chunk.index]
                                if (delta := chunk.delta).type == "text_delta":
                                    block["text"] += delta.text
                                    if logger: logger(delta.text)
                                if delta.type == "input_json_delta":
                                    block["input"] += delta.partial_json
                                    if logger: logger(delta.partial_json)

                        elif chunk.type == "content_block_stop":
                            if chunk.index in current_blocks:
                                block = current_blocks.pop(chunk.index)
                                if block["type"] == "text":
                                    content.append(ContentBlock(text=_lstr(block["text"],origin_trace=origin_id)))
                                elif block["type"] == "tool_use":
                                    try:
                                        matching_tool = ell_call.get_tool_by_name(block["name"])
                                        if matching_tool:
                                            content.append(
                                                ContentBlock(
                                                    tool_call=ToolCall(
                                                        tool=matching_tool,
                                                        tool_call_id=_lstr(
                                                            block['id'],origin_trace=origin_id
                                                        ),
                                                        params=json.loads(block['input']) if block['input'] else {},
                                                    )
                                                )
                                            )
                                    except json.JSONDecodeError:
                                        if logger: logger(f" - FAILED TO PARSE JSON")
                                        pass
                                    if logger: logger(f")>")

                        elif chunk.type == "message_delta":
                            message_metadata.update(chunk.delta.model_dump())
                            if chunk.usage:
                                usage.update(chunk.usage.model_dump())

                        elif chunk.type == "message_stop":
                            tracked_results.append(Message(role="assistant", content=content))

                        # print(chunk)
                metadata = message_metadata

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("input_tokens", 0)
            usage["completion_tokens"] = usage.get("output_tokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata

    # XXX: Make a singleton.
    anthropic_provider = AnthropicProvider()
    register_provider(anthropic_provider, anthropic.Anthropic)
    register_provider(anthropic_provider, anthropic.AnthropicBedrock)
    register_provider(anthropic_provider, anthropic.AnthropicVertex)

except ImportError:
    pass

--------------------------------------------------------------------------------

providers\bedrock.py::1
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent
from ell.types._lstr import _lstr
import json
from ell.configurator import config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image
from io import BytesIO
import requests
from PIL import Image as PILImage

--------------------------------------------------------------------------------



Server and API Setup:
studio\server.py::1
from typing import Optional, Dict, Any

from sqlmodel import Session
from ell.stores.sql import PostgresStore, SQLiteStore
from ell import __version__
from fastapi import FastAPI, Query, HTTPException, Depends, Response, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import logging
import json
from ell.studio.config import Config
from ell.studio.connection_manager import ConnectionManager
from ell.studio.datamodels import InvocationPublicWithConsumes, SerializedLMPWithUses

from ell.types import SerializedLMP
from datetime import datetime, timedelta
from sqlmodel import select


logger = logging.getLogger(__name__)


from ell.studio.datamodels import InvocationsAggregate


def get_serializer(config: Config):
    if config.pg_connection_string:
        return PostgresStore(config.pg_connection_string)
    elif config.storage_dir:
        return SQLiteStore(config.storage_dir)
    else:
        raise ValueError("No storage configuration found")

--------------------------------------------------------------------------------

studio\server.py::2
def create_app(config:Config):
    serializer = get_serializer(config)

    def get_session():
        with Session(serializer.engine) as session:
            yield session

    app = FastAPI(title="ell Studio", version=__version__)

    # Enable CORS for all origins
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    manager = ConnectionManager()

    @app.websocket("/ws")
    async def websocket_endpoint(websocket: WebSocket):
        await manager.connect(websocket)
        try:
            while True:
                data = await websocket.receive_text()
                # Handle incoming WebSocket messages if needed
        except WebSocketDisconnect:
            manager.disconnect(websocket)


    @app.get("/api/latest/lmps", response_model=list[SerializedLMPWithUses])
    def get_latest_lmps(
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        session: Session = Depends(get_session)
    ):
        lmps = serializer.get_latest_lmps(
            session,
            skip=skip, limit=limit,
            )
        return lmps

    # TOOD: Create a get endpoint to efficient get on the index with /api/lmp/<lmp_id>
    @app.get("/api/lmp/{lmp_id}")
    def get_lmp_by_id(lmp_id: str, session: Session = Depends(get_session)):
        lmp = serializer.get_lmps(session, lmp_id=lmp_id)[0]
        return lmp
    # ... other code

--------------------------------------------------------------------------------

studio\server.py::3
def create_app(config:Config):
    # ... other code



    @app.get("/api/lmps", response_model=list[SerializedLMPWithUses])
    def get_lmp(
        lmp_id: Optional[str] = Query(None),
        name: Optional[str] = Query(None),
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        session: Session = Depends(get_session)
    ):

        filters : Dict[str, Any] = {}
        if name:
            filters['name'] = name
        if lmp_id:
            filters['lmp_id'] = lmp_id

        lmps = serializer.get_lmps(session, skip=skip, limit=limit, **filters)

        if not lmps:
            raise HTTPException(status_code=404, detail="LMP not found")

        print(lmps[0])
        return lmps



    @app.get("/api/invocation/{invocation_id}", response_model=InvocationPublicWithConsumes)
    def get_invocation(
        invocation_id: str,
        session: Session = Depends(get_session)
    ):
        invocation = serializer.get_invocations(session, lmp_filters=dict(), filters={"id": invocation_id})[0]
        return invocation
    # ... other code

--------------------------------------------------------------------------------

studio\server.py::4
def create_app(config:Config):
    # ... other code

    @app.get("/api/invocations", response_model=list[InvocationPublicWithConsumes])
    def get_invocations(
        id: Optional[str] = Query(None),
        hierarchical: Optional[bool] = Query(False),
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        lmp_name: Optional[str] = Query(None),
        lmp_id: Optional[str] = Query(None),
        session: Session = Depends(get_session)
    ):
        lmp_filters = {}
        if lmp_name:
            lmp_filters["name"] = lmp_name
        if lmp_id:
            lmp_filters["lmp_id"] = lmp_id

        invocation_filters = {}
        if id:
            invocation_filters["id"] = id

        invocations = serializer.get_invocations(
            session,
            lmp_filters=lmp_filters,
            filters=invocation_filters,
            skip=skip,
            limit=limit,
            hierarchical=hierarchical
        )
        return invocations
    # ... other code

--------------------------------------------------------------------------------

studio\server.py::5
def create_app(config:Config):
    # ... other code


    @app.get("/api/traces")
    def get_consumption_graph(
        session: Session = Depends(get_session)
    ):
        traces = serializer.get_traces(session)
        return traces



    @app.get("/api/blob/{blob_id}", response_class=Response)
    def get_blob(
        blob_id: str,
        session: Session = Depends(get_session)
    ):
        if serializer.blob_store is None:
            raise HTTPException(status_code=400, detail="Blob storage is not configured")
        try:
            blob_data = serializer.blob_store.retrieve_blob(blob_id)
            return Response(content=blob_data.decode('utf-8'), media_type="application/json")
        except FileNotFoundError:
            raise HTTPException(status_code=404, detail="Blob not found")
        except Exception as e:
            logger.error(f"Error retrieving blob: {str(e)}")
            raise HTTPException(status_code=500, detail="Internal server error")
    # ... other code

--------------------------------------------------------------------------------

studio\server.py::6
def create_app(config:Config):
    # ... other code

    @app.get("/api/lmp-history")
    def get_lmp_history(
        days: int = Query(365, ge=1, le=3650),  # Default to 1 year, max 10 years
        session: Session = Depends(get_session)
    ):
        # Calculate the start date
        start_date = datetime.utcnow() - timedelta(days=days)

        # Query to get all LMP creation times within the date range
        query = (
            select(SerializedLMP.created_at)
            .where(SerializedLMP.created_at >= start_date)
            .order_by(SerializedLMP.created_at)
        )

        results = session.exec(query).all()

        # Convert results to a list of dictionaries
        history = [{"date": str(row), "count": 1} for row in results]

        return history
    # ... other code

--------------------------------------------------------------------------------

studio\config.py::1
from functools import lru_cache
import os
from typing import Optional
from pydantic import BaseModel

import logging

logger = logging.getLogger(__name__)


# todo. maybe we default storage dir and other things in the future to a well-known location
# like ~/.ell or something
@lru_cache
def ell_home() -> str:
    return os.path.join(os.path.expanduser("~"), ".ell")


class Config(BaseModel):
    pg_connection_string: Optional[str] = None
    storage_dir: Optional[str] = None

    @classmethod
    def create(
        cls,
        storage_dir: Optional[str] = None,
        pg_connection_string: Optional[str] = None,
    ) -> 'Config':
        pg_connection_string = pg_connection_string or os.getenv("ELL_PG_CONNECTION_STRING")
        storage_dir = storage_dir or os.getenv("ELL_STORAGE_DIR")

        # Enforce that we use either sqlite or postgres, but not both
        if pg_connection_string is not None and storage_dir is not None:
            raise ValueError("Cannot use both sqlite and postgres")

        # For now, fall back to sqlite if no PostgreSQL connection string is provided
        if pg_connection_string is None and storage_dir is None:
            # This intends to honor the default we had set in the CLI
            storage_dir = os.getcwd()

        return cls(pg_connection_string=pg_connection_string, storage_dir=storage_dir)

--------------------------------------------------------------------------------

studio\connection_manager.py::1
from fastapi import WebSocket


class ConnectionManager:
    def __init__(self):
        self.active_connections = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            print(f"Broadcasting message to {connection} {message}")
            await connection.send_text(message)

--------------------------------------------------------------------------------

studio\datamodels.py::1
from datetime import datetime
from typing import List, Optional, Dict, Any
from sqlmodel import SQLModel
from ell.types import SerializedLMPBase, InvocationBase, InvocationContentsBase


class SerializedLMPWithUses(SerializedLMPBase):
    lmp_id : str
    uses: List[SerializedLMPBase]


class InvocationPublic(InvocationBase):
    lmp: SerializedLMPBase
    uses: List["InvocationPublicWithConsumes"]
    contents: InvocationContentsBase

class InvocationPublicWithConsumes(InvocationPublic):
    consumes: List[InvocationPublic]
    consumed_by: List[InvocationPublic]



from pydantic import BaseModel

class GraphDataPoint(BaseModel):
    date: datetime
    count: int
    avg_latency: float
    tokens: int
    # cost: float

class InvocationsAggregate(BaseModel):
    total_invocations: int
    total_tokens: int
    avg_latency: float
    # total_cost: float
    unique_lmps: int
    # successful_invocations: int
    # success_rate: float
    graph_data: List[GraphDataPoint]

--------------------------------------------------------------------------------



Serialization and Data Handling:
util\serialization.py::1
# Global converter
import base64
import hashlib
from io import BytesIO
import json
import cattrs
import numpy as np
from pydantic import BaseModel
import PIL
from ell.types._lstr import _lstr


pydantic_ltype_aware_cattr = cattrs.Converter()

def serialize_image(img):
    buffer = BytesIO()
    img.save(buffer, format="PNG")
    return "data:image/png;base64," + base64.b64encode(buffer.getvalue()).decode()


# Register hooks for complex types
pydantic_ltype_aware_cattr.register_unstructure_hook(
    np.ndarray,
    lambda arr: {
        "content": serialize_image(PIL.Image.fromarray(arr)),
        "__limage": True
    } if arr.ndim == 3 else (
        {
            "content": base64.b64encode(arr.tobytes()).decode(),
            "dtype": str(arr.dtype),
            "shape": arr.shape,
            "__lndarray": True
        }
    )
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    set,
    lambda s: list(sorted(s))
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    frozenset,
    lambda s: list(sorted(s))
)


pydantic_ltype_aware_cattr.register_unstructure_hook(
    PIL.Image.Image,
    lambda obj: {
        "content": serialize_image(obj),
        "__limage": True
    }
)

def unstructure_lstr(obj):
    return dict(content=str(obj), **obj.__dict__, __lstr=True)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    _lstr,
    unstructure_lstr
)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    BaseModel,
    lambda obj: obj.model_dump(exclude_none=True, exclude_unset=True)
)

--------------------------------------------------------------------------------

util\serialization.py::2
def get_immutable_vars(vars_dict):
    converter = cattrs.Converter()

    def handle_complex_types(obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj]
        elif isinstance(obj, dict):
            return {k: handle_complex_types(v) if not isinstance(v, (int, float, str, bool, type(None))) else v for k, v in obj.items()}
        elif isinstance(obj, (set, frozenset)):
            return list(sorted(handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj))
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return f"<Object of type {type(obj).__name__}>"

    converter.register_unstructure_hook(object, handle_complex_types)
    x = converter.unstructure(vars_dict)
    return x

--------------------------------------------------------------------------------

util\serialization.py::3
def compute_state_cache_key(ipstr, fn_closure):
    _global_free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[2]), sort_keys=True, default=repr, ensure_ascii=False)}"
    _free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[3]), sort_keys=True, default=repr, ensure_ascii=False)}"
    state_cache_key = hashlib.sha256(f"{ipstr}{_global_free_vars_str}{_free_vars_str}".encode('utf-8')).hexdigest()
    return state_cache_key

--------------------------------------------------------------------------------

util\serialization.py::4
def prepare_invocation_params(params):
    invocation_params = params

    cleaned_invocation_params = pydantic_ltype_aware_cattr.unstructure(invocation_params)

    # Thisis because we wneed the caching to work on the hash of a cleaned and serialized object.
    jstr = json.dumps(cleaned_invocation_params, sort_keys=True, default=repr, ensure_ascii=False)

    consumes = set()
    import re
    # XXX: Better than registering a hook in cattrs.
    pattern = r'"__origin_trace__":\s*"frozenset\({(.+?)}\)"'

    # Find all matches in the jstr
    matches = re.findall(pattern, jstr)

    # Process each match and add to consumes set
    for match in matches:
        # Remove quotes and spaces, then split by comma
        items = [item.strip().strip("'") for item in match.split(',')]
        consumes.update(items)
    consumes = list(consumes)
    # XXX: Only need to reload because of 'input' caching., we could skip this by making ultimate model caching rather than input hash caching; if prompt same use the same output.. irrespective of version.
    return json.loads(jstr), jstr, consumes

--------------------------------------------------------------------------------

util\serialization.py::5
def is_immutable_variable(value):
    """
    Check if a value is immutable.

    This function determines whether the given value is of an immutable type in Python.
    Immutable types are objects whose state cannot be modified after they are created.

    Args:
        value: Any Python object to check for immutability.

    Returns:
        bool: True if the value is immutable, False otherwise.

    Note:
        - This function checks for common immutable types in Python.
        - Custom classes are considered mutable unless they explicitly implement
          immutability (which this function doesn't check for).
        - For some types like tuple, immutability is shallow (i.e., the tuple itself
          is immutable, but its contents might not be).
    """
    immutable_types = (
        int, float, complex, str, bytes,
        tuple, frozenset, type(None),
        bool,  # booleans are immutable
        range,  # range objects are immutable
        slice,  # slice objects are immutable
    )

    if isinstance(value, immutable_types):
        return True

    # Check for immutable instances of mutable types
    if isinstance(value, (tuple, frozenset)):
        return all(is_immutable_variable(item) for item in value)

    return False

--------------------------------------------------------------------------------

types\message.py::5
class ImageContent(BaseModel):

    @classmethod
    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, "ImageContent"]):
        if isinstance(value, cls):
            return value

        if isinstance(value, str):
            if value.startswith('http://') or value.startswith('https://'):
                return cls(url=value)
            try:
                img_data = base64.b64decode(value)
                img = PILImage.open(BytesIO(img_data))
                if img.mode not in ('L', 'RGB', 'RGBA'):
                    return cls(image=img.convert('RGB'))
            except:
                raise ValueError("Invalid base64 string or URL for image")

        if isinstance(value, np.ndarray):
            if value.ndim == 3 and value.shape[2] in (3, 4):
                mode = 'RGB' if value.shape[2] == 3 else 'RGBA'
                return cls(image=PILImage.fromarray(value, mode=mode))
            else:
                raise ValueError(f"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.")

        if isinstance(value, PILImage.Image):
            if value.mode not in ('L', 'RGB', 'RGBA'):
                value = value.convert('RGB')
            return cls(image=value)

        raise ValueError(f"Invalid image type: {type(value)}")

    @field_serializer('image')
    def serialize_image(self, image: Optional[PILImage.Image], _info):
        if image is None:
            return None
        return serialize_image(image)

--------------------------------------------------------------------------------

types\message.py::6
class ContentBlock(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    text: Optional[_lstr_generic] = Field(default=None)
    image: Optional[ImageContent] = Field(default=None)
    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)
    tool_call: Optional[ToolCall] = Field(default=None)
    parsed: Optional[BaseModel] = Field(default=None)
    tool_result: Optional[ToolResult] = Field(default=None)
    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.
    # This breaks us maintaing parity with the openai python client in some sen but so does image.

    def __init__(self, *args, **kwargs):
        if "image" in kwargs and not isinstance(kwargs["image"], ImageContent):
            im = kwargs["image"] = ImageContent.coerce(kwargs["image"])
            # XXX: Backwards compatibility, Deprecate.
            if (d := kwargs.get("image_detail", None)): im.detail = d

        super().__init__(*args, **kwargs)


    @model_validator(mode='after')
    def check_single_non_null(self):
        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]
        if len(non_null_fields) > 1:
            raise ValueError(f"Only one field can be non-null. Found: {', '.join(non_null_fields)}")
        return self

    def __str__(self):
        return repr(self)

    def __repr__(self):
        non_null_fields = [f"{field}={value}" for field, value in self.__dict__.items() if value is not None]
        return f"ContentBlock({', '.join(non_null_fields)})"

    @property
    def type(self):
        if self.text is not None:
            return "text"
        if self.image is not None:
            return "image"
        if self.audio is not None:
            return "audio"
        if self.tool_call is not None:
            return "tool_call"
        if self.parsed is not None:
            return "parsed"
        if self.tool_result is not None:
            return "tool_result"
        return None

    @property
    def content(self):
        return getattr(self, self.type)

--------------------------------------------------------------------------------

types\message.py::8
def to_content_blocks(
    content: Optional[Union[AnyContent, List[AnyContent]]] = None,
    **content_block_kwargs
) -> List[ContentBlock]:
    """
    Coerce a variety of input types into a list of ContentBlock objects.

    Args:
    content: The content to be coerced. Can be a single item or a list of items.
             Supported types include str, ContentBlock, ToolCall, ToolResult, BaseModel, Image, np.ndarray, and PILImage.Image.
    **content_block_kwargs: Additional keyword arguments to pass to ContentBlock creation if content is None.

    Returns:
    List[ContentBlock]: A list of ContentBlock objects created from the input content.

    Examples:
    >>> coerce_content_list("Hello")
    [ContentBlock(text="Hello")]

    >>> coerce_content_list([ContentBlock(text="Hello"), "World"])
    [ContentBlock(text="Hello"), ContentBlock(text="World")]

    >>> from PIL import Image as PILImage
    >>> pil_image = PILImage.new('RGB', (100, 100))
    >>> coerce_content_list(pil_image)
    [ContentBlock(image=Image(image=<PIL.Image.Image object>))]

    >>> coerce_content_list(Image(url="https://example.com/image.jpg"))
    [ContentBlock(image=Image(url="https://example.com/image.jpg"))]

    >>> coerce_content_list(None, text="Default text")
    [ContentBlock(text="Default text")]
    """
    if content is None:
        return [ContentBlock(**content_block_kwargs)]

    if not isinstance(content, list):
        content = [content]

    return [ContentBlock.model_validate(ContentBlock.coerce(c)) for c in content]

--------------------------------------------------------------------------------

util\should_import.py::1
import importlib.util
import os
import site
import sys
import sysconfig
from pathlib import Path


def should_import(module_name: str, raise_on_error: bool = False) -> bool:
    """
    Determines whether a module should be imported based on its origin.
    Excludes local modules and standard library modules.

    Args:
        module_name (str): The name of the module to check.

    Returns:
        bool: True if the module should be imported (i.e., it's a third-party module), False otherwise.
    """
    if module_name.startswith("ell"):
        return True
    try:
        try:
            spec = importlib.util.find_spec(module_name)
        except ValueError:
            return False
        if spec is None:
            return False

        origin = spec.origin
        if origin is None:
            return False
        if spec.has_location:
            origin_path = Path(origin).resolve()

            site_packages = list(site.getsitepackages()) + (list(site.getusersitepackages()) if isinstance(site.getusersitepackages(), list) else [site.getusersitepackages()])

            additional_paths = [Path(p).resolve() for p in sys.path if Path(p).resolve() not in map(Path, site_packages)]

            project_root = Path(os.environ.get("ELL_PROJECT_ROOT", os.getcwd())).resolve()

            site_packages_paths = [Path(p).resolve() for p in site_packages]
            stdlib_path = sysconfig.get_paths().get("stdlib")
            if stdlib_path:
                site_packages_paths.append(Path(stdlib_path).resolve())

            additional_paths = [Path(p).resolve() for p in additional_paths]
            local_paths = [project_root]

            cwd = Path.cwd().resolve()
            additional_paths = [path for path in additional_paths if path != cwd]

            for pkg in site_packages_paths:
                if origin_path.is_relative_to(pkg):
                    return True

            for path in additional_paths:
                if origin_path.is_relative_to(path):
                    return False

            for local in local_paths:
                if origin_path.is_relative_to(local):
                    return False

        return True

    except Exception as e:
        if raise_on_error:
            raise e
        return True

--------------------------------------------------------------------------------

util\verbosity.py::1
import sys
import hashlib
import shutil
import textwrap
from typing import Dict, Tuple, List, Any, Optional
from colorama import Fore, Style, init
from ell.types import Message
from ell.configurator import config
import logging
from functools import lru_cache
import threading
from ell.types.message import LMP, ContentBlock, _content_to_text
import requests

from ell.util.plot_ascii import plot_ascii

# Initialize colorama
init(autoreset=True)

# Define colors and styles
ELL_COLORS = {k: v for k, v in vars(Fore).items() if k not in ['RESET', 'BLACK', 'LIGHTBLACK_EX']}
BOLD = Style.BRIGHT
UNDERLINE = '\033[4m'
RESET = Style.RESET_ALL
SYSTEM_COLOR = Fore.CYAN
USER_COLOR = Fore.GREEN
ASSISTANT_COLOR = Fore.YELLOW
PIPE_COLOR = Fore.BLUE

# Set up logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

has_logged_version_statement = False

_version_check_lock = threading.Lock()
_has_logged_version_statement = False

--------------------------------------------------------------------------------



