_____ cgraph _____

###### Cluster Eval ######
Score: 15.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\examples\future\realtimewebcam.py_cv2_


import cv2
import time
from PIL import Image
import os
from ell.util.plot_ascii import plot_ascii


def clear_console():
    os.system('cls' if os.name == 'nt' else 'clear')

def main():
    print("Press Ctrl+C to stop the program.")
    cap = cv2.VideoCapture(0)  # Change to 0 for default camera

    if not cap.isOpened():
        print("Error: Could not open camera.")
        return

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("Failed to capture image from webcam.")
                continue

            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = Image.fromarray(frame)

            # Resize the frame
            # frame = frame.resize((40*4, 30*4), Image.LANCZOS)

            ascii_image = plot_ascii(frame, width=120, color=True)
            clear_console()
            print("\n".join(ascii_image))

            # Add a small delay to control frame rate
            time.sleep(0.05)

    except KeyboardInterrupt:
        print("Program stopped by user.")
    finally:
        cap.release()

if __name__ == "__main__":
    main()
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\types\message.py__HELPERS_


# HELPERS 
def system(content: Union[AnyContent, List[AnyContent]]) -> Message:
    """
    Create a system message with the given content.

    Args:
    content (str): The content of the system message.

    Returns:
    Message: A Message object with role set to 'system' and the provided content.
    """
    return Message(role="system", content=content)


def user(content: Union[AnyContent, List[AnyContent]]) -> Message:
    """
    Create a user message with the given content.

    Args:
    content (str): The content of the user message.

    Returns:
    Message: A Message object with role set to 'user' and the provided content.
    """
    return Message(role="user", content=content)


def assistant(content: Union[AnyContent, List[AnyContent]]) -> Message:
    """
    Create an assistant message with the given content.

    Args:
    content (str): The content of the assistant message.

    Returns:
    Message: A Message object with role set to 'assistant' and the provided content.
    """
    return Message(role="assistant", content=content)

#XXX: Make a mixi for these properties.
def _content_to_text_only(content: List[ContentBlock]) -> str:
    return _lstr("\n").join(
            available_text
            for c in content
            if (available_text := (c.tool_result.text_only if c.tool_result else c.text))
        )

# Do we include the .text of a tool result? or its repr as in the current implementaiton?
# What is the user using .text for? I just want to see the result of the tools. text_only should get us the text of the tool results; the tool_call_id is irrelevant.
def _content_to_text(content: List[ContentBlock]) -> str:
    return _lstr("\n").join(
            available_text
            for c in content
            if (available_text :=  c.text or repr(c.content))
        )


# want to enable a use case where the user can actually return a standrd oai chat format
# This is a placehodler will likely come back later for this
LMPParams = Dict[str, Any]
# Well this is disappointing, I wanted to effectively type hint by doign that data sync meta, but eh, at elast we can still reference role or content this way. Probably wil lcan the dict sync meta. TypedDict is the ticket ell oh ell.
MessageOrDict = Union[Message, Dict[str, str]]
# Can support iamge prompts later.
Chat = List[
    Message
]  # [{"role": "system", "content": "prompt"}, {"role": "user", "content": "message"}]
MultiTurnLMP = Callable[..., Chat]
OneTurn = Callable[..., _lstr_generic]
# This is the specific LMP that must accept history as an argument and can take any additional arguments
ChatLMP = Callable[[Chat, Any], Chat]
LMP = Union[OneTurn, MultiTurnLMP, ChatLMP]
InvocableLM = Callable[..., _lstr_generic]
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\plot_ascii.py_sys_try_.except_FileNotFoundError_.plot_ascii.return._image_


import sys
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import logging
import os

# Load pre-rendered character bitmaps
try:
    package_dir = os.path.dirname(__file__)
    bitmaps_path = os.path.join(package_dir, 'char_bitmaps.npy')
    data = np.load(bitmaps_path, allow_pickle=True).item()
    char_bitmaps = data['char_bitmaps']
    max_char_width = data['max_char_width']
    max_char_height = data['max_char_height']


    ASCII_CHARS = " .:-=+*#%@"
    def plot_ascii(
        image: Image.Image,
        width: int = 100,
        color: bool = True,
    ):
        """
        Convert a PIL Image to ASCII art using pre-rendered character bitmaps and print it to the console with optional coloring.
        """
        num_chars = len(ASCII_CHARS)


        # Adjust the scaling factor to compensate for character aspect ratio
        scale = 0.5  # You can tweak this value based on your terminal's character dimensions
        aspect_ratio = image.height / image.width
        new_width = width
        new_height = int(aspect_ratio * new_width * (max_char_height / max_char_width) * scale)
        image = image.resize((new_width * max_char_width, new_height * max_char_height)).convert('RGB')

        # Convert image to NumPy array
        img_array = np.array(image)

        # Compute brightness using luminance formula
        luminance = 0.2126 * img_array[:, :, 0] + 0.7152 * img_array[:, :, 1] + 0.0722 * img_array[:, :, 2]

        # Normalize brightness to range 0-1
        brightness_normalized = luminance / 255

        if color:
            # Get RGB values for coloring
            r = img_array[:, :, 0]
            g = img_array[:, :, 1]
            b = img_array[:, :, 2]

        # Compute the number of blocks
        y_blocks = new_height
        x_blocks = new_width

        # Reshape brightness_normalized to (y_blocks, max_char_height, x_blocks, max_char_width)
        brightness_blocks = brightness_normalized.reshape(y_blocks, max_char_height, x_blocks, max_char_width)
        brightness_blocks = brightness_blocks.mean(axis=(1, 3))  # Average over each block

        # Normalize again if necessary
        brightness_blocks = brightness_blocks / brightness_blocks.max()

        # Vectorize the selection of ASCII characters
        indices = np.digitize(brightness_blocks, np.linspace(0, 1, num_chars)) - 1
        indices = np.clip(indices, 0, num_chars - 1)
        ascii_chars = np.array(list(ASCII_CHARS))[indices]

        if color:
            # Compute average color for each block
            r_blocks = r.reshape(y_blocks, max_char_height, x_blocks, max_char_width).mean(axis=(1, 3)).astype(int)
            g_blocks = g.reshape(y_blocks, max_char_height, x_blocks, max_char_width).mean(axis=(1, 3)).astype(int)
            b_blocks = b.reshape(y_blocks, max_char_height, x_blocks, max_char_width).mean(axis=(1, 3)).astype(int)

            # Convert RGB to 8-bit color code
            color_codes = 16 + (36 * (r_blocks // 51)) + (6 * (g_blocks // 51)) + (b_blocks // 51)
            color_codes = color_codes.astype(str)

            # Create colored ASCII characters
            colored_ascii = np.char.add(np.char.add("\033[38;5;", color_codes), "m")
            colored_ascii = np.char.add(colored_ascii, np.char.add(ascii_chars, "\033[0m"))

            # Join characters into lines
            ascii_image = ["".join(row) for row in colored_ascii]
        else:
            ascii_image = ["".join(row) for row in ascii_chars]

        # Print the ASCII image
        return ascii_image
except FileNotFoundError:
    def plot_ascii(
        image: Image.Image,
        width: int = 100,
        color: bool = True,
    ):
        return "<image>"
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\verbosity.py_wrap_text_with_prefix_wrap_text_with_prefix.return.result


def wrap_text_with_prefix(message, width: int, prefix: str, subsequent_prefix: str, text_color: str) -> List[str]:
    """Wrap text while preserving the prefix and color for each line."""
    result = []
    for i, content in enumerate(message.content):
        wrapped_lines = []
        if content.image and content.image.image:
            wrapped_lines = plot_ascii(content.image.image, min(80, width - len(prefix)))
        else:
            if content.tool_result:
                contnets_to_wrap = [ContentBlock(text=f"ToolResult(tool_call_id={content.tool_result.tool_call_id}):"), *content.tool_result.result]
            else:
                contnets_to_wrap = [content]

            wrapped_lines = []
            for c in contnets_to_wrap:
                if c.image and c.image.image:
                    block_wrapped_lines = plot_ascii(c.image.image, min(80, width - len(prefix)))
                else:
                    text = _content_to_text([c])
                    paragraphs = text.split('\n')
                    wrapped_paragraphs = [textwrap.wrap(p, width=width - len(prefix)) for p in paragraphs]
                    block_wrapped_lines = [line for paragraph in wrapped_paragraphs for line in paragraph]
                wrapped_lines.extend(block_wrapped_lines)
        if i == 0:
            if wrapped_lines:
                result.append(f"{prefix}{text_color}{wrapped_lines[0]}{RESET}")
            else:
                result.append(f"{prefix}{text_color}{RESET}")
        else:
            result.append(f"{subsequent_prefix}{text_color}{wrapped_lines[0]}{RESET}")
        result.extend([f"{subsequent_prefix}{text_color}{line}{RESET}" for line in wrapped_lines[1:]])
    return result
====================================================================


###### Cluster Eval ######
Score: 14.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\examples\audio_example.py_asyncio_atexit_register_cleanup_


import asyncio
import base64
import os
from pydub import AudioSegment
import numpy as np
import sounddevice as sd
import threading
import queue
from openai_realtime import RealtimeClient, RealtimeUtils

# Helper function to load and convert audio files
def load_audio_sample(file_path):
    audio = AudioSegment.from_file(file_path)
    samples = np.array(audio.get_array_of_samples())
    return RealtimeUtils.array_buffer_to_base64(samples)

# Function to play audio with buffering
def play_audio(audio_data, sample_rate=24000):
    audio_queue.put(audio_data)


def audio_playback_worker():
    with sd.OutputStream(samplerate=sample_rate, channels=1, dtype='int16') as stream:
        while not stop_event.is_set():
            try:
                data = audio_queue.get(timeout=0.1)
                stream.write(data)
            except queue.Empty:
                continue

# Initialize buffer and threading components
audio_queue = queue.Queue()
stop_event = threading.Event()
sample_rate = 24000  # Ensure this matches your actual sample rate

# Start the background thread for audio playback
playback_thread = threading.Thread(target=audio_playback_worker, daemon=True)
playback_thread.start()

# Ensure to stop the thread gracefully on exit
import atexit
def cleanup():
    stop_event.set()
    playback_thread.join()

atexit.register(cleanup)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\src\openai_realtime\api.py_RealtimeAPI.send_RealtimeAPI.send.return.True


class RealtimeAPI(RealtimeEventHandler):

    def send(self, event_name, data=None):
        if not self.is_connected():
            raise Exception("RealtimeAPI is not connected")

        data = data or {}
        if not isinstance(data, dict):
            raise ValueError("data must be a dictionary")

        event = {
            "event_id": RealtimeUtils.generate_id("evt_"),
            "type": event_name,
            **data
        }

        self.dispatch(f"client.{event_name}", event)
        self.dispatch("client.*", event)
        self.log("sent:", event_name, event)

        asyncio.create_task(self.ws.send(json.dumps(event, ensure_ascii=False)))
        return True
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\src\openai_realtime\client.py_RealtimeClient.send_user_message_content_RealtimeClient.send_user_message_content.return.True


class RealtimeClient(RealtimeEventHandler):

    def send_user_message_content(self, content=None):
        content = content or []
        for c in content:
            if c['type'] == 'input_audio':
                if isinstance(c['audio'], (np.ndarray, bytes)):
                    c['audio'] = RealtimeUtils.array_buffer_to_base64(c['audio'])
        if content:
            self.realtime.send('conversation.item.create', {
                'item': {
                    'type': 'message',
                    'role': 'user',
                    'content': content
                }
            })
        self.create_response()
        return True
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\src\openai_realtime\client.py_RealtimeClient.append_input_audio_RealtimeClient.create_response.return.True


class RealtimeClient(RealtimeEventHandler):

    def append_input_audio(self, array_buffer):
        if len(array_buffer) > 0:
            self.realtime.send('input_audio_buffer.append', {
                'audio': RealtimeUtils.array_buffer_to_base64(array_buffer)
            })
            self.input_audio_buffer = RealtimeUtils.merge_int16_arrays(
                self.input_audio_buffer,
                array_buffer
            )
        return True

    def create_response(self):
        if self.get_turn_detection_type() is None and len(self.input_audio_buffer) > 0:
            self.realtime.send('input_audio_buffer.commit')
            self.conversation.queue_input_audio(self.input_audio_buffer)
            self.input_audio_buffer = np.array([], dtype=np.int16)
        self.realtime.send('response.create')
        return True
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\src\openai_realtime\utils.py_base64_RealtimeUtils.generate_id.return.prefix_join_random_c


import base64
import numpy as np

class RealtimeUtils:
    @staticmethod
    def float_to_16bit_pcm(float32_array):
        int16_array = (np.clip(float32_array, -1, 1) * 32767).astype(np.int16)
        return int16_array.tobytes()

    @staticmethod
    def base64_to_array_buffer(base64_string):
        return base64.b64decode(base64_string)

    @staticmethod
    def array_buffer_to_base64(array_buffer):
        if isinstance(array_buffer, np.ndarray):
            if array_buffer.dtype == np.float32:
                array_buffer = RealtimeUtils.float_to_16bit_pcm(array_buffer)
            elif array_buffer.dtype == np.int16:
                array_buffer = array_buffer.tobytes()
        return base64.b64encode(array_buffer).decode('utf-8')

    @staticmethod
    def merge_int16_arrays(left, right):
        if isinstance(left, bytes):
            left = np.frombuffer(left, dtype=np.int16)
        if isinstance(right, bytes):
            right = np.frombuffer(right, dtype=np.int16)
        if not isinstance(left, np.ndarray) or not isinstance(right, np.ndarray):
            raise ValueError("Both items must be numpy arrays or bytes objects")
        return np.concatenate((left, right))

    @staticmethod
    def generate_id(prefix, length=21):
        import random
        chars = '123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz'
        return prefix + ''.join(random.choice(chars) for _ in range(length - len(prefix)))
====================================================================


###### Cluster Eval ######
Score: 13.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\lmp\_track.py__serialize_lmp__serialize_lmp.func._has_serialized_lmp_8.True


def _serialize_lmp(func):
    # Serialize deptjh first all fo the used lmps.
    for f in func.__ell_uses__:
        _serialize_lmp(f)

    if getattr(func, "_has_serialized_lmp", False):
        return
    func._has_serialized_lmp = False
    fn_closure = func.__ell_closure__
    lmp_type = func.__ell_type__
    name = func.__qualname__
    api_params = getattr(func, "__ell_api_params__", None)

    lmps = config.store.get_versions_by_fqn(fqn=name)
    version = 0
    already_in_store = any(lmp.lmp_id == func.__ell_hash__ for lmp in lmps)

    if not already_in_store:
        commit = None
        if lmps:
            latest_lmp = max(lmps, key=lambda x: x.created_at)
            version = latest_lmp.version_number + 1
            if config.autocommit:
                # XXX: Move this out to autocommit itself.
                if not _autocommit_warning():
                    from ell.util.differ import write_commit_message_for_diff
                    commit = str(write_commit_message_for_diff(
                    f"{latest_lmp.dependencies}\n\n{latest_lmp.source}", 
                        f"{fn_closure[1]}\n\n{fn_closure[0]}")[0])

        serialized_lmp = SerializedLMP(
            lmp_id=func.__ell_hash__,
            name=name,
            created_at=utc_now(),
            source=fn_closure[0],
            dependencies=fn_closure[1],
            commit_message=commit,
            initial_global_vars=get_immutable_vars(fn_closure[2]),
            initial_free_vars=get_immutable_vars(fn_closure[3]),
            lmp_type=lmp_type,
            api_params=api_params if api_params else None,
            version_number=version,
        )
        config.store.write_lmp(serialized_lmp, [f.__ell_hash__ for f in func.__ell_uses__])
    func._has_serialized_lmp = True
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\lmp\_track.py__write_invocation_


def _write_invocation(func, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                     state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id):

    invocation_contents = InvocationContents(
        invocation_id=invocation_id,
        params=cleaned_invocation_params,
        results=result,
        invocation_api_params=invocation_api_params,
        global_vars=get_immutable_vars(func.__ell_closure__[2]),
        free_vars=get_immutable_vars(func.__ell_closure__[3])
    )

    if invocation_contents.should_externalize and config.store.has_blob_storage:
        invocation_contents.is_external = True

        # Write to the blob store 
        blob_id = config.store.blob_store.store_blob(
            json.dumps(invocation_contents.model_dump(
            ), default=str, ensure_ascii=False).encode('utf-8'),
            invocation_id
        )
        invocation_contents = InvocationContents(
            invocation_id=invocation_id,
            is_external=True,
        )

    invocation = Invocation(
        id=invocation_id,
        lmp_id=func.__ell_hash__,
        created_at=utc_now(),
        latency_ms=latency_ms,
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        state_cache_key=state_cache_key,
        used_by_id=parent_invocation_id,
        contents=invocation_contents
    )

    config.store.write_invocation(invocation, consumes)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\types\studio.py_from_datetime_import_date_utc_now.return.datetime_now_tz_timezone_


from datetime import datetime, timezone
import enum
from functools import cached_property

import sqlalchemy.types as types

from ell.types.message import Any, Any, Field, Message, Optional

from sqlmodel import Column, Field, SQLModel
from typing import Optional
from dataclasses import dataclass
from typing import Dict, List, Literal, Union, Any, Optional

from pydantic import BaseModel, field_validator

from datetime import datetime
from typing import Any, List, Optional
from sqlmodel import Field, SQLModel, Relationship, JSON, Column
from sqlalchemy import Index, func

from typing import TypeVar, Any

def utc_now() -> datetime:
    """
    Returns the current UTC timestamp.
    Serializes to ISO-8601.
    """
    return datetime.now(tz=timezone.utc)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\differ.py_from_ell_configurator_imp_write_commit_message_for_diff.return.f_Write_a_commit_messag


from ell.configurator import config
from ell.lmp.simple import simple
import difflib

# Todo: update this for single change stuff so that it doesn't summarize small chage but says it specifically.
@simple(config.autocommit_model, temperature=0.2, exempt_from_tracking=True, max_tokens=500)
def write_commit_message_for_diff(old : str, new : str) -> str:
    """You are an expert programmer whose goal is to write commit messages based on diffs.
You will be given two version of source code and their unified diff.
You will be expected to write a commit message that describes the changes between the two versions.

Follow these guidelines:
1. Your commit message should be at most one sentence and highly specific to the changes made. Don't just discuss the functions changed but how they were specifically changed.
2. Your commit message cannot be more than 10 words so use sentence fragments and be concise.
3. The @ell.simple decorator turns a function into a call to a language model program: the function's docstring is the system prompt and the string returned is the user prompt. 
4. It is extremely important that if the system prompt or user prompt changes, your commit message must say what specifically changed, rather than vaguely saying they were updated or changed.
5. It is extremely important that you never refer to a @ell.simple docstring as a docstring: it is a system prompt. 
6. Do NOT say why a change was done, say what specifically changed.
7. Consider all changes ot the program including the globals and free variables

Example response:
'''
Update model temperature and refine system prompt wording:
* Changed temperature from 0.5 to 0.7.
* Updated "with specificity, brevity, and good grammar" to "clearly and concisely" in system prompt.
* The `questions` param was assigned type List[Question]
'''
Response format:
<Short commit message summarizing all the changes with specificity>:
* <Bulleted list of each specific change>.
"""
    clean_program_of_all_bv_tags = lambda program : program.replace("# <BV>", "").replace("# </BV>", "").replace("# <BmV>", "").replace("# </BmV>", "")
    old_clean = clean_program_of_all_bv_tags(old)
    new_clean = clean_program_of_all_bv_tags(new)

    diff = "\n".join(difflib.unified_diff(old_clean.splitlines(), new_clean.splitlines(), lineterm=''))

    return f"""Write a commit message succinctly and specifically describing the changes between these two versions of a program.
Old version:
```
{old_clean}
```

New version:
```
{new_clean}
```

Unified diff:
{diff}
"""
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\serialization.py_get_immutable_vars_get_immutable_vars.return.x


def get_immutable_vars(vars_dict):
    converter = cattrs.Converter()

    def handle_complex_types(obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj]
        elif isinstance(obj, dict):
            return {k: handle_complex_types(v) if not isinstance(v, (int, float, str, bool, type(None))) else v for k, v in obj.items()}
        elif isinstance(obj, (set, frozenset)):
            return list(sorted(handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj))
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return f"<Object of type {type(obj).__name__}>"

    converter.register_unstructure_hook(object, handle_complex_types)
    x = converter.unstructure(vars_dict)
    return x
====================================================================


###### Cluster Eval ######
Score: 13.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\lmp\complex.py_complex_complex.parameterized_lm_decorator.model_call.return.result_final_api_params_


def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    default_client_from_decorator = client
    default_model_from_decorator = model
    default_api_params_from_decorator = api_params
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        _warnings(model, prompt, default_client_from_decorator)

        @wraps(prompt)
        def model_call(
            *prompt_args,
            _invocation_origin : Optional[str] = None,
            client: Optional[Any] = None,
            api_params: Optional[Dict[str, Any]] = None,
            lm_params: Optional[DeprecationWarning] = None,
            **prompt_kwargs,
        ) -> Tuple[Any, Any, Any]:
            # XXX: Deprecation in 0.1.0
            if lm_params:
                raise DeprecationWarning("lm_params is deprecated. Use api_params instead.")

            # promt -> str
            res = prompt(*prompt_args, **prompt_kwargs)
            # Convert prompt into ell messages
            messages = _get_messages(res, prompt)

            # XXX: move should log to a logger.
            should_log = not exempt_from_tracking and config.verbose
            # Cute verbose logging.
            if should_log: model_usage_logger_pre(prompt, prompt_args, prompt_kwargs, "[]", messages) #type: ignore

            # Call the model.
            # Merge API params
            merged_api_params = {**config.default_api_params, **default_api_params_from_decorator, **(api_params or {})}
            n = merged_api_params.get("n", 1)
            # Merge client overrides & client registry
            merged_client = _client_for_model(model, client or default_client_from_decorator)
            ell_call = EllCallParams(
                # XXX: Could change behaviour of overriding ell params for dyanmic tool calls.
                model=merged_api_params.pop("model", default_model_from_decorator),
                messages=messages,
                client = merged_client,
                api_params=merged_api_params,
                tools=tools or [],
            )
            # Get the provider for the model
            provider = config.get_provider_for(ell_call.client)
            assert provider is not None, f"No provider found for client {ell_call.client}."

            if should_log: model_usage_logger_post_start(n)
            with model_usage_logger_post_intermediate(n) as _logger:
                (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)
                if isinstance(result, list) and len(result) == 1:
                    result = result[0]

            result = post_callback(result) if post_callback else result
            if should_log:
                model_usage_logger_post_end()
            #
            #  These get sent to track. This is wack.           
            return result, final_api_params, metadata
        # ... other code
    # ... other code
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\lmp\simple.py_from_functools_import_wra_simple.return.complex_model_client_e


from functools import wraps
from typing import Any, Optional

from ell.lmp.complex import complex


def simple(model: str, client: Optional[Any] = None,  exempt_from_tracking=False, **api_params):
    assert 'tools' not in api_params, "tools are not supported in lm decorator, use multimodal decorator instead"
    assert 'tool_choice' not in api_params, "tool_choice is not supported in lm decorator, use multimodal decorator instead"
    assert 'response_format' not in api_params or isinstance(api_params.get('response_format', None), dict), "response_format is not supported in lm decorator, use multimodal decorator instead"

    def convert_multimodal_response_to_lstr(response):
        return [x.content[0].text for x in response] if isinstance(response, list) else response.content[0].text
    return complex(model, client,  exempt_from_tracking=exempt_from_tracking, **api_params, post_callback=convert_multimodal_response_to_lstr)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\_warnings.py__warnings_


def _warnings(model, fn, default_client_from_decorator):

        if not default_client_from_decorator:
            # Check to see if the model is registered and warn the user we're gonna defualt to OpenAI.

            if model not in config.registry:
                logger.warning(f"""{Fore.LIGHTYELLOW_EX}WARNING: Model `{model}` is used by LMP `{fn.__name__}` but no client could be found that supports `{model}`. Defaulting to use the OpenAI client `{config.default_client}` for `{model}`. This is likely because you've spelled the model name incorrectly or are using a newer model from a provider added after this ell version was released. 
                            
* If this is a mistake either specify a client explicitly in the decorator:
```python
import ell
ell.simple(model, client=my_client)
def {fn.__name__}(...):
    ...
```
or explicitly specify the client when the calling the LMP:

```python
ell.simple(model, client=my_client)(...)
```
{Style.RESET_ALL}""")
            elif (client_to_use := config.registry[model].default_client) is None or not client_to_use.api_key:
                logger.warning(_no_api_key_warning(model, fn.__name__, client_to_use, long=False))


def _autocommit_warning():
    if (config.get_client_for(config.autocommit_model)[0] is None):
        logger.warning(f"{Fore.LIGHTYELLOW_EX}WARNING: Autocommit is enabled but no client found for autocommit model '{config.autocommit_model}'. Commit messages will not be written.{Style.RESET_ALL}")
        return True
    return False
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\verbosity.py_model_usage_logger_pre_model_usage_logger_pre.print_wrapped_messages_me


def model_usage_logger_pre(
    invoking_lmp: LMP,
    lmp_args: Tuple,
    lmp_kwargs: Dict,
    lmp_hash: str,
    messages: List[Message],
    arg_max_length: int = 8
):
    """Log model usage before execution with customizable argument display length and ASCII box."""
    color =  compute_color(invoking_lmp)
    formatted_args = [format_arg(arg, arg_max_length) for arg in lmp_args]
    formatted_kwargs = [format_kwarg(key, lmp_kwargs[key], arg_max_length) for key in lmp_kwargs]
    formatted_params = ', '.join(formatted_args + formatted_kwargs)

    check_version_and_log()

    terminal_width = get_terminal_width()

    logger.info(f"Invoking LMP: {invoking_lmp.__name__} (hash: {lmp_hash[:8]})")

    print(f"{PIPE_COLOR}╔{'═' * (terminal_width - 2)}╗{RESET}")
    print(f"{PIPE_COLOR}║ {color}{BOLD}{UNDERLINE}{invoking_lmp.__name__}{RESET}{color}({formatted_params}){RESET}")
    print(f"{PIPE_COLOR}╠{'═' * (terminal_width - 2)}╣{RESET}")
    print(f"{PIPE_COLOR}║ {BOLD}Prompt:{RESET}")
    print(f"{PIPE_COLOR}╟{'─' * (terminal_width - 2)}╢{RESET}")

    max_role_length = max(len("assistant"), max(len(message.role) for message in messages))
    print_wrapped_messages(messages, max_role_length, color)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\verbosity.py_model_usage_logger_post_intermediate_model_usage_logger_post_intermediate.try_.finally_.pass


@contextmanager
def model_usage_logger_post_intermediate( n: int = 1):
    """Context manager to log intermediate model output without wrapping, only indenting if necessary."""
    terminal_width = get_terminal_width()
    prefix = f"{PIPE_COLOR}│   "
    subsequent_prefix = f"{PIPE_COLOR}│   {' ' * (len('assistant: '))}"
    chars_printed = len(subsequent_prefix)

    def log_stream_chunk(stream_chunk: str , is_refusal: bool = False):
        nonlocal chars_printed
        if stream_chunk:
            lines = stream_chunk.split('\n')
            for i, line in enumerate(lines):
                if chars_printed + len(line) > terminal_width - 6:
                    print()
                    if i == 0:
                        print(subsequent_prefix, end='')
                        chars_printed = len(prefix)
                    else:
                        print(subsequent_prefix, end='')
                        chars_printed = len(subsequent_prefix)
                    print(line.lstrip(), end='')
                else:
                    print(line, end='')
                chars_printed += len(line)

                if i < len(lines) - 1:
                    print()
                    print(subsequent_prefix, end='')
                    chars_printed = len(subsequent_prefix)  # Reset for new line
            sys.stdout.flush()

    try:
        yield log_stream_chunk
    finally:
        pass
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\verbosity.py_model_usage_logger_post_end_set_log_level.logger_setLevel_numeric_l


def model_usage_logger_post_end():
    """Log the end of model output with ASCII box closure."""
    terminal_width = get_terminal_width()
    print(f"\n{PIPE_COLOR}╚{'═' * (terminal_width - 2)}╝{RESET}")

def set_log_level(level: str):
    """Set the logging level."""
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f'Invalid log level: {level}')
    logger.setLevel(numeric_level)
====================================================================


###### Cluster Eval ######
Score: 13.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\closure.py__process_signature_dependency__process_signature_dependency.if_name_not_in_FORBIDDEN_.try_.except_Exception_as_e_._raise_error_f_Failed_to_


def _process_signature_dependency(val, dependencies, already_closed, recursion_stack, uses, name: Optional[str] = None):
    # Todo: Build general cattr like utility for unstructuring python objects with hooks that keep track of state variables.
    # Todo: break up closure into types and functions.
    # XXX: This is not exhaustive, we should determine should import on all dependencies

    if name not in FORBIDDEN_NAMES:
        try:
            dep = None
            _uses = None
            if isinstance(val, (types.FunctionType, types.MethodType)):
                dep, _, _uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            elif isinstance(val, (list, tuple, set)):
                for item in val:
                    _process_signature_dependency(item, dependencies, already_closed, recursion_stack, uses)
            else:
                val_class = val if isinstance(val, type) else val.__class__
                try:
                    is_builtin = (val_class.__module__ == "builtins" or val_class.__module__ == "__builtins__")
                except:
                    is_builtin = False

                if not is_builtin:
                    if should_import(val_class.__module__):
                        dependencies.append(dill.source.getimport(val_class, alias=val_class.__name__))
                    else:
                        dep, _, _uses = lexical_closure(val_class, already_closed=already_closed, recursion_stack=recursion_stack.copy())

            if dep: dependencies.append(dep)
            if _uses: uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of parameter or annotation {name}", e, recursion_stack)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\closure.py__process_variable__process_variable.if_isinstance_var_value_.else_._process_other_variable_v


def _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack , uses):
    """Process a single variable."""
    try:
        name = inspect.getmodule(var_value).__name__
        if should_import(name):
            imports.append(dill.source.getimport(var_value, alias=var_name))
            return
    except:
        pass

    if isinstance(var_value, (types.FunctionType, type, types.MethodType)):
        _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses)
    elif isinstance(var_value, types.ModuleType):
        _process_module(var_name, var_value, modules, imports, uses)
    elif isinstance(var_value, types.BuiltinFunctionType):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        _process_other_variable(var_name, var_value, dependencies, uses)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\closure.py__process_module__process_other_variable.if_isinstance_var_value_.else_.dependencies_append_f_


def _process_module(var_name, var_value, modules, imports, uses):
    """Process a module."""
    if should_import(var_value.__name__):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        modules.append((var_name, var_value))

def _process_other_variable(var_name, var_value, dependencies, uses):
    """Process variables that are not callables or modules."""
    if isinstance(var_value, str) and '\n' in var_value:
        dependencies.append(f"{var_name} = '''{var_value}'''")
    elif is_immutable_variable(var_value):
        dependencies.append(f"# <BV>\n{var_name} = {repr(var_value)}\n# </BV>")
    else:
        dependencies.append(f"# <BmV>\n{var_name} = <{type(var_value).__name__} object>\n# </BmV>")
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\serialization.py_is_immutable_variable_is_immutable_variable.return.False


def is_immutable_variable(value):
    """
    Check if a value is immutable.

    This function determines whether the given value is of an immutable type in Python.
    Immutable types are objects whose state cannot be modified after they are created.

    Args:
        value: Any Python object to check for immutability.

    Returns:
        bool: True if the value is immutable, False otherwise.

    Note:
        - This function checks for common immutable types in Python.
        - Custom classes are considered mutable unless they explicitly implement
          immutability (which this function doesn't check for).
        - For some types like tuple, immutability is shallow (i.e., the tuple itself
          is immutable, but its contents might not be).
    """
    immutable_types = (
        int, float, complex, str, bytes,
        tuple, frozenset, type(None),
        bool,  # booleans are immutable
        range,  # range objects are immutable
        slice,  # slice objects are immutable
    )

    if isinstance(value, immutable_types):
        return True

    # Check for immutable instances of mutable types
    if isinstance(value, (tuple, frozenset)):
        return all(is_immutable_variable(item) for item in value)

    return False
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\should_import.py_importlib.util_should_import.try_.except_Exception_as_e_.return.True


import importlib.util
import os
import site
import sys
import sysconfig
from pathlib import Path


def should_import(module_name: str, raise_on_error: bool = False) -> bool:
    """
    Determines whether a module should be imported based on its origin.
    Excludes local modules and standard library modules.

    Args:
        module_name (str): The name of the module to check.

    Returns:
        bool: True if the module should be imported (i.e., it's a third-party module), False otherwise.
    """
    if module_name.startswith("ell"):
        return True
    try:
        try:
            spec = importlib.util.find_spec(module_name)
        except ValueError:
            return False
        if spec is None:
            return False

        origin = spec.origin
        if origin is None:
            return False
        if spec.has_location:
            origin_path = Path(origin).resolve()

            site_packages = list(site.getsitepackages()) + (list(site.getusersitepackages()) if isinstance(site.getusersitepackages(), list) else [site.getusersitepackages()])

            additional_paths = [Path(p).resolve() for p in sys.path if Path(p).resolve() not in map(Path, site_packages)]

            project_root = Path(os.environ.get("ELL_PROJECT_ROOT", os.getcwd())).resolve()

            site_packages_paths = [Path(p).resolve() for p in site_packages]
            stdlib_path = sysconfig.get_paths().get("stdlib")
            if stdlib_path:
                site_packages_paths.append(Path(stdlib_path).resolve())

            additional_paths = [Path(p).resolve() for p in additional_paths]
            local_paths = [project_root]

            cwd = Path.cwd().resolve()
            additional_paths = [path for path in additional_paths if path != cwd]

            for pkg in site_packages_paths:
                if origin_path.is_relative_to(pkg):
                    return True

            for path in additional_paths:
                if origin_path.is_relative_to(path):
                    return False

            for local in local_paths:
                if origin_path.is_relative_to(local):
                    return False

        return True

    except Exception as e:
        if raise_on_error:
            raise e
        return True
====================================================================


###### Cluster Eval ######
Score: 13.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\examples\audio_example.py_main_if___name_____main___.asyncio_run_main_


async def main():
    # Initialize the RealtimeClient
    client = RealtimeClient(
        api_key=os.getenv("OPENAI_API_KEY"),
        debug=False
    )

    # Update session with instructions
    client.update_session(
        instructions=(
            ""
        ),
        output_audio_format='pcm16'  # Ensure we get PCM audio output
    )

    # Set up event handler for audio playback
    @client.realtime.on('server.response.audio.delta')
    def handle_audio_delta(event):
        audio_data = np.frombuffer(base64.b64decode(event['delta']), dtype=np.int16)
        audio_queue.put(audio_data)

    @client.realtime.on('server.response.text.delta')
    def handle_text_delta(event):
        print(event['delta'], end='', flush=True)

    # Connect to the RealtimeClient
    await client.connect()
    print("Connected to RealtimeClient")

    # Wait for session creation
    await client.wait_for_session_created()
    print("Session created")

    # Load audio sample
    audio_file_path = './tests/samples/toronto.mp3'
    audio_sample = load_audio_sample(audio_file_path)

    # Send audio content
    content = [{'type': 'input_audio', 'audio': audio_sample}]
    client.send_user_message_content(content)
    print("Audio sent")


    # Wait for and print the assistant's response transcript which happens a bit after the audio is played
    assistant_item = await client.wait_for_next_completed_item()
    print("Assistant's response:", assistant_item)
    client.send_user_message_content(content)
    print("Text sent")
    assistant_item = await client.wait_for_next_completed_item()
    print("Assistant's response:", assistant_item)

    assistant_item = await client.wait_for_next_completed_item()
    print("Assistant's response:", assistant_item)
    # Disconnect from the client
    client.disconnect()
    print("Disconnected from RealtimeClient")

if __name__ == "__main__":
    asyncio.run(main())
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\examples\chat_assistant_clone.py_RealtimeAssistant.initialize_RealtimeAssistant.initialize.self__setup_event_handler


class RealtimeAssistant:

    async def initialize(self):
        self.main_event_loop = asyncio.get_running_loop()
        self.client = RealtimeClient(api_key=self.api_key, debug=self.debug)
        self.client.update_session(
            instructions=self.instructions,
            output_audio_format='pcm16',
            input_audio_format='pcm16',
            turn_detection={
                'type': 'server_vad',
                'threshold': 0.5,
                'prefix_padding_ms': 300,
                'silence_duration_ms': 300,
            }
        )
        self._setup_event_handlers()
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\examples\discord_gpt4o.py_DiscordRealtimeAssistant.initialize_DiscordRealtimeAssistant.initialize.self__setup_event_handler


class DiscordRealtimeAssistant(commands.Cog):

    async def initialize(self):
        self.client = RealtimeClient(api_key=self.api_key, debug=self.debug, instructions=self.instructions)
        self.client.update_session(
            output_audio_format='pcm16',
            input_audio_format='pcm16',
            input_audio_transcription={
                'enabled': True,
                'model': 'whisper-1'
            },
            turn_detection={
                'type': 'server_vad',
                'threshold': 0.5,
                'prefix_padding_ms': 300,
                'silence_duration_ms': 300,
            }
        )
        self._setup_event_handlers()
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\src\openai_realtime\api.py_asyncio_RealtimeAPI.log.return.True


import asyncio
import json
import websockets
from .event_handler import RealtimeEventHandler
from .utils import RealtimeUtils

class RealtimeAPI(RealtimeEventHandler):
    def __init__(self, url=None, api_key=None, dangerously_allow_api_key_in_browser=False, debug=False):
        super().__init__()
        self.default_url = 'wss://api.openai.com/v1/realtime'
        self.url = url or self.default_url
        self.api_key = api_key
        self.debug = debug
        self.ws = None

    def is_connected(self):
        return self.ws is not None and self.ws.open

    def log(self, *args):
        if self.debug:
            print(*args)
        return True
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\src\openai_realtime\client.py_asyncio_RealtimeClient._reset_config.return.True


import asyncio
import numpy as np
from .event_handler import RealtimeEventHandler
from .api import RealtimeAPI
from .conversation import RealtimeConversation
from .utils import RealtimeUtils
import json

class RealtimeClient(RealtimeEventHandler):
    def __init__(self, url=None, api_key=None, instructions='', dangerously_allow_api_key_in_browser=False, debug=False):
        super().__init__()
        self.default_session_config = {
            'modalities': ['text', 'audio'],
            'instructions': instructions,
            'voice': 'alloy',
            'input_audio_format': 'pcm16',
            'output_audio_format': 'pcm16',
            'input_audio_transcription': None,
            'turn_detection': None,
            'tools': [],
            'tool_choice': 'auto',
            'temperature': 0.8,
            'max_response_output_tokens': 4096,
        }
        self.session_config = {}
        self.transcription_models = [{'model': 'whisper-1'}]
        self.default_server_vad_config = {
            'type': 'server_vad',
            'threshold': 0.5,
            'prefix_padding_ms': 300,
            'silence_duration_ms': 200,
        }
        self.realtime = RealtimeAPI(url, api_key, dangerously_allow_api_key_in_browser, debug)
        self.conversation = RealtimeConversation()
        self._reset_config()
        self._add_api_event_handlers()

    def _reset_config(self):
        self.session_created = False
        self.tools = {}
        self.session_config = self.default_session_config.copy()
        self.input_audio_buffer = np.array([], dtype=np.int16)
        return True
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\x\openai_realtime\src\openai_realtime\conversation.py_np_RealtimeConversation.get_items.return.self_items_copy_


import numpy as np
import json
from .utils import RealtimeUtils
import copy

class RealtimeConversation:
    def __init__(self):
        self.default_frequency = 24000  # 24,000 Hz
        self.clear()

    def clear(self):
        self.item_lookup = {}
        self.items = []
        self.response_lookup = {}
        self.responses = []
        self.queued_speech_items = {}
        self.queued_transcript_items = {}
        self.queued_input_audio = None
        return True

    def queue_input_audio(self, input_audio):
        self.queued_input_audio = input_audio
        return input_audio

    def process_event(self, event, *args):
        if 'event_id' not in event:
            raise ValueError("Missing 'event_id' on event")
        if 'type' not in event:
            raise ValueError("Missing 'type' on event")

        event_processor = getattr(self, f"_process_{event['type'].replace('.', '_')}", None)
        if not event_processor:
            raise ValueError(f"Missing conversation event processor for '{event['type']}'")

        return event_processor(event, *args)

    def get_item(self, id):
        return self.item_lookup.get(id)

    def get_items(self):
        return self.items.copy()
====================================================================


###### Cluster Eval ######
Score: 12.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\examples\providers\instructor_ex.py_InstructorProvider.translate_from_provider_InstructorProvider.translate_from_provider.return.Message_role_assistant_


class InstructorProvider(OpenAIProvider):

    def translate_from_provider(self,provider_response,
            ell_call : EllCallParams,
            provider_call_params : Dict[str, Any],
            origin_id : str, 
            logger : Optional[Callable] = None) -> Tuple[Message, Metadata]:
        """This translates the provider response (the result of calling client.chat.completions.create with the parameters from translate_to_provider)
          to an an ell message. In this case instructor just returns a pydantic type which we can use to create an ell response model. """
        instructor_response = cast(BaseModel, provider_response) # This just means that the type is a pydantic BaseModel. 
        if logger: logger(instructor_response.model_dump_json()) # Don't forget to log for verbose mode!
        return Message(role="assistant", content=ContentBlock(parsed=instructor_response)), {}
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\lmp\tool.py_tool_tool.tool_decorator.wrapper.if__tool_call_id_.else_.return.result__invocation_api_p


def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        _under_fn = fn

        @wraps(fn)
        def wrapper(
            *fn_args,
            _invocation_origin: str = None,
            _tool_call_id: str = None,
            **fn_kwargs
        ):
            #XXX: Post release, we need to wrap all tool arguments in type primitives for tracking I guess or change that tool makes the tool function inoperable.
            #XXX: Most people are not going to manually try and call the tool without a type primitive and if they do it will most likely be wrapped with l strs.

            if config.verbose and not exempt_from_tracking:
                pass
                # tool_usage_logger_pre(fn, fn_args, fn_kwargs, name, color)

            result = fn(*fn_args, **fn_kwargs)

            _invocation_api_params = dict(tool_kwargs=tool_kwargs)

            # Here you might want to add logic for tracking the tool usage
            # Similar to how it's done in the lm decorator # Use _invocation_origin

            if isinstance(result, str) and _invocation_origin:
                result = _lstr(result,origin_trace=_invocation_origin)

            #XXX: This _tool_call_id thing is a hack. Tracking should happen via params in the api
            # So if you call wiuth a _tool_callId
            if _tool_call_id:
                # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
                try:
                    if isinstance(result, ContentBlock):
                        content_results = [result]
                    elif isinstance(result, list) and all(isinstance(c, ContentBlock) for c in result):
                        content_results = result
                    else:
                        content_results = [ContentBlock(text=_lstr(json.dumps(result, ensure_ascii=False),origin_trace=_invocation_origin))]
                except TypeError as e:
                    raise TypeError(f"Failed to convert tool use result to ContentBlock: {e}. Tools must return json serializable objects. or a list of ContentBlocks.")
                # XXX: Need to support images and other content types somehow. We should look for images inside of the the result and then go from there.
                # try:
                #     content_results = coerce_content_list(result)
                # except ValueError as e:

                # TODO: poolymorphic validation here is important (cant have tool_call or formatted_response in the result)
                # XXX: Should we put this coercion here or in the tool call/result area.
                for c in content_results:
                    assert not c.tool_call, "Tool call in tool result"
                    # assert not c.formatted_response, "Formatted response in tool result"
                    if c.parsed:
                        # Warning: Formatted response in tool result will be converted to text
                        # TODO: Logging needs to produce not print.
                        print(f"Warning: Formatted response in tool result will be converted to text. Original: {c.parsed}")
                        c.text = _lstr(c.parsed.model_dump_json(),origin_trace=_invocation_origin)
                        c.parsed = None
                    assert not c.audio, "Audio in tool result"
                return ToolResult(tool_call_id=_tool_call_id, result=content_results), _invocation_api_params, {}
            else:
                return result, _invocation_api_params, {}
        # ... other code
    # ... other code
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\types\message.py_ToolResult_ToolResult.__repr__.return.f_self___class_____name_


class ToolResult(BaseModel):
    tool_call_id: _lstr_generic
    result: List["ContentBlock"]

    @property
    def text(self) -> str:
        return _content_to_text(self.result)

    @property
    def text_only(self) -> str:
        return _content_to_text_only(self.result)

    # # XXX: Possibly deprecate
    # def readable_repr(self) -> str:
    #     return f"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"

    def __repr__(self):
        return f"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\types\message.py_ContentBlock_ContentBlock.content.return.getattr_self_self_type_


class ContentBlock(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    text: Optional[_lstr_generic] = Field(default=None)
    image: Optional[ImageContent] = Field(default=None)
    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)
    tool_call: Optional[ToolCall] = Field(default=None)
    parsed: Optional[BaseModel] = Field(default=None)
    tool_result: Optional[ToolResult] = Field(default=None)
    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.
    # This breaks us maintaing parity with the openai python client in some sen but so does image.

    def __init__(self, *args, **kwargs):
        if "image" in kwargs and not isinstance(kwargs["image"], ImageContent):
            im = kwargs["image"] = ImageContent.coerce(kwargs["image"])
            # XXX: Backwards compatibility, Deprecate.
            if (d := kwargs.get("image_detail", None)): im.detail = d

        super().__init__(*args, **kwargs)


    @model_validator(mode='after')
    def check_single_non_null(self):
        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]
        if len(non_null_fields) > 1:
            raise ValueError(f"Only one field can be non-null. Found: {', '.join(non_null_fields)}")
        return self

    def __str__(self):
        return repr(self)

    def __repr__(self):
        non_null_fields = [f"{field}={value}" for field, value in self.__dict__.items() if value is not None]
        return f"ContentBlock({', '.join(non_null_fields)})"

    @property
    def type(self):
        if self.text is not None:
            return "text"
        if self.image is not None:
            return "image"
        if self.audio is not None:
            return "audio"
        if self.tool_call is not None:
            return "tool_call"
        if self.parsed is not None:
            return "parsed"
        if self.tool_result is not None:
            return "tool_result"
        return None

    @property
    def content(self):
        return getattr(self, self.type)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\types\message.py_Message_Message.text.return._content_to_text_self_con


class Message(BaseModel):
    role: str
    content: List[ContentBlock]


    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):
        content_blocks = to_content_blocks(content, **content_block_kwargs)

        super().__init__(role=role, content=content_blocks)

    # XXX: This choice of naming is unfortunate, but it is what it is.
    @property
    def text(self) -> str:
        """Returns all text content, replacing non-text content with their representations.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text
            'Hello\\n<PilImage>\\nWorld'
        """
        return _content_to_text(self.content)
====================================================================


###### Cluster Eval ######
Score: 12.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\lmp\_track.py__track__track.return.tracked_func


def _track(func_to_track: Callable, *, forced_dependencies: Optional[Dict[str, Any]] = None) -> Callable:

    lmp_type = getattr(func_to_track, "__ell_type__", LMPType.OTHER)


    # see if it exists
    if not hasattr(func_to_track, "_has_serialized_lmp"):
        func_to_track._has_serialized_lmp = False

    if not hasattr(func_to_track, "__ell_hash__") and not config.lazy_versioning:
        ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)


    @wraps(func_to_track)
    def tracked_func(*fn_args, _get_invocation_id=False, **fn_kwargs) -> str:
        # XXX: Cache keys and global variable binding is not thread safe.
        # Compute the invocation id and hash the inputs for serialization.
        invocation_id = "invocation-" + secrets.token_hex(16)

        state_cache_key : str = None
        if not config.store:
            return func_to_track(*fn_args, **fn_kwargs, _invocation_origin=invocation_id)[0]

        parent_invocation_id = get_current_invocation()
        try:
            push_invocation(invocation_id)

            # Convert all positional arguments to named keyword arguments
            sig = inspect.signature(func_to_track)
            # Filter out kwargs that are not in the function signature
            filtered_kwargs = {k: v for k, v in fn_kwargs.items() if k in sig.parameters}

            bound_args = sig.bind(*fn_args, **filtered_kwargs)
            bound_args.apply_defaults()
            all_kwargs = dict(bound_args.arguments)

            # Get the list of consumed lmps and clean the invocation params for serialization.
            cleaned_invocation_params, ipstr, consumes = prepare_invocation_params( all_kwargs)

            try_use_cache = hasattr(func_to_track.__wrapper__, "__ell_use_cache__")

            if  try_use_cache:
                # Todo: add nice logging if verbose for when using a cahced invocaiton. IN a different color with thar args..
                if not hasattr(func_to_track, "__ell_hash__")  and config.lazy_versioning:
                    fn_closure, _ = ell.util.closure.lexically_closured_source(func_to_track)

                # compute the state cachekey
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

                cache_store = func_to_track.__wrapper__.__ell_use_cache__
                cached_invocations = cache_store.get_cached_invocations(func_to_track.__ell_hash__, state_cache_key)


                if len(cached_invocations) > 0:
                    # XXX: Fix caching.
                    results =  [d.deserialize() for  d in cached_invocations[0].results]

                    logger.info(f"Using cached result for {func_to_track.__qualname__} with state cache key: {state_cache_key}")
                    if len(results) == 1:
                        return results[0]
                    else:
                        return results
                    # Todo: Unfiy this with the non-cached case. We should go through the same code pathway.
                else:
                    logger.info(f"Attempted to use cache on {func_to_track.__qualname__} but it was not cached, or did not exist in the store. Refreshing cache...")


            _start_time = utc_now()

            # XXX: thread saftey note, if I prevent yielding right here and get the global context I should be fine re: cache key problem

            # get the prompt
            (result, invocation_api_params, metadata) = (
                (func_to_track(*fn_args, **fn_kwargs), {}, {})
                if lmp_type == LMPType.OTHER
                else func_to_track(*fn_args, _invocation_origin=invocation_id, **fn_kwargs, )
                )
            latency_ms = (utc_now() - _start_time).total_seconds() * 1000
            usage = metadata.get("usage", {"prompt_tokens": 0, "completion_tokens": 0})
            prompt_tokens= usage.get("prompt_tokens", 0) if usage else 0
            completion_tokens= usage.get("completion_tokens", 0) if usage else 0


            #XXX: cattrs add invocation origin here recursively on all pirmitive types within a message.
            #XXX: This will allow all objects to be traced automatically irrespective origin rather than relying on the API to do it, it will of vourse be expensive but unify track.
            #XXX: No other code will need to consider tracking after this point.

            if not hasattr(func_to_track, "__ell_hash__") and config.lazy_versioning:
                ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)
            _serialize_lmp(func_to_track)

            if not state_cache_key:
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

            _write_invocation(func_to_track, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                            state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id)

            if _get_invocation_id:
                return result, invocation_id
            else:
                return result
        finally:
            pop_invocation()


    func_to_track.__wrapper__  = tracked_func
    if hasattr(func_to_track, "__ell_api_params__"):
        tracked_func.__ell_api_params__ = func_to_track.__ell_api_params__
    if hasattr(func_to_track, "__ell_params_model__"):
        tracked_func.__ell_params_model__ = func_to_track.__ell_params_model__
    tracked_func.__ell_func__ = func_to_track
    tracked_func.__ell_track = True

    return tracked_func
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\lmp\tool.py_tool.tool_decorator.wrapper.__ell_tool_kwargs___tool.return.tool_decorator


def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        # ... other code


        wrapper.__ell_tool_kwargs__ = tool_kwargs
        wrapper.__ell_func__ = _under_fn
        wrapper.__ell_type__ = LMPType.TOOL
        wrapper.__ell_exempt_from_tracking = exempt_from_tracking

        # Construct the pydantic mdoel for the _under_fn's function signature parameters.
        # 1. Get the function signature.

        sig = inspect.signature(fn)

        # 2. Create a dictionary of field definitions for the Pydantic model
        fields = {}
        for param_name, param in sig.parameters.items():
            # Skip *args and **kwargs
            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                continue

            # Determine the type annotation
            if param.annotation == inspect.Parameter.empty:
                raise ValueError(f"Parameter {param_name} has no type annotation, and cannot be converted into a tool schema for OpenAI and other provisders. Should OpenAI produce a string or an integer, etc, for this parameter?")
            annotation = param.annotation

            # Determine the default value
            default = param.default

            # Check if the parameter has a Field with description
            if isinstance(param.default, FieldInfo):
                field = param.default
                fields[param_name] = (annotation, field)
            elif param.default != inspect.Parameter.empty:
                fields[param_name] = (annotation, param.default)
            else:
                # If no default value, use Field without default
                fields[param_name] = (annotation, Field(...))

        # 3. Create the Pydantic model
        model_name = f"{fn.__name__}"
        ParamsModel = create_model(model_name, **fields)

        # Attach the Pydantic model to the wrapper function
        wrapper.__ell_params_model__ = ParamsModel

        # handle tracking last.
        if exempt_from_tracking:
            ret = wrapper
        else:
            ret=  _track(wrapper)

        # Helper function to get the Pydantic model for the tool
        def get_params_model():
            return wrapper.__ell_params_model__

        # Attach the helper function to the wrapper
        wrapper.get_params_model = get_params_model
        ret.get_params_model = get_params_model
        return ret

    return tool_decorator
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\types\studio.py_UTCTimestamp_LMPType.OTHER._OTHER_


class UTCTimestamp(types.TypeDecorator[datetime]):
    cache_ok = True
    impl = types.TIMESTAMP
    def process_result_value(self, value: datetime, dialect:Any):
        return value.replace(tzinfo=timezone.utc)


def UTCTimestampField(index:bool=False, **kwargs:Any):
    return Field(
        sa_column=Column(UTCTimestamp(timezone=True), index=index, **kwargs))


class LMPType(str, enum.Enum):
    LM = "LM"
    TOOL = "TOOL"
    MULTIMODAL = "MULTIMODAL"
    OTHER = "OTHER"
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\serialization.py_compute_state_cache_key_compute_state_cache_key.return.state_cache_key


def compute_state_cache_key(ipstr, fn_closure):
    _global_free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[2]), sort_keys=True, default=repr, ensure_ascii=False)}"
    _free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[3]), sort_keys=True, default=repr, ensure_ascii=False)}"
    state_cache_key = hashlib.sha256(f"{ipstr}{_global_free_vars_str}{_free_vars_str}".encode('utf-8')).hexdigest()
    return state_cache_key
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\serialization.py_prepare_invocation_params_prepare_invocation_params.return.json_loads_jstr_jstr_c


def prepare_invocation_params(params):
    invocation_params = params

    cleaned_invocation_params = pydantic_ltype_aware_cattr.unstructure(invocation_params)

    # Thisis because we wneed the caching to work on the hash of a cleaned and serialized object.
    jstr = json.dumps(cleaned_invocation_params, sort_keys=True, default=repr, ensure_ascii=False)

    consumes = set()
    import re
    # XXX: Better than registering a hook in cattrs.
    pattern = r'"__origin_trace__":\s*"frozenset\({(.+?)}\)"'

    # Find all matches in the jstr
    matches = re.findall(pattern, jstr)

    # Process each match and add to consumes set
    for match in matches:
        # Remove quotes and spaces, then split by comma
        items = [item.strip().strip("'") for item in match.split(',')]
        consumes.update(items)
    consumes = list(consumes)
    # XXX: Only need to reload because of 'input' caching., we could skip this by making ultimate model caching rather than input hash caching; if prompt same use the same output.. irrespective of version.
    return json.loads(jstr), jstr, consumes
====================================================================


###### Cluster Eval ######
Score: 12.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\lmp\complex.py__get_messages__get_messages.if_isinstance_prompt_ret_.else_.return.prompt_ret


def _get_messages(prompt_ret: Union[str, list[MessageOrDict]], prompt: LMP) -> list[Message]:
    """
    Helper function to convert the output of an LMP into a list of Messages.
    """
    if isinstance(prompt_ret, str):
        has_system_prompt = prompt.__doc__ is not None and prompt.__doc__.strip() != ""
        messages =     [Message(role="system", content=[ContentBlock(text=_lstr(prompt.__doc__ ) )])] if has_system_prompt else []
        return messages + [
            Message(role="user", content=[ContentBlock(text=prompt_ret)])
        ]
    else:
        assert isinstance(
            prompt_ret, list
        ), "Need to pass a list of Messages to the language model"
        return prompt_ret
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\provider.py__validate_messages_are_tracked_


def _validate_messages_are_tracked(
    messages: List[Message], origin_id: Optional[str] = None
):
    if origin_id is None:
        return

    for message in messages:
        assert isinstance(
            message.text, _lstr
        ), f"Provider implementation error: Message text should be an instance of _lstr, got {type(message.text)}"
        assert (
            origin_id in message.text.__origin_trace__
        ), f"Provider implementation error: Message origin_id {message.text.__origin_trace__} does not match the provided origin_id {origin_id}"
    return True
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\providers\openai.py__content_block_to_openai_format__content_block_to_openai_format.if_image_content_bloc.else_.raise_ValueError_f_Unsupp


def _content_block_to_openai_format(content_block: ContentBlock) -> Dict[str, Any]:
    if (image := content_block.image):
        image_url = dict(url=serialize_image(image.image) if image.image else image.url)
        # XXX: Solve per content params better
        if image.detail: image_url["detail"] = image.detail
        return {
            "type": "image_url",
            "image_url": image_url
        }
    elif ((text := content_block.text) is not None): return dict(type="text", text=text)
    elif (parsed := content_block.parsed): return dict(type="text", text=parsed.model_dump_json())
    else:
        raise ValueError(f"Unsupported content block type for openai: {content_block}")
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\types\_lstr.py____lstr._


"""
LM string that supports logits and keeps track of it'sorigin_trace even after mutation.
"""

import numpy as np
from typing import (
    Optional,
    Set,
    SupportsIndex,
    Union,
    FrozenSet,
    Iterable,
    List,
    Tuple,
    Any,
    Callable,
)
from typing_extensions import override
from pydantic import BaseModel, GetCoreSchemaHandler
from pydantic_core import CoreSchema

from pydantic_core import CoreSchema, core_schema


class _lstr(str):
    """
     A string class that supports logits and keeps track of itsorigin_trace even after mutation.
     This class is designed to be used in prompt engineering libraries where it is essential to associate
     logits with generated text and track the origin of the text.

     The `lstr` class inherits from the built-in `str` class and adds two additional attributes: `logits` and `origin_trace`.
     The `origin_trace` attribute is a frozen set of strings that represents theorigin_trace(s) of the string.

     The class provides various methods for manipulating the string, such as concatenation, slicing, splitting, and joining.
     These methods ensure that the logits andorigin_trace(s) are updated correctly based on the operation performed.

     The `lstr` class is particularly useful in LLM libraries for tracing the flow of prompts through various language model calls.
     By tracking theorigin_trace of each string, it is possible to visualize how outputs from one language model program influence
     the inputs of another, allowing for a detailed analysis of interactions between different large language models. This capability
     is crucial for understanding the propagation of prompts in complex LLM workflows and for building visual graphs that depict these interactions.

     It is important to note that any modification to the string (such as concatenation or replacement) will invalidate the associated logits.
     This is because the logits are specifically tied to the original string content, and any change would require a new computation of logits.
     The logic behind this is detailed elsewhere in this file.

     Example usage:
     ```
     # Create an lstr instance with logits and anorigin_trace
     logits = np.array([1.0, 2.0, 3.0])
    origin_trace = "4e9b7ec9"
     lstr_instance = lstr("Hello", logits,origin_trace)

     # Concatenate two lstr instances
     lstr_instance2 = lstr("World", None, "7f4d2c3a")
     concatenated_lstr = lstr_instance + lstr_instance2

     # Get the logits andorigin_trace of the concatenated lstr
     print(concatenated_lstr.logits)  # Output: None
     print(concatenated_lstr.origin_trace)  # Output: frozenset({'4e9b7ec9', '7f4d2c3a'})

     # Split the concatenated lstr into two parts
     parts = concatenated_lstr.split()
     print(parts)  # Output: [lstr('Hello', None, frozenset({'4e9b7ec9', '7f4d2c3a'})), lstr('World', None, frozenset({'4e9b7ec9', '7f4d2c3a'}))]
     ```
     Attributes:
        origin_trace (FrozenSet[str]): A frozen set of strings representing theorigin_trace(s) of the string.

     Methods:
         __new__: Create a new instance of lstr.
         __repr__: Return a string representation of the lstr instance.
         __add__: Concatenate this lstr instance with another string or lstr instance.
         __mod__: Perform a modulo operation between this lstr instance and another string, lstr, or a tuple of strings and lstrs.
         __mul__: Perform a multiplication operation between this lstr instance and an integer or another lstr.
         __rmul__: Perform a right multiplication operation between an integer or another lstr and this lstr instance.
         __getitem__: Get a slice or index of this lstr instance.
         __getattr__: Get an attribute from this lstr instance.
         join: Join a sequence of strings or lstr instances into a single lstr instance.
         split: Split this lstr instance into a list of lstr instances based on a separator.
         rsplit: Split this lstr instance into a list of lstr instances based on a separator, starting from the right.
         splitlines: Split this lstr instance into a list of lstr instances based on line breaks.
         partition: Partition this lstr instance into three lstr instances based on a separator.
         rpartition: Partition this lstr instance into three lstr instances based on a separator, starting from the right.
    """
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\types\message.py__todo_implement_tracing_AnyContent.Union_ContentBlock_str


# todo: implement tracing for structured outs. this a v2 feature.
import json
from ell.types._lstr import _lstr
from functools import cached_property
import numpy as np
import base64
from io import BytesIO
from PIL import Image as PILImage

from pydantic import BaseModel, ConfigDict, model_validator, field_serializer
from sqlmodel import Field

from concurrent.futures import ThreadPoolExecutor, as_completed

from typing import Any, Callable, Dict, List, Optional, Union

from ell.util.serialization import serialize_image
_lstr_generic = Union[_lstr, str]
InvocableTool = Callable[..., Union["ToolResult", _lstr_generic, List["ContentBlock"], ]]

# AnyContent represents any type that can be passed to Message.
AnyContent = Union["ContentBlock", str, "ToolCall", "ToolResult", "ImageContent", np.ndarray, PILImage.Image, BaseModel]
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\util\serialization.py__Global_converter_None_5


# Global converter
import base64
import hashlib
from io import BytesIO
import json
import cattrs
import numpy as np
from pydantic import BaseModel
import PIL
from ell.types._lstr import _lstr


pydantic_ltype_aware_cattr = cattrs.Converter()

def serialize_image(img):
    buffer = BytesIO()
    img.save(buffer, format="PNG")
    return "data:image/png;base64," + base64.b64encode(buffer.getvalue()).decode()


# Register hooks for complex types
pydantic_ltype_aware_cattr.register_unstructure_hook(
    np.ndarray,
    lambda arr: {
        "content": serialize_image(PIL.Image.fromarray(arr)),
        "__limage": True
    } if arr.ndim == 3 else (
        {
            "content": base64.b64encode(arr.tobytes()).decode(),
            "dtype": str(arr.dtype),
            "shape": arr.shape,
            "__lndarray": True
        }
    )
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    set,
    lambda s: list(sorted(s))
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    frozenset,
    lambda s: list(sorted(s))
)


pydantic_ltype_aware_cattr.register_unstructure_hook(
    PIL.Image.Image,
    lambda obj: {
        "content": serialize_image(obj),
        "__limage": True
    }
)

def unstructure_lstr(obj):
    return dict(content=str(obj), **obj.__dict__, __lstr=True)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    _lstr,
    unstructure_lstr
)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    BaseModel,
    lambda obj: obj.model_dump(exclude_none=True, exclude_unset=True)
)
====================================================================


###### Cluster Eval ######
Score: 11.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\configurator.py_Config.register_provider_Config.register_provider.with_self__lock_.self_providers_client_typ


class Config(BaseModel):

    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:
        """
        Register a provider class for a specific client type.

        :param provider_class: The provider class to register.
        :type provider_class: Type[Provider]
        """
        assert isinstance(client_type, type), "client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))"
        with self._lock:
            self.providers[client_type] = provider
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\configurator.py__Existing_helper_functio_set_store.raise_DeprecationWarning_


# Existing helper functions
def get_store() -> Union[Store, None]:
    return config.store

# Will be deprecated at 0.1.0 

# You can add more helper functions here if needed
def register_provider(provider: Provider, client_type: Type[Any]) -> None:
    return config.register_provider(provider, client_type)

# Deprecated now (remove at 0.1.0)
def set_store(*args, **kwargs) -> None:
    raise DeprecationWarning("The set_store function is deprecated and will be removed in a future version. Use ell.init(store=...) instead.")
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\provider.py__XXX_Might_leave_this_i_Metadata.Dict_str_Any_


# XXX: Might leave this internal to providers so that the complex code is simpler &
# we can literally jsut call provider.call like any openai fn.
class EllCallParams(BaseModel):
    model: str = Field(..., description="Model identifier")
    messages: List[Message] = Field(..., description="Conversation context")
    client: Any = Field(..., description="API client")
    tools: List[LMP] = Field(default_factory=list, description="Available tools")
    api_params: Dict[str, Any] = Field(
        default_factory=dict, description="API parameters"
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def get_tool_by_name(self, name: str) -> Optional[LMP]:
        """Get a tool by name."""
        return next(
            (tool for tool in (self.tools or [])  if tool.__name__ == name), None
        )


Metadata = Dict[str, Any]
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\provider.py__XXX_Needs_a_better_nam_Provider._Be_careful_to_override_


# XXX: Needs a better name.
class Provider(ABC):
    """
    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.
    For example, the OpenAI provider is an API interface to OpenAI's API but also to Ollama and Azure OpenAI.
    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.
    """
    dangerous_disable_validation = False

    ################################
    ### API PARAMETERS #############
    ################################
    @abstractmethod
    def provider_call_function(
        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None
    ) -> Callable[..., Any]:
        """
        Implement this method to return the function that makes the API call to the language model.
        For example, if you're implementing the OpenAI provider, you would return the function that makes the API call to OpenAI's API.
        """
        return NotImplemented

    def disallowed_api_params(self) -> FrozenSet[str]:
        """
        Returns a list of disallowed call params that ell will override.
        """
        return frozenset({"messages", "tools", "model", "stream", "stream_options"})

    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):
        params = _call_params(self.provider_call_function(client, api_params))
        return frozenset(params.keys()) - self.disallowed_api_params()

    ################################
    ### TRANSLATION ###############
    ################################
    @abstractmethod
    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:
        """Converts an ell call to provider call params!"""
        return NotImplemented

    @abstractmethod
    def translate_from_provider(
        self,
        provider_response: Any,
        ell_call: EllCallParams,
        provider_call_params: Dict[str, Any],
        origin_id: Optional[str] = None,
        logger: Optional[Callable[..., None]] = None,
    ) -> Tuple[List[Message], Metadata]:
        """Converts provider responses to universal format. with metadata"""
        return NotImplemented

    ################################
    ### CALL MODEL ################
    ################################
    # Be careful to override this method in your provider.
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\providers\anthropic.py_from_typing_import_Any_C_try_.except_ImportError_.pass


from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent

from ell.types._lstr import _lstr
from ell.types.message import LMP
from ell.configurator import register_provider
from ell.util.serialization import serialize_image
import base64
from io import BytesIO
import json
import requests
from PIL import Image as PILImage

try:
    import anthropic
    from anthropic import Anthropic
    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent
    from anthropic.types.message_create_params import MessageCreateParamsStreaming
    from anthropic._streaming import Stream

    class AnthropicProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            return client.messages.create

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())
            # XXX: Helper, but should be depreicated due to ssot
            assert final_call_params.get("max_tokens") is not None, f"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}}))."

            dirty_msgs = [
                MessageParam(
                    role=cast(Literal["user", "assistant"], message.role), 
                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]
            role_correct_msgs   : List[MessageParam] = []
            for msg in dirty_msgs:
                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):
                    role_correct_msgs.append(msg)
                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])

            system_message = None
            if role_correct_msgs and role_correct_msgs[0]["role"] == "system":
                system_message = role_correct_msgs.pop(0)

            if system_message:
                final_call_params["system"] = system_message["content"][0]["text"]


            final_call_params['stream'] = True
            final_call_params["model"] = ell_call.model
            final_call_params["messages"] = role_correct_msgs

            if ell_call.tools:
                final_call_params["tools"] = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        name=tool.__name__,
                        description=tool.__doc__,
                        input_schema=tool.__ell_params_model__.model_json_schema(),
                    )
                    for tool in ell_call.tools
                ]

            # print(final_call_params)
            return final_call_params

        def translate_from_provider(
            self,
            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            usage = {}
            tracked_results = []
            metadata = {}

            #XXX: Support n > 0

            if provider_call_params.get("stream", False):
                content = []
                current_blocks: Dict[int, Dict[str, Any]] = {}
                message_metadata = {}

                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:
                    for chunk in stream:
                        if chunk.type == "message_start":
                            message_metadata = chunk.message.model_dump()
                            message_metadata.pop("content", None)  # Remove content as we'll build it separately

                        elif chunk.type == "content_block_start":
                            block = chunk.content_block.model_dump()
                            current_blocks[chunk.index] = block
                            if block["type"] == "tool_use":
                                if logger: logger(f" <tool_use: {block['name']}(")
                                block["input"] = "" # force it to be a string, XXX: can implement partially parsed json later.
                        elif chunk.type == "content_block_delta":
                            if chunk.index in current_blocks:
                                block = current_blocks[chunk.index]
                                if (delta := chunk.delta).type == "text_delta":
                                    block["text"] += delta.text
                                    if logger: logger(delta.text)
                                if delta.type == "input_json_delta":
                                    block["input"] += delta.partial_json
                                    if logger: logger(delta.partial_json)

                        elif chunk.type == "content_block_stop":
                            if chunk.index in current_blocks:
                                block = current_blocks.pop(chunk.index)
                                if block["type"] == "text":
                                    content.append(ContentBlock(text=_lstr(block["text"],origin_trace=origin_id)))
                                elif block["type"] == "tool_use":
                                    try:
                                        matching_tool = ell_call.get_tool_by_name(block["name"])
                                        if matching_tool:
                                            content.append(
                                                ContentBlock(
                                                    tool_call=ToolCall(
                                                        tool=matching_tool,
                                                        tool_call_id=_lstr(
                                                            block['id'],origin_trace=origin_id
                                                        ),
                                                        params=json.loads(block['input']) if block['input'] else {},
                                                    )
                                                )
                                            )
                                    except json.JSONDecodeError:
                                        if logger: logger(f" - FAILED TO PARSE JSON")
                                        pass
                                    if logger: logger(f")>")

                        elif chunk.type == "message_delta":
                            message_metadata.update(chunk.delta.model_dump())
                            if chunk.usage:
                                usage.update(chunk.usage.model_dump())

                        elif chunk.type == "message_stop":
                            tracked_results.append(Message(role="assistant", content=content))

                        # print(chunk)
                metadata = message_metadata

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("input_tokens", 0)
            usage["completion_tokens"] = usage.get("output_tokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata

    # XXX: Make a singleton.
    anthropic_provider = AnthropicProvider()
    register_provider(anthropic_provider, anthropic.Anthropic)
    register_provider(anthropic_provider, anthropic.AnthropicBedrock)
    register_provider(anthropic_provider, anthropic.AnthropicVertex)

except ImportError:
    pass
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\providers\bedrock.py_try__try_.except_ImportError_.pass


try:
    from botocore.client import BaseClient
    from botocore.eventstream import (EventStream)
    class BedrockProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Any, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and api_call_params.get("stream", False):
                api_call_params.pop('stream')
                return client.converse_stream
            else:
                return client.converse

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = {}

            if ell_call.api_params.get('api_params',{}).get('stream', False):
                final_call_params['stream'] = ell_call.api_params.get('api_params',{}).get('stream', False)

            bedrock_converse_messages = [message_to_bedrock_message_format(message) for message in ell_call.messages]

            system_message = None
            if bedrock_converse_messages and bedrock_converse_messages[0]["role"] == "system":
                system_message = bedrock_converse_messages.pop(0)

            if system_message:
                final_call_params["system"] = [{'text':system_message["content"][0]["text"]}]

            final_call_params["modelId"] = ell_call.model
            final_call_params["messages"] = bedrock_converse_messages

            if ell_call.tools:
                tools = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        toolSpec = dict(
                            name=tool.__name__,
                            description=tool.__doc__,
                            inputSchema=dict(
                                json=tool.__ell_params_model__.model_json_schema(),
                            )
                        )
                    )
                    for tool in ell_call.tools
                ]
                final_call_params["toolConfig"] = {'tools':tools}

            return final_call_params

        def translate_from_provider(
                self,
                provider_response: Union[EventStream, Any],
                ell_call: EllCallParams,
                provider_call_params: Dict[str, Any],
                origin_id: Optional[str] = None,
                logger: Optional[Callable[..., None]] = None,
            ) -> Tuple[List[Message], Metadata]:

            usage = {}
            metadata : Metadata = {}

            metadata : Metadata = {}
            tracked_results : List[Message] = []
            did_stream = ell_call.api_params.get("api_params", {}).get('stream')

            if did_stream:
                content = []
                current_block: Optional[Dict[str, Any]] = {}
                message_metadata = {}
                for chunk in provider_response.get('stream'):

                    if "messageStart" in chunk:
                        current_block['content'] = ''
                        pass
                    elif "contentBlockStart" in chunk:
                        pass
                    elif "contentBlockDelta" in chunk:
                        delta = chunk.get("contentBlockDelta", {}).get("delta", {})
                        if "text" in delta:
                            current_block['type'] = 'text'
                            current_block['content'] += delta.get("text")
                            if logger:
                                logger(delta.get("text"))
                        else:
                            pass
                    elif "contentBlockStop" in chunk:
                        if current_block is not None:
                            if current_block["type"] == "text":
                                content.append(ContentBlock(text=_lstr(content=content, origin_trace=origin_id)))

                    elif "messageStop" in chunk:
                        tracked_results.append(Message(role="assistant", content=content))

                    elif "metadata" in chunk:
                        if "usage" in chunk["metadata"]:
                            usage["prompt_tokens"] = chunk["metadata"].get('usage').get("inputTokens", 0)
                            usage["completion_tokens"] = chunk["metadata"].get('usage').get("outputTokens", 0)
                            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']
                            message_metadata["usage"] = usage
                    else:
                        pass


                metadata = message_metadata
            else:
                # Non-streaming response processing (unchanged)
                cbs = []
                for content_block in provider_response.get('output', {}).get('message', {}).get('content', []):
                    if 'text' in content_block:
                        cbs.append(ContentBlock(text=_lstr(content_block.get('text'), origin_trace=origin_id)))
                    elif 'toolUse' in content_block:
                        assert ell_call.tools is not None, "Tools were not provided to the model when calling it and yet bedrock returned a tool use."
                        try:
                            toolUse = content_block['toolUse']
                            matching_tool = ell_call.get_tool_by_name(toolUse["name"])
                            if matching_tool:
                                cbs.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                toolUse['toolUseId'],origin_trace=origin_id
                                            ),
                                            params=toolUse['input'],
                                        )
                                    )
                                )
                        except json.JSONDecodeError:
                            if logger: logger(f" - FAILED TO PARSE JSON")
                            pass
                tracked_results.append(Message(role="assistant", content=cbs))
                if logger:
                    logger(tracked_results[0].text)


                # usage = call_result.response.usage.dict() if call_result.response.get('usage') else {}
                # metadata = call_result.response.model_dump()
                # del metadata["content"]

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("inputTokens", 0)
            usage["completion_tokens"] = usage.get("outputTokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata


    # XXX: Make a singleton.
    register_provider(BedrockProvider(), BaseClient)
except ImportError:
    pass
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\providers\groq.py___


"""
Groq provider.
"""

from ell.providers.openai import OpenAIProvider
from ell.configurator import register_provider


try:
    import groq
    class GroqProvider(OpenAIProvider):
        dangerous_disable_validation = True
        def translate_to_provider(self, *args, **kwargs):
            params = super().translate_to_provider(*args, **kwargs)
            params.pop('stream_options', None)
            return params

        def translate_from_provider(self, *args, **kwargs):
            res, meta = super().translate_from_provider(*args, **kwargs)
            if not meta['usage']:
                meta['usage'] = meta['x_groq']['usage']
            return res, meta
    register_provider(GroqProvider(), groq.Client)
except ImportError:
    pass
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\providers\openai.py_from_abc_import_ABC_abst_try_.except_ImportError_.pass


from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast

from pydantic import BaseModel
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from ell.configurator import _Model, config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image

try:
    # XXX: Could genericize.
    import openai
    from openai._streaming import Stream
    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam

    class OpenAIProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and (isinstance(fmt := api_call_params.get("response_format"), type)) and issubclass(fmt, BaseModel):
                return client.beta.chat.completions.parse
            else:
                return client.chat.completions.create

        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:
            final_call_params = ell_call.api_params.copy()
            final_call_params["model"] = ell_call.model
            # Stream by default for verbose logging.
            final_call_params["stream"] = True
            final_call_params["stream_options"] = {"include_usage": True}

            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.
            if ell_call.tools or final_call_params.get("response_format") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:
                final_call_params.pop("stream", None)
                final_call_params.pop("stream_options", None)
            if ell_call.tools:
                final_call_params.update(
                    tool_choice=final_call_params.get("tool_choice", "auto"),
                    tools=[  
                        dict(
                            type="function",
                            function=dict(
                                name=tool.__name__,
                                description=tool.__doc__,
                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore
                            )
                        ) for tool in ell_call.tools
                    ]
                )
            # messages
            openai_messages : List[ChatCompletionMessageParam] = []
            for message in ell_call.messages:
                if (tool_calls := message.tool_calls):
                    assert message.role == "assistant", "Tool calls must be from the assistant."
                    assert all(t.tool_call_id for t in tool_calls), "Tool calls must have tool call ids."
                    openai_messages.append(dict(
                        tool_calls=[
                            dict(
                                id=cast(str, tool_call.tool_call_id),
                                type="function",
                                function=dict(
                                    name=tool_call.tool.__name__,
                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)
                                )
                            ) for tool_call in tool_calls ],
                        role="assistant",
                        content=None,
                    ))
                elif (tool_results := message.tool_results):
                    for tool_result in tool_results:
                        assert all(cb.type == "text" for cb in tool_result.result), "Tool result does not match expected content blocks."
                        openai_messages.append(dict(
                            role="tool",
                            tool_call_id=tool_result.tool_call_id,
                            content=tool_result.text_only, 
                        ))
                else:
                    openai_messages.append(cast(ChatCompletionMessageParam, dict(
                        role=message.role,
                        content=[_content_block_to_openai_format(c) for c in message.content] 
                             if message.role != "system" 
                             else message.text_only
                    )))

            final_call_params["messages"] = openai_messages

            return final_call_params

        def translate_from_provider(
            self,
            provider_response: Union[
                ChatCompletion, 
                ParsedChatCompletion,
                Stream[ChatCompletionChunk], Any],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            metadata : Metadata = {}
            messages : List[Message] = []
            did_stream = provider_call_params.get("stream", False)


            if did_stream:
                stream = cast(Stream[ChatCompletionChunk], provider_response)
                message_streams = defaultdict(list)
                role : Optional[str] = None
                for chunk in stream:
                    metadata.update(chunk.model_dump(exclude={"choices"}))

                    for chat_compl_chunk in chunk.choices:
                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)
                        delta = chat_compl_chunk.delta
                        role = role or delta.role
                        if  chat_compl_chunk.index == 0 and logger:
                            logger(delta.content, is_refusal=hasattr(delta, "refusal") and delta.refusal)
                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):
                    text = "".join((choice.delta.content or "") for choice in message_stream)
                    messages.append(
                        Message(role=role, 
                                content=_lstr(content=text,origin_trace=origin_id)))
                    #XXX: Support streaming other types.
            else:
                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)
                metadata = chat_completion.model_dump(exclude={"choices"})
                for oai_choice in chat_completion.choices:
                    role = oai_choice.message.role
                    content_blocks = []
                    if (hasattr(message := oai_choice.message, "refusal") and (refusal := message.refusal)):
                        raise ValueError(refusal)
                    if hasattr(message, "parsed"):
                        if (parsed := message.parsed):
                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing
                            if logger: logger(parsed.model_dump_json())
                    else:
                        if (content := message.content):
                            content_blocks.append(
                                ContentBlock(
                                    text=_lstr(content=content,origin_trace=origin_id)))
                            if logger: logger(content)
                        if (tool_calls := message.tool_calls):
                            for tool_call in tool_calls:
                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)
                                assert matching_tool, "Model called tool not found in provided toolset."
                                content_blocks.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                tool_call.id, origin_trace= origin_id),
                                            params=json.loads(tool_call.function.arguments),
                                        )
                                    )
                                )
                                if logger: logger(repr(tool_call))
                    messages.append(Message(role=role, content=content_blocks))
            return messages, metadata


    # xx: singleton needed
    openai_provider = OpenAIProvider()
    register_provider(openai_provider, openai.Client)
except ImportError:
    pass
====================================================================


###### Cluster Eval ######
Score: 8.0
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\examples\hello_postgres.py_ell_


import ell
import numpy as np

from ell.stores.sql import PostgresStore

class MyPrompt:
    x : int

def get_random_length():
    return int(np.random.beta(2, 6) * 1500)

@ell.simple(model="gpt-4o-mini")
def hello(world : str):
    """Your goal is to be really mean to the other guy while saying hello"""
    name = world.capitalize()
    number_of_chars_in_name = get_random_length()

    return f"Say hello to {name} in {number_of_chars_in_name} characters or more!"


if __name__ == "__main__":
    ell.init(verbose=True, store=PostgresStore('postgresql://postgres:postgres@localhost:5432/ell'), autocommit=True)

    greeting = hello("sam altman") # > "hello sama! ... "
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\configurator.py_init_init.if_autocommit_model_is_no.config.autocommit_model.autocommit_model


def init(
    store: Optional[Union[Store, str]] = None,
    verbose: bool = False,
    autocommit: bool = True,
    lazy_versioning: bool = True,
    default_api_params: Optional[Dict[str, Any]] = None,
    default_client: Optional[Any] = None,
    autocommit_model: str = "gpt-4o-mini"
) -> None:
    """
    Initialize the ELL configuration with various settings.

    :param verbose: Set verbosity of ELL operations.
    :type verbose: bool
    :param store: Set the store for ELL. Can be a Store instance or a string path for SQLiteStore.
    :type store: Union[Store, str], optional
    :param autocommit: Set autocommit for the store operations.
    :type autocommit: bool
    :param lazy_versioning: Enable or disable lazy versioning.
    :type lazy_versioning: bool
    :param default_api_params: Set default parameters for language models.
    :type default_api_params: Dict[str, Any], optional
    :param default_openai_client: Set the default OpenAI client.
    :type default_openai_client: openai.Client, optional
    :param autocommit_model: Set the model used for autocommitting.
    :type autocommit_model: str
    """
    # XXX: prevent double init
    config.verbose = verbose
    config.lazy_versioning = lazy_versioning

    if isinstance(store, str):
        from ell.stores.sql import SQLiteStore
        config.store = SQLiteStore(store)
    else:
        config.store = store
    config.autocommit = autocommit or config.autocommit

    if default_api_params is not None:
        config.default_api_params.update(default_api_params)

    if default_client is not None:
        config.default_client = default_client

    if autocommit_model is not None:
        config.autocommit_model = autocommit_model
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\stores\sql.py_SQLiteStore_SQLBlobStore.retrieve_blob.with_gzip_open_file_path_.return.f_read_


class SQLiteStore(SQLStore):
    def __init__(self, db_dir: str):
        assert not db_dir.endswith('.db'), "Create store with a directory not a db."

        os.makedirs(db_dir, exist_ok=True)
        self.db_dir = db_dir
        db_path = os.path.join(db_dir, 'ell.db')
        blob_store = SQLBlobStore(db_dir)
        super().__init__(f'sqlite:///{db_path}', blob_store=blob_store)

class SQLBlobStore(ell.store.BlobStore):
    def __init__(self, db_dir: str):
        self.db_dir = db_dir

    def store_blob(self, blob: bytes, blob_id  : str) -> str:
        file_path = self._get_blob_path(blob_id)
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with gzip.open(file_path, "wb") as f:
            f.write(blob)
        return blob_id

    def retrieve_blob(self, blob_id: str) -> bytes:
        file_path = self._get_blob_path(blob_id)
        with gzip.open(file_path, "rb") as f:
            return f.read()
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\stores\sql.py_SQLBlobStore._get_blob_path_


class SQLBlobStore(ell.store.BlobStore):

    def _get_blob_path(self, id: str, depth: int = 2) -> str:
        assert "-" in id, "Blob id must have a single - in it to split on."
        _type, _id = id.split("-")
        increment = 2
        dirs = [_type] + [_id[i:i+increment] for i in range(0, depth*increment, increment)]
        file_name = _id[depth*increment:]
        return os.path.join(self.db_dir, *dirs, file_name)

class PostgresStore(SQLStore):
    def __init__(self, db_uri: str):
        super().__init__(db_uri)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\studio\__main__.py_main_main.db_path.Path_config_storage_dir_


def main():
    parser = ArgumentParser(description="ell studio")
    parser.add_argument("--storage-dir" , default=None,
                        help="Directory for filesystem serializer storage (default: current directory)")
    parser.add_argument("--pg-connection-string", default=None,
                        help="PostgreSQL connection string (default: None)")
    parser.add_argument("--host", default="127.0.0.1", help="Host to run the server on (default: localhost)")
    parser.add_argument("--port", type=int, default=5555, help="Port to run the server on (default: 5555)")
    parser.add_argument("--dev", action="store_true", help="Run in development mode")
    parser.add_argument("--dev-static-dir", default=None, help="Directory to serve static files from in development mode")
    parser.add_argument("--open", action="store_true", help="Opens the studio web UI in a browser")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enables debug logging for more verbose output")
    args = parser.parse_args()

    _setup_logging(logging.DEBUG if args.verbose else logging.INFO)

    if args.dev:
        assert args.port == 5555, "Port must be 5000 in development mode"

    config = Config.create(storage_dir=args.storage_dir,
                    pg_connection_string=args.pg_connection_string)
    app = create_app(config)

    if not args.dev:
        # In production mode, serve the built React app
        static_dir = Path(__file__).parent / "static"
        # app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")

        @app.get("/{full_path:path}")
        async def serve_react_app(full_path: str):
            file_path = static_dir / full_path
            if file_path.exists() and file_path.is_file():
                return FileResponse(file_path)
            else:
                return FileResponse(static_dir / "index.html")
    elif args.dev_static_dir:
        app.mount("/", StaticFiles(directory=args.dev_static_dir, html=True), name="static")

    # Respect Config.create behavior, which has fallback to env vars.
    db_path = Path(config.storage_dir) if config.storage_dir else None
    # ... other code
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\studio\config.py_from_functools_import_lru_Config.create.return.cls_pg_connection_string_


from functools import lru_cache
import os
from typing import Optional
from pydantic import BaseModel

import logging

logger = logging.getLogger(__name__)


# todo. maybe we default storage dir and other things in the future to a well-known location
# like ~/.ell or something
@lru_cache
def ell_home() -> str:
    return os.path.join(os.path.expanduser("~"), ".ell")


class Config(BaseModel):
    pg_connection_string: Optional[str] = None
    storage_dir: Optional[str] = None

    @classmethod
    def create(
        cls,
        storage_dir: Optional[str] = None,
        pg_connection_string: Optional[str] = None,
    ) -> 'Config':
        pg_connection_string = pg_connection_string or os.getenv("ELL_PG_CONNECTION_STRING")
        storage_dir = storage_dir or os.getenv("ELL_STORAGE_DIR")

        # Enforce that we use either sqlite or postgres, but not both
        if pg_connection_string is not None and storage_dir is not None:
            raise ValueError("Cannot use both sqlite and postgres")

        # For now, fall back to sqlite if no PostgreSQL connection string is provided
        if pg_connection_string is None and storage_dir is None:
            # This intends to honor the default we had set in the CLI
            storage_dir = os.getcwd()

        return cls(pg_connection_string=pg_connection_string, storage_dir=storage_dir)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\studio\connection_manager.py__ConnectionManager.broadcast.for_connection_in_self_ac.await_connection_send_tex


from fastapi import WebSocket


class ConnectionManager:
    def __init__(self):
        self.active_connections = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            print(f"Broadcasting message to {connection} {message}")
            await connection.send_text(message)
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\studio\server.py_from_typing_import_Option_get_serializer.if_config_pg_connection_s.else_.raise_ValueError_No_stor


from typing import Optional, Dict, Any

from sqlmodel import Session
from ell.stores.sql import PostgresStore, SQLiteStore
from ell import __version__
from fastapi import FastAPI, Query, HTTPException, Depends, Response, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import logging
import json
from ell.studio.config import Config
from ell.studio.connection_manager import ConnectionManager
from ell.studio.datamodels import InvocationPublicWithConsumes, SerializedLMPWithUses

from ell.types import SerializedLMP
from datetime import datetime, timedelta
from sqlmodel import select


logger = logging.getLogger(__name__)


from ell.studio.datamodels import InvocationsAggregate


def get_serializer(config: Config):
    if config.pg_connection_string:
        return PostgresStore(config.pg_connection_string)
    elif config.storage_dir:
        return SQLiteStore(config.storage_dir)
    else:
        raise ValueError("No storage configuration found")
====================================================================
-> Chunk: C:\Users\jpeng\Documents\projects\codesearch-data\repo\MadcowD_ell\src\ell\studio\server.py_create_app_create_app.get_lmp_by_id.return.lmp


def create_app(config:Config):
    serializer = get_serializer(config)

    def get_session():
        with Session(serializer.engine) as session:
            yield session

    app = FastAPI(title="ell Studio", version=__version__)

    # Enable CORS for all origins
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    manager = ConnectionManager()

    @app.websocket("/ws")
    async def websocket_endpoint(websocket: WebSocket):
        await manager.connect(websocket)
        try:
            while True:
                data = await websocket.receive_text()
                # Handle incoming WebSocket messages if needed
        except WebSocketDisconnect:
            manager.disconnect(websocket)


    @app.get("/api/latest/lmps", response_model=list[SerializedLMPWithUses])
    def get_latest_lmps(
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        session: Session = Depends(get_session)
    ):
        lmps = serializer.get_latest_lmps(
            session,
            skip=skip, limit=limit,
            )
        return lmps

    # TOOD: Create a get endpoint to efficient get on the index with /api/lmp/<lmp_id>
    @app.get("/api/lmp/{lmp_id}")
    def get_lmp_by_id(lmp_id: str, session: Session = Depends(get_session)):
        lmp = serializer.get_lmps(session, lmp_id=lmp_id)[0]
        return lmp
    # ... other code
====================================================================


Total coherence score: 12.363636363636363
