_____ full_code_v2 _____

###### Cluster Eval ######
Score: 14.666666666666666
Cluster name: Provider Integration Cluster
Provider Integration Cluster:

-> Chunk: providers\bedrock.py::2


try:
    from botocore.client import BaseClient
    from botocore.eventstream import (EventStream)
    class BedrockProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Any, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and api_call_params.get("stream", False):
                api_call_params.pop('stream')
                return client.converse_stream
            else:
                return client.converse

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = {}

            if ell_call.api_params.get('api_params',{}).get('stream', False):
                final_call_params['stream'] = ell_call.api_params.get('api_params',{}).get('stream', False)

            bedrock_converse_messages = [message_to_bedrock_message_format(message) for message in ell_call.messages]

            system_message = None
            if bedrock_converse_messages and bedrock_converse_messages[0]["role"] == "system":
                system_message = bedrock_converse_messages.pop(0)

            if system_message:
                final_call_params["system"] = [{'text':system_message["content"][0]["text"]}]

            final_call_params["modelId"] = ell_call.model
            final_call_params["messages"] = bedrock_converse_messages

            if ell_call.tools:
                tools = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        toolSpec = dict(
                            name=tool.__name__,
                            description=tool.__doc__,
                            inputSchema=dict(
                                json=tool.__ell_params_model__.model_json_schema(),
                            )
                        )
                    )
                    for tool in ell_call.tools
                ]
                final_call_params["toolConfig"] = {'tools':tools}

            return final_call_params

        def translate_from_provider(
                self,
                provider_response: Union[EventStream, Any],
                ell_call: EllCallParams,
                provider_call_params: Dict[str, Any],
                origin_id: Optional[str] = None,
                logger: Optional[Callable[..., None]] = None,
            ) -> Tuple[List[Message], Metadata]:

            usage = {}
            metadata : Metadata = {}

            metadata : Metadata = {}
            tracked_results : List[Message] = []
            did_stream = ell_call.api_params.get("api_params", {}).get('stream')

            if did_stream:
                content = []
                current_block: Optional[Dict[str, Any]] = {}
                message_metadata = {}
                for chunk in provider_response.get('stream'):

                    if "messageStart" in chunk:
                        current_block['content'] = ''
                        pass
                    elif "contentBlockStart" in chunk:
                        pass
                    elif "contentBlockDelta" in chunk:
                        delta = chunk.get("contentBlockDelta", {}).get("delta", {})
                        if "text" in delta:
                            current_block['type'] = 'text'
                            current_block['content'] += delta.get("text")
                            if logger:
                                logger(delta.get("text"))
                        else:
                            pass
                    elif "contentBlockStop" in chunk:
                        if current_block is not None:
                            if current_block["type"] == "text":
                                content.append(ContentBlock(text=_lstr(content=content, origin_trace=origin_id)))

                    elif "messageStop" in chunk:
                        tracked_results.append(Message(role="assistant", content=content))

                    elif "metadata" in chunk:
                        if "usage" in chunk["metadata"]:
                            usage["prompt_tokens"] = chunk["metadata"].get('usage').get("inputTokens", 0)
                            usage["completion_tokens"] = chunk["metadata"].get('usage').get("outputTokens", 0)
                            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']
                            message_metadata["usage"] = usage
                    else:
                        pass


                metadata = message_metadata
            else:
                # Non-streaming response processing (unchanged)
                cbs = []
                for content_block in provider_response.get('output', {}).get('message', {}).get('content', []):
                    if 'text' in content_block:
                        cbs.append(ContentBlock(text=_lstr(content_block.get('text'), origin_trace=origin_id)))
                    elif 'toolUse' in content_block:
                        assert ell_call.tools is not None, "Tools were not provided to the model when calling it and yet bedrock returned a tool use."
                        try:
                            toolUse = content_block['toolUse']
                            matching_tool = ell_call.get_tool_by_name(toolUse["name"])
                            if matching_tool:
                                cbs.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                toolUse['toolUseId'],origin_trace=origin_id
                                            ),
                                            params=toolUse['input'],
                                        )
                                    )
                                )
                        except json.JSONDecodeError:
                            if logger: logger(f" - FAILED TO PARSE JSON")
                            pass
                tracked_results.append(Message(role="assistant", content=cbs))
                if logger:
                    logger(tracked_results[0].text)


                # usage = call_result.response.usage.dict() if call_result.response.get('usage') else {}
                # metadata = call_result.response.model_dump()
                # del metadata["content"]

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("inputTokens", 0)
            usage["completion_tokens"] = usage.get("outputTokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata


    # XXX: Make a singleton.
    register_provider(BedrockProvider(), BaseClient)
except ImportError:
    pass
====================================================================
-> Chunk: providers\bedrock.py::1


from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent
from ell.types._lstr import _lstr
import json
from ell.configurator import config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image
from io import BytesIO
import requests
from PIL import Image as PILImage
====================================================================
-> Chunk: models\bedrock.py::1


from typing import Any
from ell.configurator import config
import logging

logger = logging.getLogger(__name__)


def register(client: Any):
    """
    Register Bedrock models with the provided client.

    This function takes an boto3 client and registers various Bedrock models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (boto3.client): An instance of the bedrock client to be used
                                        for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    model_data = [
        ('anthropic.claude-3-opus-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-sonnet-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-haiku-20240307-v1:0', 'bedrock'),
        ('anthropic.claude-3-5-sonnet-20240620-v1:0', 'bedrock'),

        ('mistral.mistral-7b-instruct-v0:2', 'bedrock'),
        ('mistral.mixtral-8x7b-instruct-v0:1', 'bedrock'),
        ('mistral.mistral-large-2402-v1:0', 'bedrock'),
        ('mistral.mistral-small-2402-v1:0', 'bedrock'),


        ('ai21.jamba-instruct-v1:0','bedrock'),
        ('ai21.j2-ultra-v1', 'bedrock'),
        ('ai21.j2-mid-v1', 'bedrock'),

        ('amazon.titan-embed-text-v1', 'bedrock'),
        ('amazon.titan-text-lite-v1', 'bedrock'),
        ('amazon.titan-text-express-v1', 'bedrock'),
        ('amazon.titan-image-generator-v2:0', 'bedrock'),
        ('amazon.titan-image-generator-v1', 'bedrock'),

        ('cohere.command-r-plus-v1:0', 'bedrock'),
        ('cohere.command-r-v1:0', 'bedrock'),
        ('cohere.embed-english-v3', 'bedrock'),
        ('cohere.embed-multilingual-v3', 'bedrock'),
        ('cohere.command-text-v14', 'bedrock'),

        ('meta.llama3-8b-instruct-v1:0', 'bedrock'),
        ('meta.llama3-70b-instruct-v1:0', 'bedrock'),
        ('meta.llama2-13b-chat-v1', 'bedrock'),
        ('meta.llama2-70b-chat-v1', 'bedrock'),
        ('meta.llama2-13b-v1', 'bedrock'),

    ]

    for model_id, owned_by in model_data:
        config.register_model(name=model_id, default_client=client, supports_streaming=True)

default_client = None
try:

    import boto3
    default_client = boto3.client('bedrock-runtime')
except Exception as e:
    pass

register(default_client)
====================================================================
-> Chunk: ell\provider.py::5


# handhold the the implementer, in production mode we can turn these off for speed.
@lru_cache(maxsize=None)
def _call_params(call: Callable[..., Any]) -> MappingProxyType[str, inspect.Parameter]:
    return inspect.signature(call).parameters


def _validate_provider_call_params(
    api_call_params: Dict[str, Any], call: Callable[..., Any]
):
    provider_call_params = _call_params(call)

    required_params = {
        name: param
        for name, param in provider_call_params.items()
        if param.default == param.empty and param.kind != param.VAR_KEYWORD
    }

    for param_name in required_params:
        assert (
            param_name in api_call_params
        ), f"Provider implementation error: Required parameter '{param_name}' is missing in the converted call parameters converted from ell call."

    for param_name, param_value in api_call_params.items():
        assert (
            param_name in provider_call_params
        ), f"Provider implementation error: Unexpected parameter '{param_name}' in the converted call parameters."

    return True
====================================================================


###### Cluster Eval ######
Score: 14.333333333333334
Cluster name: Persistent Storage Cluster
Persistent Storage Cluster:

-> Chunk: ell\store.py::2


class Store(ABC):
    """
    Abstract base class for serializers. Defines the interface for serializing and deserializing LMPs and invocations.
    """

    def __init__(self, blob_store: Optional[BlobStore] = None):
        self.blob_store = blob_store

    @property
    def has_blob_storage(self) -> bool:
        return self.blob_store is not None

    @abstractmethod
    def write_lmp(self, serialized_lmp: SerializedLMP, uses: Dict[str, Any]) -> Optional[Any]:
        """
        Write an LMP (Language Model Package) to the storage.

        :param serialized_lmp: SerializedLMP object containing all LMP details.
        :param uses: Dictionary of LMPs used by this LMP.
        :return: Optional return value.
        """
        pass

    @abstractmethod
    def write_invocation(self, invocation: Invocation,  consumes: Set[str]) -> Optional[Any]:
        """
        Write an invocation of an LMP to the storage.

        :param invocation: Invocation object containing all invocation details.
        :param results: List of SerializedLStr objects representing the results.
        :param consumes: Set of invocation IDs consumed by this invocation.
        :return: Optional return value.
        """
        pass

    @abstractmethod
    def get_cached_invocations(self, lmp_id :str, state_cache_key :str) -> List[Invocation]:
        """
        Get cached invocations for a given LMP and state cache key.
        """
        pass

    @abstractmethod
    def get_versions_by_fqn(self, fqn :str) -> List[SerializedLMP]:
        """
        Get all versions of an LMP by its fully qualified name.
        """
        pass
====================================================================
-> Chunk: ell\store.py::1


from abc import ABC, abstractmethod
from contextlib import contextmanager
from datetime import datetime
from typing import Any, Optional, Dict, List, Set, Union
from ell.types._lstr import _lstr
from ell.types import SerializedLMP, Invocation
from ell.types.message import InvocableLM

class BlobStore(ABC):
    @abstractmethod
    def store_blob(self, blob: bytes, blob_id  : str) -> str:
        """Store a blob and return its identifier."""
        pass

    @abstractmethod
    def retrieve_blob(self, blob_id: str) -> bytes:
        """Retrieve a blob by its identifier."""
        pass
====================================================================
-> Chunk: stores\sql.py::1


from datetime import datetime, timedelta
import json
import os
from typing import Any, Optional, Dict, List, Set, Union
from pydantic import BaseModel
from sqlmodel import Session, SQLModel, create_engine, select
import ell.store
import cattrs
import numpy as np
from sqlalchemy.sql import text
from ell.types import InvocationTrace, SerializedLMP, Invocation, InvocationContents
from ell.types._lstr import _lstr
from sqlalchemy import or_, func, and_, extract, FromClause
from sqlalchemy.types import TypeDecorator, VARCHAR
from ell.types.studio import SerializedLMPUses, utc_now
from ell.util.serialization import pydantic_ltype_aware_cattr
import gzip
import json
====================================================================
-> Chunk: stores\sql.py::2


class SQLStore(ell.store.Store):
    def __init__(self, db_uri: str, blob_store: Optional[ell.store.BlobStore] = None):
        self.engine = create_engine(db_uri,
                                    json_serializer=lambda obj: json.dumps(pydantic_ltype_aware_cattr.unstructure(obj), 
                                     sort_keys=True, default=repr, ensure_ascii=False))

        SQLModel.metadata.create_all(self.engine)
        self.open_files: Dict[str, Dict[str, Any]] = {}
        super().__init__(blob_store)

    def write_lmp(self, serialized_lmp: SerializedLMP, uses: Dict[str, Any]) -> Optional[Any]:
        with Session(self.engine) as session:
            # Bind the serialized_lmp to the session
            lmp = session.exec(select(SerializedLMP).filter(SerializedLMP.lmp_id == serialized_lmp.lmp_id)).first()

            if lmp:
                # Already added to the DB.
                return lmp
            else:
                session.add(serialized_lmp)

            for use_id in uses:
                used_lmp = session.exec(select(SerializedLMP).where(SerializedLMP.lmp_id == use_id)).first()
                if used_lmp:
                    serialized_lmp.uses.append(used_lmp)

            session.commit()
        return None
====================================================================


###### Cluster Eval ######
Score: 14.333333333333334
Cluster name: Build and Deployment Cluster
Build and Deployment Cluster:

-> Chunk: 0.1.0\autostreamprevention.py::1


import openai
import os

# Define the function to stream the response
def stream_openai_response(prompt):
    try:
        # Make the API call
        response = openai.chat.completions.create(
            model="o1-mini",  # Specify the model
            messages=[{"role": "user", "content": prompt}],
            stream=True  # Enable streaming
        )

        # Stream the response
        for chunk in response:
            if chunk.choices[0].delta.get("content"):
                print(chunk.choices[0].delta.content, end="", flush=True)

        print()  # Print a newline at the end

    except Exception as e:
        print(f"An error occurred: {e}")

# Example usage
prompt = "Tell me a short joke."
stream_openai_response(prompt)
====================================================================
-> Chunk: 0.1.0\mypytest.py::1


from typing import TypedDict


class Test(TypedDict):
    name: str
    age: int


def test(**t: Test):
    print(t)

# no type hinting like ts thats unfortunate.
test( )
====================================================================
-> Chunk: src\conf.py::1


# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

project = 'ell'
copyright = '2024, William Guss'
author = 'William Guss'

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.napoleon', 'sphinxawesome_theme', 'sphinxcontrib.autodoc_pydantic']

templates_path = ['_templates']
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

html_theme = "sphinxawesome_theme"


# Favicon configuration
html_favicon = '_static/favicon.ico'

# Configure syntax highlighting for Awesome Sphinx Theme
pygments_style = "default"
pygments_style_dark = "dracula"

# Additional theme configuration
====================================================================
-> Chunk: studio\__main__.py::1


import asyncio
import logging
import socket
import time
import webbrowser
import uvicorn
from argparse import ArgumentParser
from contextlib import closing
from ell.studio.config import Config
from ell.studio.server import create_app
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pathlib import Path
from watchfiles import awatch


logger = logging.getLogger(__file__)


def _socket_is_open(host, port) -> bool:
    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
        return sock.connect_ex((host, port)) == 0


def _setup_logging(level):
    logging.basicConfig(
        format='%(asctime)s %(levelname)-8s] %(message)s',
        level=level,
        datefmt='%Y-%m-%d %H:%M:%S'
    )
====================================================================


###### Cluster Eval ######
Score: 14.333333333333334
Cluster name: Invocation Management and Serialization Cluster
Invocation Management and Serialization Cluster:

-> Chunk: types\studio.py::6


class InvocationTrace(SQLModel, table=True):
    invocation_consumer_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)
    invocation_consuming_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)

# Should be subtyped for differnet kidns of LMPS.
# XXX: Move all ofh te binary data out to a different table.
# XXX: Need a flag that says dont store images.
# XXX: Deprecate the args columns
class InvocationBase(SQLModel):
    id: Optional[str] = Field(default=None, primary_key=True)
    lmp_id: str = Field(foreign_key="serializedlmp.lmp_id", index=True)
    latency_ms: float
    prompt_tokens: Optional[int] = Field(default=None)
    completion_tokens: Optional[int] = Field(default=None)
    state_cache_key: Optional[str] = Field(default=None)
    created_at: datetime = UTCTimestampField(default=func.now(), nullable=False)
    used_by_id: Optional[str] = Field(default=None, foreign_key="invocation.id", index=True)
    # global_vars and free_vars removed from here
====================================================================
-> Chunk: lmp\_track.py::1


import json
import logging
import threading
from ell.types import SerializedLMP, Invocation, InvocationTrace, InvocationContents
from ell.types.studio import LMPType, utc_now
from ell.util._warnings import _autocommit_warning
import ell.util.closure
from ell.configurator import config
from ell.types._lstr import _lstr

import inspect

import secrets
import time
from datetime import datetime
from functools import wraps
from typing import Any, Callable, Dict, Iterable, Optional, OrderedDict, Tuple

from ell.util.serialization import get_immutable_vars
from ell.util.serialization import compute_state_cache_key
from ell.util.serialization import prepare_invocation_params

logger = logging.getLogger(__name__)

# Thread-local storage for the invocation stack
_invocation_stack = threading.local()

def get_current_invocation() -> Optional[str]:
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    return _invocation_stack.stack[-1] if _invocation_stack.stack else None

def push_invocation(invocation_id: str):
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    _invocation_stack.stack.append(invocation_id)

def pop_invocation():
    if hasattr(_invocation_stack, 'stack') and _invocation_stack.stack:
        _invocation_stack.stack.pop()
====================================================================
-> Chunk: types\studio.py::5


class SerializedLMP(SerializedLMPBase, table=True):
    invocations: List["Invocation"] = Relationship(back_populates="lmp")
    used_by: Optional[List["SerializedLMP"]] = Relationship(
        back_populates="uses",
        link_model=SerializedLMPUses,
        sa_relationship_kwargs=dict(
            primaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id",
            secondaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id",
        ),
    )
    uses: List["SerializedLMP"] = Relationship(
        back_populates="used_by",
        link_model=SerializedLMPUses,
        sa_relationship_kwargs=dict(
            primaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id",
            secondaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id",
        ),
    )

    class Config:
        table_name = "serializedlmp"
        unique_together = [("version_number", "name")]
====================================================================
-> Chunk: types\studio.py::7


class InvocationContentsBase(SQLModel):
    invocation_id: str = Field(foreign_key="invocation.id", index=True, primary_key=True)
    params: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSON))
    results: Optional[Union[List[Message], Any]] = Field(default=None, sa_column=Column(JSON))
    invocation_api_params: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSON))
    global_vars: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSON))
    free_vars: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSON))
    is_external : bool = Field(default=False)
====================================================================


###### Cluster Eval ######
Score: 14.333333333333334
Cluster name: String Operations with Metadata Cluster
String Operations with Metadata Cluster:

-> Chunk: types\_lstr.py::7


class _lstr(str):

    def __mul__(self, other: SupportsIndex) -> "_lstr":
        """
        Perform a multiplication operation between this lstr instance and an integer or another lstr,
        tracing the operation by logging the operands and the result.

        Args:
            other (Union[SupportsIndex, "lstr"]): The right operand in the multiplication operation.

        Returns:
            lstr: A new lstr instance containing the result of the multiplication operation, with theorigin_trace(s) updated accordingly.
        """
        if isinstance(other, SupportsIndex):
            result_content = super(_lstr, self).__mul__(other)
            new__origin_trace__ = self.__origin_trace__
        else:
            return NotImplemented

        return _lstr(result_content, None, new__origin_trace__)
====================================================================
-> Chunk: types\_lstr.py::3


class _lstr(str):

    @classmethod
    def __get_pydantic_core_schema__(
        cls, source_type: Any, handler: GetCoreSchemaHandler
    ) -> CoreSchema:
        def validate_lstr(value):
            if isinstance(value, dict) and value.get("__lstr", False):
                content = value["content"]
                origin_trace = value["__origin_trace__"].split(",")
                return cls(content, origin_trace=origin_trace)
            elif isinstance(value, str):
                return cls(value)
            elif isinstance(value, cls):
                return value
            else:
                raise ValueError(f"Invalid value for lstr: {value}")

        return core_schema.json_or_python_schema(
            json_schema=core_schema.typed_dict_schema(
                {
                    "content": core_schema.typed_dict_field(core_schema.str_schema()),
                    "__origin_trace__": core_schema.typed_dict_field(
                        core_schema.str_schema()
                    ),
                    "__lstr": core_schema.typed_dict_field(core_schema.bool_schema()),
                }
            ),
            python_schema=core_schema.union_schema(
                [
                    core_schema.is_instance_schema(cls),
                    core_schema.no_info_plain_validator_function(validate_lstr),
                ]
            ),
            serialization=core_schema.plain_serializer_function_ser_schema(
                lambda instance: {
                    "content": str(instance),
                    "__origin_trace__": (instance.__origin_trace__),
                    "__lstr": True,
                }
            ),
        )
====================================================================
-> Chunk: types\_lstr.py::4


class _lstr(str):

    @property
    def origin_trace(self) -> FrozenSet[str]:
        """
        Get theorigin_trace(s) of this lstr instance.

        Returns:
            FrozenSet[str]: A frozen set of strings representing theorigin_trace(s) of this lstr instance.
        """
        return self.__origin_trace__

    ########################
    ## Overriding methods ##
    ########################
    def __repr__(self) -> str:
        """
        Return a string representation of this lstr instance.

        Returns:
            str: A string representation of this lstr instance, including its content, logits, andorigin_trace(s).
        """
        return super().__repr__()
====================================================================
-> Chunk: types\_lstr.py::9


class _lstr(str):

    def __getitem__(self, key: Union[SupportsIndex, slice]) -> "_lstr":
        """
        Get a slice or index of this lstr instance.

        Args:
            key (Union[SupportsIndex, slice]): The index or slice to retrieve.

        Returns:
            lstr: A new lstr instance containing the sliced or indexed content, with theorigin_trace(s) preserved.
        """
        result = super(_lstr, self).__getitem__(key)
        # This is a matter of opinon. I believe that when you Index into a language model output, you or divorcing the lodges of the indexed result from their contacts which produce them. Therefore, it is only reasonable to directly index into the lodges without changing the original context, and so any mutation on the string should invalidate the logits.
        # try:
        #     logit_subset = self._logits[key] if self._logits else None
        # except:
        #   logit_subset = None
        logit_subset = None
        return _lstr(result, logit_subset, self.__origin_trace__)
====================================================================
-> Chunk: types\_lstr.py::19


if __name__ == "__main__":
    import timeit
    import random
    import string

    def generate_random_string(length):
        return "".join(random.choices(string.ascii_letters + string.digits, k=length))

    def test_concatenation():
        s1 = generate_random_string(1000)
        s2 = generate_random_string(1000)

        lstr_time = timeit.timeit(lambda: _lstr(s1) + _lstr(s2), number=10000)
        str_time = timeit.timeit(lambda: s1 + s2, number=10000)

        print(f"Concatenation: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    def test_slicing():
        s = generate_random_string(10000)
        ls = _lstr(s)

        lstr_time = timeit.timeit(lambda: ls[1000:2000], number=10000)
        str_time = timeit.timeit(lambda: s[1000:2000], number=10000)

        print(f"Slicing: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    def test_splitting():
        s = generate_random_string(10000)
        ls = _lstr(s)

        lstr_time = timeit.timeit(lambda: ls.split(), number=1000)
        str_time = timeit.timeit(lambda: s.split(), number=1000)

        print(f"Splitting: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    def test_joining():
        words = [generate_random_string(10) for _ in range(1000)]
        lwords = [_lstr(word) for word in words]

        lstr_time = timeit.timeit(lambda: _lstr(" ").join(lwords), number=1000)
        str_time = timeit.timeit(lambda: " ".join(words), number=1000)

        print(f"Joining: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    print("Running performance tests...")
    test_concatenation()
    test_slicing()
    test_splitting()
    test_joining()

    import cProfile
    import pstats
    from io import StringIO

    def test_add():
        s1 = generate_random_string(1000)
        s2 = generate_random_string(1000)
        ls1 = _lstr(s1, None, "origin1")
        ls2 = _lstr(s2, None, "origin2")

        for _ in range(100000):
            result = ls1 + ls2

    print("\nProfiling __add__ method:")
    profiler = cProfile.Profile()
    profiler.enable()
    test_add()
    profiler.disable()

    s = StringIO()
    ps = pstats.Stats(profiler, stream=s).sort_stats("cumulative")
    ps.print_stats(20)  # Print top 20 lines
    print(s.getvalue())
====================================================================


###### Cluster Eval ######
Score: 14.0
Cluster name: Studio Server and API Cluster
Studio Server and API Cluster:

-> Chunk: studio\server.py::2


def create_app(config:Config):
    serializer = get_serializer(config)

    def get_session():
        with Session(serializer.engine) as session:
            yield session

    app = FastAPI(title="ell Studio", version=__version__)

    # Enable CORS for all origins
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    manager = ConnectionManager()

    @app.websocket("/ws")
    async def websocket_endpoint(websocket: WebSocket):
        await manager.connect(websocket)
        try:
            while True:
                data = await websocket.receive_text()
                # Handle incoming WebSocket messages if needed
        except WebSocketDisconnect:
            manager.disconnect(websocket)


    @app.get("/api/latest/lmps", response_model=list[SerializedLMPWithUses])
    def get_latest_lmps(
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        session: Session = Depends(get_session)
    ):
        lmps = serializer.get_latest_lmps(
            session,
            skip=skip, limit=limit,
            )
        return lmps

    # TOOD: Create a get endpoint to efficient get on the index with /api/lmp/<lmp_id>
    @app.get("/api/lmp/{lmp_id}")
    def get_lmp_by_id(lmp_id: str, session: Session = Depends(get_session)):
        lmp = serializer.get_lmps(session, lmp_id=lmp_id)[0]
        return lmp
    # ... other code
====================================================================
-> Chunk: studio\server.py::3


def create_app(config:Config):
    # ... other code



    @app.get("/api/lmps", response_model=list[SerializedLMPWithUses])
    def get_lmp(
        lmp_id: Optional[str] = Query(None),
        name: Optional[str] = Query(None),
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        session: Session = Depends(get_session)
    ):

        filters : Dict[str, Any] = {}
        if name:
            filters['name'] = name
        if lmp_id:
            filters['lmp_id'] = lmp_id

        lmps = serializer.get_lmps(session, skip=skip, limit=limit, **filters)

        if not lmps:
            raise HTTPException(status_code=404, detail="LMP not found")

        print(lmps[0])
        return lmps



    @app.get("/api/invocation/{invocation_id}", response_model=InvocationPublicWithConsumes)
    def get_invocation(
        invocation_id: str,
        session: Session = Depends(get_session)
    ):
        invocation = serializer.get_invocations(session, lmp_filters=dict(), filters={"id": invocation_id})[0]
        return invocation
    # ... other code
====================================================================
-> Chunk: studio\server.py::4


def create_app(config:Config):
    # ... other code

    @app.get("/api/invocations", response_model=list[InvocationPublicWithConsumes])
    def get_invocations(
        id: Optional[str] = Query(None),
        hierarchical: Optional[bool] = Query(False),
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        lmp_name: Optional[str] = Query(None),
        lmp_id: Optional[str] = Query(None),
        session: Session = Depends(get_session)
    ):
        lmp_filters = {}
        if lmp_name:
            lmp_filters["name"] = lmp_name
        if lmp_id:
            lmp_filters["lmp_id"] = lmp_id

        invocation_filters = {}
        if id:
            invocation_filters["id"] = id

        invocations = serializer.get_invocations(
            session,
            lmp_filters=lmp_filters,
            filters=invocation_filters,
            skip=skip,
            limit=limit,
            hierarchical=hierarchical
        )
        return invocations
    # ... other code
====================================================================
-> Chunk: studio\config.py::1


from functools import lru_cache
import os
from typing import Optional
from pydantic import BaseModel

import logging

logger = logging.getLogger(__name__)


# todo. maybe we default storage dir and other things in the future to a well-known location
# like ~/.ell or something
@lru_cache
def ell_home() -> str:
    return os.path.join(os.path.expanduser("~"), ".ell")


class Config(BaseModel):
    pg_connection_string: Optional[str] = None
    storage_dir: Optional[str] = None

    @classmethod
    def create(
        cls,
        storage_dir: Optional[str] = None,
        pg_connection_string: Optional[str] = None,
    ) -> 'Config':
        pg_connection_string = pg_connection_string or os.getenv("ELL_PG_CONNECTION_STRING")
        storage_dir = storage_dir or os.getenv("ELL_STORAGE_DIR")

        # Enforce that we use either sqlite or postgres, but not both
        if pg_connection_string is not None and storage_dir is not None:
            raise ValueError("Cannot use both sqlite and postgres")

        # For now, fall back to sqlite if no PostgreSQL connection string is provided
        if pg_connection_string is None and storage_dir is None:
            # This intends to honor the default we had set in the CLI
            storage_dir = os.getcwd()

        return cls(pg_connection_string=pg_connection_string, storage_dir=storage_dir)
====================================================================
-> Chunk: studio\connection_manager.py::1


from fastapi import WebSocket


class ConnectionManager:
    def __init__(self):
        self.active_connections = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            print(f"Broadcasting message to {connection} {message}")
            await connection.send_text(message)
====================================================================


###### Cluster Eval ######
Score: 14.0
Cluster name: Initialization and Core Setup Cluster
Initialization and Core Setup Cluster:

-> Chunk: ell\__init__.py::1


"""
ell is a Python library for language model programming (LMP). It provides a simple
and intuitive interface for working with large language models.
"""


from ell.lmp.simple import simple
from ell.lmp.tool import tool
from ell.lmp.complex import complex
from ell.types.message import system, user, assistant, Message, ContentBlock
from ell.__version__ import __version__

# Import all models
import ell.providers
import ell.models


# Import everything from configurator
from ell.configurator import *
====================================================================
-> Chunk: ell\__version__.py::1


try:
    from importlib.metadata import version
except ImportError:
    from importlib_metadata import version

__version__ = version("ell-ai")
====================================================================
-> Chunk: lmp\__init__.py::1


from ell.lmp.simple import simple
from ell.lmp.complex import complex
====================================================================
-> Chunk: models\__init__.py::1


"""
Attempts to registeres model names with their respective API client bindings. This allows for the creation of a unified interface for interacting with different LLM providers.

For example, to register an OpenAI model:
@ell.simple(model='gpt-4o-mini') -> @ell.simple(model='gpt-4o-mini', client=openai.OpenAI())

"""

import ell.models.openai
import ell.models.anthropic
import ell.models.ollama
import ell.models.groq
import ell.models.bedrock
====================================================================
-> Chunk: providers\__init__.py::1


import ell.providers.openai
import ell.providers.groq
import ell.providers.anthropic
import ell.providers.bedrock
# import ell.providers.mistral
# import ell.providers.cohere
# import ell.providers.gemini
# import ell.providers.elevenlabs
# import ell.providers.replicate
# import ell.providers.huggingface
====================================================================


###### Cluster Eval ######
Score: 14.0
Cluster name: Text and Data Transformation Cluster
Text and Data Transformation Cluster:

-> Chunk: types\_lstr.py::2


class _lstr(str):

    def __new__(
        cls,
        content: str,
        logits: Optional[np.ndarray] = None,
        origin_trace: Optional[Union[str, FrozenSet[str]]] = None,
    ):
        """
         Create a new instance of lstr. The `logits` should be a numpy array and `origin_trace` should be a frozen set of strings or a single string.

         Args:
         content (str): The string content of the lstr.
         logits (np.ndarray, optional): The logits associated with this string. Defaults to None.
        origin_trace (Union[str, FrozenSet[str]], optional): Theorigin_trace(s) of this string. Defaults to None.
        """
        instance = super(_lstr, cls).__new__(cls, content)
        # instance._logits = logits
        if isinstance(origin_trace, str):
            instance.__origin_trace__ = frozenset({origin_trace})
        else:
            instance.__origin_trace__ = (
                frozenset(origin_trace) if origin_trace is not None else frozenset()
            )
        return instance

    # _logits: Optional[np.ndarray]
    __origin_trace__: FrozenSet[str]
====================================================================
-> Chunk: types\_lstr.py::5


class _lstr(str):

    def __add__(self, other: Union[str, "_lstr"]) -> "_lstr":
        """
        Concatenate this lstr instance with another string or lstr instance.

        Args:
            other (Union[str, "lstr"]): The string or lstr instance to concatenate with this instance.

        Returns:
            lstr: A new lstr instance containing the concatenated content, with theorigin_trace(s) updated accordingly.
        """
        new_content = super(_lstr, self).__add__(other)
        self_origin = self.__origin_trace__

        if isinstance(other, _lstr):
            new_origin = self_origin
            new_origin = new_origin.union(other.__origin_trace__)
        else:
            new_origin = self_origin

        return _lstr(new_content, None, frozenset(new_origin))
====================================================================
-> Chunk: types\_lstr.py::12


class _lstr(str):

    @override
    def split(
        self, sep: Optional[Union[str, "_lstr"]] = None, maxsplit: SupportsIndex = -1
    ) -> List["_lstr"]:
        """
        Split this lstr instance into a list of lstr instances based on a separator.

        Args:
            sep (Optional[Union[str, "lstr"]], optional): The separator to split on. Defaults to None.
            maxsplit (SupportsIndex, optional): The maximum number of splits to perform. Defaults to -1.

        Returns:
            List["lstr"]: A list of lstr instances containing the split content, with theorigin_trace(s) preserved.
        """
        return self._split_helper(super(_lstr, self).split, sep, maxsplit)
====================================================================
-> Chunk: types\_lstr.py::17


class _lstr(str):

    def _partition_helper(
        self, method, sep: Union[str, "_lstr"]
    ) -> Tuple["_lstr", "_lstr", "_lstr"]:
        """
        Helper method for partitioning this lstr instance based on a separator.

        Args:
            method (Callable): The partitioning method to use (either partition or rpartition).
            sep (Union[str, "lstr"]): The separator to partition on.

        Returns:
            Tuple["lstr", "lstr", "lstr"]: A tuple of three lstr instances containing the content before the separator, the separator itself, and the content after the separator, with theorigin_trace(s) updated accordingly.
        """
        part1, part2, part3 = method(sep)
        new__origin_trace__ = (
            self.__origin_trace__ | sep.__origin_trace__
            if isinstance(sep, _lstr)
            else self.__origin_trace__
        )
        return (
            _lstr(part1, None, new__origin_trace__),
            _lstr(part2, None, new__origin_trace__),
            _lstr(part3, None, new__origin_trace__),
        )
====================================================================


###### Cluster Eval ######
Score: 13.666666666666666
Cluster name: Core Configuration and Registry Cluster
Core Configuration and Registry Cluster:

-> Chunk: ell\configurator.py::2


class Config(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    registry: Dict[str, _Model] = Field(default_factory=dict, description="A dictionary mapping model names to their configurations.")
    verbose: bool = Field(default=False, description="If True, enables verbose logging.")
    wrapped_logging: bool = Field(default=True, description="If True, enables wrapped logging for better readability.")
    override_wrapped_logging_width: Optional[int] = Field(default=None, description="If set, overrides the default width for wrapped logging.")
    store: Optional[Store] = Field(default=None, description="An optional Store instance for persistence.")
    autocommit: bool = Field(default=False, description="If True, enables automatic committing of changes to the store.")
    lazy_versioning: bool = Field(default=True, description="If True, enables lazy versioning for improved performance.")
    default_api_params: Dict[str, Any] = Field(default_factory=dict, description="Default parameters for language models.")
    default_client: Optional[openai.Client] = Field(default=None, description="The default OpenAI client used when a specific model client is not found.")
    autocommit_model: str = Field(default="gpt-4o-mini", description="When set, changes the default autocommit model from GPT 4o mini.")
    providers: Dict[Type, Provider] = Field(default_factory=dict, description="A dictionary mapping client types to provider classes.")
    def __init__(self, **data):
        super().__init__(**data)
        self._lock = threading.Lock()
        self._local = threading.local()
====================================================================
-> Chunk: ell\configurator.py::1


from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)
====================================================================
-> Chunk: ell\configurator.py::3


class Config(BaseModel):


    def register_model(
        self, 
        name: str,
        default_client: Optional[Union[openai.Client, Any]] = None,
        supports_streaming: Optional[bool] = None
    ) -> None:
        """
        Register a model with its configuration.
        """
        with self._lock:
            # XXX: Will be deprecated in 0.1.0
            self.registry[name] = _Model(
                name=name,
                default_client=default_client,
                supports_streaming=supports_streaming
            )
====================================================================
-> Chunk: ell\configurator.py::5


class Config(BaseModel):

    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:
        """
        Get the OpenAI client for a specific model name.

        :param model_name: The name of the model to get the client for.
        :type model_name: str
        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.
        :rtype: Tuple[Optional[openai.Client], bool]
        """
        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry
        model_config = current_registry.get(model_name)
        fallback = False
        if not model_config:
            warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."
            if self.verbose:
                from colorama import Fore, Style
                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")
            else:
                _config_logger.debug(warning_message)
            client = self.default_client
            fallback = True
        else:
            client = model_config.default_client
        return client, fallback
====================================================================
-> Chunk: ell\configurator.py::9


# Existing helper functions
def get_store() -> Union[Store, None]:
    return config.store

# Will be deprecated at 0.1.0 

# You can add more helper functions here if needed
def register_provider(provider: Provider, client_type: Type[Any]) -> None:
    return config.register_provider(provider, client_type)

# Deprecated now (remove at 0.1.0)
def set_store(*args, **kwargs) -> None:
    raise DeprecationWarning("The set_store function is deprecated and will be removed in a future version. Use ell.init(store=...) instead.")
====================================================================


###### Cluster Eval ######
Score: 13.666666666666666
Cluster name: Language Model Program Decorators and Execution Cluster
Language Model Program Decorators and Execution Cluster:

-> Chunk: lmp\complex.py::2


def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    default_client_from_decorator = client
    default_model_from_decorator = model
    default_api_params_from_decorator = api_params
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        _warnings(model, prompt, default_client_from_decorator)

        @wraps(prompt)
        def model_call(
            *prompt_args,
            _invocation_origin : Optional[str] = None,
            client: Optional[Any] = None,
            api_params: Optional[Dict[str, Any]] = None,
            lm_params: Optional[DeprecationWarning] = None,
            **prompt_kwargs,
        ) -> Tuple[Any, Any, Any]:
            # XXX: Deprecation in 0.1.0
            if lm_params:
                raise DeprecationWarning("lm_params is deprecated. Use api_params instead.")

            # promt -> str
            res = prompt(*prompt_args, **prompt_kwargs)
            # Convert prompt into ell messages
            messages = _get_messages(res, prompt)

            # XXX: move should log to a logger.
            should_log = not exempt_from_tracking and config.verbose
            # Cute verbose logging.
            if should_log: model_usage_logger_pre(prompt, prompt_args, prompt_kwargs, "[]", messages) #type: ignore

            # Call the model.
            # Merge API params
            merged_api_params = {**config.default_api_params, **default_api_params_from_decorator, **(api_params or {})}
            n = merged_api_params.get("n", 1)
            # Merge client overrides & client registry
            merged_client = _client_for_model(model, client or default_client_from_decorator)
            ell_call = EllCallParams(
                # XXX: Could change behaviour of overriding ell params for dyanmic tool calls.
                model=merged_api_params.pop("model", default_model_from_decorator),
                messages=messages,
                client = merged_client,
                api_params=merged_api_params,
                tools=tools or [],
            )
            # Get the provider for the model
            provider = config.get_provider_for(ell_call.client)
            assert provider is not None, f"No provider found for client {ell_call.client}."

            if should_log: model_usage_logger_post_start(n)
            with model_usage_logger_post_intermediate(n) as _logger:
                (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)
                if isinstance(result, list) and len(result) == 1:
                    result = result[0]

            result = post_callback(result) if post_callback else result
            if should_log:
                model_usage_logger_post_end()
            #
            #  These get sent to track. This is wack.           
            return result, final_api_params, metadata
        # ... other code
    # ... other code
====================================================================
-> Chunk: lmp\complex.py::3


def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        # ... other code



        model_call.__ell_api_params__ = default_api_params_from_decorator #type: ignore
        model_call.__ell_func__ = prompt #type: ignore
        model_call.__ell_type__ = LMPType.LM #type: ignore
        model_call.__ell_exempt_from_tracking = exempt_from_tracking #type: ignore


        if exempt_from_tracking:
            return model_call
        else:
            # XXX: Analyze decorators with AST instead.
            return _track(model_call, forced_dependencies=dict(tools=tools, response_format=api_params.get("response_format", {})))
    return parameterized_lm_decorator
====================================================================
-> Chunk: lmp\simple.py::1


from functools import wraps
from typing import Any, Optional

from ell.lmp.complex import complex


def simple(model: str, client: Optional[Any] = None,  exempt_from_tracking=False, **api_params):
    assert 'tools' not in api_params, "tools are not supported in lm decorator, use multimodal decorator instead"
    assert 'tool_choice' not in api_params, "tool_choice is not supported in lm decorator, use multimodal decorator instead"
    assert 'response_format' not in api_params or isinstance(api_params.get('response_format', None), dict), "response_format is not supported in lm decorator, use multimodal decorator instead"

    def convert_multimodal_response_to_lstr(response):
        return [x.content[0].text for x in response] if isinstance(response, list) else response.content[0].text
    return complex(model, client,  exempt_from_tracking=exempt_from_tracking, **api_params, post_callback=convert_multimodal_response_to_lstr)
====================================================================
-> Chunk: lmp\simple.py::2


simple.__doc__ = """The fundamental unit of language model programming in ell.

  This decorator simplifies the process of creating Language Model Programs (LMPs) 
  that return text-only outputs from language models, while supporting multimodal inputs.
  It wraps the more complex 'complex' decorator, providing a streamlined interface for common use cases.

  :param model: The name or identifier of the language model to use.
  :type model: str
  :param client: An optional OpenAI client instance. If not provided, a default client will be used.
  :type client: Optional[openai.Client]
  :param exempt_from_tracking: If True, the LMP usage won't be tracked. Default is False.
  :type exempt_from_tracking: bool
  :param api_params: Additional keyword arguments to pass to the underlying API call.
  :type api_params: Any

  Usage:
  The decorated function can return either a single prompt or a list of ell.Message objects:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def summarize_text(text: str) -> str:
          '''You are an expert at summarizing text.''' # System prompt
          return f"Please summarize the following text:\\n\\n{text}" # User prompt


      @ell.simple(model="gpt-4", temperature=0.7)
      def describe_image(image : PIL.Image.Image) -> List[ell.Message]:
          '''Describe the contents of an image.''' # unused because we're returning a list of Messages
          return [
              # helper function for ell.Message(text="...", role="system")
              ell.system("You are an AI trained to describe images."),
              # helper function for ell.Message(content="...", role="user")
              ell.user(["Describe this image in detail.", image]),
          ]


      image_description = describe_image(PIL.Image.open("https://example.com/image.jpg"))
      print(image_description) 
      # Output will be a string text-only description of the image

      summary = summarize_text("Long text to summarize...")
      print(summary)
      # Output will be a text-only summary

  Notes:

  - This decorator is designed for text-only model outputs, but supports multimodal inputs.
  - It simplifies complex responses from language models to text-only format, regardless of 
    the model's capability for structured outputs, function calling, or multimodal outputs.
  - For preserving complex model outputs (e.g., structured data, function calls, or multimodal 
    outputs), use the @ell.complex decorator instead. @ell.complex returns a Message object (role='assistant')
  - The decorated function can return a string or a list of ell.Message objects for more 
    complex prompts, including multimodal inputs.
  - If called with n > 1 in api_params, the wrapped LMP will return a list of strings for the n parallel outputs
    of the model instead of just one string. Otherwise, it will return a single string.
  - You can pass LM API parameters either in the decorator or when calling the decorated function.
    Parameters passed during the function call will override those set in the decorator.

  Example of passing LM API params:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def generate_story(prompt: str) -> str:
          return f"Write a short story based on this prompt: {prompt}"

      # Using default parameters
      story1 = generate_story("A day in the life of a time traveler")

      # Overriding parameters during function call
      story2 = generate_story("An AI's first day of consciousness", api_params={"temperature": 0.9, "max_tokens": 500})

  See Also:

  - :func:`ell.complex`: For LMPs that preserve full structure of model responses, including multimodal outputs.
  - :func:`ell.tool`: For defining tools that can be used within complex LMPs.
  - :mod:`ell.studio`: For visualizing and analyzing LMP executions.
    """
====================================================================
-> Chunk: lmp\tool.py::1


from functools import wraps
import json
from typing import Any, Callable, Optional

from pydantic import Field, create_model
from pydantic.fields import FieldInfo
from ell.lmp._track import _track
# from ell.types import ToolFunction, InvocableTool, ToolParams
# from ell.util.verbosity import compute_color, tool_usage_logger_pre
from ell.configurator import config
from ell.types._lstr import _lstr
from ell.types.studio import LMPType
import inspect

from ell.types.message import ContentBlock, InvocableTool, ToolResult, to_content_blocks
====================================================================


###### Cluster Eval ######
Score: 13.666666666666666
Cluster name: Configuration Management Cluster
Configuration Management Cluster:

-> Chunk: ell\configurator.py::6


class Config(BaseModel):

    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:
        """
        Register a provider class for a specific client type.

        :param provider_class: The provider class to register.
        :type provider_class: Type[Provider]
        """
        assert isinstance(client_type, type), "client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))"
        with self._lock:
            self.providers[client_type] = provider
====================================================================
-> Chunk: ell\configurator.py::4


class Config(BaseModel):



    @contextmanager
    def model_registry_override(self, overrides: Dict[str, _Model]):
        """
        Temporarily override the model registry with new model configurations.

        :param overrides: A dictionary of model names to ModelConfig instances to override.
        :type overrides: Dict[str, ModelConfig]
        """
        if not hasattr(self._local, 'stack'):
            self._local.stack = []

        with self._lock:
            current_registry = self._local.stack[-1] if self._local.stack else self.registry
            new_registry = current_registry.copy()
            new_registry.update(overrides)

        self._local.stack.append(new_registry)
        try:
            yield
        finally:
            self._local.stack.pop()
====================================================================
-> Chunk: ell\configurator.py::7


class Config(BaseModel):

    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:
        """
        Get the provider instance for a specific client instance.

        :param client: The client instance to get the provider for.
        :type client: Any
        :return: The provider instance for the specified client, or None if not found.
        :rtype: Optional[Provider]
        """

        client_type = type(client) if not isinstance(client, type) else client
        for provider_type, provider in self.providers.items():
            if issubclass(client_type, provider_type) or client_type == provider_type:
                return provider
        return None

# Single* instance
# XXX: Make a singleton
config = Config()
====================================================================
-> Chunk: models\anthropic.py::1


from ell.configurator import config
import logging

logger = logging.getLogger(__name__)


try:
    import anthropic

    def register(client: anthropic.Anthropic):
        """
        Register Anthropic models with the provided client.

        This function takes an Anthropic client and registers various Anthropic models
        with the global configuration. It allows the system to use these models
        for different AI tasks.

        Args:
            client (anthropic.Anthropic): An instance of the Anthropic client to be used
                                          for model registration.

        Note:
            The function doesn't return anything but updates the global
            configuration with the registered models.
        """
        model_data = [
            ('claude-3-opus-20240229', 'anthropic'),
            ('claude-3-sonnet-20240229', 'anthropic'),
            ('claude-3-haiku-20240307', 'anthropic'),
            ('claude-3-5-sonnet-20240620', 'anthropic'),
        ]
        for model_id, owned_by in model_data:
            config.register_model(model_id, client)

    try:
        default_client = anthropic.Anthropic()
        register(default_client)
    except Exception as e:
        # logger.warning(f"Failed to create default Anthropic client: {e}")
        pass


except ImportError:
    pass
====================================================================
-> Chunk: providers\bedrock.py::1


from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent
from ell.types._lstr import _lstr
import json
from ell.configurator import config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image
from io import BytesIO
import requests
from PIL import Image as PILImage
====================================================================


###### Cluster Eval ######
Score: 13.666666666666666
Cluster name: Lexical Closure and Dependency Management Cluster
Lexical Closure and Dependency Management Cluster:

-> Chunk: util\closure.py::15


def lexically_closured_source(func, forced_dependencies: Optional[Dict[str, Any]] = None):
    """
    Generate a lexically closured source for a given function.

    This function takes a callable object (function, method, or class) and generates
    a lexically closured source code. It captures all the dependencies, including
    global variables, free variables, and nested functions, to create a self-contained
    version of the function that can be executed independently.

    Args:
        func (Callable): The function or callable object to process.
        forced_dependencies (Optional[Dict[str, Any]]): A dictionary of additional
            dependencies to include in the closure. Keys are variable names, and
            values are the corresponding objects.

    Returns:
        Tuple[str, Set[Any]]: A tuple containing two elements:
            1. The lexically closured source code as a string.
            2. A set of function objects that this closure uses.

    Raises:
        ValueError: If the input is not a callable object.

    Example:
        def outer(x):
            y = 10
            def inner():
                return x + y
            return inner

        closured_source, uses = lexically_closured_source(outer)
        print(closured_source)
        # Output will include the source for both outer and inner functions,
        # along with any necessary imports and variable definitions.

    Note:
        This function relies on the `lexical_closure` function to perform the
        actual closure generation. It also uses the `__ell_closure__` attribute
        of the function, which is expected to be set by the `lexical_closure` function.
    """
    if not callable(func):
        raise ValueError("Input must be a callable object (function, method, or class).")
    _, fnclosure, uses = lexical_closure(func, initial_call=True, recursion_stack=[], forced_dependencies=forced_dependencies)
    return func.__ell_closure__, uses
====================================================================
-> Chunk: util\closure.py::5


def _process_default_kwargs(func, dependencies, already_closed, recursion_stack, uses):
    """Process default keyword arguments and annotations of a function."""
    ps = inspect.signature(func).parameters
    for name, param in ps.items():
        if param.default is not inspect.Parameter.empty:
            _process_signature_dependency(param.default, dependencies, already_closed, recursion_stack, uses, name)
        if param.annotation is not inspect.Parameter.empty:
            _process_signature_dependency(param.annotation, dependencies, already_closed, recursion_stack, uses, f"{name}_annotation")
    if func.__annotations__.get('return') is not None:
        _process_signature_dependency(func.__annotations__['return'], dependencies, already_closed, recursion_stack, uses, "return_annotation")
    # XXX: In order to properly analyze this we should walk the AST rather than inspexting the signature; e.g. Field is FieldInfo not Field.
    # I don't care about the actual default at time of execution just the symbols required to statically reproduce the prompt.
====================================================================
-> Chunk: util\closure.py::8


def _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses):
    """Process a callable (function, method, or class)."""
    try:
        module_is_ell = 'ell' in inspect.getmodule(var_value).__name__
    except:
        module_is_ell = False

    if var_name not in FORBIDDEN_NAMES and not module_is_ell:
        try:
            dep, _, _uses = lexical_closure(var_value, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            dependencies.append(dep)
            uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of global or free variable {var_name}", e, recursion_stack)
====================================================================
-> Chunk: util\closure_util.py::3


def get_referenced_names(code: str, module_name: str):
    """
    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)
    and walks through the tree to find all instances where an attribute of the module is referenced in the code.

    Parameters:
    code (str): The block of code to be parsed.
    module_name (str): The name of the module to look for in the code.

    Returns:
    list: A list of all attributes of the module that are referenced in the code.
    """
    tree = ast.parse(code)
    referenced_names = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Attribute):
            if isinstance(node.value, ast.Name) and node.value.id == module_name:
                referenced_names.append(node.attr)

    return referenced_names
====================================================================


###### Cluster Eval ######
Score: 13.333333333333334
Cluster name: Provider Interface Cluster
Provider Interface Cluster:

-> Chunk: ell\provider.py::3


# XXX: Needs a better name.
class Provider(ABC):
    """
    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.
    For example, the OpenAI provider is an API interface to OpenAI's API but also to Ollama and Azure OpenAI.
    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.
    """
    dangerous_disable_validation = False

    ################################
    ### API PARAMETERS #############
    ################################
    @abstractmethod
    def provider_call_function(
        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None
    ) -> Callable[..., Any]:
        """
        Implement this method to return the function that makes the API call to the language model.
        For example, if you're implementing the OpenAI provider, you would return the function that makes the API call to OpenAI's API.
        """
        return NotImplemented

    def disallowed_api_params(self) -> FrozenSet[str]:
        """
        Returns a list of disallowed call params that ell will override.
        """
        return frozenset({"messages", "tools", "model", "stream", "stream_options"})

    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):
        params = _call_params(self.provider_call_function(client, api_params))
        return frozenset(params.keys()) - self.disallowed_api_params()

    ################################
    ### TRANSLATION ###############
    ################################
    @abstractmethod
    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:
        """Converts an ell call to provider call params!"""
        return NotImplemented

    @abstractmethod
    def translate_from_provider(
        self,
        provider_response: Any,
        ell_call: EllCallParams,
        provider_call_params: Dict[str, Any],
        origin_id: Optional[str] = None,
        logger: Optional[Callable[..., None]] = None,
    ) -> Tuple[List[Message], Metadata]:
        """Converts provider responses to universal format. with metadata"""
        return NotImplemented

    ################################
    ### CALL MODEL ################
    ################################
    # Be careful to override this method in your provider.
====================================================================
-> Chunk: ell\provider.py::4


class Provider(ABC):
    def call(
        self,
        #XXX: In future refactors, we can fully enumerate the args and make ell_call's internal to the _provider implementer interface.
        # This gives us a litellm style interface for free.
        ell_call: EllCallParams,
        origin_id: Optional[str] = None,
        logger: Optional[Any] = None,
    ) -> Tuple[List[Message], Dict[str, Any], Metadata]:
        # Automatic validation of params
        assert (
            not set(ell_call.api_params.keys()).intersection(self.disallowed_api_params()) 
        ), f"Disallowed api parameters: {ell_call.api_params}"

        final_api_call_params = self.translate_to_provider(ell_call)

        call = self.provider_call_function(ell_call.client, final_api_call_params)
        assert self.dangerous_disable_validation or _validate_provider_call_params(final_api_call_params, call)


        provider_resp = call(**final_api_call_params)

        messages, metadata = self.translate_from_provider(
            provider_resp, ell_call, final_api_call_params, origin_id, logger
        )
        assert "choices" not in metadata, "choices should be in the metadata."
        assert self.dangerous_disable_validation or _validate_messages_are_tracked(messages, origin_id)

        return messages, final_api_call_params, metadata
====================================================================
-> Chunk: providers\openai.py::1


from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast

from pydantic import BaseModel
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from ell.configurator import _Model, config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image

try:
    # XXX: Could genericize.
    import openai
    from openai._streaming import Stream
    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam

    class OpenAIProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and (isinstance(fmt := api_call_params.get("response_format"), type)) and issubclass(fmt, BaseModel):
                return client.beta.chat.completions.parse
            else:
                return client.chat.completions.create

        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:
            final_call_params = ell_call.api_params.copy()
            final_call_params["model"] = ell_call.model
            # Stream by default for verbose logging.
            final_call_params["stream"] = True
            final_call_params["stream_options"] = {"include_usage": True}

            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.
            if ell_call.tools or final_call_params.get("response_format") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:
                final_call_params.pop("stream", None)
                final_call_params.pop("stream_options", None)
            if ell_call.tools:
                final_call_params.update(
                    tool_choice=final_call_params.get("tool_choice", "auto"),
                    tools=[  
                        dict(
                            type="function",
                            function=dict(
                                name=tool.__name__,
                                description=tool.__doc__,
                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore
                            )
                        ) for tool in ell_call.tools
                    ]
                )
            # messages
            openai_messages : List[ChatCompletionMessageParam] = []
            for message in ell_call.messages:
                if (tool_calls := message.tool_calls):
                    assert message.role == "assistant", "Tool calls must be from the assistant."
                    assert all(t.tool_call_id for t in tool_calls), "Tool calls must have tool call ids."
                    openai_messages.append(dict(
                        tool_calls=[
                            dict(
                                id=cast(str, tool_call.tool_call_id),
                                type="function",
                                function=dict(
                                    name=tool_call.tool.__name__,
                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)
                                )
                            ) for tool_call in tool_calls ],
                        role="assistant",
                        content=None,
                    ))
                elif (tool_results := message.tool_results):
                    for tool_result in tool_results:
                        assert all(cb.type == "text" for cb in tool_result.result), "Tool result does not match expected content blocks."
                        openai_messages.append(dict(
                            role="tool",
                            tool_call_id=tool_result.tool_call_id,
                            content=tool_result.text_only, 
                        ))
                else:
                    openai_messages.append(cast(ChatCompletionMessageParam, dict(
                        role=message.role,
                        content=[_content_block_to_openai_format(c) for c in message.content] 
                             if message.role != "system" 
                             else message.text_only
                    )))

            final_call_params["messages"] = openai_messages

            return final_call_params

        def translate_from_provider(
            self,
            provider_response: Union[
                ChatCompletion, 
                ParsedChatCompletion,
                Stream[ChatCompletionChunk], Any],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            metadata : Metadata = {}
            messages : List[Message] = []
            did_stream = provider_call_params.get("stream", False)


            if did_stream:
                stream = cast(Stream[ChatCompletionChunk], provider_response)
                message_streams = defaultdict(list)
                role : Optional[str] = None
                for chunk in stream:
                    metadata.update(chunk.model_dump(exclude={"choices"}))

                    for chat_compl_chunk in chunk.choices:
                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)
                        delta = chat_compl_chunk.delta
                        role = role or delta.role
                        if  chat_compl_chunk.index == 0 and logger:
                            logger(delta.content, is_refusal=hasattr(delta, "refusal") and delta.refusal)
                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):
                    text = "".join((choice.delta.content or "") for choice in message_stream)
                    messages.append(
                        Message(role=role, 
                                content=_lstr(content=text,origin_trace=origin_id)))
                    #XXX: Support streaming other types.
            else:
                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)
                metadata = chat_completion.model_dump(exclude={"choices"})
                for oai_choice in chat_completion.choices:
                    role = oai_choice.message.role
                    content_blocks = []
                    if (hasattr(message := oai_choice.message, "refusal") and (refusal := message.refusal)):
                        raise ValueError(refusal)
                    if hasattr(message, "parsed"):
                        if (parsed := message.parsed):
                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing
                            if logger: logger(parsed.model_dump_json())
                    else:
                        if (content := message.content):
                            content_blocks.append(
                                ContentBlock(
                                    text=_lstr(content=content,origin_trace=origin_id)))
                            if logger: logger(content)
                        if (tool_calls := message.tool_calls):
                            for tool_call in tool_calls:
                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)
                                assert matching_tool, "Model called tool not found in provided toolset."
                                content_blocks.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                tool_call.id, origin_trace= origin_id),
                                            params=json.loads(tool_call.function.arguments),
                                        )
                                    )
                                )
                                if logger: logger(repr(tool_call))
                    messages.append(Message(role=role, content=content_blocks))
            return messages, metadata


    # xx: singleton needed
    openai_provider = OpenAIProvider()
    register_provider(openai_provider, openai.Client)
except ImportError:
    pass
====================================================================
-> Chunk: providers\anthropic.py::1


from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent

from ell.types._lstr import _lstr
from ell.types.message import LMP
from ell.configurator import register_provider
from ell.util.serialization import serialize_image
import base64
from io import BytesIO
import json
import requests
from PIL import Image as PILImage

try:
    import anthropic
    from anthropic import Anthropic
    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent
    from anthropic.types.message_create_params import MessageCreateParamsStreaming
    from anthropic._streaming import Stream

    class AnthropicProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            return client.messages.create

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())
            # XXX: Helper, but should be depreicated due to ssot
            assert final_call_params.get("max_tokens") is not None, f"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}}))."

            dirty_msgs = [
                MessageParam(
                    role=cast(Literal["user", "assistant"], message.role), 
                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]
            role_correct_msgs   : List[MessageParam] = []
            for msg in dirty_msgs:
                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):
                    role_correct_msgs.append(msg)
                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])

            system_message = None
            if role_correct_msgs and role_correct_msgs[0]["role"] == "system":
                system_message = role_correct_msgs.pop(0)

            if system_message:
                final_call_params["system"] = system_message["content"][0]["text"]


            final_call_params['stream'] = True
            final_call_params["model"] = ell_call.model
            final_call_params["messages"] = role_correct_msgs

            if ell_call.tools:
                final_call_params["tools"] = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        name=tool.__name__,
                        description=tool.__doc__,
                        input_schema=tool.__ell_params_model__.model_json_schema(),
                    )
                    for tool in ell_call.tools
                ]

            # print(final_call_params)
            return final_call_params

        def translate_from_provider(
            self,
            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            usage = {}
            tracked_results = []
            metadata = {}

            #XXX: Support n > 0

            if provider_call_params.get("stream", False):
                content = []
                current_blocks: Dict[int, Dict[str, Any]] = {}
                message_metadata = {}

                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:
                    for chunk in stream:
                        if chunk.type == "message_start":
                            message_metadata = chunk.message.model_dump()
                            message_metadata.pop("content", None)  # Remove content as we'll build it separately

                        elif chunk.type == "content_block_start":
                            block = chunk.content_block.model_dump()
                            current_blocks[chunk.index] = block
                            if block["type"] == "tool_use":
                                if logger: logger(f" <tool_use: {block['name']}(")
                                block["input"] = "" # force it to be a string, XXX: can implement partially parsed json later.
                        elif chunk.type == "content_block_delta":
                            if chunk.index in current_blocks:
                                block = current_blocks[chunk.index]
                                if (delta := chunk.delta).type == "text_delta":
                                    block["text"] += delta.text
                                    if logger: logger(delta.text)
                                if delta.type == "input_json_delta":
                                    block["input"] += delta.partial_json
                                    if logger: logger(delta.partial_json)

                        elif chunk.type == "content_block_stop":
                            if chunk.index in current_blocks:
                                block = current_blocks.pop(chunk.index)
                                if block["type"] == "text":
                                    content.append(ContentBlock(text=_lstr(block["text"],origin_trace=origin_id)))
                                elif block["type"] == "tool_use":
                                    try:
                                        matching_tool = ell_call.get_tool_by_name(block["name"])
                                        if matching_tool:
                                            content.append(
                                                ContentBlock(
                                                    tool_call=ToolCall(
                                                        tool=matching_tool,
                                                        tool_call_id=_lstr(
                                                            block['id'],origin_trace=origin_id
                                                        ),
                                                        params=json.loads(block['input']) if block['input'] else {},
                                                    )
                                                )
                                            )
                                    except json.JSONDecodeError:
                                        if logger: logger(f" - FAILED TO PARSE JSON")
                                        pass
                                    if logger: logger(f")>")

                        elif chunk.type == "message_delta":
                            message_metadata.update(chunk.delta.model_dump())
                            if chunk.usage:
                                usage.update(chunk.usage.model_dump())

                        elif chunk.type == "message_stop":
                            tracked_results.append(Message(role="assistant", content=content))

                        # print(chunk)
                metadata = message_metadata

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("input_tokens", 0)
            usage["completion_tokens"] = usage.get("output_tokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata

    # XXX: Make a singleton.
    anthropic_provider = AnthropicProvider()
    register_provider(anthropic_provider, anthropic.Anthropic)
    register_provider(anthropic_provider, anthropic.AnthropicBedrock)
    register_provider(anthropic_provider, anthropic.AnthropicVertex)

except ImportError:
    pass
====================================================================


###### Cluster Eval ######
Score: 13.333333333333334
Cluster name: Provider Interface and API Integration Cluster
Provider Interface and API Integration Cluster:

-> Chunk: ell\provider.py::2


# XXX: Might leave this internal to providers so that the complex code is simpler &
# we can literally jsut call provider.call like any openai fn.
class EllCallParams(BaseModel):
    model: str = Field(..., description="Model identifier")
    messages: List[Message] = Field(..., description="Conversation context")
    client: Any = Field(..., description="API client")
    tools: List[LMP] = Field(default_factory=list, description="Available tools")
    api_params: Dict[str, Any] = Field(
        default_factory=dict, description="API parameters"
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def get_tool_by_name(self, name: str) -> Optional[LMP]:
        """Get a tool by name."""
        return next(
            (tool for tool in (self.tools or [])  if tool.__name__ == name), None
        )


Metadata = Dict[str, Any]
====================================================================
-> Chunk: ell\provider.py::3


# XXX: Needs a better name.
class Provider(ABC):
    """
    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.
    For example, the OpenAI provider is an API interface to OpenAI's API but also to Ollama and Azure OpenAI.
    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.
    """
    dangerous_disable_validation = False

    ################################
    ### API PARAMETERS #############
    ################################
    @abstractmethod
    def provider_call_function(
        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None
    ) -> Callable[..., Any]:
        """
        Implement this method to return the function that makes the API call to the language model.
        For example, if you're implementing the OpenAI provider, you would return the function that makes the API call to OpenAI's API.
        """
        return NotImplemented

    def disallowed_api_params(self) -> FrozenSet[str]:
        """
        Returns a list of disallowed call params that ell will override.
        """
        return frozenset({"messages", "tools", "model", "stream", "stream_options"})

    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):
        params = _call_params(self.provider_call_function(client, api_params))
        return frozenset(params.keys()) - self.disallowed_api_params()

    ################################
    ### TRANSLATION ###############
    ################################
    @abstractmethod
    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:
        """Converts an ell call to provider call params!"""
        return NotImplemented

    @abstractmethod
    def translate_from_provider(
        self,
        provider_response: Any,
        ell_call: EllCallParams,
        provider_call_params: Dict[str, Any],
        origin_id: Optional[str] = None,
        logger: Optional[Callable[..., None]] = None,
    ) -> Tuple[List[Message], Metadata]:
        """Converts provider responses to universal format. with metadata"""
        return NotImplemented

    ################################
    ### CALL MODEL ################
    ################################
    # Be careful to override this method in your provider.
====================================================================
-> Chunk: ell\provider.py::4


class Provider(ABC):
    def call(
        self,
        #XXX: In future refactors, we can fully enumerate the args and make ell_call's internal to the _provider implementer interface.
        # This gives us a litellm style interface for free.
        ell_call: EllCallParams,
        origin_id: Optional[str] = None,
        logger: Optional[Any] = None,
    ) -> Tuple[List[Message], Dict[str, Any], Metadata]:
        # Automatic validation of params
        assert (
            not set(ell_call.api_params.keys()).intersection(self.disallowed_api_params()) 
        ), f"Disallowed api parameters: {ell_call.api_params}"

        final_api_call_params = self.translate_to_provider(ell_call)

        call = self.provider_call_function(ell_call.client, final_api_call_params)
        assert self.dangerous_disable_validation or _validate_provider_call_params(final_api_call_params, call)


        provider_resp = call(**final_api_call_params)

        messages, metadata = self.translate_from_provider(
            provider_resp, ell_call, final_api_call_params, origin_id, logger
        )
        assert "choices" not in metadata, "choices should be in the metadata."
        assert self.dangerous_disable_validation or _validate_messages_are_tracked(messages, origin_id)

        return messages, final_api_call_params, metadata
====================================================================
-> Chunk: providers\openai.py::1


from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast

from pydantic import BaseModel
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from ell.configurator import _Model, config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image

try:
    # XXX: Could genericize.
    import openai
    from openai._streaming import Stream
    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam

    class OpenAIProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and (isinstance(fmt := api_call_params.get("response_format"), type)) and issubclass(fmt, BaseModel):
                return client.beta.chat.completions.parse
            else:
                return client.chat.completions.create

        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:
            final_call_params = ell_call.api_params.copy()
            final_call_params["model"] = ell_call.model
            # Stream by default for verbose logging.
            final_call_params["stream"] = True
            final_call_params["stream_options"] = {"include_usage": True}

            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.
            if ell_call.tools or final_call_params.get("response_format") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:
                final_call_params.pop("stream", None)
                final_call_params.pop("stream_options", None)
            if ell_call.tools:
                final_call_params.update(
                    tool_choice=final_call_params.get("tool_choice", "auto"),
                    tools=[  
                        dict(
                            type="function",
                            function=dict(
                                name=tool.__name__,
                                description=tool.__doc__,
                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore
                            )
                        ) for tool in ell_call.tools
                    ]
                )
            # messages
            openai_messages : List[ChatCompletionMessageParam] = []
            for message in ell_call.messages:
                if (tool_calls := message.tool_calls):
                    assert message.role == "assistant", "Tool calls must be from the assistant."
                    assert all(t.tool_call_id for t in tool_calls), "Tool calls must have tool call ids."
                    openai_messages.append(dict(
                        tool_calls=[
                            dict(
                                id=cast(str, tool_call.tool_call_id),
                                type="function",
                                function=dict(
                                    name=tool_call.tool.__name__,
                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)
                                )
                            ) for tool_call in tool_calls ],
                        role="assistant",
                        content=None,
                    ))
                elif (tool_results := message.tool_results):
                    for tool_result in tool_results:
                        assert all(cb.type == "text" for cb in tool_result.result), "Tool result does not match expected content blocks."
                        openai_messages.append(dict(
                            role="tool",
                            tool_call_id=tool_result.tool_call_id,
                            content=tool_result.text_only, 
                        ))
                else:
                    openai_messages.append(cast(ChatCompletionMessageParam, dict(
                        role=message.role,
                        content=[_content_block_to_openai_format(c) for c in message.content] 
                             if message.role != "system" 
                             else message.text_only
                    )))

            final_call_params["messages"] = openai_messages

            return final_call_params

        def translate_from_provider(
            self,
            provider_response: Union[
                ChatCompletion, 
                ParsedChatCompletion,
                Stream[ChatCompletionChunk], Any],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            metadata : Metadata = {}
            messages : List[Message] = []
            did_stream = provider_call_params.get("stream", False)


            if did_stream:
                stream = cast(Stream[ChatCompletionChunk], provider_response)
                message_streams = defaultdict(list)
                role : Optional[str] = None
                for chunk in stream:
                    metadata.update(chunk.model_dump(exclude={"choices"}))

                    for chat_compl_chunk in chunk.choices:
                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)
                        delta = chat_compl_chunk.delta
                        role = role or delta.role
                        if  chat_compl_chunk.index == 0 and logger:
                            logger(delta.content, is_refusal=hasattr(delta, "refusal") and delta.refusal)
                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):
                    text = "".join((choice.delta.content or "") for choice in message_stream)
                    messages.append(
                        Message(role=role, 
                                content=_lstr(content=text,origin_trace=origin_id)))
                    #XXX: Support streaming other types.
            else:
                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)
                metadata = chat_completion.model_dump(exclude={"choices"})
                for oai_choice in chat_completion.choices:
                    role = oai_choice.message.role
                    content_blocks = []
                    if (hasattr(message := oai_choice.message, "refusal") and (refusal := message.refusal)):
                        raise ValueError(refusal)
                    if hasattr(message, "parsed"):
                        if (parsed := message.parsed):
                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing
                            if logger: logger(parsed.model_dump_json())
                    else:
                        if (content := message.content):
                            content_blocks.append(
                                ContentBlock(
                                    text=_lstr(content=content,origin_trace=origin_id)))
                            if logger: logger(content)
                        if (tool_calls := message.tool_calls):
                            for tool_call in tool_calls:
                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)
                                assert matching_tool, "Model called tool not found in provided toolset."
                                content_blocks.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                tool_call.id, origin_trace= origin_id),
                                            params=json.loads(tool_call.function.arguments),
                                        )
                                    )
                                )
                                if logger: logger(repr(tool_call))
                    messages.append(Message(role=role, content=content_blocks))
            return messages, metadata


    # xx: singleton needed
    openai_provider = OpenAIProvider()
    register_provider(openai_provider, openai.Client)
except ImportError:
    pass
====================================================================
-> Chunk: providers\anthropic.py::1


from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent

from ell.types._lstr import _lstr
from ell.types.message import LMP
from ell.configurator import register_provider
from ell.util.serialization import serialize_image
import base64
from io import BytesIO
import json
import requests
from PIL import Image as PILImage

try:
    import anthropic
    from anthropic import Anthropic
    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent
    from anthropic.types.message_create_params import MessageCreateParamsStreaming
    from anthropic._streaming import Stream

    class AnthropicProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            return client.messages.create

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())
            # XXX: Helper, but should be depreicated due to ssot
            assert final_call_params.get("max_tokens") is not None, f"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}}))."

            dirty_msgs = [
                MessageParam(
                    role=cast(Literal["user", "assistant"], message.role), 
                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]
            role_correct_msgs   : List[MessageParam] = []
            for msg in dirty_msgs:
                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):
                    role_correct_msgs.append(msg)
                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])

            system_message = None
            if role_correct_msgs and role_correct_msgs[0]["role"] == "system":
                system_message = role_correct_msgs.pop(0)

            if system_message:
                final_call_params["system"] = system_message["content"][0]["text"]


            final_call_params['stream'] = True
            final_call_params["model"] = ell_call.model
            final_call_params["messages"] = role_correct_msgs

            if ell_call.tools:
                final_call_params["tools"] = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        name=tool.__name__,
                        description=tool.__doc__,
                        input_schema=tool.__ell_params_model__.model_json_schema(),
                    )
                    for tool in ell_call.tools
                ]

            # print(final_call_params)
            return final_call_params

        def translate_from_provider(
            self,
            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            usage = {}
            tracked_results = []
            metadata = {}

            #XXX: Support n > 0

            if provider_call_params.get("stream", False):
                content = []
                current_blocks: Dict[int, Dict[str, Any]] = {}
                message_metadata = {}

                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:
                    for chunk in stream:
                        if chunk.type == "message_start":
                            message_metadata = chunk.message.model_dump()
                            message_metadata.pop("content", None)  # Remove content as we'll build it separately

                        elif chunk.type == "content_block_start":
                            block = chunk.content_block.model_dump()
                            current_blocks[chunk.index] = block
                            if block["type"] == "tool_use":
                                if logger: logger(f" <tool_use: {block['name']}(")
                                block["input"] = "" # force it to be a string, XXX: can implement partially parsed json later.
                        elif chunk.type == "content_block_delta":
                            if chunk.index in current_blocks:
                                block = current_blocks[chunk.index]
                                if (delta := chunk.delta).type == "text_delta":
                                    block["text"] += delta.text
                                    if logger: logger(delta.text)
                                if delta.type == "input_json_delta":
                                    block["input"] += delta.partial_json
                                    if logger: logger(delta.partial_json)

                        elif chunk.type == "content_block_stop":
                            if chunk.index in current_blocks:
                                block = current_blocks.pop(chunk.index)
                                if block["type"] == "text":
                                    content.append(ContentBlock(text=_lstr(block["text"],origin_trace=origin_id)))
                                elif block["type"] == "tool_use":
                                    try:
                                        matching_tool = ell_call.get_tool_by_name(block["name"])
                                        if matching_tool:
                                            content.append(
                                                ContentBlock(
                                                    tool_call=ToolCall(
                                                        tool=matching_tool,
                                                        tool_call_id=_lstr(
                                                            block['id'],origin_trace=origin_id
                                                        ),
                                                        params=json.loads(block['input']) if block['input'] else {},
                                                    )
                                                )
                                            )
                                    except json.JSONDecodeError:
                                        if logger: logger(f" - FAILED TO PARSE JSON")
                                        pass
                                    if logger: logger(f")>")

                        elif chunk.type == "message_delta":
                            message_metadata.update(chunk.delta.model_dump())
                            if chunk.usage:
                                usage.update(chunk.usage.model_dump())

                        elif chunk.type == "message_stop":
                            tracked_results.append(Message(role="assistant", content=content))

                        # print(chunk)
                metadata = message_metadata

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("input_tokens", 0)
            usage["completion_tokens"] = usage.get("output_tokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata

    # XXX: Make a singleton.
    anthropic_provider = AnthropicProvider()
    register_provider(anthropic_provider, anthropic.Anthropic)
    register_provider(anthropic_provider, anthropic.AnthropicBedrock)
    register_provider(anthropic_provider, anthropic.AnthropicVertex)

except ImportError:
    pass
====================================================================


###### Cluster Eval ######
Score: 13.333333333333334
Cluster name: SQL Store for Data Persistency Cluster
SQL Store for Data Persistency Cluster:

-> Chunk: stores\sql.py::2


class SQLStore(ell.store.Store):
    def __init__(self, db_uri: str, blob_store: Optional[ell.store.BlobStore] = None):
        self.engine = create_engine(db_uri,
                                    json_serializer=lambda obj: json.dumps(pydantic_ltype_aware_cattr.unstructure(obj), 
                                     sort_keys=True, default=repr, ensure_ascii=False))

        SQLModel.metadata.create_all(self.engine)
        self.open_files: Dict[str, Dict[str, Any]] = {}
        super().__init__(blob_store)

    def write_lmp(self, serialized_lmp: SerializedLMP, uses: Dict[str, Any]) -> Optional[Any]:
        with Session(self.engine) as session:
            # Bind the serialized_lmp to the session
            lmp = session.exec(select(SerializedLMP).filter(SerializedLMP.lmp_id == serialized_lmp.lmp_id)).first()

            if lmp:
                # Already added to the DB.
                return lmp
            else:
                session.add(serialized_lmp)

            for use_id in uses:
                used_lmp = session.exec(select(SerializedLMP).where(SerializedLMP.lmp_id == use_id)).first()
                if used_lmp:
                    serialized_lmp.uses.append(used_lmp)

            session.commit()
        return None
====================================================================
-> Chunk: stores\sql.py::4


class SQLStore(ell.store.Store):

    def get_cached_invocations(self, lmp_id :str, state_cache_key :str) -> List[Invocation]:
        with Session(self.engine) as session:
            return self.get_invocations(session, lmp_filters={"lmp_id": lmp_id}, filters={"state_cache_key": state_cache_key})

    def get_versions_by_fqn(self, fqn :str) -> List[SerializedLMP]:
        with Session(self.engine) as session:
            return self.get_lmps(session, name=fqn)

    ## HELPER METHODS FOR ELL STUDIO! :) 
====================================================================
-> Chunk: stores\sql.py::5


class SQLStore(ell.store.Store):
    def get_latest_lmps(self, session: Session, skip: int = 0, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Gets all the lmps grouped by unique name with the highest created at
        """
        subquery = (
            select(SerializedLMP.name, func.max(SerializedLMP.created_at).label("max_created_at"))
            .group_by(SerializedLMP.name)
            .subquery()
        )

        filters = {
            "name": subquery.c.name,
            "created_at": subquery.c.max_created_at
        }

        return self.get_lmps(session, skip=skip, limit=limit, subquery=subquery, **filters)
====================================================================
-> Chunk: types\studio.py::4


class SerializedLMPBase(SQLModel):
    lmp_id: Optional[str] = Field(default=None, primary_key=True)
    name: str = Field(index=True)
    source: str
    dependencies: str
    created_at: datetime = UTCTimestampField(index=True, nullable=False)

    lmp_type: LMPType
    api_params: Optional[Dict[str, Any]] = Field(default_factory=dict, sa_column=Column(JSON))
    initial_free_vars: Optional[Dict[str, Any]] = Field(default_factory=dict, sa_column=Column(JSON))
    initial_global_vars: Optional[Dict[str, Any]] = Field(default_factory=dict, sa_column=Column(JSON))
    num_invocations: Optional[int] = Field(default=0)
    commit_message: Optional[str] = Field(default=None)
    version_number: Optional[int] = Field(default=None)
====================================================================
-> Chunk: stores\sql.py::10


class SQLiteStore(SQLStore):
    def __init__(self, db_dir: str):
        assert not db_dir.endswith('.db'), "Create store with a directory not a db."

        os.makedirs(db_dir, exist_ok=True)
        self.db_dir = db_dir
        db_path = os.path.join(db_dir, 'ell.db')
        blob_store = SQLBlobStore(db_dir)
        super().__init__(f'sqlite:///{db_path}', blob_store=blob_store)

class SQLBlobStore(ell.store.BlobStore):
    def __init__(self, db_dir: str):
        self.db_dir = db_dir

    def store_blob(self, blob: bytes, blob_id  : str) -> str:
        file_path = self._get_blob_path(blob_id)
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with gzip.open(file_path, "wb") as f:
            f.write(blob)
        return blob_id

    def retrieve_blob(self, blob_id: str) -> bytes:
        file_path = self._get_blob_path(blob_id)
        with gzip.open(file_path, "rb") as f:
            return f.read()
====================================================================


###### Cluster Eval ######
Score: 13.333333333333334
Cluster name: Serialization and Data Preparation Cluster
Serialization and Data Preparation Cluster:

-> Chunk: util\serialization.py::1


# Global converter
import base64
import hashlib
from io import BytesIO
import json
import cattrs
import numpy as np
from pydantic import BaseModel
import PIL
from ell.types._lstr import _lstr


pydantic_ltype_aware_cattr = cattrs.Converter()

def serialize_image(img):
    buffer = BytesIO()
    img.save(buffer, format="PNG")
    return "data:image/png;base64," + base64.b64encode(buffer.getvalue()).decode()


# Register hooks for complex types
pydantic_ltype_aware_cattr.register_unstructure_hook(
    np.ndarray,
    lambda arr: {
        "content": serialize_image(PIL.Image.fromarray(arr)),
        "__limage": True
    } if arr.ndim == 3 else (
        {
            "content": base64.b64encode(arr.tobytes()).decode(),
            "dtype": str(arr.dtype),
            "shape": arr.shape,
            "__lndarray": True
        }
    )
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    set,
    lambda s: list(sorted(s))
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    frozenset,
    lambda s: list(sorted(s))
)


pydantic_ltype_aware_cattr.register_unstructure_hook(
    PIL.Image.Image,
    lambda obj: {
        "content": serialize_image(obj),
        "__limage": True
    }
)

def unstructure_lstr(obj):
    return dict(content=str(obj), **obj.__dict__, __lstr=True)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    _lstr,
    unstructure_lstr
)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    BaseModel,
    lambda obj: obj.model_dump(exclude_none=True, exclude_unset=True)
)
====================================================================
-> Chunk: util\serialization.py::2


def get_immutable_vars(vars_dict):
    converter = cattrs.Converter()

    def handle_complex_types(obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj]
        elif isinstance(obj, dict):
            return {k: handle_complex_types(v) if not isinstance(v, (int, float, str, bool, type(None))) else v for k, v in obj.items()}
        elif isinstance(obj, (set, frozenset)):
            return list(sorted(handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj))
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return f"<Object of type {type(obj).__name__}>"

    converter.register_unstructure_hook(object, handle_complex_types)
    x = converter.unstructure(vars_dict)
    return x
====================================================================
-> Chunk: util\serialization.py::3


def compute_state_cache_key(ipstr, fn_closure):
    _global_free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[2]), sort_keys=True, default=repr, ensure_ascii=False)}"
    _free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[3]), sort_keys=True, default=repr, ensure_ascii=False)}"
    state_cache_key = hashlib.sha256(f"{ipstr}{_global_free_vars_str}{_free_vars_str}".encode('utf-8')).hexdigest()
    return state_cache_key
====================================================================
-> Chunk: util\serialization.py::4


def prepare_invocation_params(params):
    invocation_params = params

    cleaned_invocation_params = pydantic_ltype_aware_cattr.unstructure(invocation_params)

    # Thisis because we wneed the caching to work on the hash of a cleaned and serialized object.
    jstr = json.dumps(cleaned_invocation_params, sort_keys=True, default=repr, ensure_ascii=False)

    consumes = set()
    import re
    # XXX: Better than registering a hook in cattrs.
    pattern = r'"__origin_trace__":\s*"frozenset\({(.+?)}\)"'

    # Find all matches in the jstr
    matches = re.findall(pattern, jstr)

    # Process each match and add to consumes set
    for match in matches:
        # Remove quotes and spaces, then split by comma
        items = [item.strip().strip("'") for item in match.split(',')]
        consumes.update(items)
    consumes = list(consumes)
    # XXX: Only need to reload because of 'input' caching., we could skip this by making ultimate model caching rather than input hash caching; if prompt same use the same output.. irrespective of version.
    return json.loads(jstr), jstr, consumes
====================================================================
-> Chunk: types\_lstr.py::1


"""
LM string that supports logits and keeps track of it'sorigin_trace even after mutation.
"""

import numpy as np
from typing import (
    Optional,
    Set,
    SupportsIndex,
    Union,
    FrozenSet,
    Iterable,
    List,
    Tuple,
    Any,
    Callable,
)
from typing_extensions import override
from pydantic import BaseModel, GetCoreSchemaHandler
from pydantic_core import CoreSchema

from pydantic_core import CoreSchema, core_schema


class _lstr(str):
    """
     A string class that supports logits and keeps track of itsorigin_trace even after mutation.
     This class is designed to be used in prompt engineering libraries where it is essential to associate
     logits with generated text and track the origin of the text.

     The `lstr` class inherits from the built-in `str` class and adds two additional attributes: `logits` and `origin_trace`.
     The `origin_trace` attribute is a frozen set of strings that represents theorigin_trace(s) of the string.

     The class provides various methods for manipulating the string, such as concatenation, slicing, splitting, and joining.
     These methods ensure that the logits andorigin_trace(s) are updated correctly based on the operation performed.

     The `lstr` class is particularly useful in LLM libraries for tracing the flow of prompts through various language model calls.
     By tracking theorigin_trace of each string, it is possible to visualize how outputs from one language model program influence
     the inputs of another, allowing for a detailed analysis of interactions between different large language models. This capability
     is crucial for understanding the propagation of prompts in complex LLM workflows and for building visual graphs that depict these interactions.

     It is important to note that any modification to the string (such as concatenation or replacement) will invalidate the associated logits.
     This is because the logits are specifically tied to the original string content, and any change would require a new computation of logits.
     The logic behind this is detailed elsewhere in this file.

     Example usage:
     ```
     # Create an lstr instance with logits and anorigin_trace
     logits = np.array([1.0, 2.0, 3.0])
    origin_trace = "4e9b7ec9"
     lstr_instance = lstr("Hello", logits,origin_trace)

     # Concatenate two lstr instances
     lstr_instance2 = lstr("World", None, "7f4d2c3a")
     concatenated_lstr = lstr_instance + lstr_instance2

     # Get the logits andorigin_trace of the concatenated lstr
     print(concatenated_lstr.logits)  # Output: None
     print(concatenated_lstr.origin_trace)  # Output: frozenset({'4e9b7ec9', '7f4d2c3a'})

     # Split the concatenated lstr into two parts
     parts = concatenated_lstr.split()
     print(parts)  # Output: [lstr('Hello', None, frozenset({'4e9b7ec9', '7f4d2c3a'})), lstr('World', None, frozenset({'4e9b7ec9', '7f4d2c3a'}))]
     ```
     Attributes:
        origin_trace (FrozenSet[str]): A frozen set of strings representing theorigin_trace(s) of the string.

     Methods:
         __new__: Create a new instance of lstr.
         __repr__: Return a string representation of the lstr instance.
         __add__: Concatenate this lstr instance with another string or lstr instance.
         __mod__: Perform a modulo operation between this lstr instance and another string, lstr, or a tuple of strings and lstrs.
         __mul__: Perform a multiplication operation between this lstr instance and an integer or another lstr.
         __rmul__: Perform a right multiplication operation between an integer or another lstr and this lstr instance.
         __getitem__: Get a slice or index of this lstr instance.
         __getattr__: Get an attribute from this lstr instance.
         join: Join a sequence of strings or lstr instances into a single lstr instance.
         split: Split this lstr instance into a list of lstr instances based on a separator.
         rsplit: Split this lstr instance into a list of lstr instances based on a separator, starting from the right.
         splitlines: Split this lstr instance into a list of lstr instances based on line breaks.
         partition: Partition this lstr instance into three lstr instances based on a separator.
         rpartition: Partition this lstr instance into three lstr instances based on a separator, starting from the right.
    """
====================================================================


###### Cluster Eval ######
Score: 13.333333333333334
Cluster name: Code Serialization and Closure Cluster
Code Serialization and Closure Cluster:

-> Chunk: util\closure.py::2


def lexical_closure(
    func: Any,
    already_closed: Set[int] = None,
    initial_call: bool = False,
    recursion_stack: list = None,
    forced_dependencies: Optional[Dict[str, Any]] = None
) -> Tuple[str, Tuple[str, str], Set[str]]:
    """
    Generate a lexical closure for a given function or callable.

    Args:
        func: The function or callable to process.
        already_closed: Set of already processed function hashes.
        initial_call: Whether this is the initial call to the function.
        recursion_stack: Stack to keep track of the recursion path.

    Returns:
        A tuple containing:
        - The full source code of the closure
        - A tuple of (function source, dependencies source)
        - A set of function hashes that this closure uses
    """
    already_closed = already_closed or set()
    uses = set()
    forced_dependencies = forced_dependencies or {}
    recursion_stack = recursion_stack or []

    if hash(func) in already_closed:
        return "", ("", ""), set()

    recursion_stack.append(getattr(func, '__qualname__', str(func)))

    outer_ell_func = func
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__

    source = getsource(func, lstrip=True)
    already_closed.add(hash(func))

    globals_and_frees = _get_globals_and_frees(func)
    dependencies, imports, modules = _process_dependencies(func, globals_and_frees, already_closed, recursion_stack, uses)
    for k,v in forced_dependencies.items():
        # Todo: dictionary not necessary
        _process_signature_dependency(v, dependencies, already_closed, recursion_stack, uses, k)

    cur_src = _build_initial_source(imports, dependencies, source)

    module_src = _process_modules(modules, cur_src, already_closed, recursion_stack, uses)

    dirty_src = _build_final_source(imports, module_src, dependencies, source)
    dirty_src_without_func = _build_final_source(imports, module_src, dependencies, "")

    CLOSURE_SOURCE[hash(func)] = dirty_src

    dsrc = _clean_src(dirty_src_without_func)

    # Format the sorce and dsrc soruce using Black
    source = _format_source(source)
    dsrc = _format_source(dsrc)

    fn_hash = _generate_function_hash(source, dsrc, func.__qualname__)

    _update_ell_func(outer_ell_func, source, dsrc, globals_and_frees['globals'], globals_and_frees['frees'], fn_hash, uses)

    return (dirty_src, (source, dsrc), ({outer_ell_func} if not initial_call and hasattr(outer_ell_func, "__ell_func__") else uses))
====================================================================
-> Chunk: util\closure.py::6


def _process_signature_dependency(val, dependencies, already_closed, recursion_stack, uses, name: Optional[str] = None):
    # Todo: Build general cattr like utility for unstructuring python objects with hooks that keep track of state variables.
    # Todo: break up closure into types and functions.
    # XXX: This is not exhaustive, we should determine should import on all dependencies

    if name not in FORBIDDEN_NAMES:
        try:
            dep = None
            _uses = None
            if isinstance(val, (types.FunctionType, types.MethodType)):
                dep, _, _uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            elif isinstance(val, (list, tuple, set)):
                for item in val:
                    _process_signature_dependency(item, dependencies, already_closed, recursion_stack, uses)
            else:
                val_class = val if isinstance(val, type) else val.__class__
                try:
                    is_builtin = (val_class.__module__ == "builtins" or val_class.__module__ == "__builtins__")
                except:
                    is_builtin = False

                if not is_builtin:
                    if should_import(val_class.__module__):
                        dependencies.append(dill.source.getimport(val_class, alias=val_class.__name__))
                    else:
                        dep, _, _uses = lexical_closure(val_class, already_closed=already_closed, recursion_stack=recursion_stack.copy())

            if dep: dependencies.append(dep)
            if _uses: uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of parameter or annotation {name}", e, recursion_stack)
====================================================================
-> Chunk: util\closure.py::7


def _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack , uses):
    """Process a single variable."""
    try:
        name = inspect.getmodule(var_value).__name__
        if should_import(name):
            imports.append(dill.source.getimport(var_value, alias=var_name))
            return
    except:
        pass

    if isinstance(var_value, (types.FunctionType, type, types.MethodType)):
        _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses)
    elif isinstance(var_value, types.ModuleType):
        _process_module(var_name, var_value, modules, imports, uses)
    elif isinstance(var_value, types.BuiltinFunctionType):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        _process_other_variable(var_name, var_value, dependencies, uses)
====================================================================
-> Chunk: util\closure.py::13


def _update_ell_func(outer_ell_func, source, dsrc, globals_dict, frees_dict, fn_hash, uses):
    """Update the ell function attributes."""
    formatted_source = _format_source(source)
    formatted_dsrc = _format_source(dsrc)

    if hasattr(outer_ell_func, "__ell_func__"):

        outer_ell_func.__ell_closure__ = (formatted_source, formatted_dsrc, globals_dict, frees_dict)
        outer_ell_func.__ell_hash__ = fn_hash
        outer_ell_func.__ell_uses__ = uses

def _raise_error(message, exception, recursion_stack):
    """Raise an error with detailed information."""
    error_msg = f"{message}. Error: {str(exception)}\n"
    error_msg += f"Recursion stack: {' -> '.join(recursion_stack)}"
    # print(error_msg)
    raise Exception(error_msg)
====================================================================


###### Cluster Eval ######
Score: 13.333333333333334
Cluster name: OpenAI Realtime Integration Cluster
OpenAI Realtime Integration Cluster:

-> Chunk: openai_realtime\api.py::2


class RealtimeAPI(RealtimeEventHandler):

    async def connect(self, model='gpt-4o-realtime-preview-2024-10-01'):
        if self.is_connected():
            raise Exception("Already connected")

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'OpenAI-Beta': 'realtime=v1'
        }

        self.ws = await websockets.connect(f"{self.url}?model={model}", extra_headers=headers)

        self.log(f"Connected to {self.url}")

        asyncio.create_task(self._message_handler())

        return True
====================================================================
-> Chunk: openai_realtime\client.py::1


import asyncio
import numpy as np
from .event_handler import RealtimeEventHandler
from .api import RealtimeAPI
from .conversation import RealtimeConversation
from .utils import RealtimeUtils
import json

class RealtimeClient(RealtimeEventHandler):
    def __init__(self, url=None, api_key=None, instructions='', dangerously_allow_api_key_in_browser=False, debug=False):
        super().__init__()
        self.default_session_config = {
            'modalities': ['text', 'audio'],
            'instructions': instructions,
            'voice': 'alloy',
            'input_audio_format': 'pcm16',
            'output_audio_format': 'pcm16',
            'input_audio_transcription': None,
            'turn_detection': None,
            'tools': [],
            'tool_choice': 'auto',
            'temperature': 0.8,
            'max_response_output_tokens': 4096,
        }
        self.session_config = {}
        self.transcription_models = [{'model': 'whisper-1'}]
        self.default_server_vad_config = {
            'type': 'server_vad',
            'threshold': 0.5,
            'prefix_padding_ms': 300,
            'silence_duration_ms': 200,
        }
        self.realtime = RealtimeAPI(url, api_key, dangerously_allow_api_key_in_browser, debug)
        self.conversation = RealtimeConversation()
        self._reset_config()
        self._add_api_event_handlers()

    def _reset_config(self):
        self.session_created = False
        self.tools = {}
        self.session_config = self.default_session_config.copy()
        self.input_audio_buffer = np.array([], dtype=np.int16)
        return True
====================================================================
-> Chunk: openai_realtime\api.py::3


class RealtimeAPI(RealtimeEventHandler):

    async def _message_handler(self):
        try:
            async for message in self.ws:
                data = json.loads(message)
                self.receive(data['type'], data)
        except websockets.exceptions.ConnectionClosed:
            self.disconnect()
            self.dispatch('close', {'error': True})

    def disconnect(self):
        if self.ws:
            asyncio.create_task(self.ws.close())
            self.ws = None
        return True

    def receive(self, event_name, event):
        self.log("received:", event_name, event)
        self.dispatch(f"server.{event_name}", event)
        self.dispatch("server.*", event)
        return True
====================================================================
-> Chunk: openai_realtime\conversation.py::1


import numpy as np
import json
from .utils import RealtimeUtils
import copy

class RealtimeConversation:
    def __init__(self):
        self.default_frequency = 24000  # 24,000 Hz
        self.clear()

    def clear(self):
        self.item_lookup = {}
        self.items = []
        self.response_lookup = {}
        self.responses = []
        self.queued_speech_items = {}
        self.queued_transcript_items = {}
        self.queued_input_audio = None
        return True

    def queue_input_audio(self, input_audio):
        self.queued_input_audio = input_audio
        return input_audio

    def process_event(self, event, *args):
        if 'event_id' not in event:
            raise ValueError("Missing 'event_id' on event")
        if 'type' not in event:
            raise ValueError("Missing 'type' on event")

        event_processor = getattr(self, f"_process_{event['type'].replace('.', '_')}", None)
        if not event_processor:
            raise ValueError(f"Missing conversation event processor for '{event['type']}'")

        return event_processor(event, *args)

    def get_item(self, id):
        return self.item_lookup.get(id)

    def get_items(self):
        return self.items.copy()
====================================================================


###### Cluster Eval ######
Score: 13.333333333333334
Cluster name: Real-time Client and Event Management Cluster
Real-time Client and Event Management Cluster:

-> Chunk: openai_realtime\client.py::3


class RealtimeClient(RealtimeEventHandler):



    def is_connected(self):
        return self.realtime.is_connected() and self.session_created

    def reset(self):
        self.disconnect()
        self.clear_event_handlers()
        self.realtime.clear_event_handlers()
        self._reset_config()
        self._add_api_event_handlers()
        return True

    async def connect(self):
        if self.is_connected():
            raise Exception("Already connected, use .disconnect() first")
        await self.realtime.connect()
        self.update_session()
        return True

    async def wait_for_session_created(self):
        if not self.realtime.is_connected():
            raise Exception("Not connected, use .connect() first")
        while not self.session_created:
            await asyncio.sleep(0.001)
        return True

    def disconnect(self):
        self.session_created = False
        self.conversation.clear()
        if self.realtime.is_connected():
            self.realtime.disconnect()

    def get_turn_detection_type(self):
        turn_detection = self.session_config.get('turn_detection')
        if isinstance(turn_detection, dict):
            return turn_detection.get('type')
        return None
====================================================================
-> Chunk: openai_realtime\api.py::4


class RealtimeAPI(RealtimeEventHandler):

    def send(self, event_name, data=None):
        if not self.is_connected():
            raise Exception("RealtimeAPI is not connected")

        data = data or {}
        if not isinstance(data, dict):
            raise ValueError("data must be a dictionary")

        event = {
            "event_id": RealtimeUtils.generate_id("evt_"),
            "type": event_name,
            **data
        }

        self.dispatch(f"client.{event_name}", event)
        self.dispatch("client.*", event)
        self.log("sent:", event_name, event)

        asyncio.create_task(self.ws.send(json.dumps(event, ensure_ascii=False)))
        return True
====================================================================
-> Chunk: openai_realtime\event_handler.py::1


import asyncio
from typing import Callable, Dict, List, Any

class RealtimeEventHandler:
    def __init__(self):
        self.event_handlers: Dict[str, List[Callable]] = {}
        self.next_event_handlers: Dict[str, List[Callable]] = {}

    def clear_event_handlers(self):
        self.event_handlers.clear()
        self.next_event_handlers.clear()
        return True

    def on(self, event_name: str, callback: Callable = None):
        def decorator(func):
            if event_name not in self.event_handlers:
                self.event_handlers[event_name] = []
            self.event_handlers[event_name].append(func)
            return func

        if callback is None:
            return decorator
        else:
            return decorator(callback)

    def on_next(self, event_name: str, callback: Callable):
        if event_name not in self.next_event_handlers:
            self.next_event_handlers[event_name] = []
        self.next_event_handlers[event_name].append(callback)

    def off(self, event_name: str, callback: Callable = None):
        if event_name in self.event_handlers:
            if callback:
                self.event_handlers[event_name].remove(callback)
            else:
                del self.event_handlers[event_name]
        return True

    def off_next(self, event_name: str, callback: Callable = None):
        if event_name in self.next_event_handlers:
            if callback:
                self.next_event_handlers[event_name].remove(callback)
            else:
                del self.next_event_handlers[event_name]
        return True

    async def wait_for_next(self, event_name: str, timeout: float = None):
        next_event = None
        def set_next_event(event):
            nonlocal next_event
            next_event = event

        self.on_next(event_name, set_next_event)

        start_time = asyncio.get_event_loop().time()
        while not next_event:
            if timeout and asyncio.get_event_loop().time() - start_time > timeout:
                return None
            await asyncio.sleep(0.001)

        return next_event

    def dispatch(self, event_name: str, event: Any):
        handlers = self.event_handlers.get(event_name, []).copy()
        for handler in handlers:
            handler(event)

        next_handlers = self.next_event_handlers.pop(event_name, [])
        for next_handler in next_handlers:
            next_handler(event)

        return True
====================================================================
-> Chunk: openai_realtime\conversation.py::1


import numpy as np
import json
from .utils import RealtimeUtils
import copy

class RealtimeConversation:
    def __init__(self):
        self.default_frequency = 24000  # 24,000 Hz
        self.clear()

    def clear(self):
        self.item_lookup = {}
        self.items = []
        self.response_lookup = {}
        self.responses = []
        self.queued_speech_items = {}
        self.queued_transcript_items = {}
        self.queued_input_audio = None
        return True

    def queue_input_audio(self, input_audio):
        self.queued_input_audio = input_audio
        return input_audio

    def process_event(self, event, *args):
        if 'event_id' not in event:
            raise ValueError("Missing 'event_id' on event")
        if 'type' not in event:
            raise ValueError("Missing 'type' on event")

        event_processor = getattr(self, f"_process_{event['type'].replace('.', '_')}", None)
        if not event_processor:
            raise ValueError(f"Missing conversation event processor for '{event['type']}'")

        return event_processor(event, *args)

    def get_item(self, id):
        return self.item_lookup.get(id)

    def get_items(self):
        return self.items.copy()
====================================================================
-> Chunk: openai_realtime\conversation.py::6


class RealtimeConversation:

    def _process_response_created(self, event):
        response = event['response']
        if response['id'] not in self.response_lookup:
            self.response_lookup[response['id']] = response
            self.responses.append(response)
        return {'item': None, 'delta': None}

    def _process_response_output_item_added(self, event):
        response_id, item = event['response_id'], event['item']
        response = self.response_lookup.get(response_id)
        if not response:
            raise ValueError(f"response.output_item.added: Response '{response_id}' not found")
        response['output'].append(item['id'])
        return {'item': None, 'delta': None}

    def _process_response_output_item_done(self, event):
        item = event['item']
        if not item:
            raise ValueError("response.output_item.done: Missing 'item'")
        found_item = self.item_lookup.get(item['id'])
        if not found_item:
            raise ValueError(f"response.output_item.done: Item '{item['id']}' not found")
        found_item['status'] = item['status']
        return {'item': found_item, 'delta': None}

    def _process_response_content_part_added(self, event):
        item_id, part = event['item_id'], event['part']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.content_part.added: Item '{item_id}' not found")
        item['content'].append(part)
        return {'item': item, 'delta': None}
====================================================================


###### Cluster Eval ######
Score: 13.0
Cluster name: Configuration Management Cluster
Configuration Management Cluster:

-> Chunk: ell\configurator.py::2


class Config(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    registry: Dict[str, _Model] = Field(default_factory=dict, description="A dictionary mapping model names to their configurations.")
    verbose: bool = Field(default=False, description="If True, enables verbose logging.")
    wrapped_logging: bool = Field(default=True, description="If True, enables wrapped logging for better readability.")
    override_wrapped_logging_width: Optional[int] = Field(default=None, description="If set, overrides the default width for wrapped logging.")
    store: Optional[Store] = Field(default=None, description="An optional Store instance for persistence.")
    autocommit: bool = Field(default=False, description="If True, enables automatic committing of changes to the store.")
    lazy_versioning: bool = Field(default=True, description="If True, enables lazy versioning for improved performance.")
    default_api_params: Dict[str, Any] = Field(default_factory=dict, description="Default parameters for language models.")
    default_client: Optional[openai.Client] = Field(default=None, description="The default OpenAI client used when a specific model client is not found.")
    autocommit_model: str = Field(default="gpt-4o-mini", description="When set, changes the default autocommit model from GPT 4o mini.")
    providers: Dict[Type, Provider] = Field(default_factory=dict, description="A dictionary mapping client types to provider classes.")
    def __init__(self, **data):
        super().__init__(**data)
        self._lock = threading.Lock()
        self._local = threading.local()
====================================================================
-> Chunk: ell\configurator.py::1


from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)
====================================================================
-> Chunk: ell\configurator.py::3


class Config(BaseModel):


    def register_model(
        self, 
        name: str,
        default_client: Optional[Union[openai.Client, Any]] = None,
        supports_streaming: Optional[bool] = None
    ) -> None:
        """
        Register a model with its configuration.
        """
        with self._lock:
            # XXX: Will be deprecated in 0.1.0
            self.registry[name] = _Model(
                name=name,
                default_client=default_client,
                supports_streaming=supports_streaming
            )
====================================================================
-> Chunk: ell\configurator.py::8


def init(
    store: Optional[Union[Store, str]] = None,
    verbose: bool = False,
    autocommit: bool = True,
    lazy_versioning: bool = True,
    default_api_params: Optional[Dict[str, Any]] = None,
    default_client: Optional[Any] = None,
    autocommit_model: str = "gpt-4o-mini"
) -> None:
    """
    Initialize the ELL configuration with various settings.

    :param verbose: Set verbosity of ELL operations.
    :type verbose: bool
    :param store: Set the store for ELL. Can be a Store instance or a string path for SQLiteStore.
    :type store: Union[Store, str], optional
    :param autocommit: Set autocommit for the store operations.
    :type autocommit: bool
    :param lazy_versioning: Enable or disable lazy versioning.
    :type lazy_versioning: bool
    :param default_api_params: Set default parameters for language models.
    :type default_api_params: Dict[str, Any], optional
    :param default_openai_client: Set the default OpenAI client.
    :type default_openai_client: openai.Client, optional
    :param autocommit_model: Set the model used for autocommitting.
    :type autocommit_model: str
    """
    # XXX: prevent double init
    config.verbose = verbose
    config.lazy_versioning = lazy_versioning

    if isinstance(store, str):
        from ell.stores.sql import SQLiteStore
        config.store = SQLiteStore(store)
    else:
        config.store = store
    config.autocommit = autocommit or config.autocommit

    if default_api_params is not None:
        config.default_api_params.update(default_api_params)

    if default_client is not None:
        config.default_client = default_client

    if autocommit_model is not None:
        config.autocommit_model = autocommit_model
====================================================================


###### Cluster Eval ######
Score: 13.0
Cluster name: Logging and Verbosity Cluster
Logging and Verbosity Cluster:

-> Chunk: util\verbosity.py::6


def model_usage_logger_pre(
    invoking_lmp: LMP,
    lmp_args: Tuple,
    lmp_kwargs: Dict,
    lmp_hash: str,
    messages: List[Message],
    arg_max_length: int = 8
):
    """Log model usage before execution with customizable argument display length and ASCII box."""
    color =  compute_color(invoking_lmp)
    formatted_args = [format_arg(arg, arg_max_length) for arg in lmp_args]
    formatted_kwargs = [format_kwarg(key, lmp_kwargs[key], arg_max_length) for key in lmp_kwargs]
    formatted_params = ', '.join(formatted_args + formatted_kwargs)

    check_version_and_log()

    terminal_width = get_terminal_width()

    logger.info(f"Invoking LMP: {invoking_lmp.__name__} (hash: {lmp_hash[:8]})")

    print(f"{PIPE_COLOR}╔{'═' * (terminal_width - 2)}╗{RESET}")
    print(f"{PIPE_COLOR}║ {color}{BOLD}{UNDERLINE}{invoking_lmp.__name__}{RESET}{color}({formatted_params}){RESET}")
    print(f"{PIPE_COLOR}╠{'═' * (terminal_width - 2)}╣{RESET}")
    print(f"{PIPE_COLOR}║ {BOLD}Prompt:{RESET}")
    print(f"{PIPE_COLOR}╟{'─' * (terminal_width - 2)}╢{RESET}")

    max_role_length = max(len("assistant"), max(len(message.role) for message in messages))
    print_wrapped_messages(messages, max_role_length, color)
====================================================================
-> Chunk: util\verbosity.py::3


@lru_cache(maxsize=128)
def compute_color(invoking_lmp: LMP) -> str:
    """Compute and cache a consistent color for a given LMP."""
    name_hash = hashlib.md5(invoking_lmp.__name__.encode()).hexdigest()
    color_index = int(name_hash, 16) % len(ELL_COLORS)
    return list(ELL_COLORS.values())[color_index]

def format_arg(arg: Any, max_length: int = 8) -> str:
    """Format an argument for display with customizable max length."""
    str_arg = str(arg)
    return f"{Fore.MAGENTA}{str_arg[:max_length]}..{Style.RESET_ALL}" if len(str_arg) > max_length else f"{Fore.MAGENTA}{str_arg}{Style.RESET_ALL}"

def format_kwarg(key: str, value: Any, max_length: int = 8) -> str:
    """Format a keyword argument for display with customizable max length."""
    return f"{Style.DIM}{key}{Style.RESET_ALL}={Fore.MAGENTA}{str(value)[:max_length]}..{Style.RESET_ALL}"

def get_terminal_width() -> int:
    """Get the terminal width, defaulting to 80 if it can't be determined."""
    try:
        return shutil.get_terminal_size((80, 20)).columns
    except Exception:
        logger.warning("Unable to determine terminal size. Defaulting to 80 columns.")
        return 80
====================================================================
-> Chunk: util\verbosity.py::4


def wrap_text_with_prefix(message, width: int, prefix: str, subsequent_prefix: str, text_color: str) -> List[str]:
    """Wrap text while preserving the prefix and color for each line."""
    result = []
    for i, content in enumerate(message.content):
        wrapped_lines = []
        if content.image and content.image.image:
            wrapped_lines = plot_ascii(content.image.image, min(80, width - len(prefix)))
        else:
            if content.tool_result:
                contnets_to_wrap = [ContentBlock(text=f"ToolResult(tool_call_id={content.tool_result.tool_call_id}):"), *content.tool_result.result]
            else:
                contnets_to_wrap = [content]

            wrapped_lines = []
            for c in contnets_to_wrap:
                if c.image and c.image.image:
                    block_wrapped_lines = plot_ascii(c.image.image, min(80, width - len(prefix)))
                else:
                    text = _content_to_text([c])
                    paragraphs = text.split('\n')
                    wrapped_paragraphs = [textwrap.wrap(p, width=width - len(prefix)) for p in paragraphs]
                    block_wrapped_lines = [line for paragraph in wrapped_paragraphs for line in paragraph]
                wrapped_lines.extend(block_wrapped_lines)
        if i == 0:
            if wrapped_lines:
                result.append(f"{prefix}{text_color}{wrapped_lines[0]}{RESET}")
            else:
                result.append(f"{prefix}{text_color}{RESET}")
        else:
            result.append(f"{subsequent_prefix}{text_color}{wrapped_lines[0]}{RESET}")
        result.extend([f"{subsequent_prefix}{text_color}{line}{RESET}" for line in wrapped_lines[1:]])
    return result
====================================================================
-> Chunk: util\verbosity.py::5


def print_wrapped_messages(messages: List[Message], max_role_length: int, color: str, wrap_width: Optional[int] = None):
    """Print wrapped messages with proper indentation, customizable wrap width, and consistent ASCII piping."""
    terminal_width = get_terminal_width()
    prefix = f"{PIPE_COLOR}│   "
    role_prefix = ' ' * (max_role_length + 2)
    subsequent_prefix = f"{PIPE_COLOR}│   {role_prefix}"
    wrapping_width = wrap_width or (terminal_width - len(prefix))

    for i, message in enumerate(messages):
        role = message.role
        m = message.content[0]


        role_color = SYSTEM_COLOR if role == "system" else USER_COLOR if role == "user" else ASSISTANT_COLOR

        role_line = f"{prefix}{role_color}{role.rjust(max_role_length)}: {RESET}"
        wrapped_lines = wrap_text_with_prefix(message, wrapping_width - len(role_prefix), '', subsequent_prefix, role_color)

        print(f"{role_line}{wrapped_lines[0]}")
        for line in wrapped_lines[1:]:
            print(line)

        if i < len(messages) - 1:
            print(f"{PIPE_COLOR}│{RESET}")
====================================================================


###### Cluster Eval ######
Score: 13.0
Cluster name: Asynchronous Database and Web Server Cluster
Asynchronous Database and Web Server Cluster:

-> Chunk: studio\__main__.py::4


def main():
    # ... other code

    async def open_browser(host, port):
        while True:
            logger.debug(f"Checking TCP port {port} on {host} for readiness.")
            if _socket_is_open(host, port):
                url = f"http://{host}:{port}"
                logger.debug(f"Port is open, launching {url}.")
                webbrowser.open_new(url)
                return

            logger.debug(f"Port {port} was not open, retrying.")
            await asyncio.sleep(.1)

    # Start the database watcher
    loop = asyncio.new_event_loop()

    config = uvicorn.Config(app=app, host=args.host, port=args.port, loop=loop)
    server = uvicorn.Server(config)
    loop.create_task(server.serve())
    if db_path:
        loop.create_task(db_watcher(db_path, app))
    if args.open:
        loop.create_task(open_browser(args.host, args.port))
    loop.run_forever()

if __name__ == "__main__":
    main()
====================================================================
-> Chunk: studio\__main__.py::2


def main():
    parser = ArgumentParser(description="ell studio")
    parser.add_argument("--storage-dir" , default=None,
                        help="Directory for filesystem serializer storage (default: current directory)")
    parser.add_argument("--pg-connection-string", default=None,
                        help="PostgreSQL connection string (default: None)")
    parser.add_argument("--host", default="127.0.0.1", help="Host to run the server on (default: localhost)")
    parser.add_argument("--port", type=int, default=5555, help="Port to run the server on (default: 5555)")
    parser.add_argument("--dev", action="store_true", help="Run in development mode")
    parser.add_argument("--open", action="store_true", help="Opens the studio web UI in a browser")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enables debug logging for more verbose output")
    args = parser.parse_args()

    _setup_logging(logging.DEBUG if args.verbose else logging.INFO)

    if args.dev:
        assert args.port == 5555, "Port must be 5000 in development mode"

    config = Config.create(storage_dir=args.storage_dir,
                    pg_connection_string=args.pg_connection_string)
    app = create_app(config)

    if not args.dev:
        # In production mode, serve the built React app
        static_dir = Path(__file__).parent / "static"
        # app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")

        @app.get("/{full_path:path}")
        async def serve_react_app(full_path: str):
            file_path = static_dir / full_path
            if file_path.exists() and file_path.is_file():
                return FileResponse(file_path)
            else:
                return FileResponse(static_dir / "index.html")

    # Respect Config.create behavior, which has fallback to env vars.
    db_path = Path(config.storage_dir) if config.storage_dir else None
    # ... other code
====================================================================
-> Chunk: studio\datamodels.py::1


from datetime import datetime
from typing import List, Optional, Dict, Any
from sqlmodel import SQLModel
from ell.types import SerializedLMPBase, InvocationBase, InvocationContentsBase


class SerializedLMPWithUses(SerializedLMPBase):
    lmp_id : str
    uses: List[SerializedLMPBase]


class InvocationPublic(InvocationBase):
    lmp: SerializedLMPBase
    uses: List["InvocationPublicWithConsumes"]
    contents: InvocationContentsBase

class InvocationPublicWithConsumes(InvocationPublic):
    consumes: List[InvocationPublic]
    consumed_by: List[InvocationPublic]



from pydantic import BaseModel

class GraphDataPoint(BaseModel):
    date: datetime
    count: int
    avg_latency: float
    tokens: int
    # cost: float

class InvocationsAggregate(BaseModel):
    total_invocations: int
    total_tokens: int
    avg_latency: float
    # total_cost: float
    unique_lmps: int
    # successful_invocations: int
    # success_rate: float
    graph_data: List[GraphDataPoint]
====================================================================
-> Chunk: studio\server.py::1


from typing import Optional, Dict, Any

from sqlmodel import Session
from ell.stores.sql import PostgresStore, SQLiteStore
from ell import __version__
from fastapi import FastAPI, Query, HTTPException, Depends, Response, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import logging
import json
from ell.studio.config import Config
from ell.studio.connection_manager import ConnectionManager
from ell.studio.datamodels import InvocationPublicWithConsumes, SerializedLMPWithUses

from ell.types import SerializedLMP
from datetime import datetime, timedelta
from sqlmodel import select


logger = logging.getLogger(__name__)


from ell.studio.datamodels import InvocationsAggregate


def get_serializer(config: Config):
    if config.pg_connection_string:
        return PostgresStore(config.pg_connection_string)
    elif config.storage_dir:
        return SQLiteStore(config.storage_dir)
    else:
        raise ValueError("No storage configuration found")
====================================================================
-> Chunk: studio\server.py::6


def create_app(config:Config):
    # ... other code

    @app.get("/api/lmp-history")
    def get_lmp_history(
        days: int = Query(365, ge=1, le=3650),  # Default to 1 year, max 10 years
        session: Session = Depends(get_session)
    ):
        # Calculate the start date
        start_date = datetime.utcnow() - timedelta(days=days)

        # Query to get all LMP creation times within the date range
        query = (
            select(SerializedLMP.created_at)
            .where(SerializedLMP.created_at >= start_date)
            .order_by(SerializedLMP.created_at)
        )

        results = session.exec(query).all()

        # Convert results to a list of dictionaries
        history = [{"date": str(row), "count": 1} for row in results]

        return history
    # ... other code
====================================================================


###### Cluster Eval ######
Score: 12.666666666666666
Cluster name: Messaging and Content Management Cluster
Messaging and Content Management Cluster:

-> Chunk: types\message.py::1


# todo: implement tracing for structured outs. this a v2 feature.
import json
from ell.types._lstr import _lstr
from functools import cached_property
import numpy as np
import base64
from io import BytesIO
from PIL import Image as PILImage

from pydantic import BaseModel, ConfigDict, model_validator, field_serializer
from sqlmodel import Field

from concurrent.futures import ThreadPoolExecutor, as_completed

from typing import Any, Callable, Dict, List, Optional, Union

from ell.util.serialization import serialize_image
_lstr_generic = Union[_lstr, str]
InvocableTool = Callable[..., Union["ToolResult", _lstr_generic, List["ContentBlock"], ]]

# AnyContent represents any type that can be passed to Message.
AnyContent = Union["ContentBlock", str, "ToolCall", "ToolResult", "ImageContent", np.ndarray, PILImage.Image, BaseModel]
====================================================================
-> Chunk: types\message.py::2


class ToolResult(BaseModel):
    tool_call_id: _lstr_generic
    result: List["ContentBlock"]

    @property
    def text(self) -> str:
        return _content_to_text(self.result)

    @property
    def text_only(self) -> str:
        return _content_to_text_only(self.result)

    # # XXX: Possibly deprecate
    # def readable_repr(self) -> str:
    #     return f"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"

    def __repr__(self):
        return f"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"
====================================================================
-> Chunk: types\message.py::5


class ImageContent(BaseModel):

    @classmethod
    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, "ImageContent"]):
        if isinstance(value, cls):
            return value

        if isinstance(value, str):
            if value.startswith('http://') or value.startswith('https://'):
                return cls(url=value)
            try:
                img_data = base64.b64decode(value)
                img = PILImage.open(BytesIO(img_data))
                if img.mode not in ('L', 'RGB', 'RGBA'):
                    return cls(image=img.convert('RGB'))
            except:
                raise ValueError("Invalid base64 string or URL for image")

        if isinstance(value, np.ndarray):
            if value.ndim == 3 and value.shape[2] in (3, 4):
                mode = 'RGB' if value.shape[2] == 3 else 'RGBA'
                return cls(image=PILImage.fromarray(value, mode=mode))
            else:
                raise ValueError(f"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.")

        if isinstance(value, PILImage.Image):
            if value.mode not in ('L', 'RGB', 'RGBA'):
                value = value.convert('RGB')
            return cls(image=value)

        raise ValueError(f"Invalid image type: {type(value)}")

    @field_serializer('image')
    def serialize_image(self, image: Optional[PILImage.Image], _info):
        if image is None:
            return None
        return serialize_image(image)
====================================================================
-> Chunk: types\message.py::6


class ContentBlock(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    text: Optional[_lstr_generic] = Field(default=None)
    image: Optional[ImageContent] = Field(default=None)
    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)
    tool_call: Optional[ToolCall] = Field(default=None)
    parsed: Optional[BaseModel] = Field(default=None)
    tool_result: Optional[ToolResult] = Field(default=None)
    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.
    # This breaks us maintaing parity with the openai python client in some sen but so does image.

    def __init__(self, *args, **kwargs):
        if "image" in kwargs and not isinstance(kwargs["image"], ImageContent):
            im = kwargs["image"] = ImageContent.coerce(kwargs["image"])
            # XXX: Backwards compatibility, Deprecate.
            if (d := kwargs.get("image_detail", None)): im.detail = d

        super().__init__(*args, **kwargs)


    @model_validator(mode='after')
    def check_single_non_null(self):
        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]
        if len(non_null_fields) > 1:
            raise ValueError(f"Only one field can be non-null. Found: {', '.join(non_null_fields)}")
        return self

    def __str__(self):
        return repr(self)

    def __repr__(self):
        non_null_fields = [f"{field}={value}" for field, value in self.__dict__.items() if value is not None]
        return f"ContentBlock({', '.join(non_null_fields)})"

    @property
    def type(self):
        if self.text is not None:
            return "text"
        if self.image is not None:
            return "image"
        if self.audio is not None:
            return "audio"
        if self.tool_call is not None:
            return "tool_call"
        if self.parsed is not None:
            return "parsed"
        if self.tool_result is not None:
            return "tool_result"
        return None

    @property
    def content(self):
        return getattr(self, self.type)
====================================================================
-> Chunk: types\message.py::9


class Message(BaseModel):
    role: str
    content: List[ContentBlock]


    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):
        content_blocks = to_content_blocks(content, **content_block_kwargs)

        super().__init__(role=role, content=content_blocks)

    # XXX: This choice of naming is unfortunate, but it is what it is.
    @property
    def text(self) -> str:
        """Returns all text content, replacing non-text content with their representations.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text
            'Hello\\n<PilImage>\\nWorld'
        """
        return _content_to_text(self.content)
====================================================================


###### Cluster Eval ######
Score: 12.666666666666666
Cluster name: Reinforcement Learning Cluster
Reinforcement Learning Cluster:

-> Chunk: 0.1.0\cem.py::6


# Main execution code
if __name__ == '__main__':
    # Initialize environments
    env_fns = [make_env(ENV_NAME, SEED + i) for i in range(NUM_ENVIRONMENTS)]
    envs = AsyncVectorEnv(env_fns)

    # Get environment details
    dummy_env = gym.make(ENV_NAME)
    state_dim = dummy_env.observation_space.shape[0]
    action_dim = dummy_env.action_space.n
    dummy_env.close()

    # Initialize policy network and optimizer
    policy = PolicyNetwork(state_dim, action_dim)
    optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    # Training Loop
    for iteration in range(1, NUM_ITERATIONS + 1):
        try:
            # Step 1: Collect Trajectories
            trajectories = collect_trajectories(envs, policy, TRAJECTORIES_PER_ITER, MAX_STEPS)
        except Exception as e:
            print(f"Error during trajectory collection at iteration {iteration}: {e}")
            break

        # Step 2: Select Elite Trajectories
        elite_trajectories = select_elite(trajectories, ELITE_PERCENT)

        if len(elite_trajectories) == 0:
            print(f"Iteration {iteration}: No elite trajectories found. Skipping update.")
            continue

        # Step 3: Create Training Data
        states, actions = create_training_data(elite_trajectories)

        if states is None or actions is None:
            print(f"Iteration {iteration}: No training data available. Skipping update.")
            continue

        # Step 4: Behavioral Cloning (Policy Update)
        dataset_size = states.size(0)
        indices = np.arange(dataset_size)
        np.random.shuffle(indices)

        for start in range(0, dataset_size, BATCH_SIZE):
            end = start + BATCH_SIZE
            batch_indices = indices[start:end]
            batch_states = states[batch_indices]
            batch_actions = actions[batch_indices]

            optimizer.zero_grad()
            logits = policy(batch_states)
            loss = criterion(logits, batch_actions)
            loss.backward()
            optimizer.step()

        # Step 5: Evaluate Current Policy
        avg_reward = np.mean([traj['reward'] for traj in elite_trajectories])
        print(f"Iteration {iteration}: Elite Trajectories: {len(elite_trajectories)}, Average Reward: {avg_reward:.2f}")

    # Close environments
    envs.close()

    # Testing the Trained Policy
    def test_policy(policy, env_name=ENV_NAME, episodes=5, max_steps=500):
        env = gym.make(env_name)
        total_rewards = []
        for episode in range(episodes):
            obs, _ = env.reset()
            done = False
            episode_reward = 0
            for _ in range(max_steps):
                obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)
                with torch.no_grad():
                    action = policy.get_action(obs_tensor).item()
                obs, reward, done, info, _ = env.step(action)
                episode_reward += reward
                if done:
                    break
            total_rewards.append(episode_reward)
            print(f"Test Episode {episode + 1}: Reward: {episode_reward}")
        env.close()
        print(f"Average Test Reward over {episodes} episodes: {np.mean(total_rewards):.2f}")

    # Run the test
    test_policy(policy)
====================================================================
-> Chunk: 0.1.0\cem.py::1


import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from gym.vector import AsyncVectorEnv
import random

# Set random seeds for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# Hyperparameters
NUM_ENVIRONMENTS = 4           # Reduced for simplicity
NUM_ITERATIONS = 50            # Number of training iterations
TRAJECTORIES_PER_ITER = 100    # Total number of trajectories per iteration
ELITE_PERCENT = 10             # Top k% trajectories to select
LEARNING_RATE = 1e-3
BATCH_SIZE = 64
MAX_STEPS = 500                # Max steps per trajectory
ENV_NAME = 'CartPole-v1'
====================================================================
-> Chunk: 0.1.0\cem.py::2


       # Gym environment

# Define the Policy Network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state):
        logits = self.fc(state)
        return logits

    def get_action(self, state):
        logits = self.forward(state)
        action_probs = torch.softmax(logits, dim=-1)
        action = torch.multinomial(action_probs, num_samples=1)
        return action.squeeze(-1)

# Function to create multiple environments
def make_env(env_name, seed):
    def _init():
        env = gym.make(env_name)
        return env
    return _init
====================================================================
-> Chunk: 0.1.0\cem.py::4


def collect_trajectories(envs, policy, num_trajectories, max_steps):
    # ... other code

    while total_collected < num_trajectories and steps < max_steps:
        # Convert observations to tensor efficiently
        try:
            # Ensure 'obs' is a NumPy array
            if not isinstance(obs, np.ndarray):
                print(f"Unexpected type for observations: {type(obs)}")
                raise ValueError("Observations are not a NumPy array.")

            # Convert observations to tensor using from_numpy for efficiency
            obs_tensor = torch.from_numpy(obs).float()
            # Ensure the observation dimension matches expected
            assert obs_tensor.shape[1] == 4, f"Expected observation dimension 4, got {obs_tensor.shape[1]}"
        except Exception as e:
            print(f"Error converting observations to tensor at step {steps}: {e}")
            print(f"Observations: {obs}")
            raise e

        with torch.no_grad():
            actions = policy.get_action(obs_tensor).cpu().numpy()

        # Unpack step based on Gym version
        try:
            # For Gym versions >=0.26, step returns five values
            next_obs, rewards, dones, truncs, infos = envs.step(actions)
        except ValueError:
            # For older Gym versions, step returns four values
            next_obs, rewards, dones, infos = envs.step(actions)
            truncs = [False] * len(dones)  # Assume no truncations if not provided

        # Handle the reset output of step()
        if isinstance(next_obs, tuple) or isinstance(next_obs, list):
            next_obs = next_obs[0]  # Extract observations

        # Ensure infos is a list
        if not isinstance(infos, list):
            infos = [{} for _ in range(num_envs)]  # Default to empty dicts

        for i in range(num_envs):
            if not done_envs[i]:
                # Check if obs[i] has the correct shape
                if len(obs[i]) != 4:
                    print(f"Unexpected observation shape for env {i}: {obs[i]}")
                    continue  # Skip this step for the problematic environment

                env_states[i].append(obs[i])
                env_actions[i].append(actions[i])
                env_rewards[i] += rewards[i]
                if dones[i] or truncs[i]:
                    # Extract reward from infos
                    if isinstance(infos[i], dict):
                        episode_info = infos[i].get('episode', {})
                        traj_reward = episode_info.get('r') if 'r' in episode_info else env_rewards[i]
                    else:
                        # Handle cases where infos[i] is not a dict
                        traj_reward = env_rewards[i]
                        print(f"Warning: infos[{i}] is not a dict. Received type: {type(infos[i])}")

                    trajectories.append({
                        'states': env_states[i],
                        'actions': env_actions[i],
                        'reward': traj_reward
                    })
                    total_collected += 1
                    env_states[i] = []
                    env_actions[i] = []
                    env_rewards[i] = 0.0
                    done_envs[i] = True

        obs = next_obs
        steps += 1

        # Reset environments that are done
        if any(done_envs):
            indices = [i for i, done in enumerate(done_envs) if done]
            if total_collected < num_trajectories:
                for i in indices:
                    try:
                        # Directly reset the environment
                        reset_output = envs.envs[i].reset()
                        if isinstance(reset_output, tuple) or isinstance(reset_output, list):
                            # For Gym versions where reset returns (obs, info)
                            obs[i] = reset_output[0]
                        else:
                            # For Gym versions where reset returns only obs
                            obs[i] = reset_output
                        done_envs[i] = False
                    except Exception as e:
                        print(f"Error resetting environment {i}: {e}")
                        # Optionally, handle the failure (e.g., retry, terminate the environment)
                        done_envs[i] = False  # Prevent infinite loop

    return trajectories
====================================================================


###### Cluster Eval ######
Score: 12.333333333333334
Cluster name: OpenAI Model Registration Cluster
OpenAI Model Registration Cluster:

-> Chunk: models\openai.py::2


def register(client: openai.Client):
    """
    Register OpenAI models with the provided client.

    This function takes an OpenAI client and registers various OpenAI models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (openai.Client): An instance of the OpenAI client to be used
                                for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    #XXX: Deprecation in 0.1.0
    standard_models = [
        'gpt-4-1106-preview',
        'gpt-4-32k-0314',
        'text-embedding-3-large',
        'gpt-4-0125-preview',
        'babbage-002',
        'gpt-4-turbo-preview',
        'gpt-4o',
        'gpt-4o-2024-05-13',
        'gpt-4o-mini-2024-07-18',
        'gpt-4o-mini',
        'gpt-4o-2024-08-06',
        'gpt-3.5-turbo-0301',
        'gpt-3.5-turbo-0613',
        'tts-1',
        'gpt-3.5-turbo',
        'gpt-3.5-turbo-16k',
        'davinci-002',
        'gpt-3.5-turbo-16k-0613',
        'gpt-4-turbo-2024-04-09',
        'gpt-3.5-turbo-0125',
        'gpt-4-turbo',
        'gpt-3.5-turbo-1106',
        'gpt-3.5-turbo-instruct-0914',
        'gpt-3.5-turbo-instruct',
        'gpt-4-0613',
        'gpt-4',
        'gpt-4-0314',
        'gpt-4o-audio-preview',
        'gpt-4o-realtime',
    ]
    for model_id in standard_models:
        config.register_model(model_id, client)

    #XXX: Deprecation in 0.1.0
    config.register_model('o1-preview', client, supports_streaming=False)
    config.register_model('o1-mini', client, supports_streaming=False)

default_client = None
try:
    default_client = openai.Client()
except openai.OpenAIError as e:
    pass

register(default_client)
config.default_client = default_client
====================================================================
-> Chunk: models\openai.py::1


"""
This module handles the registration of OpenAI models within the ell framework.

It provides functionality to register various OpenAI models with a given OpenAI client,
making them available for use throughout the system. The module also sets up a default
client behavior for unregistered models.

Key features:
1. Registration of specific OpenAI models with their respective types (system, openai, openai-internal).
2. Utilization of a default OpenAI client for any unregistered models,

The default client behavior ensures that even if a specific model is not explicitly
registered, the system can still attempt to use it with the default OpenAI client.
This fallback mechanism provides flexibility in model usage while maintaining a
structured approach to model registration.

Note: The actual model availability may depend on your OpenAI account's access and the
current offerings from OpenAI.

Additionally, due to the registration of default mdoels, the OpenAI client may be used for
anthropic, cohere, groq, etc. models if their clients are not registered or fail
to register due to an error (lack of API keys, rate limits, etc.)
"""

from ell.configurator import config
import openai

import logging
import colorama

logger = logging.getLogger(__name__)
====================================================================
-> Chunk: ell\provider.py::6


def _validate_messages_are_tracked(
    messages: List[Message], origin_id: Optional[str] = None
):
    if origin_id is None:
        return

    for message in messages:
        assert isinstance(
            message.text, _lstr
        ), f"Provider implementation error: Message text should be an instance of _lstr, got {type(message.text)}"
        assert (
            origin_id in message.text.__origin_trace__
        ), f"Provider implementation error: Message origin_id {message.text.__origin_trace__} does not match the provided origin_id {origin_id}"
    return True
====================================================================
-> Chunk: ell\configurator.py::5


class Config(BaseModel):

    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:
        """
        Get the OpenAI client for a specific model name.

        :param model_name: The name of the model to get the client for.
        :type model_name: str
        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.
        :rtype: Tuple[Optional[openai.Client], bool]
        """
        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry
        model_config = current_registry.get(model_name)
        fallback = False
        if not model_config:
            warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."
            if self.verbose:
                from colorama import Fore, Style
                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")
            else:
                _config_logger.debug(warning_message)
            client = self.default_client
            fallback = True
        else:
            client = model_config.default_client
        return client, fallback
====================================================================


###### Cluster Eval ######
Score: 12.333333333333334
Cluster name: Execution Tracing and Tracking Cluster
Execution Tracing and Tracking Cluster:

-> Chunk: lmp\_track.py::2


def _track(func_to_track: Callable, *, forced_dependencies: Optional[Dict[str, Any]] = None) -> Callable:

    lmp_type = getattr(func_to_track, "__ell_type__", LMPType.OTHER)


    # see if it exists
    if not hasattr(func_to_track, "_has_serialized_lmp"):
        func_to_track._has_serialized_lmp = False

    if not hasattr(func_to_track, "__ell_hash__") and not config.lazy_versioning:
        ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)


    @wraps(func_to_track)
    def tracked_func(*fn_args, _get_invocation_id=False, **fn_kwargs) -> str:
        # XXX: Cache keys and global variable binding is not thread safe.
        # Compute the invocation id and hash the inputs for serialization.
        invocation_id = "invocation-" + secrets.token_hex(16)

        state_cache_key : str = None
        if not config.store:
            return func_to_track(*fn_args, **fn_kwargs, _invocation_origin=invocation_id)[0]

        parent_invocation_id = get_current_invocation()
        try:
            push_invocation(invocation_id)

            # Convert all positional arguments to named keyword arguments
            sig = inspect.signature(func_to_track)
            # Filter out kwargs that are not in the function signature
            filtered_kwargs = {k: v for k, v in fn_kwargs.items() if k in sig.parameters}

            bound_args = sig.bind(*fn_args, **filtered_kwargs)
            bound_args.apply_defaults()
            all_kwargs = dict(bound_args.arguments)

            # Get the list of consumed lmps and clean the invocation params for serialization.
            cleaned_invocation_params, ipstr, consumes = prepare_invocation_params( all_kwargs)

            try_use_cache = hasattr(func_to_track.__wrapper__, "__ell_use_cache__")

            if  try_use_cache:
                # Todo: add nice logging if verbose for when using a cahced invocaiton. IN a different color with thar args..
                if not hasattr(func_to_track, "__ell_hash__")  and config.lazy_versioning:
                    fn_closure, _ = ell.util.closure.lexically_closured_source(func_to_track)

                # compute the state cachekey
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

                cache_store = func_to_track.__wrapper__.__ell_use_cache__
                cached_invocations = cache_store.get_cached_invocations(func_to_track.__ell_hash__, state_cache_key)


                if len(cached_invocations) > 0:
                    # XXX: Fix caching.
                    results =  [d.deserialize() for  d in cached_invocations[0].results]

                    logger.info(f"Using cached result for {func_to_track.__qualname__} with state cache key: {state_cache_key}")
                    if len(results) == 1:
                        return results[0]
                    else:
                        return results
                    # Todo: Unfiy this with the non-cached case. We should go through the same code pathway.
                else:
                    logger.info(f"Attempted to use cache on {func_to_track.__qualname__} but it was not cached, or did not exist in the store. Refreshing cache...")


            _start_time = utc_now()

            # XXX: thread saftey note, if I prevent yielding right here and get the global context I should be fine re: cache key problem

            # get the prompt
            (result, invocation_api_params, metadata) = (
                (func_to_track(*fn_args, **fn_kwargs), {}, {})
                if lmp_type == LMPType.OTHER
                else func_to_track(*fn_args, _invocation_origin=invocation_id, **fn_kwargs, )
                )
            latency_ms = (utc_now() - _start_time).total_seconds() * 1000
            usage = metadata.get("usage", {"prompt_tokens": 0, "completion_tokens": 0})
            prompt_tokens= usage.get("prompt_tokens", 0) if usage else 0
            completion_tokens= usage.get("completion_tokens", 0) if usage else 0


            #XXX: cattrs add invocation origin here recursively on all pirmitive types within a message.
            #XXX: This will allow all objects to be traced automatically irrespective origin rather than relying on the API to do it, it will of vourse be expensive but unify track.
            #XXX: No other code will need to consider tracking after this point.

            if not hasattr(func_to_track, "__ell_hash__") and config.lazy_versioning:
                ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)
            _serialize_lmp(func_to_track)

            if not state_cache_key:
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

            _write_invocation(func_to_track, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                            state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id)

            if _get_invocation_id:
                return result, invocation_id
            else:
                return result
        finally:
            pop_invocation()


    func_to_track.__wrapper__  = tracked_func
    if hasattr(func_to_track, "__ell_api_params__"):
        tracked_func.__ell_api_params__ = func_to_track.__ell_api_params__
    if hasattr(func_to_track, "__ell_params_model__"):
        tracked_func.__ell_params_model__ = func_to_track.__ell_params_model__
    tracked_func.__ell_func__ = func_to_track
    tracked_func.__ell_track = True

    return tracked_func
====================================================================
-> Chunk: lmp\_track.py::3


def _serialize_lmp(func):
    # Serialize deptjh first all fo the used lmps.
    for f in func.__ell_uses__:
        _serialize_lmp(f)

    if getattr(func, "_has_serialized_lmp", False):
        return
    func._has_serialized_lmp = False
    fn_closure = func.__ell_closure__
    lmp_type = func.__ell_type__
    name = func.__qualname__
    api_params = getattr(func, "__ell_api_params__", None)

    lmps = config.store.get_versions_by_fqn(fqn=name)
    version = 0
    already_in_store = any(lmp.lmp_id == func.__ell_hash__ for lmp in lmps)

    if not already_in_store:
        commit = None
        if lmps:
            latest_lmp = max(lmps, key=lambda x: x.created_at)
            version = latest_lmp.version_number + 1
            if config.autocommit:
                # XXX: Move this out to autocommit itself.
                if not _autocommit_warning():
                    from ell.util.differ import write_commit_message_for_diff
                    commit = str(write_commit_message_for_diff(
                    f"{latest_lmp.dependencies}\n\n{latest_lmp.source}", 
                        f"{fn_closure[1]}\n\n{fn_closure[0]}")[0])

        serialized_lmp = SerializedLMP(
            lmp_id=func.__ell_hash__,
            name=name,
            created_at=utc_now(),
            source=fn_closure[0],
            dependencies=fn_closure[1],
            commit_message=commit,
            initial_global_vars=get_immutable_vars(fn_closure[2]),
            initial_free_vars=get_immutable_vars(fn_closure[3]),
            lmp_type=lmp_type,
            api_params=api_params if api_params else None,
            version_number=version,
        )
        config.store.write_lmp(serialized_lmp, [f.__ell_hash__ for f in func.__ell_uses__])
    func._has_serialized_lmp = True
====================================================================
-> Chunk: lmp\_track.py::4


def _write_invocation(func, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                     state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id):

    invocation_contents = InvocationContents(
        invocation_id=invocation_id,
        params=cleaned_invocation_params,
        results=result,
        invocation_api_params=invocation_api_params,
        global_vars=get_immutable_vars(func.__ell_closure__[2]),
        free_vars=get_immutable_vars(func.__ell_closure__[3])
    )

    if invocation_contents.should_externalize and config.store.has_blob_storage:
        invocation_contents.is_external = True

        # Write to the blob store 
        blob_id = config.store.blob_store.store_blob(
            json.dumps(invocation_contents.model_dump(
            ), default=str, ensure_ascii=False).encode('utf-8'),
            invocation_id
        )
        invocation_contents = InvocationContents(
            invocation_id=invocation_id,
            is_external=True,
        )

    invocation = Invocation(
        id=invocation_id,
        lmp_id=func.__ell_hash__,
        created_at=utc_now(),
        latency_ms=latency_ms,
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        state_cache_key=state_cache_key,
        used_by_id=parent_invocation_id,
        contents=invocation_contents
    )

    config.store.write_invocation(invocation, consumes)
====================================================================
-> Chunk: stores\sql.py::3


class SQLStore(ell.store.Store):

    def write_invocation(self, invocation: Invocation, consumes: Set[str]) -> Optional[Any]:
        with Session(self.engine) as session:
            lmp = session.exec(select(SerializedLMP).filter(SerializedLMP.lmp_id == invocation.lmp_id)).first()
            assert lmp is not None, f"LMP with id {invocation.lmp_id} not found. Writing invocation erroneously"

            # Increment num_invocations
            if lmp.num_invocations is None:
                lmp.num_invocations = 1
            else:
                lmp.num_invocations += 1

            # Add the invocation contents
            session.add(invocation.contents)

            # Add the invocation
            session.add(invocation)

            # Now create traces.
            for consumed_id in consumes:
                session.add(InvocationTrace(
                    invocation_consumer_id=invocation.id,
                    invocation_consuming_id=consumed_id
                ))

            session.commit()
            return None
====================================================================
-> Chunk: util\closure.py::1


"""
This should do the following.
# prompt_consts.py
import math
def test():
    return math.sin(10)

# lol3.py
import prompt_consts

X = 7
def xD():
    print(X)
    return prompt_consts.test()

###
Our goal is to use AST & dill to get a full lexical closured source of xD, with the exception of modules that are stored in site-packages. For example.

lexical_extration(xD) returns
#closure.py
import math
def test():
    return math.sin(10)

X = 7 
def xD():
    print(X)
    return test()

"""
import collections
import ast
import hashlib
import itertools
from typing import Any, Dict, Iterable, Optional, Set, Tuple, Callable
import dill
import inspect
import types
from dill.source import getsource
import re
from collections import deque
import black

from ell.util.serialization import is_immutable_variable
from ell.util.should_import import should_import

DELIM = "$$$$$$$$$$$$$$$$$$$$$$$$$"
FORBIDDEN_NAMES = ["ell", "lstr"]
====================================================================


###### Cluster Eval ######
Score: 12.0
Cluster name: Reinforcement Learning and Elite Selection Cluster
Reinforcement Learning and Elite Selection Cluster:

-> Chunk: 0.1.0\cem.py::5


def select_elite(trajectories, percentile=ELITE_PERCENT):
    rewards = [traj['reward'] for traj in trajectories]
    if not rewards:
        return []
    reward_threshold = np.percentile(rewards, 100 - percentile)
    elite_trajectories = [traj for traj in trajectories if traj['reward'] >= reward_threshold]
    return elite_trajectories

# Function to create training dataset from elite trajectories
def create_training_data(elite_trajectories):
    states = []
    actions = []
    for traj in elite_trajectories:
        states.extend(traj['states'])
        actions.extend(traj['actions'])
    if not states or not actions:
        return None, None
    # Convert lists to NumPy arrays first for efficiency
    states = np.array(states, dtype=np.float32)
    actions = np.array(actions, dtype=np.int64)
    # Convert to PyTorch tensors
    states = torch.from_numpy(states)
    actions = torch.from_numpy(actions)
    return states, actions
====================================================================
-> Chunk: 0.1.0\cem.py::3


def collect_trajectories(envs, policy, num_trajectories, max_steps):
    trajectories = []
    num_envs = envs.num_envs

    # Handle the return type of reset()
    reset_output = envs.reset()
    if isinstance(reset_output, tuple) or isinstance(reset_output, list):
        obs = reset_output[0]  # Extract observations
    else:
        obs = reset_output

    done_envs = [False] * num_envs
    steps = 0

    # Initialize storage for states, actions, and rewards per environment
    env_states = [[] for _ in range(num_envs)]
    env_actions = [[] for _ in range(num_envs)]
    env_rewards = [0.0 for _ in range(num_envs)]
    total_collected = 0
    # ... other code
====================================================================
-> Chunk: 0.1.0\cem.py::2


       # Gym environment

# Define the Policy Network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state):
        logits = self.fc(state)
        return logits

    def get_action(self, state):
        logits = self.forward(state)
        action_probs = torch.softmax(logits, dim=-1)
        action = torch.multinomial(action_probs, num_samples=1)
        return action.squeeze(-1)

# Function to create multiple environments
def make_env(env_name, seed):
    def _init():
        env = gym.make(env_name)
        return env
    return _init
====================================================================
-> Chunk: 0.1.0\cpbo.py::6


# Main CBPO algorithm
def CBPO(env_name='CartPole-v1', num_epochs=10, num_episodes_per_epoch=100, gamma=0.99, 
         batch_size=64, learning_rate=1e-3, device='cpu'):

    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")

        # 1. Collect trajectories
        trajectories = collect_trajectories(env, policy, num_episodes_per_epoch, device)

        # 2. Create labeled dataset
        states, actions, labels = create_labeled_dataset(trajectories, gamma, device)

        # 3. Create DataLoader
        dataset = TensorDataset(states, actions, labels)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        # 4. Behavioral Cloning Update
        behavioral_cloning_update(policy, optimizer, dataloader, device)

        # 5. Evaluate current policy
        avg_reward = evaluate_policy(env, policy, device)
        print(f"Average Reward: {avg_reward}")

        # Early stopping if solved
        if avg_reward >= env.spec.reward_threshold:
            print(f"Environment solved in {epoch+1} epochs!")
            break

    env.close()
    return policy
====================================================================
-> Chunk: 0.1.0\cpbo.py::5


# Evaluation function
def evaluate_policy(env, policy, device, episodes=5):
    policy.eval()
    total_rewards = []
    for _ in range(episodes):
        state, info = env.reset()
        done = False
        ep_reward = 0
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                action_probs = policy(state_tensor)
            action = torch.argmax(action_probs, dim=1).item()
            # Handle Gym's updated step() API
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            ep_reward += reward
            state = next_state
        total_rewards.append(ep_reward)
    average_reward = np.mean(total_rewards)
    return average_reward
====================================================================


###### Cluster Eval ######
Score: 11.666666666666666
Cluster name: Caching and Store Operations Cluster
Caching and Store Operations Cluster:

-> Chunk: ell\store.py::3


class Store(ABC):


    @contextmanager
    def freeze(self, *lmps: InvocableLM):
        """
        A context manager for caching operations using a particular store.

        Args:
            *lmps: InvocableLM objects to freeze.

        Yields:
            None
        """
        old_cache_values = {}
        try:
            for lmp in lmps:
                old_cache_values[lmp] = getattr(lmp, '__ell_use_cache__', None)
                setattr(lmp, '__ell_use_cache__', self)
            yield
        finally:
            # TODO: Implement cache storage logic here
            for lmp in lmps:
                if lmp in old_cache_values:
                    setattr(lmp, '__ell_use_cache__', old_cache_values[lmp])
                else:
                    delattr(lmp, '__ell_use_cache__')
====================================================================
-> Chunk: stores\sql.py::6


class SQLStore(ell.store.Store):


    def get_lmps(self, session: Session, skip: int = 0, limit: int = 10, subquery=None, **filters: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:

        query = select(SerializedLMP)

        if subquery is not None:
            query = query.join(subquery, and_(
                SerializedLMP.name == subquery.c.name,
                SerializedLMP.created_at == subquery.c.max_created_at
            ))

        if filters:
            for key, value in filters.items():
                query = query.where(getattr(SerializedLMP, key) == value)

        query = query.order_by(SerializedLMP.created_at.desc())  # Sort by created_at in descending order
        query = query.offset(skip).limit(limit)
        results = session.exec(query).all()

        return results
====================================================================
-> Chunk: stores\sql.py::7


class SQLStore(ell.store.Store):

    def get_invocations(self, session: Session, lmp_filters: Dict[str, Any], skip: int = 0, limit: int = 10, filters: Optional[Dict[str, Any]] = None, hierarchical: bool = False) -> List[Dict[str, Any]]:

        query = select(Invocation).join(SerializedLMP)

        # Apply LMP filters
        for key, value in lmp_filters.items():
            query = query.where(getattr(SerializedLMP, key) == value)

        # Apply invocation filters
        if filters:
            for key, value in filters.items():
                query = query.where(getattr(Invocation, key) == value)

        # Sort from newest to oldest
        query = query.order_by(Invocation.created_at.desc()).offset(skip).limit(limit)

        invocations = session.exec(query).all()
        return invocations
====================================================================
-> Chunk: stores\sql.py::8


class SQLStore(ell.store.Store):


    def get_traces(self, session: Session):
        query = text("""
        SELECT 
            consumer.lmp_id, 
            trace.*, 
            consumed.lmp_id
        FROM 
            invocation AS consumer
        JOIN 
            invocationtrace AS trace ON consumer.id = trace.invocation_consumer_id
        JOIN 
            invocation AS consumed ON trace.invocation_consuming_id = consumed.id
        """)
        results = session.exec(query).all()

        traces = []
        for (consumer_lmp_id, consumer_invocation_id, consumed_invocation_id, consumed_lmp_id) in results:
            traces.append({
                'consumer': consumer_lmp_id,
                'consumed': consumed_lmp_id
            })

        return traces
====================================================================
-> Chunk: types\studio.py::9


class Invocation(InvocationBase, table=True):
    lmp: SerializedLMP = Relationship(back_populates="invocations")
    consumed_by: List["Invocation"] = Relationship(
        back_populates="consumes",
        link_model=InvocationTrace,
        sa_relationship_kwargs=dict(
            primaryjoin="Invocation.id==InvocationTrace.invocation_consumer_id",
            secondaryjoin="Invocation.id==InvocationTrace.invocation_consuming_id",
        ),
    )
    consumes: List["Invocation"] = Relationship(
        back_populates="consumed_by",
        link_model=InvocationTrace,
        sa_relationship_kwargs=dict(
            primaryjoin="Invocation.id==InvocationTrace.invocation_consuming_id",
            secondaryjoin="Invocation.id==InvocationTrace.invocation_consumer_id",
        ),
    )
    used_by: Optional["Invocation"] = Relationship(back_populates="uses", sa_relationship_kwargs={"remote_side": "Invocation.id"})
    uses: List["Invocation"] = Relationship(back_populates="used_by")
    contents: InvocationContents = Relationship(back_populates="invocation")

    __table_args__ = (
        Index('ix_invocation_lmp_id_created_at', 'lmp_id', 'created_at'),
        Index('ix_invocation_created_at_latency_ms', 'created_at', 'latency_ms'),
        Index('ix_invocation_created_at_tokens', 'created_at', 'prompt_tokens', 'completion_tokens'),
    )
====================================================================


###### Cluster Eval ######
Score: 10.0
Cluster name: Tool Definition and Usage Cluster
Tool Definition and Usage Cluster:

-> Chunk: lmp\tool.py::2


def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        _under_fn = fn

        @wraps(fn)
        def wrapper(
            *fn_args,
            _invocation_origin: str = None,
            _tool_call_id: str = None,
            **fn_kwargs
        ):
            #XXX: Post release, we need to wrap all tool arguments in type primitives for tracking I guess or change that tool makes the tool function inoperable.
            #XXX: Most people are not going to manually try and call the tool without a type primitive and if they do it will most likely be wrapped with l strs.

            if config.verbose and not exempt_from_tracking:
                pass
                # tool_usage_logger_pre(fn, fn_args, fn_kwargs, name, color)

            result = fn(*fn_args, **fn_kwargs)

            _invocation_api_params = dict(tool_kwargs=tool_kwargs)

            # Here you might want to add logic for tracking the tool usage
            # Similar to how it's done in the lm decorator # Use _invocation_origin

            if isinstance(result, str) and _invocation_origin:
                result = _lstr(result,origin_trace=_invocation_origin)

            #XXX: This _tool_call_id thing is a hack. Tracking should happen via params in the api
            # So if you call wiuth a _tool_callId
            if _tool_call_id:
                # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
                try:
                    if isinstance(result, ContentBlock):
                        content_results = [result]
                    elif isinstance(result, list) and all(isinstance(c, ContentBlock) for c in result):
                        content_results = result
                    else:
                        content_results = [ContentBlock(text=_lstr(json.dumps(result, ensure_ascii=False),origin_trace=_invocation_origin))]
                except TypeError as e:
                    raise TypeError(f"Failed to convert tool use result to ContentBlock: {e}. Tools must return json serializable objects. or a list of ContentBlocks.")
                # XXX: Need to support images and other content types somehow. We should look for images inside of the the result and then go from there.
                # try:
                #     content_results = coerce_content_list(result)
                # except ValueError as e:

                # TODO: poolymorphic validation here is important (cant have tool_call or formatted_response in the result)
                # XXX: Should we put this coercion here or in the tool call/result area.
                for c in content_results:
                    assert not c.tool_call, "Tool call in tool result"
                    # assert not c.formatted_response, "Formatted response in tool result"
                    if c.parsed:
                        # Warning: Formatted response in tool result will be converted to text
                        # TODO: Logging needs to produce not print.
                        print(f"Warning: Formatted response in tool result will be converted to text. Original: {c.parsed}")
                        c.text = _lstr(c.parsed.model_dump_json(),origin_trace=_invocation_origin)
                        c.parsed = None
                    assert not c.audio, "Audio in tool result"
                return ToolResult(tool_call_id=_tool_call_id, result=content_results), _invocation_api_params, {}
            else:
                return result, _invocation_api_params, {}
        # ... other code
    # ... other code
====================================================================
-> Chunk: lmp\tool.py::3


def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        # ... other code


        wrapper.__ell_tool_kwargs__ = tool_kwargs
        wrapper.__ell_func__ = _under_fn
        wrapper.__ell_type__ = LMPType.TOOL
        wrapper.__ell_exempt_from_tracking = exempt_from_tracking

        # Construct the pydantic mdoel for the _under_fn's function signature parameters.
        # 1. Get the function signature.

        sig = inspect.signature(fn)

        # 2. Create a dictionary of field definitions for the Pydantic model
        fields = {}
        for param_name, param in sig.parameters.items():
            # Skip *args and **kwargs
            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                continue

            # Determine the type annotation
            if param.annotation == inspect.Parameter.empty:
                raise ValueError(f"Parameter {param_name} has no type annotation, and cannot be converted into a tool schema for OpenAI and other provisders. Should OpenAI produce a string or an integer, etc, for this parameter?")
            annotation = param.annotation

            # Determine the default value
            default = param.default

            # Check if the parameter has a Field with description
            if isinstance(param.default, FieldInfo):
                field = param.default
                fields[param_name] = (annotation, field)
            elif param.default != inspect.Parameter.empty:
                fields[param_name] = (annotation, param.default)
            else:
                # If no default value, use Field without default
                fields[param_name] = (annotation, Field(...))

        # 3. Create the Pydantic model
        model_name = f"{fn.__name__}"
        ParamsModel = create_model(model_name, **fields)

        # Attach the Pydantic model to the wrapper function
        wrapper.__ell_params_model__ = ParamsModel

        # handle tracking last.
        if exempt_from_tracking:
            ret = wrapper
        else:
            ret=  _track(wrapper)

        # Helper function to get the Pydantic model for the tool
        def get_params_model():
            return wrapper.__ell_params_model__

        # Attach the helper function to the wrapper
        wrapper.get_params_model = get_params_model
        ret.get_params_model = get_params_model
        return ret

    return tool_decorator
====================================================================
-> Chunk: lmp\tool.py::4


tool.__doc__ = """Defines a tool for use in language model programs (LMPs) that support tool use.

This decorator wraps a function, adding metadata and handling for tool invocations.
It automatically extracts the tool's description and parameters from the function's
docstring and type annotations, creating a structured representation for LMs to use.

:param exempt_from_tracking: If True, the tool usage won't be tracked. Default is False.
:type exempt_from_tracking: bool
:param tool_kwargs: Additional keyword arguments for tool configuration.
:return: A wrapped version of the original function, usable as a tool by LMs.
:rtype: Callable

Requirements:

- Function must have fully typed arguments (Pydantic-serializable).
- Return value must be one of: str, JSON-serializable object, Pydantic model, or List[ContentBlock].
- All parameters must have type annotations.
- Complex types should be Pydantic models.
- Function should have a descriptive docstring.
- Can only be used in LMPs with @ell.complex decorators

Functionality:

1. Metadata Extraction:
    - Uses function docstring as tool description.
    - Extracts parameter info from type annotations and docstring.
    - Creates a Pydantic model for parameter validation and schema generation.

2. Integration with LMs:
    - Can be passed to @ell.complex decorators.
    - Provides structured tool information to LMs.

3. Invocation Handling:
    - Manages tracking, logging, and result processing.
    - Wraps results in appropriate types (e.g., _lstr) for tracking.

Usage Modes:

1. Normal Function Call:
    - Behaves like a regular Python function.
    - Example: result = my_tool(arg1="value", arg2=123)

2. LMP Tool Call:
    - Used within LMPs or with explicit _tool_call_id.
    - Returns a ToolResult object.
    - Example: result = my_tool(arg1="value", arg2=123, _tool_call_id="unique_id")

Result Coercion:

- String → ContentBlock(text=result)
- Pydantic BaseModel → ContentBlock(parsed=result)
- List[ContentBlock] → Used as-is
- Other types → ContentBlock(text=json.dumps(result))

Example::

    @ell.tool()
    def create_claim_draft(
        claim_details: str,
        claim_type: str,
        claim_amount: float,
        claim_date: str = Field(description="Date format: YYYY-MM-DD")
    ) -> str:
        '''Create a claim draft. Returns the created claim ID.'''
        return "12345"

    # For use in a complex LMP:
    @ell.complex(model="gpt-4", tools=[create_claim_draft], temperature=0.1)
    def insurance_chatbot(message_history: List[Message]) -> List[Message]:
        # Chatbot implementation...

    x = insurance_chatbot([
        ell.user("I crashed my car into a tree."),
        ell.assistant("I'm sorry to hear that. Can you provide more details?"),
        ell.user("The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000")
    ]) 
    print(x)
    '''ell.Message(content=[
        ContentBlock(tool_call(
            tool_call_id="asdas4e",
            tool_fn=create_claim_draft,
            input=create_claim_draftParams({
                claim_details="The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000",
                claim_type="car",
                claim_amount=5000,
                claim_date="2024-08-01"
            })
        ))
    ], role='assistant')'''
    
    if x.tool_calls:
        next_user_message = response_message.call_tools_and_collect_as_message()
        # This actually calls create_claim_draft
        print(next_user_message)
        '''
        ell.Message(content=[
            ContentBlock(tool_result=ToolResult(
                tool_call_id="asdas4e",
                result=[ContentBlock(text="12345")]
            ))
        ], role='user')
        '''
        y = insurance_chatbot(message_history + [x, next_user_message])
        print(y)
        '''
        ell.Message("I've filed that for you!", role='assistant')
        '''

Note:
- Tools are integrated into LMP calls via the 'tools' parameter in @ell.complex.
- LMs receive structured tool information, enabling understanding and usage within the conversation context.
    """
====================================================================
-> Chunk: types\message.py::3


class ToolCall(BaseModel):
    tool : InvocableTool
    tool_call_id : Optional[_lstr_generic] = Field(default=None)
    params : BaseModel

    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):
        if not isinstance(params, BaseModel):
            params = tool.__ell_params_model__(**params) #convenience.
        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)

    def __call__(self, **kwargs):
        assert not kwargs, "Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall."

        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
        return self.tool(**self.params.model_dump())

    # XXX: Deprecate in 0.1.0
    def call_and_collect_as_message_block(self):
        raise DeprecationWarning("call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.")

    def call_and_collect_as_content_block(self):
        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)
        return ContentBlock(tool_result=res)

    def call_and_collect_as_message(self):
        return Message(role="user", content=[self.call_and_collect_as_message_block()])

    def __repr__(self):
        return f"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id='{self.tool_call_id}')"
====================================================================
-> Chunk: types\message.py::4


class ImageContent(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    image: Optional[PILImage.Image] = Field(default=None)
    url: Optional[str] = Field(default=None)
    detail: Optional[str] = Field(default=None)

    @model_validator(mode='after')
    def check_image_or_url(self):
        if self.image is not None and self.url is not None:
            raise ValueError("Both 'image' and 'url' cannot be set simultaneously.")
        if self.image is None and self.url is None:
            raise ValueError("Either 'image' or 'url' must be set.")
        return self
====================================================================


###### Cluster Eval ######
Score: 8.666666666666666
Cluster name: Message and Content Handling Cluster
Message and Content Handling Cluster:

-> Chunk: types\message.py::1


# todo: implement tracing for structured outs. this a v2 feature.
import json
from ell.types._lstr import _lstr
from functools import cached_property
import numpy as np
import base64
from io import BytesIO
from PIL import Image as PILImage

from pydantic import BaseModel, ConfigDict, model_validator, field_serializer
from sqlmodel import Field

from concurrent.futures import ThreadPoolExecutor, as_completed

from typing import Any, Callable, Dict, List, Optional, Union

from ell.util.serialization import serialize_image
_lstr_generic = Union[_lstr, str]
InvocableTool = Callable[..., Union["ToolResult", _lstr_generic, List["ContentBlock"], ]]

# AnyContent represents any type that can be passed to Message.
AnyContent = Union["ContentBlock", str, "ToolCall", "ToolResult", "ImageContent", np.ndarray, PILImage.Image, BaseModel]
====================================================================
-> Chunk: types\message.py::3


class ToolCall(BaseModel):
    tool : InvocableTool
    tool_call_id : Optional[_lstr_generic] = Field(default=None)
    params : BaseModel

    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):
        if not isinstance(params, BaseModel):
            params = tool.__ell_params_model__(**params) #convenience.
        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)

    def __call__(self, **kwargs):
        assert not kwargs, "Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall."

        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
        return self.tool(**self.params.model_dump())

    # XXX: Deprecate in 0.1.0
    def call_and_collect_as_message_block(self):
        raise DeprecationWarning("call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.")

    def call_and_collect_as_content_block(self):
        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)
        return ContentBlock(tool_result=res)

    def call_and_collect_as_message(self):
        return Message(role="user", content=[self.call_and_collect_as_message_block()])

    def __repr__(self):
        return f"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id='{self.tool_call_id}')"
====================================================================
-> Chunk: types\message.py::4


class ImageContent(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    image: Optional[PILImage.Image] = Field(default=None)
    url: Optional[str] = Field(default=None)
    detail: Optional[str] = Field(default=None)

    @model_validator(mode='after')
    def check_image_or_url(self):
        if self.image is not None and self.url is not None:
            raise ValueError("Both 'image' and 'url' cannot be set simultaneously.")
        if self.image is None and self.url is None:
            raise ValueError("Either 'image' or 'url' must be set.")
        return self
====================================================================
-> Chunk: types\message.py::5


class ImageContent(BaseModel):

    @classmethod
    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, "ImageContent"]):
        if isinstance(value, cls):
            return value

        if isinstance(value, str):
            if value.startswith('http://') or value.startswith('https://'):
                return cls(url=value)
            try:
                img_data = base64.b64decode(value)
                img = PILImage.open(BytesIO(img_data))
                if img.mode not in ('L', 'RGB', 'RGBA'):
                    return cls(image=img.convert('RGB'))
            except:
                raise ValueError("Invalid base64 string or URL for image")

        if isinstance(value, np.ndarray):
            if value.ndim == 3 and value.shape[2] in (3, 4):
                mode = 'RGB' if value.shape[2] == 3 else 'RGBA'
                return cls(image=PILImage.fromarray(value, mode=mode))
            else:
                raise ValueError(f"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.")

        if isinstance(value, PILImage.Image):
            if value.mode not in ('L', 'RGB', 'RGBA'):
                value = value.convert('RGB')
            return cls(image=value)

        raise ValueError(f"Invalid image type: {type(value)}")

    @field_serializer('image')
    def serialize_image(self, image: Optional[PILImage.Image], _info):
        if image is None:
            return None
        return serialize_image(image)
====================================================================


Total coherence score: 13.111111111111109
