_____ summary_v2 _____

###### Cluster Eval ######
Score: 4.666666666666667
Cluster name: Reinforcement Learning Environment Setup Cluster
Reinforcement Learning Environment Setup Cluster:

-> Chunk: 0.1.0\cem.py::1


import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from gym.vector import AsyncVectorEnv
import random

# Set random seeds for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# Hyperparameters
NUM_ENVIRONMENTS = 4           # Reduced for simplicity
NUM_ITERATIONS = 50            # Number of training iterations
TRAJECTORIES_PER_ITER = 100    # Total number of trajectories per iteration
ELITE_PERCENT = 10             # Top k% trajectories to select
LEARNING_RATE = 1e-3
BATCH_SIZE = 64
MAX_STEPS = 500                # Max steps per trajectory
ENV_NAME = 'CartPole-v1'
====================================================================
-> Chunk: 0.1.0\cpbo.py::1


import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import namedtuple
from torch.utils.data import DataLoader, TensorDataset

# Define a simple policy network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)  # Output action probabilities
        )

    def forward(self, x):
        return self.network(x)
====================================================================
-> Chunk: 0.1.0\cem.py::3


def collect_trajectories(envs, policy, num_trajectories, max_steps):
    trajectories = []
    num_envs = envs.num_envs

    # Handle the return type of reset()
    reset_output = envs.reset()
    if isinstance(reset_output, tuple) or isinstance(reset_output, list):
        obs = reset_output[0]  # Extract observations
    else:
        obs = reset_output

    done_envs = [False] * num_envs
    steps = 0

    # Initialize storage for states, actions, and rewards per environment
    env_states = [[] for _ in range(num_envs)]
    env_actions = [[] for _ in range(num_envs)]
    env_rewards = [0.0 for _ in range(num_envs)]
    total_collected = 0
    # ... other code
====================================================================
-> Chunk: 0.1.0\cem.py::5


def select_elite(trajectories, percentile=ELITE_PERCENT):
    rewards = [traj['reward'] for traj in trajectories]
    if not rewards:
        return []
    reward_threshold = np.percentile(rewards, 100 - percentile)
    elite_trajectories = [traj for traj in trajectories if traj['reward'] >= reward_threshold]
    return elite_trajectories

# Function to create training dataset from elite trajectories
def create_training_data(elite_trajectories):
    states = []
    actions = []
    for traj in elite_trajectories:
        states.extend(traj['states'])
        actions.extend(traj['actions'])
    if not states or not actions:
        return None, None
    # Convert lists to NumPy arrays first for efficiency
    states = np.array(states, dtype=np.float32)
    actions = np.array(actions, dtype=np.int64)
    # Convert to PyTorch tensors
    states = torch.from_numpy(states)
    actions = torch.from_numpy(actions)
    return states, actions
====================================================================
-> Chunk: 0.1.0\cem.py::6


# Main execution code
if __name__ == '__main__':
    # Initialize environments
    env_fns = [make_env(ENV_NAME, SEED + i) for i in range(NUM_ENVIRONMENTS)]
    envs = AsyncVectorEnv(env_fns)

    # Get environment details
    dummy_env = gym.make(ENV_NAME)
    state_dim = dummy_env.observation_space.shape[0]
    action_dim = dummy_env.action_space.n
    dummy_env.close()

    # Initialize policy network and optimizer
    policy = PolicyNetwork(state_dim, action_dim)
    optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    # Training Loop
    for iteration in range(1, NUM_ITERATIONS + 1):
        try:
            # Step 1: Collect Trajectories
            trajectories = collect_trajectories(envs, policy, TRAJECTORIES_PER_ITER, MAX_STEPS)
        except Exception as e:
            print(f"Error during trajectory collection at iteration {iteration}: {e}")
            break

        # Step 2: Select Elite Trajectories
        elite_trajectories = select_elite(trajectories, ELITE_PERCENT)

        if len(elite_trajectories) == 0:
            print(f"Iteration {iteration}: No elite trajectories found. Skipping update.")
            continue

        # Step 3: Create Training Data
        states, actions = create_training_data(elite_trajectories)

        if states is None or actions is None:
            print(f"Iteration {iteration}: No training data available. Skipping update.")
            continue

        # Step 4: Behavioral Cloning (Policy Update)
        dataset_size = states.size(0)
        indices = np.arange(dataset_size)
        np.random.shuffle(indices)

        for start in range(0, dataset_size, BATCH_SIZE):
            end = start + BATCH_SIZE
            batch_indices = indices[start:end]
            batch_states = states[batch_indices]
            batch_actions = actions[batch_indices]

            optimizer.zero_grad()
            logits = policy(batch_states)
            loss = criterion(logits, batch_actions)
            loss.backward()
            optimizer.step()

        # Step 5: Evaluate Current Policy
        avg_reward = np.mean([traj['reward'] for traj in elite_trajectories])
        print(f"Iteration {iteration}: Elite Trajectories: {len(elite_trajectories)}, Average Reward: {avg_reward:.2f}")

    # Close environments
    envs.close()

    # Testing the Trained Policy
    def test_policy(policy, env_name=ENV_NAME, episodes=5, max_steps=500):
        env = gym.make(env_name)
        total_rewards = []
        for episode in range(episodes):
            obs, _ = env.reset()
            done = False
            episode_reward = 0
            for _ in range(max_steps):
                obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)
                with torch.no_grad():
                    action = policy.get_action(obs_tensor).item()
                obs, reward, done, info, _ = env.step(action)
                episode_reward += reward
                if done:
                    break
            total_rewards.append(episode_reward)
            print(f"Test Episode {episode + 1}: Reward: {episode_reward}")
        env.close()
        print(f"Average Test Reward over {episodes} episodes: {np.mean(total_rewards):.2f}")

    # Run the test
    test_policy(policy)
====================================================================


###### Cluster Eval ######
Score: 4.666666666666667
Cluster name: Model Configuration and Persistence Cluster
Model Configuration and Persistence Cluster:

-> Chunk: ell\configurator.py::2


class Config(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    registry: Dict[str, _Model] = Field(default_factory=dict, description="A dictionary mapping model names to their configurations.")
    verbose: bool = Field(default=False, description="If True, enables verbose logging.")
    wrapped_logging: bool = Field(default=True, description="If True, enables wrapped logging for better readability.")
    override_wrapped_logging_width: Optional[int] = Field(default=None, description="If set, overrides the default width for wrapped logging.")
    store: Optional[Store] = Field(default=None, description="An optional Store instance for persistence.")
    autocommit: bool = Field(default=False, description="If True, enables automatic committing of changes to the store.")
    lazy_versioning: bool = Field(default=True, description="If True, enables lazy versioning for improved performance.")
    default_api_params: Dict[str, Any] = Field(default_factory=dict, description="Default parameters for language models.")
    default_client: Optional[openai.Client] = Field(default=None, description="The default OpenAI client used when a specific model client is not found.")
    autocommit_model: str = Field(default="gpt-4o-mini", description="When set, changes the default autocommit model from GPT 4o mini.")
    providers: Dict[Type, Provider] = Field(default_factory=dict, description="A dictionary mapping client types to provider classes.")
    def __init__(self, **data):
        super().__init__(**data)
        self._lock = threading.Lock()
        self._local = threading.local()
====================================================================
-> Chunk: ell\configurator.py::1


from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)
====================================================================
-> Chunk: ell\configurator.py::3


class Config(BaseModel):


    def register_model(
        self, 
        name: str,
        default_client: Optional[Union[openai.Client, Any]] = None,
        supports_streaming: Optional[bool] = None
    ) -> None:
        """
        Register a model with its configuration.
        """
        with self._lock:
            # XXX: Will be deprecated in 0.1.0
            self.registry[name] = _Model(
                name=name,
                default_client=default_client,
                supports_streaming=supports_streaming
            )
====================================================================
-> Chunk: ell\configurator.py::6


class Config(BaseModel):

    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:
        """
        Register a provider class for a specific client type.

        :param provider_class: The provider class to register.
        :type provider_class: Type[Provider]
        """
        assert isinstance(client_type, type), "client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))"
        with self._lock:
            self.providers[client_type] = provider
====================================================================
-> Chunk: ell\configurator.py::5


class Config(BaseModel):

    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:
        """
        Get the OpenAI client for a specific model name.

        :param model_name: The name of the model to get the client for.
        :type model_name: str
        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.
        :rtype: Tuple[Optional[openai.Client], bool]
        """
        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry
        model_config = current_registry.get(model_name)
        fallback = False
        if not model_config:
            warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."
            if self.verbose:
                from colorama import Fore, Style
                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")
            else:
                _config_logger.debug(warning_message)
            client = self.default_client
            fallback = True
        else:
            client = model_config.default_client
        return client, fallback
====================================================================


###### Cluster Eval ######
Score: 4.333333333333333
Cluster name: Real-Time Client Management Cluster
Real-Time Client Management Cluster:

-> Chunk: openai_realtime\client.py::2


class RealtimeClient(RealtimeEventHandler):

    def _add_api_event_handlers(self):
        self.realtime.on('client.*', lambda event: self.dispatch('realtime.event', {
            'time': RealtimeUtils.generate_id('time_'),
            'source': 'client',
            'event': event
        }))
        self.realtime.on('server.*', lambda event: self.dispatch('realtime.event', {
            'time': RealtimeUtils.generate_id('time_'),
            'source': 'server',
            'event': event
        }))
        self.realtime.on('server.session.created', lambda _: setattr(self, 'session_created', True))

        def handle_conversation_event(event, *args):
            result = self.conversation.process_event(event, *args)
            if result['item']:
                self.dispatch('conversation.updated', result)
            return result

        self.realtime.on('server.response.created', handle_conversation_event)
        self.realtime.on('server.response.output_item.added', handle_conversation_event)
        self.realtime.on('server.response.content_part.added', handle_conversation_event)
        self.realtime.on('server.input_audio_buffer.speech_started', lambda event: (
            handle_conversation_event(event),
            self.dispatch('conversation.interrupted', event)
        ))
        self.realtime.on('server.input_audio_buffer.speech_stopped', lambda event: 
            handle_conversation_event(event, self.input_audio_buffer)
        )
        self.realtime.on('server.conversation.item.created', lambda event: (
            handle_conversation_event(event),
            self.dispatch('conversation.item.appended', {'item': event['item']})
        ))
        self.realtime.on('server.conversation.item.truncated', handle_conversation_event)
        self.realtime.on('server.conversation.item.deleted', handle_conversation_event)
        self.realtime.on('server.conversation.item.input_audio_transcription.completed', handle_conversation_event)
        self.realtime.on('server.response.audio_transcript.delta', handle_conversation_event)
        self.realtime.on('server.response.audio.delta', handle_conversation_event)
        self.realtime.on('server.response.text.delta', handle_conversation_event)
        self.realtime.on('server.response.function_call_arguments.delta', handle_conversation_event)
        def handle_output_item_done( event):
            handle_conversation_event(event)
            item = event.get('item', {})

            if item.get('status') == 'completed':
                self.dispatch('conversation.item.completed', {'item': item})

            formatted = item.get('formatted', {})
            tool = formatted.get('tool') if isinstance(formatted, dict) else None

            if tool:
                asyncio.create_task(self._call_tool(tool))
        self.realtime.on('server.response.output_item.done', handle_output_item_done)
====================================================================
-> Chunk: openai_realtime\api.py::2


class RealtimeAPI(RealtimeEventHandler):

    async def connect(self, model='gpt-4o-realtime-preview-2024-10-01'):
        if self.is_connected():
            raise Exception("Already connected")

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'OpenAI-Beta': 'realtime=v1'
        }

        self.ws = await websockets.connect(f"{self.url}?model={model}", extra_headers=headers)

        self.log(f"Connected to {self.url}")

        asyncio.create_task(self._message_handler())

        return True
====================================================================
-> Chunk: openai_realtime\conversation.py::1


import numpy as np
import json
from .utils import RealtimeUtils
import copy

class RealtimeConversation:
    def __init__(self):
        self.default_frequency = 24000  # 24,000 Hz
        self.clear()

    def clear(self):
        self.item_lookup = {}
        self.items = []
        self.response_lookup = {}
        self.responses = []
        self.queued_speech_items = {}
        self.queued_transcript_items = {}
        self.queued_input_audio = None
        return True

    def queue_input_audio(self, input_audio):
        self.queued_input_audio = input_audio
        return input_audio

    def process_event(self, event, *args):
        if 'event_id' not in event:
            raise ValueError("Missing 'event_id' on event")
        if 'type' not in event:
            raise ValueError("Missing 'type' on event")

        event_processor = getattr(self, f"_process_{event['type'].replace('.', '_')}", None)
        if not event_processor:
            raise ValueError(f"Missing conversation event processor for '{event['type']}'")

        return event_processor(event, *args)

    def get_item(self, id):
        return self.item_lookup.get(id)

    def get_items(self):
        return self.items.copy()
====================================================================
-> Chunk: openai_realtime\client.py::6


class RealtimeClient(RealtimeEventHandler):

    def send_user_message_content(self, content=None):
        content = content or []
        for c in content:
            if c['type'] == 'input_audio':
                if isinstance(c['audio'], (np.ndarray, bytes)):
                    c['audio'] = RealtimeUtils.array_buffer_to_base64(c['audio'])
        if content:
            self.realtime.send('conversation.item.create', {
                'item': {
                    'type': 'message',
                    'role': 'user',
                    'content': content
                }
            })
        self.create_response()
        return True
====================================================================
-> Chunk: openai_realtime\conversation.py::3


class RealtimeConversation:

    def _process_conversation_item_truncated(self, event):
        item_id, audio_end_ms = event['item_id'], event['audio_end_ms']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"item.truncated: Item '{item_id}' not found")

        end_index = int((audio_end_ms * self.default_frequency) / 1000)
        item['formatted']['transcript'] = ''
        item['formatted']['audio'] = item['formatted']['audio'][:end_index]
        return {'item': item, 'delta': None}

    def _process_conversation_item_deleted(self, event):
        item_id = event['item_id']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"item.deleted: Item '{item_id}' not found")

        del self.item_lookup[item['id']]
        self.items = [i for i in self.items if i['id'] != item['id']]
        return {'item': item, 'delta': None}
====================================================================


###### Cluster Eval ######
Score: 4.333333333333333
Cluster name: Content Representation Framework Cluster
Content Representation Framework Cluster:

-> Chunk: types\message.py::6


class ContentBlock(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    text: Optional[_lstr_generic] = Field(default=None)
    image: Optional[ImageContent] = Field(default=None)
    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)
    tool_call: Optional[ToolCall] = Field(default=None)
    parsed: Optional[BaseModel] = Field(default=None)
    tool_result: Optional[ToolResult] = Field(default=None)
    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.
    # This breaks us maintaing parity with the openai python client in some sen but so does image.

    def __init__(self, *args, **kwargs):
        if "image" in kwargs and not isinstance(kwargs["image"], ImageContent):
            im = kwargs["image"] = ImageContent.coerce(kwargs["image"])
            # XXX: Backwards compatibility, Deprecate.
            if (d := kwargs.get("image_detail", None)): im.detail = d

        super().__init__(*args, **kwargs)


    @model_validator(mode='after')
    def check_single_non_null(self):
        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]
        if len(non_null_fields) > 1:
            raise ValueError(f"Only one field can be non-null. Found: {', '.join(non_null_fields)}")
        return self

    def __str__(self):
        return repr(self)

    def __repr__(self):
        non_null_fields = [f"{field}={value}" for field, value in self.__dict__.items() if value is not None]
        return f"ContentBlock({', '.join(non_null_fields)})"

    @property
    def type(self):
        if self.text is not None:
            return "text"
        if self.image is not None:
            return "image"
        if self.audio is not None:
            return "audio"
        if self.tool_call is not None:
            return "tool_call"
        if self.parsed is not None:
            return "parsed"
        if self.tool_result is not None:
            return "tool_result"
        return None

    @property
    def content(self):
        return getattr(self, self.type)
====================================================================
-> Chunk: types\message.py::5


class ImageContent(BaseModel):

    @classmethod
    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, "ImageContent"]):
        if isinstance(value, cls):
            return value

        if isinstance(value, str):
            if value.startswith('http://') or value.startswith('https://'):
                return cls(url=value)
            try:
                img_data = base64.b64decode(value)
                img = PILImage.open(BytesIO(img_data))
                if img.mode not in ('L', 'RGB', 'RGBA'):
                    return cls(image=img.convert('RGB'))
            except:
                raise ValueError("Invalid base64 string or URL for image")

        if isinstance(value, np.ndarray):
            if value.ndim == 3 and value.shape[2] in (3, 4):
                mode = 'RGB' if value.shape[2] == 3 else 'RGBA'
                return cls(image=PILImage.fromarray(value, mode=mode))
            else:
                raise ValueError(f"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.")

        if isinstance(value, PILImage.Image):
            if value.mode not in ('L', 'RGB', 'RGBA'):
                value = value.convert('RGB')
            return cls(image=value)

        raise ValueError(f"Invalid image type: {type(value)}")

    @field_serializer('image')
    def serialize_image(self, image: Optional[PILImage.Image], _info):
        if image is None:
            return None
        return serialize_image(image)
====================================================================
-> Chunk: types\message.py::4


class ImageContent(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    image: Optional[PILImage.Image] = Field(default=None)
    url: Optional[str] = Field(default=None)
    detail: Optional[str] = Field(default=None)

    @model_validator(mode='after')
    def check_image_or_url(self):
        if self.image is not None and self.url is not None:
            raise ValueError("Both 'image' and 'url' cannot be set simultaneously.")
        if self.image is None and self.url is None:
            raise ValueError("Either 'image' or 'url' must be set.")
        return self
====================================================================
-> Chunk: types\message.py::7


class ContentBlock(BaseModel):

    @classmethod
    def coerce(cls, content: AnyContent) -> "ContentBlock":
        """
        Coerce various types of content into a ContentBlock.

        This method provides a flexible way to create ContentBlock instances from different types of input.

        Args:
        content: The content to be coerced into a ContentBlock. Can be one of the following types:
        - str: Will be converted to a text ContentBlock.
        - ToolCall: Will be converted to a tool_call ContentBlock.
        - ToolResult: Will be converted to a tool_result ContentBlock.
        - BaseModel: Will be converted to a parsed ContentBlock.
        - ContentBlock: Will be returned as-is.
        - Image: Will be converted to an image ContentBlock.
        - np.ndarray: Will be converted to an image ContentBlock.
        - PILImage.Image: Will be converted to an image ContentBlock.

        Returns:
        ContentBlock: A new ContentBlock instance containing the coerced content.

        Raises:
        ValueError: If the content cannot be coerced into a valid ContentBlock.

        Examples:
        >>> ContentBlock.coerce("Hello, world!")
        ContentBlock(text="Hello, world!")

        >>> tool_call = ToolCall(...)
        >>> ContentBlock.coerce(tool_call)
        ContentBlock(tool_call=tool_call)

        >>> tool_result = ToolResult(...)
        >>> ContentBlock.coerce(tool_result)
        ContentBlock(tool_result=tool_result)

        >>> class MyModel(BaseModel):
        ...     field: str
        >>> model_instance = MyModel(field="value")
        >>> ContentBlock.coerce(model_instance)
        ContentBlock(parsed=model_instance)

        >>> from PIL import Image as PILImage
        >>> img = PILImage.new('RGB', (100, 100))
        >>> ContentBlock.coerce(img)
        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))

        >>> import numpy as np
        >>> arr = np.random.rand(100, 100, 3)
        >>> ContentBlock.coerce(arr)
        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))

        >>> image = Image(url="https://example.com/image.jpg")
        >>> ContentBlock.coerce(image)
        ContentBlock(image=ImageContent(url="https://example.com/image.jpg"))

        Notes:
        - This method is particularly useful when working with heterogeneous content types
          and you want to ensure they are all properly encapsulated in ContentBlock instances.
        - The method performs type checking and appropriate conversions to ensure the resulting
          ContentBlock is valid according to the model's constraints.
        - For image content, Image objects, PIL Image objects, and numpy arrays are supported,
          with automatic conversion to the appropriate format.
        - As a last resort, the method will attempt to create an image from the input before
          raising a ValueError.
        """
        if isinstance(content, ContentBlock):
            return content
        if isinstance(content, str):
            return cls(text=content)
        if isinstance(content, ToolCall):
            return cls(tool_call=content)
        if isinstance(content, ToolResult):
            return cls(tool_result=content)
        if isinstance(content, (ImageContent, np.ndarray, PILImage.Image)):
            return cls(image=ImageContent.coerce(content))
        if isinstance(content, BaseModel):
            return cls(parsed=content)

        raise ValueError(f"Invalid content type: {type(content)}")

    @field_serializer('parsed')
    def serialize_parsed(self, value: Optional[BaseModel], _info):
        if value is None:
            return None
        return value.model_dump(exclude_none=True, exclude_unset=True)
====================================================================
-> Chunk: util\serialization.py::1


# Global converter
import base64
import hashlib
from io import BytesIO
import json
import cattrs
import numpy as np
from pydantic import BaseModel
import PIL
from ell.types._lstr import _lstr


pydantic_ltype_aware_cattr = cattrs.Converter()

def serialize_image(img):
    buffer = BytesIO()
    img.save(buffer, format="PNG")
    return "data:image/png;base64," + base64.b64encode(buffer.getvalue()).decode()


# Register hooks for complex types
pydantic_ltype_aware_cattr.register_unstructure_hook(
    np.ndarray,
    lambda arr: {
        "content": serialize_image(PIL.Image.fromarray(arr)),
        "__limage": True
    } if arr.ndim == 3 else (
        {
            "content": base64.b64encode(arr.tobytes()).decode(),
            "dtype": str(arr.dtype),
            "shape": arr.shape,
            "__lndarray": True
        }
    )
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    set,
    lambda s: list(sorted(s))
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    frozenset,
    lambda s: list(sorted(s))
)


pydantic_ltype_aware_cattr.register_unstructure_hook(
    PIL.Image.Image,
    lambda obj: {
        "content": serialize_image(obj),
        "__limage": True
    }
)

def unstructure_lstr(obj):
    return dict(content=str(obj), **obj.__dict__, __lstr=True)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    _lstr,
    unstructure_lstr
)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    BaseModel,
    lambda obj: obj.model_dump(exclude_none=True, exclude_unset=True)
)
====================================================================


###### Cluster Eval ######
Score: 4.333333333333333
Cluster name: Language Model Parameter Management Cluster
Language Model Parameter Management Cluster:

-> Chunk: types\studio.py::3


class UTCTimestamp(types.TypeDecorator[datetime]):
    cache_ok = True
    impl = types.TIMESTAMP
    def process_result_value(self, value: datetime, dialect:Any):
        return value.replace(tzinfo=timezone.utc)


def UTCTimestampField(index:bool=False, **kwargs:Any):
    return Field(
        sa_column=Column(UTCTimestamp(timezone=True), index=index, **kwargs))


class LMPType(str, enum.Enum):
    LM = "LM"
    TOOL = "TOOL"
    MULTIMODAL = "MULTIMODAL"
    OTHER = "OTHER"
====================================================================
-> Chunk: types\studio.py::5


class SerializedLMP(SerializedLMPBase, table=True):
    invocations: List["Invocation"] = Relationship(back_populates="lmp")
    used_by: Optional[List["SerializedLMP"]] = Relationship(
        back_populates="uses",
        link_model=SerializedLMPUses,
        sa_relationship_kwargs=dict(
            primaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id",
            secondaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id",
        ),
    )
    uses: List["SerializedLMP"] = Relationship(
        back_populates="used_by",
        link_model=SerializedLMPUses,
        sa_relationship_kwargs=dict(
            primaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id",
            secondaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id",
        ),
    )

    class Config:
        table_name = "serializedlmp"
        unique_together = [("version_number", "name")]
====================================================================
-> Chunk: types\studio.py::6


class InvocationTrace(SQLModel, table=True):
    invocation_consumer_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)
    invocation_consuming_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)

# Should be subtyped for differnet kidns of LMPS.
# XXX: Move all ofh te binary data out to a different table.
# XXX: Need a flag that says dont store images.
# XXX: Deprecate the args columns
class InvocationBase(SQLModel):
    id: Optional[str] = Field(default=None, primary_key=True)
    lmp_id: str = Field(foreign_key="serializedlmp.lmp_id", index=True)
    latency_ms: float
    prompt_tokens: Optional[int] = Field(default=None)
    completion_tokens: Optional[int] = Field(default=None)
    state_cache_key: Optional[str] = Field(default=None)
    created_at: datetime = UTCTimestampField(default=func.now(), nullable=False)
    used_by_id: Optional[str] = Field(default=None, foreign_key="invocation.id", index=True)
    # global_vars and free_vars removed from here
====================================================================
-> Chunk: stores\sql.py::6


class SQLStore(ell.store.Store):


    def get_lmps(self, session: Session, skip: int = 0, limit: int = 10, subquery=None, **filters: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:

        query = select(SerializedLMP)

        if subquery is not None:
            query = query.join(subquery, and_(
                SerializedLMP.name == subquery.c.name,
                SerializedLMP.created_at == subquery.c.max_created_at
            ))

        if filters:
            for key, value in filters.items():
                query = query.where(getattr(SerializedLMP, key) == value)

        query = query.order_by(SerializedLMP.created_at.desc())  # Sort by created_at in descending order
        query = query.offset(skip).limit(limit)
        results = session.exec(query).all()

        return results
====================================================================
-> Chunk: stores\sql.py::11


class SQLBlobStore(ell.store.BlobStore):

    def _get_blob_path(self, id: str, depth: int = 2) -> str:
        assert "-" in id, "Blob id must have a single - in it to split on."
        _type, _id = id.split("-")
        increment = 2
        dirs = [_type] + [_id[i:i+increment] for i in range(0, depth*increment, increment)]
        file_name = _id[depth*increment:]
        return os.path.join(self.db_dir, *dirs, file_name)

class PostgresStore(SQLStore):
    def __init__(self, db_uri: str):
        super().__init__(db_uri)
====================================================================


###### Cluster Eval ######
Score: 4.333333333333333
Cluster name: Real-Time Client Management Cluster
Real-Time Client Management Cluster:

-> Chunk: openai_realtime\client.py::7


class RealtimeClient(RealtimeEventHandler):

    def append_input_audio(self, array_buffer):
        if len(array_buffer) > 0:
            self.realtime.send('input_audio_buffer.append', {
                'audio': RealtimeUtils.array_buffer_to_base64(array_buffer)
            })
            self.input_audio_buffer = RealtimeUtils.merge_int16_arrays(
                self.input_audio_buffer,
                array_buffer
            )
        return True

    def create_response(self):
        if self.get_turn_detection_type() is None and len(self.input_audio_buffer) > 0:
            self.realtime.send('input_audio_buffer.commit')
            self.conversation.queue_input_audio(self.input_audio_buffer)
            self.input_audio_buffer = np.array([], dtype=np.int16)
        self.realtime.send('response.create')
        return True
====================================================================
-> Chunk: openai_realtime\api.py::3


class RealtimeAPI(RealtimeEventHandler):

    async def _message_handler(self):
        try:
            async for message in self.ws:
                data = json.loads(message)
                self.receive(data['type'], data)
        except websockets.exceptions.ConnectionClosed:
            self.disconnect()
            self.dispatch('close', {'error': True})

    def disconnect(self):
        if self.ws:
            asyncio.create_task(self.ws.close())
            self.ws = None
        return True

    def receive(self, event_name, event):
        self.log("received:", event_name, event)
        self.dispatch(f"server.{event_name}", event)
        self.dispatch("server.*", event)
        return True
====================================================================
-> Chunk: openai_realtime\client.py::1


import asyncio
import numpy as np
from .event_handler import RealtimeEventHandler
from .api import RealtimeAPI
from .conversation import RealtimeConversation
from .utils import RealtimeUtils
import json

class RealtimeClient(RealtimeEventHandler):
    def __init__(self, url=None, api_key=None, instructions='', dangerously_allow_api_key_in_browser=False, debug=False):
        super().__init__()
        self.default_session_config = {
            'modalities': ['text', 'audio'],
            'instructions': instructions,
            'voice': 'alloy',
            'input_audio_format': 'pcm16',
            'output_audio_format': 'pcm16',
            'input_audio_transcription': None,
            'turn_detection': None,
            'tools': [],
            'tool_choice': 'auto',
            'temperature': 0.8,
            'max_response_output_tokens': 4096,
        }
        self.session_config = {}
        self.transcription_models = [{'model': 'whisper-1'}]
        self.default_server_vad_config = {
            'type': 'server_vad',
            'threshold': 0.5,
            'prefix_padding_ms': 300,
            'silence_duration_ms': 200,
        }
        self.realtime = RealtimeAPI(url, api_key, dangerously_allow_api_key_in_browser, debug)
        self.conversation = RealtimeConversation()
        self._reset_config()
        self._add_api_event_handlers()

    def _reset_config(self):
        self.session_created = False
        self.tools = {}
        self.session_config = self.default_session_config.copy()
        self.input_audio_buffer = np.array([], dtype=np.int16)
        return True
====================================================================
-> Chunk: openai_realtime\conversation.py::4


class RealtimeConversation:

    def _process_conversation_item_input_audio_transcription_completed(self, event):
        item_id, content_index, transcript = event['item_id'], event['content_index'], event['transcript']
        item = self.item_lookup.get(item_id)
        formatted_transcript = transcript or ' '

        if not item:
            self.queued_transcript_items[item_id] = {'transcript': formatted_transcript}
            return {'item': None, 'delta': None}

        item['content'][content_index]['transcript'] = transcript
        item['formatted']['transcript'] = formatted_transcript
        return {'item': item, 'delta': {'transcript': transcript}}

    def _process_input_audio_buffer_speech_started(self, event):
        item_id, audio_start_ms = event['item_id'], event['audio_start_ms']
        self.queued_speech_items[item_id] = {'audio_start_ms': audio_start_ms}
        return {'item': None, 'delta': None}
====================================================================
-> Chunk: openai_realtime\event_handler.py::1


import asyncio
from typing import Callable, Dict, List, Any

class RealtimeEventHandler:
    def __init__(self):
        self.event_handlers: Dict[str, List[Callable]] = {}
        self.next_event_handlers: Dict[str, List[Callable]] = {}

    def clear_event_handlers(self):
        self.event_handlers.clear()
        self.next_event_handlers.clear()
        return True

    def on(self, event_name: str, callback: Callable = None):
        def decorator(func):
            if event_name not in self.event_handlers:
                self.event_handlers[event_name] = []
            self.event_handlers[event_name].append(func)
            return func

        if callback is None:
            return decorator
        else:
            return decorator(callback)

    def on_next(self, event_name: str, callback: Callable):
        if event_name not in self.next_event_handlers:
            self.next_event_handlers[event_name] = []
        self.next_event_handlers[event_name].append(callback)

    def off(self, event_name: str, callback: Callable = None):
        if event_name in self.event_handlers:
            if callback:
                self.event_handlers[event_name].remove(callback)
            else:
                del self.event_handlers[event_name]
        return True

    def off_next(self, event_name: str, callback: Callable = None):
        if event_name in self.next_event_handlers:
            if callback:
                self.next_event_handlers[event_name].remove(callback)
            else:
                del self.next_event_handlers[event_name]
        return True

    async def wait_for_next(self, event_name: str, timeout: float = None):
        next_event = None
        def set_next_event(event):
            nonlocal next_event
            next_event = event

        self.on_next(event_name, set_next_event)

        start_time = asyncio.get_event_loop().time()
        while not next_event:
            if timeout and asyncio.get_event_loop().time() - start_time > timeout:
                return None
            await asyncio.sleep(0.001)

        return next_event

    def dispatch(self, event_name: str, event: Any):
        handlers = self.event_handlers.get(event_name, []).copy()
        for handler in handlers:
            handler(event)

        next_handlers = self.next_event_handlers.pop(event_name, [])
        for next_handler in next_handlers:
            next_handler(event)

        return True
====================================================================
-> Chunk: openai_realtime\utils.py::1


import base64
import numpy as np

class RealtimeUtils:
    @staticmethod
    def float_to_16bit_pcm(float32_array):
        int16_array = (np.clip(float32_array, -1, 1) * 32767).astype(np.int16)
        return int16_array.tobytes()

    @staticmethod
    def base64_to_array_buffer(base64_string):
        return base64.b64decode(base64_string)

    @staticmethod
    def array_buffer_to_base64(array_buffer):
        if isinstance(array_buffer, np.ndarray):
            if array_buffer.dtype == np.float32:
                array_buffer = RealtimeUtils.float_to_16bit_pcm(array_buffer)
            elif array_buffer.dtype == np.int16:
                array_buffer = array_buffer.tobytes()
        return base64.b64encode(array_buffer).decode('utf-8')

    @staticmethod
    def merge_int16_arrays(left, right):
        if isinstance(left, bytes):
            left = np.frombuffer(left, dtype=np.int16)
        if isinstance(right, bytes):
            right = np.frombuffer(right, dtype=np.int16)
        if not isinstance(left, np.ndarray) or not isinstance(right, np.ndarray):
            raise ValueError("Both items must be numpy arrays or bytes objects")
        return np.concatenate((left, right))

    @staticmethod
    def generate_id(prefix, length=21):
        import random
        chars = '123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz'
        return prefix + ''.join(random.choice(chars) for _ in range(length - len(prefix)))
====================================================================


###### Cluster Eval ######
Score: 4.0
Cluster name: Tool and Data Validation Cluster
Tool and Data Validation Cluster:

-> Chunk: lmp\tool.py::2


def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        _under_fn = fn

        @wraps(fn)
        def wrapper(
            *fn_args,
            _invocation_origin: str = None,
            _tool_call_id: str = None,
            **fn_kwargs
        ):
            #XXX: Post release, we need to wrap all tool arguments in type primitives for tracking I guess or change that tool makes the tool function inoperable.
            #XXX: Most people are not going to manually try and call the tool without a type primitive and if they do it will most likely be wrapped with l strs.

            if config.verbose and not exempt_from_tracking:
                pass
                # tool_usage_logger_pre(fn, fn_args, fn_kwargs, name, color)

            result = fn(*fn_args, **fn_kwargs)

            _invocation_api_params = dict(tool_kwargs=tool_kwargs)

            # Here you might want to add logic for tracking the tool usage
            # Similar to how it's done in the lm decorator # Use _invocation_origin

            if isinstance(result, str) and _invocation_origin:
                result = _lstr(result,origin_trace=_invocation_origin)

            #XXX: This _tool_call_id thing is a hack. Tracking should happen via params in the api
            # So if you call wiuth a _tool_callId
            if _tool_call_id:
                # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
                try:
                    if isinstance(result, ContentBlock):
                        content_results = [result]
                    elif isinstance(result, list) and all(isinstance(c, ContentBlock) for c in result):
                        content_results = result
                    else:
                        content_results = [ContentBlock(text=_lstr(json.dumps(result, ensure_ascii=False),origin_trace=_invocation_origin))]
                except TypeError as e:
                    raise TypeError(f"Failed to convert tool use result to ContentBlock: {e}. Tools must return json serializable objects. or a list of ContentBlocks.")
                # XXX: Need to support images and other content types somehow. We should look for images inside of the the result and then go from there.
                # try:
                #     content_results = coerce_content_list(result)
                # except ValueError as e:

                # TODO: poolymorphic validation here is important (cant have tool_call or formatted_response in the result)
                # XXX: Should we put this coercion here or in the tool call/result area.
                for c in content_results:
                    assert not c.tool_call, "Tool call in tool result"
                    # assert not c.formatted_response, "Formatted response in tool result"
                    if c.parsed:
                        # Warning: Formatted response in tool result will be converted to text
                        # TODO: Logging needs to produce not print.
                        print(f"Warning: Formatted response in tool result will be converted to text. Original: {c.parsed}")
                        c.text = _lstr(c.parsed.model_dump_json(),origin_trace=_invocation_origin)
                        c.parsed = None
                    assert not c.audio, "Audio in tool result"
                return ToolResult(tool_call_id=_tool_call_id, result=content_results), _invocation_api_params, {}
            else:
                return result, _invocation_api_params, {}
        # ... other code
    # ... other code
====================================================================
-> Chunk: lmp\tool.py::1


from functools import wraps
import json
from typing import Any, Callable, Optional

from pydantic import Field, create_model
from pydantic.fields import FieldInfo
from ell.lmp._track import _track
# from ell.types import ToolFunction, InvocableTool, ToolParams
# from ell.util.verbosity import compute_color, tool_usage_logger_pre
from ell.configurator import config
from ell.types._lstr import _lstr
from ell.types.studio import LMPType
import inspect

from ell.types.message import ContentBlock, InvocableTool, ToolResult, to_content_blocks
====================================================================
-> Chunk: lmp\tool.py::3


def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        # ... other code


        wrapper.__ell_tool_kwargs__ = tool_kwargs
        wrapper.__ell_func__ = _under_fn
        wrapper.__ell_type__ = LMPType.TOOL
        wrapper.__ell_exempt_from_tracking = exempt_from_tracking

        # Construct the pydantic mdoel for the _under_fn's function signature parameters.
        # 1. Get the function signature.

        sig = inspect.signature(fn)

        # 2. Create a dictionary of field definitions for the Pydantic model
        fields = {}
        for param_name, param in sig.parameters.items():
            # Skip *args and **kwargs
            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                continue

            # Determine the type annotation
            if param.annotation == inspect.Parameter.empty:
                raise ValueError(f"Parameter {param_name} has no type annotation, and cannot be converted into a tool schema for OpenAI and other provisders. Should OpenAI produce a string or an integer, etc, for this parameter?")
            annotation = param.annotation

            # Determine the default value
            default = param.default

            # Check if the parameter has a Field with description
            if isinstance(param.default, FieldInfo):
                field = param.default
                fields[param_name] = (annotation, field)
            elif param.default != inspect.Parameter.empty:
                fields[param_name] = (annotation, param.default)
            else:
                # If no default value, use Field without default
                fields[param_name] = (annotation, Field(...))

        # 3. Create the Pydantic model
        model_name = f"{fn.__name__}"
        ParamsModel = create_model(model_name, **fields)

        # Attach the Pydantic model to the wrapper function
        wrapper.__ell_params_model__ = ParamsModel

        # handle tracking last.
        if exempt_from_tracking:
            ret = wrapper
        else:
            ret=  _track(wrapper)

        # Helper function to get the Pydantic model for the tool
        def get_params_model():
            return wrapper.__ell_params_model__

        # Attach the helper function to the wrapper
        wrapper.get_params_model = get_params_model
        ret.get_params_model = get_params_model
        return ret

    return tool_decorator
====================================================================
-> Chunk: lmp\tool.py::4


tool.__doc__ = """Defines a tool for use in language model programs (LMPs) that support tool use.

This decorator wraps a function, adding metadata and handling for tool invocations.
It automatically extracts the tool's description and parameters from the function's
docstring and type annotations, creating a structured representation for LMs to use.

:param exempt_from_tracking: If True, the tool usage won't be tracked. Default is False.
:type exempt_from_tracking: bool
:param tool_kwargs: Additional keyword arguments for tool configuration.
:return: A wrapped version of the original function, usable as a tool by LMs.
:rtype: Callable

Requirements:

- Function must have fully typed arguments (Pydantic-serializable).
- Return value must be one of: str, JSON-serializable object, Pydantic model, or List[ContentBlock].
- All parameters must have type annotations.
- Complex types should be Pydantic models.
- Function should have a descriptive docstring.
- Can only be used in LMPs with @ell.complex decorators

Functionality:

1. Metadata Extraction:
    - Uses function docstring as tool description.
    - Extracts parameter info from type annotations and docstring.
    - Creates a Pydantic model for parameter validation and schema generation.

2. Integration with LMs:
    - Can be passed to @ell.complex decorators.
    - Provides structured tool information to LMs.

3. Invocation Handling:
    - Manages tracking, logging, and result processing.
    - Wraps results in appropriate types (e.g., _lstr) for tracking.

Usage Modes:

1. Normal Function Call:
    - Behaves like a regular Python function.
    - Example: result = my_tool(arg1="value", arg2=123)

2. LMP Tool Call:
    - Used within LMPs or with explicit _tool_call_id.
    - Returns a ToolResult object.
    - Example: result = my_tool(arg1="value", arg2=123, _tool_call_id="unique_id")

Result Coercion:

- String  ContentBlock(text=result)
- Pydantic BaseModel  ContentBlock(parsed=result)
- List[ContentBlock]  Used as-is
- Other types  ContentBlock(text=json.dumps(result))

Example::

    @ell.tool()
    def create_claim_draft(
        claim_details: str,
        claim_type: str,
        claim_amount: float,
        claim_date: str = Field(description="Date format: YYYY-MM-DD")
    ) -> str:
        '''Create a claim draft. Returns the created claim ID.'''
        return "12345"

    # For use in a complex LMP:
    @ell.complex(model="gpt-4", tools=[create_claim_draft], temperature=0.1)
    def insurance_chatbot(message_history: List[Message]) -> List[Message]:
        # Chatbot implementation...

    x = insurance_chatbot([
        ell.user("I crashed my car into a tree."),
        ell.assistant("I'm sorry to hear that. Can you provide more details?"),
        ell.user("The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000")
    ]) 
    print(x)
    '''ell.Message(content=[
        ContentBlock(tool_call(
            tool_call_id="asdas4e",
            tool_fn=create_claim_draft,
            input=create_claim_draftParams({
                claim_details="The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000",
                claim_type="car",
                claim_amount=5000,
                claim_date="2024-08-01"
            })
        ))
    ], role='assistant')'''
    
    if x.tool_calls:
        next_user_message = response_message.call_tools_and_collect_as_message()
        # This actually calls create_claim_draft
        print(next_user_message)
        '''
        ell.Message(content=[
            ContentBlock(tool_result=ToolResult(
                tool_call_id="asdas4e",
                result=[ContentBlock(text="12345")]
            ))
        ], role='user')
        '''
        y = insurance_chatbot(message_history + [x, next_user_message])
        print(y)
        '''
        ell.Message("I've filed that for you!", role='assistant')
        '''

Note:
- Tools are integrated into LMP calls via the 'tools' parameter in @ell.complex.
- LMs receive structured tool information, enabling understanding and usage within the conversation context.
    """
====================================================================


###### Cluster Eval ######
Score: 4.0
Cluster name: Lexical Closure and Code Analysis Cluster
Lexical Closure and Code Analysis Cluster:

-> Chunk: util\closure.py::7


def _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack , uses):
    """Process a single variable."""
    try:
        name = inspect.getmodule(var_value).__name__
        if should_import(name):
            imports.append(dill.source.getimport(var_value, alias=var_name))
            return
    except:
        pass

    if isinstance(var_value, (types.FunctionType, type, types.MethodType)):
        _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses)
    elif isinstance(var_value, types.ModuleType):
        _process_module(var_name, var_value, modules, imports, uses)
    elif isinstance(var_value, types.BuiltinFunctionType):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        _process_other_variable(var_name, var_value, dependencies, uses)
====================================================================
-> Chunk: util\closure.py::6


def _process_signature_dependency(val, dependencies, already_closed, recursion_stack, uses, name: Optional[str] = None):
    # Todo: Build general cattr like utility for unstructuring python objects with hooks that keep track of state variables.
    # Todo: break up closure into types and functions.
    # XXX: This is not exhaustive, we should determine should import on all dependencies

    if name not in FORBIDDEN_NAMES:
        try:
            dep = None
            _uses = None
            if isinstance(val, (types.FunctionType, types.MethodType)):
                dep, _, _uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            elif isinstance(val, (list, tuple, set)):
                for item in val:
                    _process_signature_dependency(item, dependencies, already_closed, recursion_stack, uses)
            else:
                val_class = val if isinstance(val, type) else val.__class__
                try:
                    is_builtin = (val_class.__module__ == "builtins" or val_class.__module__ == "__builtins__")
                except:
                    is_builtin = False

                if not is_builtin:
                    if should_import(val_class.__module__):
                        dependencies.append(dill.source.getimport(val_class, alias=val_class.__name__))
                    else:
                        dep, _, _uses = lexical_closure(val_class, already_closed=already_closed, recursion_stack=recursion_stack.copy())

            if dep: dependencies.append(dep)
            if _uses: uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of parameter or annotation {name}", e, recursion_stack)
====================================================================
-> Chunk: util\closure.py::8


def _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses):
    """Process a callable (function, method, or class)."""
    try:
        module_is_ell = 'ell' in inspect.getmodule(var_value).__name__
    except:
        module_is_ell = False

    if var_name not in FORBIDDEN_NAMES and not module_is_ell:
        try:
            dep, _, _uses = lexical_closure(var_value, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            dependencies.append(dep)
            uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of global or free variable {var_name}", e, recursion_stack)
====================================================================
-> Chunk: util\closure.py::13


def _update_ell_func(outer_ell_func, source, dsrc, globals_dict, frees_dict, fn_hash, uses):
    """Update the ell function attributes."""
    formatted_source = _format_source(source)
    formatted_dsrc = _format_source(dsrc)

    if hasattr(outer_ell_func, "__ell_func__"):

        outer_ell_func.__ell_closure__ = (formatted_source, formatted_dsrc, globals_dict, frees_dict)
        outer_ell_func.__ell_hash__ = fn_hash
        outer_ell_func.__ell_uses__ = uses

def _raise_error(message, exception, recursion_stack):
    """Raise an error with detailed information."""
    error_msg = f"{message}. Error: {str(exception)}\n"
    error_msg += f"Recursion stack: {' -> '.join(recursion_stack)}"
    # print(error_msg)
    raise Exception(error_msg)
====================================================================
-> Chunk: util\closure.py::5


def _process_default_kwargs(func, dependencies, already_closed, recursion_stack, uses):
    """Process default keyword arguments and annotations of a function."""
    ps = inspect.signature(func).parameters
    for name, param in ps.items():
        if param.default is not inspect.Parameter.empty:
            _process_signature_dependency(param.default, dependencies, already_closed, recursion_stack, uses, name)
        if param.annotation is not inspect.Parameter.empty:
            _process_signature_dependency(param.annotation, dependencies, already_closed, recursion_stack, uses, f"{name}_annotation")
    if func.__annotations__.get('return') is not None:
        _process_signature_dependency(func.__annotations__['return'], dependencies, already_closed, recursion_stack, uses, "return_annotation")
    # XXX: In order to properly analyze this we should walk the AST rather than inspexting the signature; e.g. Field is FieldInfo not Field.
    # I don't care about the actual default at time of execution just the symbols required to statically reproduce the prompt.
====================================================================


###### Cluster Eval ######
Score: 4.0
Cluster name: Model Configuration and Management Cluster
Model Configuration and Management Cluster:

-> Chunk: ell\configurator.py::1


from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)
====================================================================
-> Chunk: ell\configurator.py::2


class Config(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    registry: Dict[str, _Model] = Field(default_factory=dict, description="A dictionary mapping model names to their configurations.")
    verbose: bool = Field(default=False, description="If True, enables verbose logging.")
    wrapped_logging: bool = Field(default=True, description="If True, enables wrapped logging for better readability.")
    override_wrapped_logging_width: Optional[int] = Field(default=None, description="If set, overrides the default width for wrapped logging.")
    store: Optional[Store] = Field(default=None, description="An optional Store instance for persistence.")
    autocommit: bool = Field(default=False, description="If True, enables automatic committing of changes to the store.")
    lazy_versioning: bool = Field(default=True, description="If True, enables lazy versioning for improved performance.")
    default_api_params: Dict[str, Any] = Field(default_factory=dict, description="Default parameters for language models.")
    default_client: Optional[openai.Client] = Field(default=None, description="The default OpenAI client used when a specific model client is not found.")
    autocommit_model: str = Field(default="gpt-4o-mini", description="When set, changes the default autocommit model from GPT 4o mini.")
    providers: Dict[Type, Provider] = Field(default_factory=dict, description="A dictionary mapping client types to provider classes.")
    def __init__(self, **data):
        super().__init__(**data)
        self._lock = threading.Lock()
        self._local = threading.local()
====================================================================
-> Chunk: ell\configurator.py::4


class Config(BaseModel):



    @contextmanager
    def model_registry_override(self, overrides: Dict[str, _Model]):
        """
        Temporarily override the model registry with new model configurations.

        :param overrides: A dictionary of model names to ModelConfig instances to override.
        :type overrides: Dict[str, ModelConfig]
        """
        if not hasattr(self._local, 'stack'):
            self._local.stack = []

        with self._lock:
            current_registry = self._local.stack[-1] if self._local.stack else self.registry
            new_registry = current_registry.copy()
            new_registry.update(overrides)

        self._local.stack.append(new_registry)
        try:
            yield
        finally:
            self._local.stack.pop()
====================================================================
-> Chunk: ell\configurator.py::5


class Config(BaseModel):

    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:
        """
        Get the OpenAI client for a specific model name.

        :param model_name: The name of the model to get the client for.
        :type model_name: str
        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.
        :rtype: Tuple[Optional[openai.Client], bool]
        """
        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry
        model_config = current_registry.get(model_name)
        fallback = False
        if not model_config:
            warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."
            if self.verbose:
                from colorama import Fore, Style
                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")
            else:
                _config_logger.debug(warning_message)
            client = self.default_client
            fallback = True
        else:
            client = model_config.default_client
        return client, fallback
====================================================================
-> Chunk: ell\provider.py::2


# XXX: Might leave this internal to providers so that the complex code is simpler &
# we can literally jsut call provider.call like any openai fn.
class EllCallParams(BaseModel):
    model: str = Field(..., description="Model identifier")
    messages: List[Message] = Field(..., description="Conversation context")
    client: Any = Field(..., description="API client")
    tools: List[LMP] = Field(default_factory=list, description="Available tools")
    api_params: Dict[str, Any] = Field(
        default_factory=dict, description="API parameters"
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def get_tool_by_name(self, name: str) -> Optional[LMP]:
        """Get a tool by name."""
        return next(
            (tool for tool in (self.tools or [])  if tool.__name__ == name), None
        )


Metadata = Dict[str, Any]
====================================================================


###### Cluster Eval ######
Score: 4.0
Cluster name: Language Model Interaction Cluster
Language Model Interaction Cluster:

-> Chunk: lmp\complex.py::3


def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        # ... other code



        model_call.__ell_api_params__ = default_api_params_from_decorator #type: ignore
        model_call.__ell_func__ = prompt #type: ignore
        model_call.__ell_type__ = LMPType.LM #type: ignore
        model_call.__ell_exempt_from_tracking = exempt_from_tracking #type: ignore


        if exempt_from_tracking:
            return model_call
        else:
            # XXX: Analyze decorators with AST instead.
            return _track(model_call, forced_dependencies=dict(tools=tools, response_format=api_params.get("response_format", {})))
    return parameterized_lm_decorator
====================================================================
-> Chunk: models\openai.py::1


"""
This module handles the registration of OpenAI models within the ell framework.

It provides functionality to register various OpenAI models with a given OpenAI client,
making them available for use throughout the system. The module also sets up a default
client behavior for unregistered models.

Key features:
1. Registration of specific OpenAI models with their respective types (system, openai, openai-internal).
2. Utilization of a default OpenAI client for any unregistered models,

The default client behavior ensures that even if a specific model is not explicitly
registered, the system can still attempt to use it with the default OpenAI client.
This fallback mechanism provides flexibility in model usage while maintaining a
structured approach to model registration.

Note: The actual model availability may depend on your OpenAI account's access and the
current offerings from OpenAI.

Additionally, due to the registration of default mdoels, the OpenAI client may be used for
anthropic, cohere, groq, etc. models if their clients are not registered or fail
to register due to an error (lack of API keys, rate limits, etc.)
"""

from ell.configurator import config
import openai

import logging
import colorama

logger = logging.getLogger(__name__)
====================================================================
-> Chunk: openai_realtime\client.py::7


class RealtimeClient(RealtimeEventHandler):

    def append_input_audio(self, array_buffer):
        if len(array_buffer) > 0:
            self.realtime.send('input_audio_buffer.append', {
                'audio': RealtimeUtils.array_buffer_to_base64(array_buffer)
            })
            self.input_audio_buffer = RealtimeUtils.merge_int16_arrays(
                self.input_audio_buffer,
                array_buffer
            )
        return True

    def create_response(self):
        if self.get_turn_detection_type() is None and len(self.input_audio_buffer) > 0:
            self.realtime.send('input_audio_buffer.commit')
            self.conversation.queue_input_audio(self.input_audio_buffer)
            self.input_audio_buffer = np.array([], dtype=np.int16)
        self.realtime.send('response.create')
        return True
====================================================================
-> Chunk: lmp\simple.py::2


simple.__doc__ = """The fundamental unit of language model programming in ell.

  This decorator simplifies the process of creating Language Model Programs (LMPs) 
  that return text-only outputs from language models, while supporting multimodal inputs.
  It wraps the more complex 'complex' decorator, providing a streamlined interface for common use cases.

  :param model: The name or identifier of the language model to use.
  :type model: str
  :param client: An optional OpenAI client instance. If not provided, a default client will be used.
  :type client: Optional[openai.Client]
  :param exempt_from_tracking: If True, the LMP usage won't be tracked. Default is False.
  :type exempt_from_tracking: bool
  :param api_params: Additional keyword arguments to pass to the underlying API call.
  :type api_params: Any

  Usage:
  The decorated function can return either a single prompt or a list of ell.Message objects:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def summarize_text(text: str) -> str:
          '''You are an expert at summarizing text.''' # System prompt
          return f"Please summarize the following text:\\n\\n{text}" # User prompt


      @ell.simple(model="gpt-4", temperature=0.7)
      def describe_image(image : PIL.Image.Image) -> List[ell.Message]:
          '''Describe the contents of an image.''' # unused because we're returning a list of Messages
          return [
              # helper function for ell.Message(text="...", role="system")
              ell.system("You are an AI trained to describe images."),
              # helper function for ell.Message(content="...", role="user")
              ell.user(["Describe this image in detail.", image]),
          ]


      image_description = describe_image(PIL.Image.open("https://example.com/image.jpg"))
      print(image_description) 
      # Output will be a string text-only description of the image

      summary = summarize_text("Long text to summarize...")
      print(summary)
      # Output will be a text-only summary

  Notes:

  - This decorator is designed for text-only model outputs, but supports multimodal inputs.
  - It simplifies complex responses from language models to text-only format, regardless of 
    the model's capability for structured outputs, function calling, or multimodal outputs.
  - For preserving complex model outputs (e.g., structured data, function calls, or multimodal 
    outputs), use the @ell.complex decorator instead. @ell.complex returns a Message object (role='assistant')
  - The decorated function can return a string or a list of ell.Message objects for more 
    complex prompts, including multimodal inputs.
  - If called with n > 1 in api_params, the wrapped LMP will return a list of strings for the n parallel outputs
    of the model instead of just one string. Otherwise, it will return a single string.
  - You can pass LM API parameters either in the decorator or when calling the decorated function.
    Parameters passed during the function call will override those set in the decorator.

  Example of passing LM API params:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def generate_story(prompt: str) -> str:
          return f"Write a short story based on this prompt: {prompt}"

      # Using default parameters
      story1 = generate_story("A day in the life of a time traveler")

      # Overriding parameters during function call
      story2 = generate_story("An AI's first day of consciousness", api_params={"temperature": 0.9, "max_tokens": 500})

  See Also:

  - :func:`ell.complex`: For LMPs that preserve full structure of model responses, including multimodal outputs.
  - :func:`ell.tool`: For defining tools that can be used within complex LMPs.
  - :mod:`ell.studio`: For visualizing and analyzing LMP executions.
    """
====================================================================
-> Chunk: ell\provider.py::4


class Provider(ABC):
    def call(
        self,
        #XXX: In future refactors, we can fully enumerate the args and make ell_call's internal to the _provider implementer interface.
        # This gives us a litellm style interface for free.
        ell_call: EllCallParams,
        origin_id: Optional[str] = None,
        logger: Optional[Any] = None,
    ) -> Tuple[List[Message], Dict[str, Any], Metadata]:
        # Automatic validation of params
        assert (
            not set(ell_call.api_params.keys()).intersection(self.disallowed_api_params()) 
        ), f"Disallowed api parameters: {ell_call.api_params}"

        final_api_call_params = self.translate_to_provider(ell_call)

        call = self.provider_call_function(ell_call.client, final_api_call_params)
        assert self.dangerous_disable_validation or _validate_provider_call_params(final_api_call_params, call)


        provider_resp = call(**final_api_call_params)

        messages, metadata = self.translate_from_provider(
            provider_resp, ell_call, final_api_call_params, origin_id, logger
        )
        assert "choices" not in metadata, "choices should be in the metadata."
        assert self.dangerous_disable_validation or _validate_messages_are_tracked(messages, origin_id)

        return messages, final_api_call_params, metadata
====================================================================
-> Chunk: util\verbosity.py::7


def model_usage_logger_post_start(color: str = "", n: int = 1):
    """Log the start of model output with ASCII box."""
    terminal_width = get_terminal_width()
    print(f"{PIPE_COLOR}{'' * (terminal_width - 2)}{RESET}")
    print(f"{PIPE_COLOR} {BOLD}Output{f'[0 of {n}]' if n > 1 else ''}:{RESET}")
    print(f"{PIPE_COLOR}{'' * (terminal_width - 2)}{RESET}")
    print(f"{PIPE_COLOR}   {ASSISTANT_COLOR}assistant: {RESET}", end='')
    sys.stdout.flush()

from contextlib import contextmanager
====================================================================


###### Cluster Eval ######
Score: 4.0
Cluster name: Content Handling and Messaging Cluster
Content Handling and Messaging Cluster:

-> Chunk: types\message.py::9


class Message(BaseModel):
    role: str
    content: List[ContentBlock]


    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):
        content_blocks = to_content_blocks(content, **content_block_kwargs)

        super().__init__(role=role, content=content_blocks)

    # XXX: This choice of naming is unfortunate, but it is what it is.
    @property
    def text(self) -> str:
        """Returns all text content, replacing non-text content with their representations.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text
            'Hello\\n<PilImage>\\nWorld'
        """
        return _content_to_text(self.content)
====================================================================
-> Chunk: types\message.py::1


# todo: implement tracing for structured outs. this a v2 feature.
import json
from ell.types._lstr import _lstr
from functools import cached_property
import numpy as np
import base64
from io import BytesIO
from PIL import Image as PILImage

from pydantic import BaseModel, ConfigDict, model_validator, field_serializer
from sqlmodel import Field

from concurrent.futures import ThreadPoolExecutor, as_completed

from typing import Any, Callable, Dict, List, Optional, Union

from ell.util.serialization import serialize_image
_lstr_generic = Union[_lstr, str]
InvocableTool = Callable[..., Union["ToolResult", _lstr_generic, List["ContentBlock"], ]]

# AnyContent represents any type that can be passed to Message.
AnyContent = Union["ContentBlock", str, "ToolCall", "ToolResult", "ImageContent", np.ndarray, PILImage.Image, BaseModel]
====================================================================
-> Chunk: types\message.py::10


class Message(BaseModel):

    @property
    def images(self) -> List[ImageContent]:
        """Returns a list of all image content.

        Example:
            >>> from PIL import Image as PILImage
            >>> image1 = Image(url="https://example.com/image.jpg")
            >>> image2 = Image(image=PILImage.new('RGB', (200, 200)))
            >>> message = Message(role="user", content=["Text", image1, "More text", image2])
            >>> len(message.images)
            2
            >>> isinstance(message.images[0], Image)
            True
            >>> message.images[0].url
            'https://example.com/image.jpg'
            >>> isinstance(message.images[1].image, PILImage.Image)
            True
        """
        return [c.image for c in self.content if c.image]
====================================================================
-> Chunk: types\message.py::8


def to_content_blocks(
    content: Optional[Union[AnyContent, List[AnyContent]]] = None,
    **content_block_kwargs
) -> List[ContentBlock]:
    """
    Coerce a variety of input types into a list of ContentBlock objects.

    Args:
    content: The content to be coerced. Can be a single item or a list of items.
             Supported types include str, ContentBlock, ToolCall, ToolResult, BaseModel, Image, np.ndarray, and PILImage.Image.
    **content_block_kwargs: Additional keyword arguments to pass to ContentBlock creation if content is None.

    Returns:
    List[ContentBlock]: A list of ContentBlock objects created from the input content.

    Examples:
    >>> coerce_content_list("Hello")
    [ContentBlock(text="Hello")]

    >>> coerce_content_list([ContentBlock(text="Hello"), "World"])
    [ContentBlock(text="Hello"), ContentBlock(text="World")]

    >>> from PIL import Image as PILImage
    >>> pil_image = PILImage.new('RGB', (100, 100))
    >>> coerce_content_list(pil_image)
    [ContentBlock(image=Image(image=<PIL.Image.Image object>))]

    >>> coerce_content_list(Image(url="https://example.com/image.jpg"))
    [ContentBlock(image=Image(url="https://example.com/image.jpg"))]

    >>> coerce_content_list(None, text="Default text")
    [ContentBlock(text="Default text")]
    """
    if content is None:
        return [ContentBlock(**content_block_kwargs)]

    if not isinstance(content, list):
        content = [content]

    return [ContentBlock.model_validate(ContentBlock.coerce(c)) for c in content]
====================================================================
-> Chunk: util\serialization.py::3


def compute_state_cache_key(ipstr, fn_closure):
    _global_free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[2]), sort_keys=True, default=repr, ensure_ascii=False)}"
    _free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[3]), sort_keys=True, default=repr, ensure_ascii=False)}"
    state_cache_key = hashlib.sha256(f"{ipstr}{_global_free_vars_str}{_free_vars_str}".encode('utf-8')).hexdigest()
    return state_cache_key
====================================================================
-> Chunk: lmp\_track.py::4


def _write_invocation(func, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                     state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id):

    invocation_contents = InvocationContents(
        invocation_id=invocation_id,
        params=cleaned_invocation_params,
        results=result,
        invocation_api_params=invocation_api_params,
        global_vars=get_immutable_vars(func.__ell_closure__[2]),
        free_vars=get_immutable_vars(func.__ell_closure__[3])
    )

    if invocation_contents.should_externalize and config.store.has_blob_storage:
        invocation_contents.is_external = True

        # Write to the blob store 
        blob_id = config.store.blob_store.store_blob(
            json.dumps(invocation_contents.model_dump(
            ), default=str, ensure_ascii=False).encode('utf-8'),
            invocation_id
        )
        invocation_contents = InvocationContents(
            invocation_id=invocation_id,
            is_external=True,
        )

    invocation = Invocation(
        id=invocation_id,
        lmp_id=func.__ell_hash__,
        created_at=utc_now(),
        latency_ms=latency_ms,
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        state_cache_key=state_cache_key,
        used_by_id=parent_invocation_id,
        contents=invocation_contents
    )

    config.store.write_invocation(invocation, consumes)
====================================================================


###### Cluster Eval ######
Score: 4.0
Cluster name: Tool Call and Result Management Cluster
Tool Call and Result Management Cluster:

-> Chunk: types\message.py::1


# todo: implement tracing for structured outs. this a v2 feature.
import json
from ell.types._lstr import _lstr
from functools import cached_property
import numpy as np
import base64
from io import BytesIO
from PIL import Image as PILImage

from pydantic import BaseModel, ConfigDict, model_validator, field_serializer
from sqlmodel import Field

from concurrent.futures import ThreadPoolExecutor, as_completed

from typing import Any, Callable, Dict, List, Optional, Union

from ell.util.serialization import serialize_image
_lstr_generic = Union[_lstr, str]
InvocableTool = Callable[..., Union["ToolResult", _lstr_generic, List["ContentBlock"], ]]

# AnyContent represents any type that can be passed to Message.
AnyContent = Union["ContentBlock", str, "ToolCall", "ToolResult", "ImageContent", np.ndarray, PILImage.Image, BaseModel]
====================================================================
-> Chunk: types\message.py::2


class ToolResult(BaseModel):
    tool_call_id: _lstr_generic
    result: List["ContentBlock"]

    @property
    def text(self) -> str:
        return _content_to_text(self.result)

    @property
    def text_only(self) -> str:
        return _content_to_text_only(self.result)

    # # XXX: Possibly deprecate
    # def readable_repr(self) -> str:
    #     return f"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"

    def __repr__(self):
        return f"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"
====================================================================
-> Chunk: types\message.py::3


class ToolCall(BaseModel):
    tool : InvocableTool
    tool_call_id : Optional[_lstr_generic] = Field(default=None)
    params : BaseModel

    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):
        if not isinstance(params, BaseModel):
            params = tool.__ell_params_model__(**params) #convenience.
        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)

    def __call__(self, **kwargs):
        assert not kwargs, "Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall."

        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
        return self.tool(**self.params.model_dump())

    # XXX: Deprecate in 0.1.0
    def call_and_collect_as_message_block(self):
        raise DeprecationWarning("call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.")

    def call_and_collect_as_content_block(self):
        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)
        return ContentBlock(tool_result=res)

    def call_and_collect_as_message(self):
        return Message(role="user", content=[self.call_and_collect_as_message_block()])

    def __repr__(self):
        return f"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id='{self.tool_call_id}')"
====================================================================
-> Chunk: types\message.py::9


class Message(BaseModel):
    role: str
    content: List[ContentBlock]


    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):
        content_blocks = to_content_blocks(content, **content_block_kwargs)

        super().__init__(role=role, content=content_blocks)

    # XXX: This choice of naming is unfortunate, but it is what it is.
    @property
    def text(self) -> str:
        """Returns all text content, replacing non-text content with their representations.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text
            'Hello\\n<PilImage>\\nWorld'
        """
        return _content_to_text(self.content)
====================================================================
-> Chunk: models\__init__.py::1


"""
Attempts to registeres model names with their respective API client bindings. This allows for the creation of a unified interface for interacting with different LLM providers.

For example, to register an OpenAI model:
@ell.simple(model='gpt-4o-mini') -> @ell.simple(model='gpt-4o-mini', client=openai.OpenAI())

"""

import ell.models.openai
import ell.models.anthropic
import ell.models.ollama
import ell.models.groq
import ell.models.bedrock
====================================================================


###### Cluster Eval ######
Score: 3.6666666666666665
Cluster name: Server and API Interaction Cluster
Server and API Interaction Cluster:

-> Chunk: studio\server.py::1


from typing import Optional, Dict, Any

from sqlmodel import Session
from ell.stores.sql import PostgresStore, SQLiteStore
from ell import __version__
from fastapi import FastAPI, Query, HTTPException, Depends, Response, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import logging
import json
from ell.studio.config import Config
from ell.studio.connection_manager import ConnectionManager
from ell.studio.datamodels import InvocationPublicWithConsumes, SerializedLMPWithUses

from ell.types import SerializedLMP
from datetime import datetime, timedelta
from sqlmodel import select


logger = logging.getLogger(__name__)


from ell.studio.datamodels import InvocationsAggregate


def get_serializer(config: Config):
    if config.pg_connection_string:
        return PostgresStore(config.pg_connection_string)
    elif config.storage_dir:
        return SQLiteStore(config.storage_dir)
    else:
        raise ValueError("No storage configuration found")
====================================================================
-> Chunk: studio\__main__.py::2


def main():
    parser = ArgumentParser(description="ell studio")
    parser.add_argument("--storage-dir" , default=None,
                        help="Directory for filesystem serializer storage (default: current directory)")
    parser.add_argument("--pg-connection-string", default=None,
                        help="PostgreSQL connection string (default: None)")
    parser.add_argument("--host", default="127.0.0.1", help="Host to run the server on (default: localhost)")
    parser.add_argument("--port", type=int, default=5555, help="Port to run the server on (default: 5555)")
    parser.add_argument("--dev", action="store_true", help="Run in development mode")
    parser.add_argument("--open", action="store_true", help="Opens the studio web UI in a browser")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enables debug logging for more verbose output")
    args = parser.parse_args()

    _setup_logging(logging.DEBUG if args.verbose else logging.INFO)

    if args.dev:
        assert args.port == 5555, "Port must be 5000 in development mode"

    config = Config.create(storage_dir=args.storage_dir,
                    pg_connection_string=args.pg_connection_string)
    app = create_app(config)

    if not args.dev:
        # In production mode, serve the built React app
        static_dir = Path(__file__).parent / "static"
        # app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")

        @app.get("/{full_path:path}")
        async def serve_react_app(full_path: str):
            file_path = static_dir / full_path
            if file_path.exists() and file_path.is_file():
                return FileResponse(file_path)
            else:
                return FileResponse(static_dir / "index.html")

    # Respect Config.create behavior, which has fallback to env vars.
    db_path = Path(config.storage_dir) if config.storage_dir else None
    # ... other code
====================================================================
-> Chunk: lmp\complex.py::4


def _get_messages(prompt_ret: Union[str, list[MessageOrDict]], prompt: LMP) -> list[Message]:
    """
    Helper function to convert the output of an LMP into a list of Messages.
    """
    if isinstance(prompt_ret, str):
        has_system_prompt = prompt.__doc__ is not None and prompt.__doc__.strip() != ""
        messages =     [Message(role="system", content=[ContentBlock(text=_lstr(prompt.__doc__ ) )])] if has_system_prompt else []
        return messages + [
            Message(role="user", content=[ContentBlock(text=prompt_ret)])
        ]
    else:
        assert isinstance(
            prompt_ret, list
        ), "Need to pass a list of Messages to the language model"
        return prompt_ret
====================================================================
-> Chunk: openai_realtime\conversation.py::9


class RealtimeConversation:

    def _process_response_text_delta(self, event):
        item_id, content_index, delta = event['item_id'], event['content_index'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.text.delta: Item '{item_id}' not found")
        item['content'][content_index]['text'] += delta
        item['formatted']['text'] += delta
        return {'item': item, 'delta': {'text': delta}}

    def _process_response_function_call_arguments_delta(self, event):
        item_id, delta = event['item_id'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.function_call_arguments.delta: Item '{item_id}' not found")
        item['arguments'] += delta
        item['formatted']['tool']['arguments'] += delta
        return {'item': item, 'delta': {'arguments': delta}}
====================================================================
-> Chunk: types\message.py::13


class Message(BaseModel):

    @property
    def parsed(self) -> Union[BaseModel, List[BaseModel]]:
        """Returns a list of all parsed content.

        Example:
            >>> class CustomModel(BaseModel):
            ...     value: int
            >>> parsed_content = CustomModel(value=42)
            >>> message = Message(role="user", content=["Text", ContentBlock(parsed=parsed_content)])
            >>> len(message.parsed)
            1
        """
        parsed_content = [c.parsed for c in self.content if c.parsed is not None]
        return parsed_content[0] if len(parsed_content) == 1 else parsed_content
====================================================================


###### Cluster Eval ######
Score: 3.6666666666666665
Cluster name: Function Dependency and Closure Management Cluster
Function Dependency and Closure Management Cluster:

-> Chunk: util\closure.py::2


def lexical_closure(
    func: Any,
    already_closed: Set[int] = None,
    initial_call: bool = False,
    recursion_stack: list = None,
    forced_dependencies: Optional[Dict[str, Any]] = None
) -> Tuple[str, Tuple[str, str], Set[str]]:
    """
    Generate a lexical closure for a given function or callable.

    Args:
        func: The function or callable to process.
        already_closed: Set of already processed function hashes.
        initial_call: Whether this is the initial call to the function.
        recursion_stack: Stack to keep track of the recursion path.

    Returns:
        A tuple containing:
        - The full source code of the closure
        - A tuple of (function source, dependencies source)
        - A set of function hashes that this closure uses
    """
    already_closed = already_closed or set()
    uses = set()
    forced_dependencies = forced_dependencies or {}
    recursion_stack = recursion_stack or []

    if hash(func) in already_closed:
        return "", ("", ""), set()

    recursion_stack.append(getattr(func, '__qualname__', str(func)))

    outer_ell_func = func
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__

    source = getsource(func, lstrip=True)
    already_closed.add(hash(func))

    globals_and_frees = _get_globals_and_frees(func)
    dependencies, imports, modules = _process_dependencies(func, globals_and_frees, already_closed, recursion_stack, uses)
    for k,v in forced_dependencies.items():
        # Todo: dictionary not necessary
        _process_signature_dependency(v, dependencies, already_closed, recursion_stack, uses, k)

    cur_src = _build_initial_source(imports, dependencies, source)

    module_src = _process_modules(modules, cur_src, already_closed, recursion_stack, uses)

    dirty_src = _build_final_source(imports, module_src, dependencies, source)
    dirty_src_without_func = _build_final_source(imports, module_src, dependencies, "")

    CLOSURE_SOURCE[hash(func)] = dirty_src

    dsrc = _clean_src(dirty_src_without_func)

    # Format the sorce and dsrc soruce using Black
    source = _format_source(source)
    dsrc = _format_source(dsrc)

    fn_hash = _generate_function_hash(source, dsrc, func.__qualname__)

    _update_ell_func(outer_ell_func, source, dsrc, globals_and_frees['globals'], globals_and_frees['frees'], fn_hash, uses)

    return (dirty_src, (source, dsrc), ({outer_ell_func} if not initial_call and hasattr(outer_ell_func, "__ell_func__") else uses))
====================================================================
-> Chunk: util\closure.py::17


def is_function_called(func_name, source_code):
    """
    Check if a function is called in the given source code.

    Parameters:
    func_name (str): The name of the function to check.
    source_code (str): The source code to check.

    Returns:
    bool: True if the function is called, False otherwise.
    """
    # Parse the source code into an AST
    tree = ast.parse(source_code)

    # Walk through all the nodes in the AST
    for node in ast.walk(tree):
        # If the node is a function call
        if isinstance(node, ast.Call):
            # If the function being called is the function we're looking for
            if isinstance(node.func, ast.Name) and node.func.id == func_name:
                return True

    # If we've gone through all the nodes and haven't found a call to the function, it's not called
    return False

#!/usr/bin/env python
#
from dill.detect import nestedglobals
import inspect
====================================================================
-> Chunk: util\closure.py::10


def _build_initial_source(imports, dependencies, source):
    """Build the initial source code."""
    return f"{DELIM}\n" + f"\n{DELIM}\n".join(imports + dependencies + [source]) + f"\n{DELIM}\n"

def _process_modules(modules, cur_src, already_closed, recursion_stack, uses):
    """Process module dependencies."""
    reverse_module_src = deque()
    while modules:
        mname, mval = modules.popleft()
        mdeps = []
        attrs_to_extract = get_referenced_names(cur_src.replace(DELIM, ""), mname)
        for attr in attrs_to_extract:
            _process_module_attribute(mname, mval, attr, mdeps, modules, already_closed, recursion_stack, uses)

        mdeps.insert(0, f"# Extracted from module: {mname}")
        reverse_module_src.appendleft("\n".join(mdeps))

        cur_src = _dereference_module_names(cur_src, mname, attrs_to_extract)

    return list(reverse_module_src)
====================================================================
-> Chunk: util\closure.py::9


def _process_module(var_name, var_value, modules, imports, uses):
    """Process a module."""
    if should_import(var_value.__name__):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        modules.append((var_name, var_value))

def _process_other_variable(var_name, var_value, dependencies, uses):
    """Process variables that are not callables or modules."""
    if isinstance(var_value, str) and '\n' in var_value:
        dependencies.append(f"{var_name} = '''{var_value}'''")
    elif is_immutable_variable(var_value):
        dependencies.append(f"#<BV>\n{var_name} = {repr(var_value)}\n#</BV>")
    else:
        dependencies.append(f"#<BmV>\n{var_name} = <{type(var_value).__name__} object>\n#</BmV>")
====================================================================
-> Chunk: util\closure.py::15


def lexically_closured_source(func, forced_dependencies: Optional[Dict[str, Any]] = None):
    """
    Generate a lexically closured source for a given function.

    This function takes a callable object (function, method, or class) and generates
    a lexically closured source code. It captures all the dependencies, including
    global variables, free variables, and nested functions, to create a self-contained
    version of the function that can be executed independently.

    Args:
        func (Callable): The function or callable object to process.
        forced_dependencies (Optional[Dict[str, Any]]): A dictionary of additional
            dependencies to include in the closure. Keys are variable names, and
            values are the corresponding objects.

    Returns:
        Tuple[str, Set[Any]]: A tuple containing two elements:
            1. The lexically closured source code as a string.
            2. A set of function objects that this closure uses.

    Raises:
        ValueError: If the input is not a callable object.

    Example:
        def outer(x):
            y = 10
            def inner():
                return x + y
            return inner

        closured_source, uses = lexically_closured_source(outer)
        print(closured_source)
        # Output will include the source for both outer and inner functions,
        # along with any necessary imports and variable definitions.

    Note:
        This function relies on the `lexical_closure` function to perform the
        actual closure generation. It also uses the `__ell_closure__` attribute
        of the function, which is expected to be set by the `lexical_closure` function.
    """
    if not callable(func):
        raise ValueError("Input must be a callable object (function, method, or class).")
    _, fnclosure, uses = lexical_closure(func, initial_call=True, recursion_stack=[], forced_dependencies=forced_dependencies)
    return func.__ell_closure__, uses
====================================================================
-> Chunk: types\message.py::3


class ToolCall(BaseModel):
    tool : InvocableTool
    tool_call_id : Optional[_lstr_generic] = Field(default=None)
    params : BaseModel

    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):
        if not isinstance(params, BaseModel):
            params = tool.__ell_params_model__(**params) #convenience.
        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)

    def __call__(self, **kwargs):
        assert not kwargs, "Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall."

        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
        return self.tool(**self.params.model_dump())

    # XXX: Deprecate in 0.1.0
    def call_and_collect_as_message_block(self):
        raise DeprecationWarning("call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.")

    def call_and_collect_as_content_block(self):
        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)
        return ContentBlock(tool_result=res)

    def call_and_collect_as_message(self):
        return Message(role="user", content=[self.call_and_collect_as_message_block()])

    def __repr__(self):
        return f"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id='{self.tool_call_id}')"
====================================================================


###### Cluster Eval ######
Score: 3.3333333333333335
Cluster name: Behavioral Cloning in Reinforcement Learning Cluster
Behavioral Cloning in Reinforcement Learning Cluster:

-> Chunk: 0.1.0\cpbo.py::6


# Main CBPO algorithm
def CBPO(env_name='CartPole-v1', num_epochs=10, num_episodes_per_epoch=100, gamma=0.99, 
         batch_size=64, learning_rate=1e-3, device='cpu'):

    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")

        # 1. Collect trajectories
        trajectories = collect_trajectories(env, policy, num_episodes_per_epoch, device)

        # 2. Create labeled dataset
        states, actions, labels = create_labeled_dataset(trajectories, gamma, device)

        # 3. Create DataLoader
        dataset = TensorDataset(states, actions, labels)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        # 4. Behavioral Cloning Update
        behavioral_cloning_update(policy, optimizer, dataloader, device)

        # 5. Evaluate current policy
        avg_reward = evaluate_policy(env, policy, device)
        print(f"Average Reward: {avg_reward}")

        # Early stopping if solved
        if avg_reward >= env.spec.reward_threshold:
            print(f"Environment solved in {epoch+1} epochs!")
            break

    env.close()
    return policy
====================================================================
-> Chunk: 0.1.0\cpbo.py::2


# Function to collect trajectories
def collect_trajectories(env, policy, num_episodes, device):
    trajectories = []
    Episode = namedtuple('Episode', ['states', 'actions', 'rewards'])

    for episode_num in range(num_episodes):
        states = []
        actions = []
        rewards = []
        # Handle Gym's updated reset() API
        state, info = env.reset(seed=42 + episode_num)  # Optional: set seed for reproducibility
        done = False

        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                action_probs = policy(state_tensor)
            action_dist = torch.distributions.Categorical(action_probs)
            action = action_dist.sample().item()

            # Handle Gym's updated step() API
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            states.append(state)
            actions.append(action)
            rewards.append(reward)

            state = next_state

        trajectories.append(Episode(states, actions, rewards))

    return trajectories

# Function to compute returns
def compute_returns(trajectories, gamma=0.99):
    all_returns = []
    for episode in trajectories:
        returns = []
        G = 0
        for reward in reversed(episode.rewards):
            G = reward + gamma * G
            returns.insert(0, G)
        all_returns.extend(returns)
    return all_returns
====================================================================
-> Chunk: 0.1.0\cpbo.py::3


# Function to create labeled dataset
def create_labeled_dataset(trajectories, gamma=0.99, device='cpu'):
    states = []
    actions = []
    labels = []

    all_returns = compute_returns(trajectories, gamma)
    all_returns = np.array(all_returns)
    median_return = np.median(all_returns)

    for episode in trajectories:
        for t in range(len(episode.rewards)):
            # Compute return from timestep t
            G = sum([gamma**k * episode.rewards[t + k] for k in range(len(episode.rewards) - t)])
            label = 1 if G >= median_return else 0
            states.append(episode.states[t])
            actions.append(episode.actions[t])
            labels.append(label)

    # Convert lists to NumPy arrays first for efficiency
    states = np.array(states)
    actions = np.array(actions)
    labels = np.array(labels)

    # Convert to PyTorch tensors
    states = torch.FloatTensor(states).to(device)
    actions = torch.LongTensor(actions).to(device)
    labels = torch.FloatTensor(labels).to(device)

    return states, actions, labels
====================================================================
-> Chunk: 0.1.0\cpbo.py::5


# Evaluation function
def evaluate_policy(env, policy, device, episodes=5):
    policy.eval()
    total_rewards = []
    for _ in range(episodes):
        state, info = env.reset()
        done = False
        ep_reward = 0
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                action_probs = policy(state_tensor)
            action = torch.argmax(action_probs, dim=1).item()
            # Handle Gym's updated step() API
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            ep_reward += reward
            state = next_state
        total_rewards.append(ep_reward)
    average_reward = np.mean(total_rewards)
    return average_reward
====================================================================
-> Chunk: 0.1.0\mypytest.py::1


from typing import TypedDict


class Test(TypedDict):
    name: str
    age: int


def test(**t: Test):
    print(t)

# no type hinting like ts thats unfortunate.
test( )
====================================================================


###### Cluster Eval ######
Score: 3.3333333333333335
Cluster name: Database Interaction and Serialization Cluster
Database Interaction and Serialization Cluster:

-> Chunk: stores\sql.py::1


from datetime import datetime, timedelta
import json
import os
from typing import Any, Optional, Dict, List, Set, Union
from pydantic import BaseModel
from sqlmodel import Session, SQLModel, create_engine, select
import ell.store
import cattrs
import numpy as np
from sqlalchemy.sql import text
from ell.types import InvocationTrace, SerializedLMP, Invocation, InvocationContents
from ell.types._lstr import _lstr
from sqlalchemy import or_, func, and_, extract, FromClause
from sqlalchemy.types import TypeDecorator, VARCHAR
from ell.types.studio import SerializedLMPUses, utc_now
from ell.util.serialization import pydantic_ltype_aware_cattr
import gzip
import json
====================================================================
-> Chunk: stores\sql.py::2


class SQLStore(ell.store.Store):
    def __init__(self, db_uri: str, blob_store: Optional[ell.store.BlobStore] = None):
        self.engine = create_engine(db_uri,
                                    json_serializer=lambda obj: json.dumps(pydantic_ltype_aware_cattr.unstructure(obj), 
                                     sort_keys=True, default=repr, ensure_ascii=False))

        SQLModel.metadata.create_all(self.engine)
        self.open_files: Dict[str, Dict[str, Any]] = {}
        super().__init__(blob_store)

    def write_lmp(self, serialized_lmp: SerializedLMP, uses: Dict[str, Any]) -> Optional[Any]:
        with Session(self.engine) as session:
            # Bind the serialized_lmp to the session
            lmp = session.exec(select(SerializedLMP).filter(SerializedLMP.lmp_id == serialized_lmp.lmp_id)).first()

            if lmp:
                # Already added to the DB.
                return lmp
            else:
                session.add(serialized_lmp)

            for use_id in uses:
                used_lmp = session.exec(select(SerializedLMP).where(SerializedLMP.lmp_id == use_id)).first()
                if used_lmp:
                    serialized_lmp.uses.append(used_lmp)

            session.commit()
        return None
====================================================================
-> Chunk: stores\sql.py::5


class SQLStore(ell.store.Store):
    def get_latest_lmps(self, session: Session, skip: int = 0, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Gets all the lmps grouped by unique name with the highest created at
        """
        subquery = (
            select(SerializedLMP.name, func.max(SerializedLMP.created_at).label("max_created_at"))
            .group_by(SerializedLMP.name)
            .subquery()
        )

        filters = {
            "name": subquery.c.name,
            "created_at": subquery.c.max_created_at
        }

        return self.get_lmps(session, skip=skip, limit=limit, subquery=subquery, **filters)
====================================================================
-> Chunk: util\serialization.py::4


def prepare_invocation_params(params):
    invocation_params = params

    cleaned_invocation_params = pydantic_ltype_aware_cattr.unstructure(invocation_params)

    # Thisis because we wneed the caching to work on the hash of a cleaned and serialized object.
    jstr = json.dumps(cleaned_invocation_params, sort_keys=True, default=repr, ensure_ascii=False)

    consumes = set()
    import re
    # XXX: Better than registering a hook in cattrs.
    pattern = r'"__origin_trace__":\s*"frozenset\({(.+?)}\)"'

    # Find all matches in the jstr
    matches = re.findall(pattern, jstr)

    # Process each match and add to consumes set
    for match in matches:
        # Remove quotes and spaces, then split by comma
        items = [item.strip().strip("'") for item in match.split(',')]
        consumes.update(items)
    consumes = list(consumes)
    # XXX: Only need to reload because of 'input' caching., we could skip this by making ultimate model caching rather than input hash caching; if prompt same use the same output.. irrespective of version.
    return json.loads(jstr), jstr, consumes
====================================================================
-> Chunk: studio\config.py::1


from functools import lru_cache
import os
from typing import Optional
from pydantic import BaseModel

import logging

logger = logging.getLogger(__name__)


# todo. maybe we default storage dir and other things in the future to a well-known location
# like ~/.ell or something
@lru_cache
def ell_home() -> str:
    return os.path.join(os.path.expanduser("~"), ".ell")


class Config(BaseModel):
    pg_connection_string: Optional[str] = None
    storage_dir: Optional[str] = None

    @classmethod
    def create(
        cls,
        storage_dir: Optional[str] = None,
        pg_connection_string: Optional[str] = None,
    ) -> 'Config':
        pg_connection_string = pg_connection_string or os.getenv("ELL_PG_CONNECTION_STRING")
        storage_dir = storage_dir or os.getenv("ELL_STORAGE_DIR")

        # Enforce that we use either sqlite or postgres, but not both
        if pg_connection_string is not None and storage_dir is not None:
            raise ValueError("Cannot use both sqlite and postgres")

        # For now, fall back to sqlite if no PostgreSQL connection string is provided
        if pg_connection_string is None and storage_dir is None:
            # This intends to honor the default we had set in the CLI
            storage_dir = os.getcwd()

        return cls(pg_connection_string=pg_connection_string, storage_dir=storage_dir)
====================================================================


###### Cluster Eval ######
Score: 3.3333333333333335
Cluster name: Data Storage and Management Cluster
Data Storage and Management Cluster:

-> Chunk: providers\openai.py::1


from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast

from pydantic import BaseModel
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from ell.configurator import _Model, config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image

try:
    # XXX: Could genericize.
    import openai
    from openai._streaming import Stream
    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam

    class OpenAIProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and (isinstance(fmt := api_call_params.get("response_format"), type)) and issubclass(fmt, BaseModel):
                return client.beta.chat.completions.parse
            else:
                return client.chat.completions.create

        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:
            final_call_params = ell_call.api_params.copy()
            final_call_params["model"] = ell_call.model
            # Stream by default for verbose logging.
            final_call_params["stream"] = True
            final_call_params["stream_options"] = {"include_usage": True}

            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.
            if ell_call.tools or final_call_params.get("response_format") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:
                final_call_params.pop("stream", None)
                final_call_params.pop("stream_options", None)
            if ell_call.tools:
                final_call_params.update(
                    tool_choice=final_call_params.get("tool_choice", "auto"),
                    tools=[  
                        dict(
                            type="function",
                            function=dict(
                                name=tool.__name__,
                                description=tool.__doc__,
                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore
                            )
                        ) for tool in ell_call.tools
                    ]
                )
            # messages
            openai_messages : List[ChatCompletionMessageParam] = []
            for message in ell_call.messages:
                if (tool_calls := message.tool_calls):
                    assert message.role == "assistant", "Tool calls must be from the assistant."
                    assert all(t.tool_call_id for t in tool_calls), "Tool calls must have tool call ids."
                    openai_messages.append(dict(
                        tool_calls=[
                            dict(
                                id=cast(str, tool_call.tool_call_id),
                                type="function",
                                function=dict(
                                    name=tool_call.tool.__name__,
                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)
                                )
                            ) for tool_call in tool_calls ],
                        role="assistant",
                        content=None,
                    ))
                elif (tool_results := message.tool_results):
                    for tool_result in tool_results:
                        assert all(cb.type == "text" for cb in tool_result.result), "Tool result does not match expected content blocks."
                        openai_messages.append(dict(
                            role="tool",
                            tool_call_id=tool_result.tool_call_id,
                            content=tool_result.text_only, 
                        ))
                else:
                    openai_messages.append(cast(ChatCompletionMessageParam, dict(
                        role=message.role,
                        content=[_content_block_to_openai_format(c) for c in message.content] 
                             if message.role != "system" 
                             else message.text_only
                    )))

            final_call_params["messages"] = openai_messages

            return final_call_params

        def translate_from_provider(
            self,
            provider_response: Union[
                ChatCompletion, 
                ParsedChatCompletion,
                Stream[ChatCompletionChunk], Any],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            metadata : Metadata = {}
            messages : List[Message] = []
            did_stream = provider_call_params.get("stream", False)


            if did_stream:
                stream = cast(Stream[ChatCompletionChunk], provider_response)
                message_streams = defaultdict(list)
                role : Optional[str] = None
                for chunk in stream:
                    metadata.update(chunk.model_dump(exclude={"choices"}))

                    for chat_compl_chunk in chunk.choices:
                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)
                        delta = chat_compl_chunk.delta
                        role = role or delta.role
                        if  chat_compl_chunk.index == 0 and logger:
                            logger(delta.content, is_refusal=hasattr(delta, "refusal") and delta.refusal)
                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):
                    text = "".join((choice.delta.content or "") for choice in message_stream)
                    messages.append(
                        Message(role=role, 
                                content=_lstr(content=text,origin_trace=origin_id)))
                    #XXX: Support streaming other types.
            else:
                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)
                metadata = chat_completion.model_dump(exclude={"choices"})
                for oai_choice in chat_completion.choices:
                    role = oai_choice.message.role
                    content_blocks = []
                    if (hasattr(message := oai_choice.message, "refusal") and (refusal := message.refusal)):
                        raise ValueError(refusal)
                    if hasattr(message, "parsed"):
                        if (parsed := message.parsed):
                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing
                            if logger: logger(parsed.model_dump_json())
                    else:
                        if (content := message.content):
                            content_blocks.append(
                                ContentBlock(
                                    text=_lstr(content=content,origin_trace=origin_id)))
                            if logger: logger(content)
                        if (tool_calls := message.tool_calls):
                            for tool_call in tool_calls:
                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)
                                assert matching_tool, "Model called tool not found in provided toolset."
                                content_blocks.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                tool_call.id, origin_trace= origin_id),
                                            params=json.loads(tool_call.function.arguments),
                                        )
                                    )
                                )
                                if logger: logger(repr(tool_call))
                    messages.append(Message(role=role, content=content_blocks))
            return messages, metadata


    # xx: singleton needed
    openai_provider = OpenAIProvider()
    register_provider(openai_provider, openai.Client)
except ImportError:
    pass
====================================================================
-> Chunk: stores\sql.py::10


class SQLiteStore(SQLStore):
    def __init__(self, db_dir: str):
        assert not db_dir.endswith('.db'), "Create store with a directory not a db."

        os.makedirs(db_dir, exist_ok=True)
        self.db_dir = db_dir
        db_path = os.path.join(db_dir, 'ell.db')
        blob_store = SQLBlobStore(db_dir)
        super().__init__(f'sqlite:///{db_path}', blob_store=blob_store)

class SQLBlobStore(ell.store.BlobStore):
    def __init__(self, db_dir: str):
        self.db_dir = db_dir

    def store_blob(self, blob: bytes, blob_id  : str) -> str:
        file_path = self._get_blob_path(blob_id)
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with gzip.open(file_path, "wb") as f:
            f.write(blob)
        return blob_id

    def retrieve_blob(self, blob_id: str) -> bytes:
        file_path = self._get_blob_path(blob_id)
        with gzip.open(file_path, "rb") as f:
            return f.read()
====================================================================
-> Chunk: studio\__main__.py::1


import asyncio
import logging
import socket
import time
import webbrowser
import uvicorn
from argparse import ArgumentParser
from contextlib import closing
from ell.studio.config import Config
from ell.studio.server import create_app
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pathlib import Path
from watchfiles import awatch


logger = logging.getLogger(__file__)


def _socket_is_open(host, port) -> bool:
    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
        return sock.connect_ex((host, port)) == 0


def _setup_logging(level):
    logging.basicConfig(
        format='%(asctime)s %(levelname)-8s] %(message)s',
        level=level,
        datefmt='%Y-%m-%d %H:%M:%S'
    )
====================================================================
-> Chunk: ell\provider.py::6


def _validate_messages_are_tracked(
    messages: List[Message], origin_id: Optional[str] = None
):
    if origin_id is None:
        return

    for message in messages:
        assert isinstance(
            message.text, _lstr
        ), f"Provider implementation error: Message text should be an instance of _lstr, got {type(message.text)}"
        assert (
            origin_id in message.text.__origin_trace__
        ), f"Provider implementation error: Message origin_id {message.text.__origin_trace__} does not match the provided origin_id {origin_id}"
    return True
====================================================================
-> Chunk: studio\server.py::3


def create_app(config:Config):
    # ... other code



    @app.get("/api/lmps", response_model=list[SerializedLMPWithUses])
    def get_lmp(
        lmp_id: Optional[str] = Query(None),
        name: Optional[str] = Query(None),
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        session: Session = Depends(get_session)
    ):

        filters : Dict[str, Any] = {}
        if name:
            filters['name'] = name
        if lmp_id:
            filters['lmp_id'] = lmp_id

        lmps = serializer.get_lmps(session, skip=skip, limit=limit, **filters)

        if not lmps:
            raise HTTPException(status_code=404, detail="LMP not found")

        print(lmps[0])
        return lmps



    @app.get("/api/invocation/{invocation_id}", response_model=InvocationPublicWithConsumes)
    def get_invocation(
        invocation_id: str,
        session: Session = Depends(get_session)
    ):
        invocation = serializer.get_invocations(session, lmp_filters=dict(), filters={"id": invocation_id})[0]
        return invocation
    # ... other code
====================================================================


###### Cluster Eval ######
Score: 3.3333333333333335
Cluster name: Invocation Content Management Cluster
Invocation Content Management Cluster:

-> Chunk: types\studio.py::8


class InvocationContentsBase(SQLModel):

    @cached_property
    def should_externalize(self) -> bool:
        import json

        json_fields = [
            self.params,
            self.results,
            self.invocation_api_params,
            self.global_vars,
            self.free_vars
        ]

        total_size = sum(
            len(json.dumps(field, default=(lambda x: json.dumps(x.model_dump(), default=str, ensure_ascii=False)
                                           if isinstance(x, BaseModel) else str(x)), ensure_ascii=False).encode('utf-8'))
            for field in json_fields if field is not None
        )
        # print("total_size", total_size)

        return total_size > 102400  # Precisely 100kb in bytes

class InvocationContents(InvocationContentsBase, table=True):
    invocation: "Invocation" = Relationship(back_populates="contents")
====================================================================
-> Chunk: ell\configurator.py::7


class Config(BaseModel):

    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:
        """
        Get the provider instance for a specific client instance.

        :param client: The client instance to get the provider for.
        :type client: Any
        :return: The provider instance for the specified client, or None if not found.
        :rtype: Optional[Provider]
        """

        client_type = type(client) if not isinstance(client, type) else client
        for provider_type, provider in self.providers.items():
            if issubclass(client_type, provider_type) or client_type == provider_type:
                return provider
        return None

# Single* instance
# XXX: Make a singleton
config = Config()
====================================================================
-> Chunk: ell\store.py::2


class Store(ABC):
    """
    Abstract base class for serializers. Defines the interface for serializing and deserializing LMPs and invocations.
    """

    def __init__(self, blob_store: Optional[BlobStore] = None):
        self.blob_store = blob_store

    @property
    def has_blob_storage(self) -> bool:
        return self.blob_store is not None

    @abstractmethod
    def write_lmp(self, serialized_lmp: SerializedLMP, uses: Dict[str, Any]) -> Optional[Any]:
        """
        Write an LMP (Language Model Package) to the storage.

        :param serialized_lmp: SerializedLMP object containing all LMP details.
        :param uses: Dictionary of LMPs used by this LMP.
        :return: Optional return value.
        """
        pass

    @abstractmethod
    def write_invocation(self, invocation: Invocation,  consumes: Set[str]) -> Optional[Any]:
        """
        Write an invocation of an LMP to the storage.

        :param invocation: Invocation object containing all invocation details.
        :param results: List of SerializedLStr objects representing the results.
        :param consumes: Set of invocation IDs consumed by this invocation.
        :return: Optional return value.
        """
        pass

    @abstractmethod
    def get_cached_invocations(self, lmp_id :str, state_cache_key :str) -> List[Invocation]:
        """
        Get cached invocations for a given LMP and state cache key.
        """
        pass

    @abstractmethod
    def get_versions_by_fqn(self, fqn :str) -> List[SerializedLMP]:
        """
        Get all versions of an LMP by its fully qualified name.
        """
        pass
====================================================================
-> Chunk: studio\server.py::4


def create_app(config:Config):
    # ... other code

    @app.get("/api/invocations", response_model=list[InvocationPublicWithConsumes])
    def get_invocations(
        id: Optional[str] = Query(None),
        hierarchical: Optional[bool] = Query(False),
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        lmp_name: Optional[str] = Query(None),
        lmp_id: Optional[str] = Query(None),
        session: Session = Depends(get_session)
    ):
        lmp_filters = {}
        if lmp_name:
            lmp_filters["name"] = lmp_name
        if lmp_id:
            lmp_filters["lmp_id"] = lmp_id

        invocation_filters = {}
        if id:
            invocation_filters["id"] = id

        invocations = serializer.get_invocations(
            session,
            lmp_filters=lmp_filters,
            filters=invocation_filters,
            skip=skip,
            limit=limit,
            hierarchical=hierarchical
        )
        return invocations
    # ... other code
====================================================================
-> Chunk: util\should_import.py::1


import importlib.util
import os
import site
import sys
import sysconfig
from pathlib import Path


def should_import(module_name: str, raise_on_error: bool = False) -> bool:
    """
    Determines whether a module should be imported based on its origin.
    Excludes local modules and standard library modules.

    Args:
        module_name (str): The name of the module to check.

    Returns:
        bool: True if the module should be imported (i.e., it's a third-party module), False otherwise.
    """
    if module_name.startswith("ell"):
        return True
    try:
        try:
            spec = importlib.util.find_spec(module_name)
        except ValueError:
            return False
        if spec is None:
            return False

        origin = spec.origin
        if origin is None:
            return False
        if spec.has_location:
            origin_path = Path(origin).resolve()

            site_packages = list(site.getsitepackages()) + (list(site.getusersitepackages()) if isinstance(site.getusersitepackages(), list) else [site.getusersitepackages()])

            additional_paths = [Path(p).resolve() for p in sys.path if Path(p).resolve() not in map(Path, site_packages)]

            project_root = Path(os.environ.get("ELL_PROJECT_ROOT", os.getcwd())).resolve()

            site_packages_paths = [Path(p).resolve() for p in site_packages]
            stdlib_path = sysconfig.get_paths().get("stdlib")
            if stdlib_path:
                site_packages_paths.append(Path(stdlib_path).resolve())

            additional_paths = [Path(p).resolve() for p in additional_paths]
            local_paths = [project_root]

            cwd = Path.cwd().resolve()
            additional_paths = [path for path in additional_paths if path != cwd]

            for pkg in site_packages_paths:
                if origin_path.is_relative_to(pkg):
                    return True

            for path in additional_paths:
                if origin_path.is_relative_to(path):
                    return False

            for local in local_paths:
                if origin_path.is_relative_to(local):
                    return False

        return True

    except Exception as e:
        if raise_on_error:
            raise e
        return True
====================================================================


###### Cluster Eval ######
Score: 3.3333333333333335
Cluster name: Dependency Management and Code Structure Cluster
Dependency Management and Code Structure Cluster:

-> Chunk: util\closure_util.py::4


def should_import(module_name : str):
    """
    This function checks if a module should be imported based on its origin.
    It returns False if the module is in the local directory or if the module's spec is None.
    Otherwise, it returns True.

    Returns:
    bool: True if the module should be imported, False otherwise.
    """

    # Define the local directory
    DIRECTORY_TO_WATCH = os.environ.get("DIRECTORY_TO_WATCH", os.getcwd())

    # Get the module's spec
    spec = importlib.util.find_spec(module_name)

    if module_name.startswith("ell"):
        return True

    # Return False if the spec is None or if the spec's origin starts with the local directory
    if spec is None or (spec.origin is not None and spec.origin.startswith(DIRECTORY_TO_WATCH)):
        return False

    # Otherwise, return True
    return True


def format_source(source: str) -> str:
    """Format the source code using Black."""
    try:
        return black.format_str(source, mode=black.Mode())
    except:
        # If Black formatting fails, return the original source
        return source
====================================================================
-> Chunk: util\closure.py::14


def get_referenced_names(code: str, module_name: str):
    """
    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)
    and walks through the tree to find all instances where an attribute of the module is referenced in the code.

    Parameters:
    code (str): The block of code to be parsed.
    module_name (str): The name of the module to look for in the code.

    Returns:
    list: A list of all attributes of the module that are referenced in the code.
    """
    # Remove content between #<BV> and #</BV> tags
    code = re.sub(r'#<BV>\n.*?\n#</BV>', '', code, flags=re.DOTALL)

    # Remove content between #<BmV> and #</BmV> tags
    code = re.sub(r'#<BmV>\n.*?\n#</BmV>', '', code, flags=re.DOTALL)

    tree = ast.parse(code)
    referenced_names = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Attribute):
            if isinstance(node.value, ast.Name) and node.value.id == module_name:
                referenced_names.append(node.attr)

    return referenced_names

CLOSURE_SOURCE: Dict[str, str] = {}
====================================================================
-> Chunk: util\differ.py::1


from ell.configurator import config
from ell.lmp.simple import simple
import difflib

# Todo: update this for single change stuff so that it doesn't summarize small chage but says it specifically.
@simple(config.autocommit_model, temperature=0.2, exempt_from_tracking=True, max_tokens=500)
def write_commit_message_for_diff(old : str, new : str) -> str:
    """You are an expert programmer whose goal is to write commit messages based on diffs.
You will be given two version of source code and their unified diff.
You will be expected to write a commit message that describes the changes between the two versions.

Follow these guidelines:
1. Your commit message should be at most one sentence and highly specific to the changes made. Don't just discuss the functions changed but how they were specifically changed.
2. Your commit message cannot be more than 10 words so use sentence fragments and be concise.
3. The @ell.simple decorator turns a function into a call to a language model program: the function's docstring is the system prompt and the string returned is the user prompt. 
4. It is extremely important that if the system prompt or user prompt changes, your commit message must say what specifically changed, rather than vaguely saying they were updated or changed.
5. It is extremely important that you never refer to a @ell.simple docstring as a docstring: it is a system prompt. 
6. Do NOT say why a change was done, say what specifically changed.
7. Consider all changes ot the program including the globals and free variables

Example response:
'''
Update model temperature and refine system prompt wording:
* Changed temperature from 0.5 to 0.7.
* Updated "with specificity, brevity, and good grammar" to "clearly and concisely" in system prompt.
* The `questions` param was assigned type List[Question]
'''
Response format:
<Short commit message summarizing all the changes with specificity>:
* <Bulleted list of each specific change>.
"""
    clean_program_of_all_bv_tags = lambda program : program.replace("#<BV>", "").replace("#</BV>", "").replace("#<BmV>", "").replace("#</BmV>", "")
    old_clean = clean_program_of_all_bv_tags(old)
    new_clean = clean_program_of_all_bv_tags(new)

    diff = "\n".join(difflib.unified_diff(old_clean.splitlines(), new_clean.splitlines(), lineterm=''))

    return f"""Write a commit message succinctly and specifically describing the changes between these two versions of a program.
Old version:
```
{old_clean}
```

New version:
```
{new_clean}
```

Unified diff:
{diff}
"""
====================================================================
-> Chunk: src\conf.py::2


html_theme_options = {
    "show_prev_next": True,
    "show_scrolltop": True,
    "main_nav_links": {
        "Docs": "index",
        "API Reference": "reference/index",
        "AI Jobs Board": "https://jobs.ell.so",
    },
    "extra_header_link_icons": {
        "Discord": {
        "link": "https://discord.gg/vWntgU52Xb",
            "icon": """<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" height="18" fill="currentColor"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M524.5 69.8a1.5 1.5 0 0 0 -.8-.7A485.1 485.1 0 0 0 404.1 32a1.8 1.8 0 0 0 -1.9 .9 337.5 337.5 0 0 0 -14.9 30.6 447.8 447.8 0 0 0 -134.4 0 309.5 309.5 0 0 0 -15.1-30.6 1.9 1.9 0 0 0 -1.9-.9A483.7 483.7 0 0 0 116.1 69.1a1.7 1.7 0 0 0 -.8 .7C39.1 183.7 18.2 294.7 28.4 404.4a2 2 0 0 0 .8 1.4A487.7 487.7 0 0 0 176 479.9a1.9 1.9 0 0 0 2.1-.7A348.2 348.2 0 0 0 208.1 430.4a1.9 1.9 0 0 0 -1-2.6 321.2 321.2 0 0 1 -45.9-21.9 1.9 1.9 0 0 1 -.2-3.1c3.1-2.3 6.2-4.7 9.1-7.1a1.8 1.8 0 0 1 1.9-.3c96.2 43.9 200.4 43.9 295.5 0a1.8 1.8 0 0 1 1.9 .2c2.9 2.4 6 4.9 9.1 7.2a1.9 1.9 0 0 1 -.2 3.1 301.4 301.4 0 0 1 -45.9 21.8 1.9 1.9 0 0 0 -1 2.6 391.1 391.1 0 0 0 30 48.8 1.9 1.9 0 0 0 2.1 .7A486 486 0 0 0 610.7 405.7a1.9 1.9 0 0 0 .8-1.4C623.7 277.6 590.9 167.5 524.5 69.8zM222.5 337.6c-29 0-52.8-26.6-52.8-59.2S193.1 219.1 222.5 219.1c29.7 0 53.3 26.8 52.8 59.2C275.3 311 251.9 337.6 222.5 337.6zm195.4 0c-29 0-52.8-26.6-52.8-59.2S388.4 219.1 417.9 219.1c29.7 0 53.3 26.8 52.8 59.2C470.7 311 447.5 337.6 417.9 337.6z"/></svg>""",
            "type": "font-awesome",
            "name": "Discord",
        },
    },

    "logo_light": "_static/ell-wide-light.png",
    "logo_dark": "_static/ell-wide-dark.png",
    
}

html_static_path = ['_static']



templates_path = ['_templates']
====================================================================


###### Cluster Eval ######
Score: 3.0
Cluster name: Tool Creation and Validation Cluster
Tool Creation and Validation Cluster:

-> Chunk: lmp\tool.py::2


def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        _under_fn = fn

        @wraps(fn)
        def wrapper(
            *fn_args,
            _invocation_origin: str = None,
            _tool_call_id: str = None,
            **fn_kwargs
        ):
            #XXX: Post release, we need to wrap all tool arguments in type primitives for tracking I guess or change that tool makes the tool function inoperable.
            #XXX: Most people are not going to manually try and call the tool without a type primitive and if they do it will most likely be wrapped with l strs.

            if config.verbose and not exempt_from_tracking:
                pass
                # tool_usage_logger_pre(fn, fn_args, fn_kwargs, name, color)

            result = fn(*fn_args, **fn_kwargs)

            _invocation_api_params = dict(tool_kwargs=tool_kwargs)

            # Here you might want to add logic for tracking the tool usage
            # Similar to how it's done in the lm decorator # Use _invocation_origin

            if isinstance(result, str) and _invocation_origin:
                result = _lstr(result,origin_trace=_invocation_origin)

            #XXX: This _tool_call_id thing is a hack. Tracking should happen via params in the api
            # So if you call wiuth a _tool_callId
            if _tool_call_id:
                # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
                try:
                    if isinstance(result, ContentBlock):
                        content_results = [result]
                    elif isinstance(result, list) and all(isinstance(c, ContentBlock) for c in result):
                        content_results = result
                    else:
                        content_results = [ContentBlock(text=_lstr(json.dumps(result, ensure_ascii=False),origin_trace=_invocation_origin))]
                except TypeError as e:
                    raise TypeError(f"Failed to convert tool use result to ContentBlock: {e}. Tools must return json serializable objects. or a list of ContentBlocks.")
                # XXX: Need to support images and other content types somehow. We should look for images inside of the the result and then go from there.
                # try:
                #     content_results = coerce_content_list(result)
                # except ValueError as e:

                # TODO: poolymorphic validation here is important (cant have tool_call or formatted_response in the result)
                # XXX: Should we put this coercion here or in the tool call/result area.
                for c in content_results:
                    assert not c.tool_call, "Tool call in tool result"
                    # assert not c.formatted_response, "Formatted response in tool result"
                    if c.parsed:
                        # Warning: Formatted response in tool result will be converted to text
                        # TODO: Logging needs to produce not print.
                        print(f"Warning: Formatted response in tool result will be converted to text. Original: {c.parsed}")
                        c.text = _lstr(c.parsed.model_dump_json(),origin_trace=_invocation_origin)
                        c.parsed = None
                    assert not c.audio, "Audio in tool result"
                return ToolResult(tool_call_id=_tool_call_id, result=content_results), _invocation_api_params, {}
            else:
                return result, _invocation_api_params, {}
        # ... other code
    # ... other code
====================================================================
-> Chunk: lmp\tool.py::3


def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        # ... other code


        wrapper.__ell_tool_kwargs__ = tool_kwargs
        wrapper.__ell_func__ = _under_fn
        wrapper.__ell_type__ = LMPType.TOOL
        wrapper.__ell_exempt_from_tracking = exempt_from_tracking

        # Construct the pydantic mdoel for the _under_fn's function signature parameters.
        # 1. Get the function signature.

        sig = inspect.signature(fn)

        # 2. Create a dictionary of field definitions for the Pydantic model
        fields = {}
        for param_name, param in sig.parameters.items():
            # Skip *args and **kwargs
            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                continue

            # Determine the type annotation
            if param.annotation == inspect.Parameter.empty:
                raise ValueError(f"Parameter {param_name} has no type annotation, and cannot be converted into a tool schema for OpenAI and other provisders. Should OpenAI produce a string or an integer, etc, for this parameter?")
            annotation = param.annotation

            # Determine the default value
            default = param.default

            # Check if the parameter has a Field with description
            if isinstance(param.default, FieldInfo):
                field = param.default
                fields[param_name] = (annotation, field)
            elif param.default != inspect.Parameter.empty:
                fields[param_name] = (annotation, param.default)
            else:
                # If no default value, use Field without default
                fields[param_name] = (annotation, Field(...))

        # 3. Create the Pydantic model
        model_name = f"{fn.__name__}"
        ParamsModel = create_model(model_name, **fields)

        # Attach the Pydantic model to the wrapper function
        wrapper.__ell_params_model__ = ParamsModel

        # handle tracking last.
        if exempt_from_tracking:
            ret = wrapper
        else:
            ret=  _track(wrapper)

        # Helper function to get the Pydantic model for the tool
        def get_params_model():
            return wrapper.__ell_params_model__

        # Attach the helper function to the wrapper
        wrapper.get_params_model = get_params_model
        ret.get_params_model = get_params_model
        return ret

    return tool_decorator
====================================================================
-> Chunk: lmp\tool.py::4


tool.__doc__ = """Defines a tool for use in language model programs (LMPs) that support tool use.

This decorator wraps a function, adding metadata and handling for tool invocations.
It automatically extracts the tool's description and parameters from the function's
docstring and type annotations, creating a structured representation for LMs to use.

:param exempt_from_tracking: If True, the tool usage won't be tracked. Default is False.
:type exempt_from_tracking: bool
:param tool_kwargs: Additional keyword arguments for tool configuration.
:return: A wrapped version of the original function, usable as a tool by LMs.
:rtype: Callable

Requirements:

- Function must have fully typed arguments (Pydantic-serializable).
- Return value must be one of: str, JSON-serializable object, Pydantic model, or List[ContentBlock].
- All parameters must have type annotations.
- Complex types should be Pydantic models.
- Function should have a descriptive docstring.
- Can only be used in LMPs with @ell.complex decorators

Functionality:

1. Metadata Extraction:
    - Uses function docstring as tool description.
    - Extracts parameter info from type annotations and docstring.
    - Creates a Pydantic model for parameter validation and schema generation.

2. Integration with LMs:
    - Can be passed to @ell.complex decorators.
    - Provides structured tool information to LMs.

3. Invocation Handling:
    - Manages tracking, logging, and result processing.
    - Wraps results in appropriate types (e.g., _lstr) for tracking.

Usage Modes:

1. Normal Function Call:
    - Behaves like a regular Python function.
    - Example: result = my_tool(arg1="value", arg2=123)

2. LMP Tool Call:
    - Used within LMPs or with explicit _tool_call_id.
    - Returns a ToolResult object.
    - Example: result = my_tool(arg1="value", arg2=123, _tool_call_id="unique_id")

Result Coercion:

- String  ContentBlock(text=result)
- Pydantic BaseModel  ContentBlock(parsed=result)
- List[ContentBlock]  Used as-is
- Other types  ContentBlock(text=json.dumps(result))

Example::

    @ell.tool()
    def create_claim_draft(
        claim_details: str,
        claim_type: str,
        claim_amount: float,
        claim_date: str = Field(description="Date format: YYYY-MM-DD")
    ) -> str:
        '''Create a claim draft. Returns the created claim ID.'''
        return "12345"

    # For use in a complex LMP:
    @ell.complex(model="gpt-4", tools=[create_claim_draft], temperature=0.1)
    def insurance_chatbot(message_history: List[Message]) -> List[Message]:
        # Chatbot implementation...

    x = insurance_chatbot([
        ell.user("I crashed my car into a tree."),
        ell.assistant("I'm sorry to hear that. Can you provide more details?"),
        ell.user("The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000")
    ]) 
    print(x)
    '''ell.Message(content=[
        ContentBlock(tool_call(
            tool_call_id="asdas4e",
            tool_fn=create_claim_draft,
            input=create_claim_draftParams({
                claim_details="The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000",
                claim_type="car",
                claim_amount=5000,
                claim_date="2024-08-01"
            })
        ))
    ], role='assistant')'''
    
    if x.tool_calls:
        next_user_message = response_message.call_tools_and_collect_as_message()
        # This actually calls create_claim_draft
        print(next_user_message)
        '''
        ell.Message(content=[
            ContentBlock(tool_result=ToolResult(
                tool_call_id="asdas4e",
                result=[ContentBlock(text="12345")]
            ))
        ], role='user')
        '''
        y = insurance_chatbot(message_history + [x, next_user_message])
        print(y)
        '''
        ell.Message("I've filed that for you!", role='assistant')
        '''

Note:
- Tools are integrated into LMP calls via the 'tools' parameter in @ell.complex.
- LMs receive structured tool information, enabling understanding and usage within the conversation context.
    """
====================================================================
-> Chunk: types\message.py::14


class Message(BaseModel):

    def call_tools_and_collect_as_message(self, parallel=False, max_workers=None):
        if parallel:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(c.tool_call.call_and_collect_as_content_block) for c in self.content if c.tool_call]
                content = [future.result() for future in as_completed(futures)]
        else:
            content = [c.tool_call.call_and_collect_as_content_block() for c in self.content if c.tool_call]
        return Message(role="user", content=content)
====================================================================
-> Chunk: util\closure.py::1


"""
This should do the following.
# prompt_consts.py
import math
def test():
    return math.sin(10)

# lol3.py
import prompt_consts

X = 7
def xD():
    print(X)
    return prompt_consts.test()

###
Our goal is to use AST & dill to get a full lexical closured source of xD, with the exception of modules that are stored in site-packages. For example.

lexical_extration(xD) returns
#closure.py
import math
def test():
    return math.sin(10)

X = 7 
def xD():
    print(X)
    return test()

"""
import collections
import ast
import hashlib
import itertools
from typing import Any, Dict, Iterable, Optional, Set, Tuple, Callable
import dill
import inspect
import types
from dill.source import getsource
import re
from collections import deque
import black

from ell.util.serialization import is_immutable_variable
from ell.util.should_import import should_import

DELIM = "$$$$$$$$$$$$$$$$$$$$$$$$$"
FORBIDDEN_NAMES = ["ell", "lstr"]
====================================================================


###### Cluster Eval ######
Score: 3.0
Cluster name: Data Handling and Timestamp Management Cluster
Data Handling and Timestamp Management Cluster:

-> Chunk: types\studio.py::1


from datetime import datetime, timezone
import enum
from functools import cached_property

import sqlalchemy.types as types

from ell.types.message import Any, Any, Field, Message, Optional

from sqlmodel import Column, Field, SQLModel
from typing import Optional
from dataclasses import dataclass
from typing import Dict, List, Literal, Union, Any, Optional

from pydantic import BaseModel, field_validator

from datetime import datetime
from typing import Any, List, Optional
from sqlmodel import Field, SQLModel, Relationship, JSON, Column
from sqlalchemy import Index, func

from typing import TypeVar, Any

def utc_now() -> datetime:
    """
    Returns the current UTC timestamp.
    Serializes to ISO-8601.
    """
    return datetime.now(tz=timezone.utc)
====================================================================
-> Chunk: types\studio.py::2


class SerializedLMPUses(SQLModel, table=True):
    """
    Represents the many-to-many relationship between SerializedLMPs.

    This class is used to track which LMPs use or are used by other LMPs.
    """

    lmp_user_id: Optional[str] = Field(default=None, foreign_key="serializedlmp.lmp_id", primary_key=True, index=True)  # ID of the LMP that is being used
    lmp_using_id: Optional[str] = Field(default=None, foreign_key="serializedlmp.lmp_id", primary_key=True, index=True)  # ID of the LMP that is using the other LMP
====================================================================
-> Chunk: studio\server.py::6


def create_app(config:Config):
    # ... other code

    @app.get("/api/lmp-history")
    def get_lmp_history(
        days: int = Query(365, ge=1, le=3650),  # Default to 1 year, max 10 years
        session: Session = Depends(get_session)
    ):
        # Calculate the start date
        start_date = datetime.utcnow() - timedelta(days=days)

        # Query to get all LMP creation times within the date range
        query = (
            select(SerializedLMP.created_at)
            .where(SerializedLMP.created_at >= start_date)
            .order_by(SerializedLMP.created_at)
        )

        results = session.exec(query).all()

        # Convert results to a list of dictionaries
        history = [{"date": str(row), "count": 1} for row in results]

        return history
    # ... other code
====================================================================
-> Chunk: studio\server.py::5


def create_app(config:Config):
    # ... other code


    @app.get("/api/traces")
    def get_consumption_graph(
        session: Session = Depends(get_session)
    ):
        traces = serializer.get_traces(session)
        return traces



    @app.get("/api/blob/{blob_id}", response_class=Response)
    def get_blob(
        blob_id: str,
        session: Session = Depends(get_session)
    ):
        if serializer.blob_store is None:
            raise HTTPException(status_code=400, detail="Blob storage is not configured")
        try:
            blob_data = serializer.blob_store.retrieve_blob(blob_id)
            return Response(content=blob_data.decode('utf-8'), media_type="application/json")
        except FileNotFoundError:
            raise HTTPException(status_code=404, detail="Blob not found")
        except Exception as e:
            logger.error(f"Error retrieving blob: {str(e)}")
            raise HTTPException(status_code=500, detail="Internal server error")
    # ... other code
====================================================================
-> Chunk: stores\sql.py::7


class SQLStore(ell.store.Store):

    def get_invocations(self, session: Session, lmp_filters: Dict[str, Any], skip: int = 0, limit: int = 10, filters: Optional[Dict[str, Any]] = None, hierarchical: bool = False) -> List[Dict[str, Any]]:

        query = select(Invocation).join(SerializedLMP)

        # Apply LMP filters
        for key, value in lmp_filters.items():
            query = query.where(getattr(SerializedLMP, key) == value)

        # Apply invocation filters
        if filters:
            for key, value in filters.items():
                query = query.where(getattr(Invocation, key) == value)

        # Sort from newest to oldest
        query = query.order_by(Invocation.created_at.desc()).offset(skip).limit(limit)

        invocations = session.exec(query).all()
        return invocations
====================================================================


###### Cluster Eval ######
Score: 3.0
Cluster name: Message and Content Processing Cluster
Message and Content Processing Cluster:

-> Chunk: types\message.py::11


class Message(BaseModel):

    @property
    def audios(self) -> List[Union[np.ndarray, List[float]]]:
        """Returns a list of all audio content.

        Example:
            >>> audio1 = np.array([0.1, 0.2, 0.3])
            >>> audio2 = np.array([0.4, 0.5, 0.6])
            >>> message = Message(role="user", content=["Text", audio1, "More text", audio2])
            >>> len(message.audios)
            2
        """
        return [c.audio for c in self.content if c.audio]
====================================================================
-> Chunk: types\studio.py::4


class SerializedLMPBase(SQLModel):
    lmp_id: Optional[str] = Field(default=None, primary_key=True)
    name: str = Field(index=True)
    source: str
    dependencies: str
    created_at: datetime = UTCTimestampField(index=True, nullable=False)

    lmp_type: LMPType
    api_params: Optional[Dict[str, Any]] = Field(default_factory=dict, sa_column=Column(JSON))
    initial_free_vars: Optional[Dict[str, Any]] = Field(default_factory=dict, sa_column=Column(JSON))
    initial_global_vars: Optional[Dict[str, Any]] = Field(default_factory=dict, sa_column=Column(JSON))
    num_invocations: Optional[int] = Field(default=0)
    commit_message: Optional[str] = Field(default=None)
    version_number: Optional[int] = Field(default=None)
====================================================================
-> Chunk: providers\anthropic.py::3


def _content_block_to_anthropic_format(content_block: ContentBlock):
        if (image := content_block.image): return serialize_image_for_anthropic(image)
        elif ((text := content_block.text) is not None): return dict(type="text", text=text)
        elif (parsed := content_block.parsed):
            return dict(type="text", text=json.dumps(parsed.model_dump(), ensure_ascii=False))
        elif (tool_call := content_block.tool_call):
            return dict(
                type="tool_use",
                id=tool_call.tool_call_id,
                name=tool_call.tool.__name__,
                input=tool_call.params.model_dump()
            )
        elif (tool_result := content_block.tool_result):
            return dict(
                type="tool_result",
                tool_use_id=tool_result.tool_call_id,
                content=[_content_block_to_anthropic_format(c) for c in tool_result.result]
            )
        else:
            raise ValueError("Content block is not supported by anthropic")
====================================================================
-> Chunk: lmp\_track.py::1


import json
import logging
import threading
from ell.types import SerializedLMP, Invocation, InvocationTrace, InvocationContents
from ell.types.studio import LMPType, utc_now
from ell.util._warnings import _autocommit_warning
import ell.util.closure
from ell.configurator import config
from ell.types._lstr import _lstr

import inspect

import secrets
import time
from datetime import datetime
from functools import wraps
from typing import Any, Callable, Dict, Iterable, Optional, OrderedDict, Tuple

from ell.util.serialization import get_immutable_vars
from ell.util.serialization import compute_state_cache_key
from ell.util.serialization import prepare_invocation_params

logger = logging.getLogger(__name__)

# Thread-local storage for the invocation stack
_invocation_stack = threading.local()

def get_current_invocation() -> Optional[str]:
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    return _invocation_stack.stack[-1] if _invocation_stack.stack else None

def push_invocation(invocation_id: str):
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    _invocation_stack.stack.append(invocation_id)

def pop_invocation():
    if hasattr(_invocation_stack, 'stack') and _invocation_stack.stack:
        _invocation_stack.stack.pop()
====================================================================
-> Chunk: types\_lstr.py::10


class _lstr(str):

    def __getattribute__(self, name: str) -> Union[Callable, Any]:
        """
        Get an attribute from this lstr instance.

        Args:
            name (str): The name of the attribute to retrieve.

        Returns:
            Union[Callable, Any]: The requested attribute, which may be a method or a value.
        """
        # Get the attribute from the superclass (str)
        # First, try to get the attribute from the current class instance

        # Get the attribute using the superclass method
        attr = super().__getattribute__(name)

        # Check if the attribute is a callable and not defined in lstr class itself

        if name == "__class__":
            return type(self)

        if callable(attr) and name not in _lstr.__dict__:
            def wrapped(*args: Any, **kwargs: Any) -> Any:
                result = attr(*args, **kwargs)
                # If the result is a string, return an lstr instance
                if isinstance(result, str):
                    origin_traces = self.__origin_trace__
                    for arg in args:
                        if isinstance(arg, _lstr):
                            origin_traces = origin_traces.union(arg.__origin_trace__)
                    for key, value in kwargs.items():
                        if isinstance(value, _lstr):
                            origin_traces = origin_traces.union(value.__origin_trace__)
                    return _lstr(result, None, origin_traces)

                return result

            return wrapped

        return attr
====================================================================


###### Cluster Eval ######
Score: 2.6666666666666665
Cluster name: Code Dependency and Analysis Cluster
Code Dependency and Analysis Cluster:

-> Chunk: util\closure_util.py::3


def get_referenced_names(code: str, module_name: str):
    """
    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)
    and walks through the tree to find all instances where an attribute of the module is referenced in the code.

    Parameters:
    code (str): The block of code to be parsed.
    module_name (str): The name of the module to look for in the code.

    Returns:
    list: A list of all attributes of the module that are referenced in the code.
    """
    tree = ast.parse(code)
    referenced_names = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Attribute):
            if isinstance(node.value, ast.Name) and node.value.id == module_name:
                referenced_names.append(node.attr)

    return referenced_names
====================================================================
-> Chunk: util\closure_util.py::2


def is_function_called(func_name, source_code):
    """
    Check if a function is called in the given source code.

    Parameters:
    func_name (str): The name of the function to check.
    source_code (str): The source code to check.

    Returns:
    bool: True if the function is called, False otherwise.
    """
    # Parse the source code into an AST
    tree = ast.parse(source_code)

    # Walk through all the nodes in the AST
    for node in ast.walk(tree):
        # If the node is a function call
        if isinstance(node, ast.Call):
            # If the function being called is the function we're looking for
            if isinstance(node.func, ast.Name) and node.func.id == func_name:
                return True

    # If we've gone through all the nodes and haven't found a call to the function, it's not called
    return False
====================================================================
-> Chunk: util\closure.py::7


def _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack , uses):
    """Process a single variable."""
    try:
        name = inspect.getmodule(var_value).__name__
        if should_import(name):
            imports.append(dill.source.getimport(var_value, alias=var_name))
            return
    except:
        pass

    if isinstance(var_value, (types.FunctionType, type, types.MethodType)):
        _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses)
    elif isinstance(var_value, types.ModuleType):
        _process_module(var_name, var_value, modules, imports, uses)
    elif isinstance(var_value, types.BuiltinFunctionType):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        _process_other_variable(var_name, var_value, dependencies, uses)
====================================================================
-> Chunk: util\closure.py::13


def _update_ell_func(outer_ell_func, source, dsrc, globals_dict, frees_dict, fn_hash, uses):
    """Update the ell function attributes."""
    formatted_source = _format_source(source)
    formatted_dsrc = _format_source(dsrc)

    if hasattr(outer_ell_func, "__ell_func__"):

        outer_ell_func.__ell_closure__ = (formatted_source, formatted_dsrc, globals_dict, frees_dict)
        outer_ell_func.__ell_hash__ = fn_hash
        outer_ell_func.__ell_uses__ = uses

def _raise_error(message, exception, recursion_stack):
    """Raise an error with detailed information."""
    error_msg = f"{message}. Error: {str(exception)}\n"
    error_msg += f"Recursion stack: {' -> '.join(recursion_stack)}"
    # print(error_msg)
    raise Exception(error_msg)
====================================================================
-> Chunk: util\closure_util.py::1


import ast
import importlib
import os
import black
from dill.detect import nestedglobals

import inspect

import inspect

#!/usr/bin/env python
#
def globalvars(func, recurse=True, builtin=False):
    """get objects defined in global scope that are referred to by func

    return a dict of {name:object}"""
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__
    if inspect.ismethod(func): func = func.__func__
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__
    if inspect.isfunction(func):
        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}
        # get references from within closure
        orig_func, func = func, set()
        for obj in orig_func.__closure__ or {}:
            try:
                cell_contents = obj.cell_contents
            except ValueError: # cell is empty
                pass
            else:
                _vars = globalvars(cell_contents, recurse, builtin) or {}
                func.update(_vars) #XXX: (above) be wary of infinte recursion?
                globs.update(_vars)
        # get globals
        globs.update(orig_func.__globals__ or {})
        # get names of references
        if not recurse:
            func.update(orig_func.__code__.co_names)
        else:
            func.update(nestedglobals(orig_func.__code__))
            # find globals for all entries of func
            for key in func.copy(): #XXX: unnecessary...?
                nested_func = globs.get(key)
                if nested_func is orig_func:
                   #func.remove(key) if key in func else None
                    continue  #XXX: globalvars(func, False)?
                func.update(globalvars(nested_func, True, builtin))
    elif inspect.iscode(func):
        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}
       #globs.update(globals())
        if not recurse:
            func = func.co_names # get names
        else:
            orig_func = func.co_name # to stop infinite recursion
            func = set(nestedglobals(func))
            # find globals for all entries of func
            for key in func.copy(): #XXX: unnecessary...?
                if key is orig_func:
                   #func.remove(key) if key in func else None
                    continue  #XXX: globalvars(func, False)?
                nested_func = globs.get(key)
                func.update(globalvars(nested_func, True, builtin))
    else:
        return {}
    #NOTE: if name not in __globals__, then we skip it...
    return dict((name,globs[name]) for name in func if name in globs)
====================================================================


###### Cluster Eval ######
Score: 2.6666666666666665
Cluster name: Anthropic Model Registration and Integration Cluster
Anthropic Model Registration and Integration Cluster:

-> Chunk: models\anthropic.py::1


from ell.configurator import config
import logging

logger = logging.getLogger(__name__)


try:
    import anthropic

    def register(client: anthropic.Anthropic):
        """
        Register Anthropic models with the provided client.

        This function takes an Anthropic client and registers various Anthropic models
        with the global configuration. It allows the system to use these models
        for different AI tasks.

        Args:
            client (anthropic.Anthropic): An instance of the Anthropic client to be used
                                          for model registration.

        Note:
            The function doesn't return anything but updates the global
            configuration with the registered models.
        """
        model_data = [
            ('claude-3-opus-20240229', 'anthropic'),
            ('claude-3-sonnet-20240229', 'anthropic'),
            ('claude-3-haiku-20240307', 'anthropic'),
            ('claude-3-5-sonnet-20240620', 'anthropic'),
        ]
        for model_id, owned_by in model_data:
            config.register_model(model_id, client)

    try:
        default_client = anthropic.Anthropic()
        register(default_client)
    except Exception as e:
        # logger.warning(f"Failed to create default Anthropic client: {e}")
        pass


except ImportError:
    pass
====================================================================
-> Chunk: providers\anthropic.py::1


from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent

from ell.types._lstr import _lstr
from ell.types.message import LMP
from ell.configurator import register_provider
from ell.util.serialization import serialize_image
import base64
from io import BytesIO
import json
import requests
from PIL import Image as PILImage

try:
    import anthropic
    from anthropic import Anthropic
    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent
    from anthropic.types.message_create_params import MessageCreateParamsStreaming
    from anthropic._streaming import Stream

    class AnthropicProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            return client.messages.create

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())
            # XXX: Helper, but should be depreicated due to ssot
            assert final_call_params.get("max_tokens") is not None, f"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}}))."

            dirty_msgs = [
                MessageParam(
                    role=cast(Literal["user", "assistant"], message.role), 
                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]
            role_correct_msgs   : List[MessageParam] = []
            for msg in dirty_msgs:
                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):
                    role_correct_msgs.append(msg)
                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])

            system_message = None
            if role_correct_msgs and role_correct_msgs[0]["role"] == "system":
                system_message = role_correct_msgs.pop(0)

            if system_message:
                final_call_params["system"] = system_message["content"][0]["text"]


            final_call_params['stream'] = True
            final_call_params["model"] = ell_call.model
            final_call_params["messages"] = role_correct_msgs

            if ell_call.tools:
                final_call_params["tools"] = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        name=tool.__name__,
                        description=tool.__doc__,
                        input_schema=tool.__ell_params_model__.model_json_schema(),
                    )
                    for tool in ell_call.tools
                ]

            # print(final_call_params)
            return final_call_params

        def translate_from_provider(
            self,
            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            usage = {}
            tracked_results = []
            metadata = {}

            #XXX: Support n > 0

            if provider_call_params.get("stream", False):
                content = []
                current_blocks: Dict[int, Dict[str, Any]] = {}
                message_metadata = {}

                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:
                    for chunk in stream:
                        if chunk.type == "message_start":
                            message_metadata = chunk.message.model_dump()
                            message_metadata.pop("content", None)  # Remove content as we'll build it separately

                        elif chunk.type == "content_block_start":
                            block = chunk.content_block.model_dump()
                            current_blocks[chunk.index] = block
                            if block["type"] == "tool_use":
                                if logger: logger(f" <tool_use: {block['name']}(")
                                block["input"] = "" # force it to be a string, XXX: can implement partially parsed json later.
                        elif chunk.type == "content_block_delta":
                            if chunk.index in current_blocks:
                                block = current_blocks[chunk.index]
                                if (delta := chunk.delta).type == "text_delta":
                                    block["text"] += delta.text
                                    if logger: logger(delta.text)
                                if delta.type == "input_json_delta":
                                    block["input"] += delta.partial_json
                                    if logger: logger(delta.partial_json)

                        elif chunk.type == "content_block_stop":
                            if chunk.index in current_blocks:
                                block = current_blocks.pop(chunk.index)
                                if block["type"] == "text":
                                    content.append(ContentBlock(text=_lstr(block["text"],origin_trace=origin_id)))
                                elif block["type"] == "tool_use":
                                    try:
                                        matching_tool = ell_call.get_tool_by_name(block["name"])
                                        if matching_tool:
                                            content.append(
                                                ContentBlock(
                                                    tool_call=ToolCall(
                                                        tool=matching_tool,
                                                        tool_call_id=_lstr(
                                                            block['id'],origin_trace=origin_id
                                                        ),
                                                        params=json.loads(block['input']) if block['input'] else {},
                                                    )
                                                )
                                            )
                                    except json.JSONDecodeError:
                                        if logger: logger(f" - FAILED TO PARSE JSON")
                                        pass
                                    if logger: logger(f")>")

                        elif chunk.type == "message_delta":
                            message_metadata.update(chunk.delta.model_dump())
                            if chunk.usage:
                                usage.update(chunk.usage.model_dump())

                        elif chunk.type == "message_stop":
                            tracked_results.append(Message(role="assistant", content=content))

                        # print(chunk)
                metadata = message_metadata

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("input_tokens", 0)
            usage["completion_tokens"] = usage.get("output_tokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata

    # XXX: Make a singleton.
    anthropic_provider = AnthropicProvider()
    register_provider(anthropic_provider, anthropic.Anthropic)
    register_provider(anthropic_provider, anthropic.AnthropicBedrock)
    register_provider(anthropic_provider, anthropic.AnthropicVertex)

except ImportError:
    pass
====================================================================
-> Chunk: models\bedrock.py::1


from typing import Any
from ell.configurator import config
import logging

logger = logging.getLogger(__name__)


def register(client: Any):
    """
    Register Bedrock models with the provided client.

    This function takes an boto3 client and registers various Bedrock models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (boto3.client): An instance of the bedrock client to be used
                                        for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    model_data = [
        ('anthropic.claude-3-opus-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-sonnet-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-haiku-20240307-v1:0', 'bedrock'),
        ('anthropic.claude-3-5-sonnet-20240620-v1:0', 'bedrock'),

        ('mistral.mistral-7b-instruct-v0:2', 'bedrock'),
        ('mistral.mixtral-8x7b-instruct-v0:1', 'bedrock'),
        ('mistral.mistral-large-2402-v1:0', 'bedrock'),
        ('mistral.mistral-small-2402-v1:0', 'bedrock'),


        ('ai21.jamba-instruct-v1:0','bedrock'),
        ('ai21.j2-ultra-v1', 'bedrock'),
        ('ai21.j2-mid-v1', 'bedrock'),

        ('amazon.titan-embed-text-v1', 'bedrock'),
        ('amazon.titan-text-lite-v1', 'bedrock'),
        ('amazon.titan-text-express-v1', 'bedrock'),
        ('amazon.titan-image-generator-v2:0', 'bedrock'),
        ('amazon.titan-image-generator-v1', 'bedrock'),

        ('cohere.command-r-plus-v1:0', 'bedrock'),
        ('cohere.command-r-v1:0', 'bedrock'),
        ('cohere.embed-english-v3', 'bedrock'),
        ('cohere.embed-multilingual-v3', 'bedrock'),
        ('cohere.command-text-v14', 'bedrock'),

        ('meta.llama3-8b-instruct-v1:0', 'bedrock'),
        ('meta.llama3-70b-instruct-v1:0', 'bedrock'),
        ('meta.llama2-13b-chat-v1', 'bedrock'),
        ('meta.llama2-70b-chat-v1', 'bedrock'),
        ('meta.llama2-13b-v1', 'bedrock'),

    ]

    for model_id, owned_by in model_data:
        config.register_model(name=model_id, default_client=client, supports_streaming=True)

default_client = None
try:

    import boto3
    default_client = boto3.client('bedrock-runtime')
except Exception as e:
    pass

register(default_client)
====================================================================
-> Chunk: models\groq.py::1


from typing import Optional
from ell.configurator import config

try:
    from groq import Groq
    def register(client: Optional[Groq] = None, **client_kwargs):
        if client is None:
            client = Groq(**client_kwargs)
        for model in client.models.list().data:
            config.register_model(model.id, default_client=client, supports_streaming=True)
except ImportError:
    pass
====================================================================
-> Chunk: models\openai.py::2


def register(client: openai.Client):
    """
    Register OpenAI models with the provided client.

    This function takes an OpenAI client and registers various OpenAI models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (openai.Client): An instance of the OpenAI client to be used
                                for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    #XXX: Deprecation in 0.1.0
    standard_models = [
        'gpt-4-1106-preview',
        'gpt-4-32k-0314',
        'text-embedding-3-large',
        'gpt-4-0125-preview',
        'babbage-002',
        'gpt-4-turbo-preview',
        'gpt-4o',
        'gpt-4o-2024-05-13',
        'gpt-4o-mini-2024-07-18',
        'gpt-4o-mini',
        'gpt-4o-2024-08-06',
        'gpt-3.5-turbo-0301',
        'gpt-3.5-turbo-0613',
        'tts-1',
        'gpt-3.5-turbo',
        'gpt-3.5-turbo-16k',
        'davinci-002',
        'gpt-3.5-turbo-16k-0613',
        'gpt-4-turbo-2024-04-09',
        'gpt-3.5-turbo-0125',
        'gpt-4-turbo',
        'gpt-3.5-turbo-1106',
        'gpt-3.5-turbo-instruct-0914',
        'gpt-3.5-turbo-instruct',
        'gpt-4-0613',
        'gpt-4',
        'gpt-4-0314',
        'gpt-4o-audio-preview',
        'gpt-4o-realtime',
    ]
    for model_id in standard_models:
        config.register_model(model_id, client)

    #XXX: Deprecation in 0.1.0
    config.register_model('o1-preview', client, supports_streaming=False)
    config.register_model('o1-mini', client, supports_streaming=False)

default_client = None
try:
    default_client = openai.Client()
except openai.OpenAIError as e:
    pass

register(default_client)
config.default_client = default_client
====================================================================


###### Cluster Eval ######
Score: 2.6666666666666665
Cluster name: Invocation Data and Serialization Cluster
Invocation Data and Serialization Cluster:

-> Chunk: studio\connection_manager.py::1


from fastapi import WebSocket


class ConnectionManager:
    def __init__(self):
        self.active_connections = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            print(f"Broadcasting message to {connection} {message}")
            await connection.send_text(message)
====================================================================
-> Chunk: ell\configurator.py::9


# Existing helper functions
def get_store() -> Union[Store, None]:
    return config.store

# Will be deprecated at 0.1.0 

# You can add more helper functions here if needed
def register_provider(provider: Provider, client_type: Type[Any]) -> None:
    return config.register_provider(provider, client_type)

# Deprecated now (remove at 0.1.0)
def set_store(*args, **kwargs) -> None:
    raise DeprecationWarning("The set_store function is deprecated and will be removed in a future version. Use ell.init(store=...) instead.")
====================================================================
-> Chunk: lmp\_track.py::3


def _serialize_lmp(func):
    # Serialize deptjh first all fo the used lmps.
    for f in func.__ell_uses__:
        _serialize_lmp(f)

    if getattr(func, "_has_serialized_lmp", False):
        return
    func._has_serialized_lmp = False
    fn_closure = func.__ell_closure__
    lmp_type = func.__ell_type__
    name = func.__qualname__
    api_params = getattr(func, "__ell_api_params__", None)

    lmps = config.store.get_versions_by_fqn(fqn=name)
    version = 0
    already_in_store = any(lmp.lmp_id == func.__ell_hash__ for lmp in lmps)

    if not already_in_store:
        commit = None
        if lmps:
            latest_lmp = max(lmps, key=lambda x: x.created_at)
            version = latest_lmp.version_number + 1
            if config.autocommit:
                # XXX: Move this out to autocommit itself.
                if not _autocommit_warning():
                    from ell.util.differ import write_commit_message_for_diff
                    commit = str(write_commit_message_for_diff(
                    f"{latest_lmp.dependencies}\n\n{latest_lmp.source}", 
                        f"{fn_closure[1]}\n\n{fn_closure[0]}")[0])

        serialized_lmp = SerializedLMP(
            lmp_id=func.__ell_hash__,
            name=name,
            created_at=utc_now(),
            source=fn_closure[0],
            dependencies=fn_closure[1],
            commit_message=commit,
            initial_global_vars=get_immutable_vars(fn_closure[2]),
            initial_free_vars=get_immutable_vars(fn_closure[3]),
            lmp_type=lmp_type,
            api_params=api_params if api_params else None,
            version_number=version,
        )
        config.store.write_lmp(serialized_lmp, [f.__ell_hash__ for f in func.__ell_uses__])
    func._has_serialized_lmp = True
====================================================================
-> Chunk: types\studio.py::8


class InvocationContentsBase(SQLModel):

    @cached_property
    def should_externalize(self) -> bool:
        import json

        json_fields = [
            self.params,
            self.results,
            self.invocation_api_params,
            self.global_vars,
            self.free_vars
        ]

        total_size = sum(
            len(json.dumps(field, default=(lambda x: json.dumps(x.model_dump(), default=str, ensure_ascii=False)
                                           if isinstance(x, BaseModel) else str(x)), ensure_ascii=False).encode('utf-8'))
            for field in json_fields if field is not None
        )
        # print("total_size", total_size)

        return total_size > 102400  # Precisely 100kb in bytes

class InvocationContents(InvocationContentsBase, table=True):
    invocation: "Invocation" = Relationship(back_populates="contents")
====================================================================
-> Chunk: util\serialization.py::2


def get_immutable_vars(vars_dict):
    converter = cattrs.Converter()

    def handle_complex_types(obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj]
        elif isinstance(obj, dict):
            return {k: handle_complex_types(v) if not isinstance(v, (int, float, str, bool, type(None))) else v for k, v in obj.items()}
        elif isinstance(obj, (set, frozenset)):
            return list(sorted(handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj))
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return f"<Object of type {type(obj).__name__}>"

    converter.register_unstructure_hook(object, handle_complex_types)
    x = converter.unstructure(vars_dict)
    return x
====================================================================


Total coherence score: 3.6666666666666665
