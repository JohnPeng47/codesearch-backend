{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "from src.cluster.cluster import (\n",
    "    generate_full_code_clusters, \n",
    "    generate_summarized_clusters,\n",
    "    generate_graph_clusters,\n",
    "    generate_random_clusters\n",
    ")\n",
    "\n",
    "from src.cluster.types import (\n",
    "    CodeChunk,\n",
    "    SummaryChunk,\n",
    "    ClusterInput,\n",
    "    ClusteredTopic,\n",
    "    ClusterInputType,\n",
    "    LMClusteredTopicList\n",
    ")\n",
    "\n",
    "\n",
    "# repo_name = \"ell\"\n",
    "repo_name = \"CrashOffsetFinder\"\n",
    "repo_path = Path(\"../../src/cluster/repos\") / repo_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\CrashOffsetFinder\n",
      "[Chunker]: 33 chunks used\n",
      "Unclassified chunks, iter:[1]:  1\n",
      "Unclassified chunks, iter:[2]:  0\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\CrashOffsetFinder\n",
      "[Chunker]: 33 chunks used\n",
      "Unclassified chunks, iter:[1]:  3\n",
      "Unclassified chunks, iter:[2]:  0\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\CrashOffsetFinder\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\CrashOffsetFinder\n",
      "[Chunker]: 33 chunks used\n"
     ]
    }
   ],
   "source": [
    "# Generate clusters\n",
    "full_code_clusters = generate_full_code_clusters(repo_path)\n",
    "summary_clusters = generate_summarized_clusters(repo_path)\n",
    "graph_clusters = generate_graph_clusters(repo_path)\n",
    "random_clusers = generate_random_clusters(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUll Code:\n",
      "Total number of chunks: 33\n",
      "Mean chunks per cluster: 2.75\n",
      "Median chunks per cluster: 1.50\n",
      "Summary:\n",
      "Total number of chunks: 34\n",
      "Mean chunks per cluster: 2.83\n",
      "Median chunks per cluster: 2.50\n",
      "Graph:\n",
      "Total number of chunks: 21\n",
      "Mean chunks per cluster: 3.50\n",
      "Median chunks per cluster: 3.00\n",
      "Random:\n",
      "Total number of chunks: 16\n",
      "Mean chunks per cluster: 4.00\n",
      "Median chunks per cluster: 4.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print some cluster stats\n",
    "def get_cluster_stats(clusters: List[ClusteredTopic]):\n",
    "    # Calculate total number of chunks\n",
    "    total_chunks = sum(len(cluster.chunks) for cluster in clusters)\n",
    "    \n",
    "    # Calculate mean and median number of chunks per cluster\n",
    "    chunks_per_cluster = [len(cluster.chunks) for cluster in clusters]\n",
    "    mean_chunks = np.mean(chunks_per_cluster)\n",
    "    median_chunks = np.median(chunks_per_cluster)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total number of chunks: {total_chunks}\")\n",
    "    print(f\"Mean chunks per cluster: {mean_chunks:.2f}\")\n",
    "    print(f\"Median chunks per cluster: {median_chunks:.2f}\")\n",
    "    \n",
    "    # Create histogram\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.hist(chunks_per_cluster, bins='auto', edgecolor='black')\n",
    "    # plt.title('Histogram of Chunks per Cluster')\n",
    "    # plt.xlabel('Number of Chunks')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.show()\n",
    "\n",
    "print(\"FUll Code:\")\n",
    "get_cluster_stats(full_code_clusters)\n",
    "print(\"Summary:\")\n",
    "get_cluster_stats(summary_clusters)\n",
    "print(\"Graph:\")\n",
    "get_cluster_stats(graph_clusters)\n",
    "print(\"Random:\")\n",
    "get_cluster_stats(random_clusers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_code_ids = [chunk.id for cluster in full_code_clusters for chunk in cluster.chunks]\n",
    "summary_code_ids = [chunk.id for cluster in summary_clusters for chunk in cluster.chunks]\n",
    "graph_code_ids = [chunk.id for cluster in graph_clusters for chunk in cluster.chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full code clusters: 61\n",
      "Summary clusters: 38\n",
      "Graph clusters: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Full code clusters: {len(full_code_clusters)}\")\n",
    "print(f\"Summary clusters: {len(summary_clusters)}\")\n",
    "print(f\"Graph clusters: {len(graph_clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### EVALS ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full code crossfile score:  0.14814814814814814\n",
      "Summary crossfile score:  0.24444444444444444\n",
      "Graph crossfile score:  0.7305555555555555\n",
      "Random crossfile score:  1.75\n"
     ]
    }
   ],
   "source": [
    "from src.llm.evals.eval_cluster import eval_cross_file_cluster\n",
    "\n",
    "full_code_crossfile = eval_cross_file_cluster(full_code_clusters)\n",
    "print(\"Full code crossfile score: \", full_code_crossfile)\n",
    "\n",
    "summary_crossfile = eval_cross_file_cluster(summary_clusters)\n",
    "print(\"Summary crossfile score: \", summary_crossfile)\n",
    "\n",
    "graph_crossfile = eval_cross_file_cluster(graph_clusters)\n",
    "print(\"Graph crossfile score: \", graph_crossfile)\n",
    "\n",
    "random_crossfile = eval_cross_file_cluster(random_clusers)\n",
    "print(\"Random crossfile score: \", random_crossfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full code coherence score:  4.333333333333333\n"
     ]
    }
   ],
   "source": [
    "from src.llm.evals.eval_cluster import eval_coherence_cluster\n",
    "\n",
    "full_code_cohere = eval_coherence_cluster(full_code_clusters, output_file=\"full_code_cohere.txt\")\n",
    "# graph_cohere = eval_coherence_cluster(graph_clusters)\n",
    "\n",
    "print(\"Full code coherence score: \", full_code_cohere)\n",
    "# print(\"Graph coherence score: \", graph_cohere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-Time API and Client Management | Conversation APIs and Real-time Communication | 7\n",
      "Web Application and Server Setup | Interactive CLI with Visual Representation | 6\n",
      "Factorial Calculation | Interactive CLI with Visual Representation | 5\n",
      "Store and Database Management | SQL Store and Query Operations | 5\n",
      "Store Management and Configuration | Configuration and Initialization | 4\n",
      "User Input Validation and Formatting | OpenAI and LLM Capabilities | 3\n",
      "Reinforcement Learning Environment Setup and Evaluation | RL Training Using Gym | 3\n",
      "Data Collection and Processing in Reinforcement Learning | CBPO Reinforcement Learning Algorithm | 3\n",
      "Model Registration and Handling | Ell Language Modeling | 3\n",
      "Language Model and Prompt Handling | Language Model Decorator Utilities | 3\n",
      "Main Application and Serving | Studio Command-Line Interface | 3\n",
      "String and Data Manipulation | Basic Classes and Methods | 3\n",
      "Real-time Audio Handling | Real-time Client and Event Handling | 3\n",
      "Content and Message Processing | Model of Language Models | 3\n",
      "Logging and Debugging | Environment Settings and Commands | 3\n",
      "Real-Time Client and Communication | Conversation APIs and Real-time Communication | 3\n",
      "Policy Networks in Reinforcement Learning | CBPO Reinforcement Learning Algorithm | 2\n",
      "String and Factorial Operations | Basic Classes and Methods | 2\n",
      "Data Processing and Serialization | Persistent Storage and Serialization | 2\n",
      "Client and Provider Management | Basic APIs and Util Functions | 2\n",
      "Function Tracking and Invocation Logging | Invocation and Tracking | 2\n",
      "API Interactions and Middleware | Anthropic Interaction Implementation | 2\n",
      "Real-time Communication and Event Handling | Real-time API Client | 2\n",
      "Event Callback Management | Real-time Event Dispatch and Utilities | 2\n",
      "Data Serialization and Transformation | Persistent Storage and Serialization | 2\n",
      "Model and Invocation Management | Basic Classes and Methods | 2\n",
      "Custom Data Structures and String Operations | Processing and Mapping Functions | 2\n",
      "Configuration and Dependency Management | Lexical Closure and Dependency Management | 2\n",
      "Image and Graphics Processing | Plotting ASCII Art | 2\n",
      "NPM Operations and Testing | Build Process and Commands | 1\n",
      "OpenAI API Interaction | Real-time API Client | 1\n",
      "Abstract Syntax Tree (AST) Manipulation | Context Versioning Using AST | 1\n",
      "Code Cloning and Updation | Configuration and Initialization | 1\n",
      "Testing and Validation | Basic APIs and Util Functions | 1\n",
      "Network Connections and Logging | Markov Decision Process | 1\n",
      "Configuration Management | Interactive CLI with Visual Representation | 1\n",
      "Content Transformation and Serialization | Anthropic Interaction Implementation | 1\n",
      "Tool and Invocation Management | Basic Classes and Methods | 1\n"
     ]
    }
   ],
   "source": [
    "# full_code_ids contain the superset of all code chunks\n",
    "id_map = {chunk_id: i for i, chunk_id in enumerate(full_code_ids)}\n",
    "\n",
    "# match clusters to find ones with the most shared chunks\n",
    "def compare_clusters(cluster_a: List[ClusteredTopic], \n",
    "                 cluster_b: List[ClusteredTopic]) -> List[Tuple[ClusteredTopic, ClusteredTopic, int]]:\n",
    "    \"\"\"\n",
    "    Loops through all clusters to find the best match for each cluster in the other set.\n",
    "    \"\"\"\n",
    "    seen = []\n",
    "    matched_clusters = []\n",
    "    for i, a in enumerate(cluster_a):\n",
    "        best_match = None\n",
    "        best_score = -1\n",
    "        for b in cluster_b:\n",
    "            # if b.name in seen:\n",
    "            #     continue\n",
    "            \n",
    "            a_chunk_ids = [id_map[chunk.id] for chunk in a.chunks]\n",
    "            b_chunk_ids = [id_map[chunk.id] for chunk in b.chunks]\n",
    "            score = len(set(a_chunk_ids) & set(b_chunk_ids))\n",
    "\n",
    "            # if i == 12:\n",
    "            #     print(\"a chunks: \", [id_map[chunk.id] for chunk in a.chunks])\n",
    "            #     print(\"b chunks: \", [id_map[chunk.id] for chunk in b.chunks])\n",
    "            #     print(score)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = b\n",
    "        \n",
    "        if best_match: \n",
    "            matched_clusters.append((a, best_match, best_score))\n",
    "            seen.append(best_match.name)\n",
    "\n",
    "    return matched_clusters\n",
    "\n",
    "matched_clusters = compare_clusters(summary_clusters, full_code_clusters)\n",
    "for c1, c2, score in sorted(matched_clusters, key=lambda x: x[2], reverse=True):\n",
    "    print(f\"{c1.name} | {c2.name} | {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_by_name(clusters: List[ClusteredTopic], name: str) -> ClusteredTopic:\n",
    "    for cluster in clusters:\n",
    "        if cluster.name == name:\n",
    "            print(cluster.name)\n",
    "            for i, chunk in enumerate(cluster.chunks):\n",
    "                print(f\"Chunk {i} ----------------------------------\")\n",
    "                print(chunk.get_content())\n",
    "                if isinstance(chunk, SummaryChunk):\n",
    "                    print(chunk.get_filecontent())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Interactions and Middleware\n",
      "Chunk 0 ----------------------------------\n",
      "This code cluster defines functions related to API calls and parameter handling for a provider, including translation methods. It facilitates interaction with external APIs by managing parameters and logging.\n",
      "\n",
      "Provider:provider_call_function(api_call_params: Optional[Dict[str, Any]] = None)\n",
      "Provider:available_api_params(api_params: Optional[Dict[str, Any]] = None)\n",
      "Provider:translate_from_provider()\n",
      "Provider:translate_from_provider(origin_id: Optional[str] = None, logger: Optional[Callable[..., None]] = None)\n",
      "\n",
      "Chunk 1 ----------------------------------\n",
      "This code defines a function to serialize an image for use with the Anthropic API. It takes an ImageContent object as input to prepare the image data.\n",
      "serialize_image_for_anthropic(img : ImageContent)\n",
      "\n",
      "Chunk 2 ----------------------------------\n",
      "This code defines a function that converts a content block into a format suitable for the Anthropic API. It is primarily focused on transforming data structures for compatibility with external systems.\n",
      "_content_block_to_anthropic_format(content_block: ContentBlock)\n",
      "\n",
      "Chunk 3 ----------------------------------\n",
      "The code defines methods for a BedrockProvider class to handle API calls and translate responses from a provider. It manages parameters for API interactions and response processing.\n",
      "\n",
      "BedrockProvider:provider_call_function(api_call_params : Optional[Dict[str, Any]] = None)\n",
      "BedrockProvider:translate_from_provider(self, provider_response: Union[EventStream, Any], ell_call: EllCallParams, provider_call_params: Dict[str, Any], origin_id: Optional[str] = None, logger: Optional[Callable[..., None]] = None)\n",
      "\n",
      "Chunk 4 ----------------------------------\n",
      "This code defines a Message class with an initializer that can accept various types of content and a method to retrieve the text of the message.\n",
      "\n",
      "Message:__init__()\n",
      "Message:__init__(content: Union[AnyContent, List[AnyContent], None] = None)\n",
      "Message:text(self)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_cluster_by_name(summary_clusters, \"API Interactions and Middleware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic Interaction Implementation\n",
      "Chunk 0 ----------------------------------\n",
      "from ell.configurator import config\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "try:\n",
      "    import anthropic\n",
      "\n",
      "    def register(client: anthropic.Anthropic):\n",
      "        \"\"\"\n",
      "        Register Anthropic models with the provided client.\n",
      "\n",
      "        This function takes an Anthropic client and registers various Anthropic models\n",
      "        with the global configuration. It allows the system to use these models\n",
      "        for different AI tasks.\n",
      "\n",
      "        Args:\n",
      "            client (anthropic.Anthropic): An instance of the Anthropic client to be used\n",
      "                                          for model registration.\n",
      "\n",
      "        Note:\n",
      "            The function doesn't return anything but updates the global\n",
      "            configuration with the registered models.\n",
      "        \"\"\"\n",
      "        model_data = [\n",
      "            ('claude-3-opus-20240229', 'anthropic'),\n",
      "            ('claude-3-sonnet-20240229', 'anthropic'),\n",
      "            ('claude-3-haiku-20240307', 'anthropic'),\n",
      "            ('claude-3-5-sonnet-20240620', 'anthropic'),\n",
      "        ]\n",
      "        for model_id, owned_by in model_data:\n",
      "            config.register_model(model_id, client)\n",
      "\n",
      "    try:\n",
      "        default_client = anthropic.Anthropic()\n",
      "        register(default_client)\n",
      "    except Exception as e:\n",
      "        # logger.warning(f\"Failed to create default Anthropic client: {e}\")\n",
      "        pass\n",
      "\n",
      "\n",
      "except ImportError:\n",
      "    pass\n",
      "Chunk 1 ----------------------------------\n",
      "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast\n",
      "from ell.provider import  EllCallParams, Metadata, Provider\n",
      "from ell.types import Message, ContentBlock, ToolCall, ImageContent\n",
      "\n",
      "from ell.types._lstr import _lstr\n",
      "from ell.types.message import LMP\n",
      "from ell.configurator import register_provider\n",
      "from ell.util.serialization import serialize_image\n",
      "import base64\n",
      "from io import BytesIO\n",
      "import json\n",
      "import requests\n",
      "from PIL import Image as PILImage\n",
      "\n",
      "try:\n",
      "    import anthropic\n",
      "    from anthropic import Anthropic\n",
      "    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent\n",
      "    from anthropic.types.message_create_params import MessageCreateParamsStreaming\n",
      "    from anthropic._streaming import Stream\n",
      "\n",
      "    class AnthropicProvider(Provider):\n",
      "        dangerous_disable_validation = True\n",
      "\n",
      "        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\n",
      "            return client.messages.create\n",
      "\n",
      "        def translate_to_provider(self, ell_call : EllCallParams):\n",
      "            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())\n",
      "            # XXX: Helper, but should be depreicated due to ssot\n",
      "            assert final_call_params.get(\"max_tokens\") is not None, f\"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}})).\"\n",
      "\n",
      "            dirty_msgs = [\n",
      "                MessageParam(\n",
      "                    role=cast(Literal[\"user\", \"assistant\"], message.role), \n",
      "                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]\n",
      "            role_correct_msgs   : List[MessageParam] = []\n",
      "            for msg in dirty_msgs:\n",
      "                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):\n",
      "                    role_correct_msgs.append(msg)\n",
      "                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])\n",
      "\n",
      "            system_message = None\n",
      "            if role_correct_msgs and role_correct_msgs[0][\"role\"] == \"system\":\n",
      "                system_message = role_correct_msgs.pop(0)\n",
      "\n",
      "            if system_message:\n",
      "                final_call_params[\"system\"] = system_message[\"content\"][0][\"text\"]\n",
      "\n",
      "\n",
      "            final_call_params['stream'] = True\n",
      "            final_call_params[\"model\"] = ell_call.model\n",
      "            final_call_params[\"messages\"] = role_correct_msgs\n",
      "\n",
      "            if ell_call.tools:\n",
      "                final_call_params[\"tools\"] = [\n",
      "                    #XXX: Cleaner with LMP's as a class.\n",
      "                    dict(\n",
      "                        name=tool.__name__,\n",
      "                        description=tool.__doc__,\n",
      "                        input_schema=tool.__ell_params_model__.model_json_schema(),\n",
      "                    )\n",
      "                    for tool in ell_call.tools\n",
      "                ]\n",
      "\n",
      "            # print(final_call_params)\n",
      "            return final_call_params\n",
      "\n",
      "        def translate_from_provider(\n",
      "            self,\n",
      "            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],\n",
      "            ell_call: EllCallParams,\n",
      "            provider_call_params: Dict[str, Any],\n",
      "            origin_id: Optional[str] = None,\n",
      "            logger: Optional[Callable[..., None]] = None,\n",
      "        ) -> Tuple[List[Message], Metadata]:\n",
      "\n",
      "            usage = {}\n",
      "            tracked_results = []\n",
      "            metadata = {}\n",
      "\n",
      "            #XXX: Support n > 0\n",
      "\n",
      "            if provider_call_params.get(\"stream\", False):\n",
      "                content = []\n",
      "                current_blocks: Dict[int, Dict[str, Any]] = {}\n",
      "                message_metadata = {}\n",
      "\n",
      "                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:\n",
      "                    for chunk in stream:\n",
      "                        if chunk.type == \"message_start\":\n",
      "                            message_metadata = chunk.message.model_dump()\n",
      "                            message_metadata.pop(\"content\", None)  # Remove content as we'll build it separately\n",
      "\n",
      "                        elif chunk.type == \"content_block_start\":\n",
      "                            block = chunk.content_block.model_dump()\n",
      "                            current_blocks[chunk.index] = block\n",
      "                            if block[\"type\"] == \"tool_use\":\n",
      "                                if logger: logger(f\" <tool_use: {block['name']}(\")\n",
      "                                block[\"input\"] = \"\" # force it to be a string, XXX: can implement partially parsed json later.\n",
      "                        elif chunk.type == \"content_block_delta\":\n",
      "                            if chunk.index in current_blocks:\n",
      "                                block = current_blocks[chunk.index]\n",
      "                                if (delta := chunk.delta).type == \"text_delta\":\n",
      "                                    block[\"text\"] += delta.text\n",
      "                                    if logger: logger(delta.text)\n",
      "                                if delta.type == \"input_json_delta\":\n",
      "                                    block[\"input\"] += delta.partial_json\n",
      "                                    if logger: logger(delta.partial_json)\n",
      "\n",
      "                        elif chunk.type == \"content_block_stop\":\n",
      "                            if chunk.index in current_blocks:\n",
      "                                block = current_blocks.pop(chunk.index)\n",
      "                                if block[\"type\"] == \"text\":\n",
      "                                    content.append(ContentBlock(text=_lstr(block[\"text\"],origin_trace=origin_id)))\n",
      "                                elif block[\"type\"] == \"tool_use\":\n",
      "                                    try:\n",
      "                                        matching_tool = ell_call.get_tool_by_name(block[\"name\"])\n",
      "                                        if matching_tool:\n",
      "                                            content.append(\n",
      "                                                ContentBlock(\n",
      "                                                    tool_call=ToolCall(\n",
      "                                                        tool=matching_tool,\n",
      "                                                        tool_call_id=_lstr(\n",
      "                                                            block['id'],origin_trace=origin_id\n",
      "                                                        ),\n",
      "                                                        params=json.loads(block['input']) if block['input'] else {},\n",
      "                                                    )\n",
      "                                                )\n",
      "                                            )\n",
      "                                    except json.JSONDecodeError:\n",
      "                                        if logger: logger(f\" - FAILED TO PARSE JSON\")\n",
      "                                        pass\n",
      "                                    if logger: logger(f\")>\")\n",
      "\n",
      "                        elif chunk.type == \"message_delta\":\n",
      "                            message_metadata.update(chunk.delta.model_dump())\n",
      "                            if chunk.usage:\n",
      "                                usage.update(chunk.usage.model_dump())\n",
      "\n",
      "                        elif chunk.type == \"message_stop\":\n",
      "                            tracked_results.append(Message(role=\"assistant\", content=content))\n",
      "\n",
      "                        # print(chunk)\n",
      "                metadata = message_metadata\n",
      "\n",
      "            # process metadata for ell\n",
      "            # XXX: Unify an ell metadata format for ell studio.\n",
      "            usage[\"prompt_tokens\"] = usage.get(\"input_tokens\", 0)\n",
      "            usage[\"completion_tokens\"] = usage.get(\"output_tokens\", 0)\n",
      "            usage[\"total_tokens\"] = usage['prompt_tokens'] + usage['completion_tokens']\n",
      "\n",
      "            metadata[\"usage\"] = usage\n",
      "            return tracked_results, metadata\n",
      "\n",
      "    # XXX: Make a singleton.\n",
      "    anthropic_provider = AnthropicProvider()\n",
      "    register_provider(anthropic_provider, anthropic.Anthropic)\n",
      "    register_provider(anthropic_provider, anthropic.AnthropicBedrock)\n",
      "    register_provider(anthropic_provider, anthropic.AnthropicVertex)\n",
      "\n",
      "except ImportError:\n",
      "    pass\n",
      "Chunk 2 ----------------------------------\n",
      "def serialize_image_for_anthropic(img : ImageContent):\n",
      "    if img.url:\n",
      "        # Download the image from the URL\n",
      "        response = requests.get(img.url)\n",
      "        response.raise_for_status()  # Raise an exception for bad responses\n",
      "        pil_image = PILImage.open(BytesIO(response.content))\n",
      "    elif img.image:\n",
      "        pil_image = img.image\n",
      "    else:\n",
      "        raise ValueError(\"Image object has neither url nor image data.\")\n",
      "    buffer = BytesIO()\n",
      "    pil_image.save(buffer, format=\"PNG\")\n",
      "    base64_image =  base64.b64encode(buffer.getvalue()).decode()\n",
      "    return dict(\n",
      "        type=\"image\",\n",
      "        source=dict(\n",
      "            type=\"base64\",\n",
      "            media_type=\"image/png\",\n",
      "            data=base64_image\n",
      "        )\n",
      "    )\n",
      "Chunk 3 ----------------------------------\n",
      "def _content_block_to_anthropic_format(content_block: ContentBlock):\n",
      "        if (image := content_block.image): return serialize_image_for_anthropic(image)\n",
      "        elif ((text := content_block.text) is not None): return dict(type=\"text\", text=text)\n",
      "        elif (parsed := content_block.parsed):\n",
      "            return dict(type=\"text\", text=json.dumps(parsed.model_dump(), ensure_ascii=False))\n",
      "        elif (tool_call := content_block.tool_call):\n",
      "            return dict(\n",
      "                type=\"tool_use\",\n",
      "                id=tool_call.tool_call_id,\n",
      "                name=tool_call.tool.__name__,\n",
      "                input=tool_call.params.model_dump()\n",
      "            )\n",
      "        elif (tool_result := content_block.tool_result):\n",
      "            return dict(\n",
      "                type=\"tool_result\",\n",
      "                tool_use_id=tool_result.tool_call_id,\n",
      "                content=[_content_block_to_anthropic_format(c) for c in tool_result.result]\n",
      "            )\n",
      "        else:\n",
      "            raise ValueError(\"Content block is not supported by anthropic\")\n"
     ]
    }
   ],
   "source": [
    "print_cluster_by_name(full_code_clusters, \"Anthropic Interaction Implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
