{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "from src.cluster.cluster import (\n",
    "    generate_full_code_clusters, \n",
    "    generate_summarized_clusters,\n",
    "    generate_graph_clusters,\n",
    "    generate_random_clusters\n",
    ")\n",
    "\n",
    "\n",
    "from src.cluster.types import (\n",
    "    CodeChunk,\n",
    "    SummaryChunk,\n",
    "    ClusterInput,\n",
    "    ClusteredTopic,\n",
    "    ClusterInputType,\n",
    "    LMClusteredTopicList\n",
    ")\n",
    "\n",
    "from src.cluster.chunk_repo import ChunkStrat\n",
    "\n",
    "# repo_name = \"ell\"\n",
    "repo_name = \"ell\"\n",
    "repo_path = Path(\"../../src/cluster/repos\") / repo_name\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# AutoSearching for n, k:\n",
    "# Goal: want to automatically find the values of n and k given that\n",
    "# the cohere_score evaluation function is probably gonna be changed in the\n",
    "# future\n",
    "#\n",
    "# Add a function that automatically recalculates n and k in:\n",
    "# n * cohere_score\n",
    "# num_files ** k / num_clusters (for cross_file)\n",
    "# This should be a simple search for increasing values of k over a range \n",
    "# R (of length ... 7?) such that the following condition holds:\n",
    "# For the first half (1, R/2]\n",
    "# The ranking of the score is: (Fullcode, Cgraph, ..., Random)\n",
    "# For the second half (R/2 + 1, R)\n",
    "# The ranking of the score is: (Cgraph, Fullcode, ..., Random)\n",
    "# \n",
    "# Then once the lowest value of n is found\n",
    "# R is found that satisfies above, take k to be R / 2 and call it a day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src.config import GRAPH_ROOT, REPOS_ROOT\n",
    "from rtfs.chunk_resolution.chunk_graph import ChunkGraph\n",
    "from rtfs.transforms.cluster import cluster\n",
    "\n",
    "def generate_cgraph_clusters() -> List[ClusteredTopic]:\n",
    "    ell_json = json.loads(open(GRAPH_ROOT / \"MadcowD_ell_standard.json\", \"r\").read())\n",
    "    cg = ChunkGraph.from_json(REPOS_ROOT / \"MadcowD_ell\", ell_json)\n",
    "\n",
    "    cluster(cg)\n",
    "\n",
    "    return [\n",
    "        ClusteredTopic(\n",
    "            name=\"Graph Cluster\",\n",
    "            chunks=[\n",
    "                CodeChunk(\n",
    "                    id=chunk.og_id,\n",
    "                    content=chunk.content,\n",
    "                    filepath=chunk.file_path,\n",
    "                    input_type=ClusterInputType.CHUNK,\n",
    "                ).dict() for chunk in cluster.chunks\n",
    "            ],\n",
    "        ) \n",
    "        for cluster in cg.get_clusters()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n",
      "Unclassified chunks, iter:[1]:  65\n",
      "Unclassified chunks, iter:[2]:  2\n",
      "Unclassified chunks, iter:[3]:  0\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n",
      "[Summarize Chunk] Chunk too long: 8743, continuing...\n",
      "Summary tokens: 8604,           Code tokens: 60918,           Ratio: 0.14123904264749335\n",
      "Unclassified chunks, iter:[1]:  147\n",
      "Unclassified chunks, iter:[2]:  64\n",
      "Unclassified chunks, iter:[3]:  2\n",
      "Unclassified chunks, iter:[4]:  0\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Generate clusters\n",
    "full_code_clusters = generate_full_code_clusters(repo_path)\n",
    "summary_clusters = generate_summarized_clusters(repo_path)\n",
    "graph_clusters = generate_graph_clusters(repo_path)\n",
    "cgraph_clusters = generate_cgraph_clusters()\n",
    "random_clusters = generate_random_clusters(repo_path, num_clusters = 10)\n",
    "rsummarized_clusters = generate_summarized_clusters(repo_path, chunk_strat=ChunkStrat.RANDOM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full code clusters: 45\n",
      "Summary clusters: 52\n",
      "Graph clusters: 10\n",
      "CGraph clusters: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Full code clusters: {len(full_code_clusters)}\")\n",
    "print(f\"Summary clusters: {len(summary_clusters)}\")\n",
    "print(f\"Random Summary clusters: {len(rsummarized_clusters)}\")\n",
    "print(f\"Graph clusters: {len(graph_clusters)}\")\n",
    "print(f\"CGraph clusters: {len(cgraph_clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### EVALS ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new instance for c:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\llm\\evals\\eval_cluster.py\n"
     ]
    }
   ],
   "source": [
    "from src.llm.evals.eval_cluster import eval_coherence_clusters\n",
    "\n",
    "iters = 3\n",
    "\n",
    "full_code_coherence = eval_coherence_clusters(full_code_clusters, iters, \"full_code\", repo_name, subdir=\"full_code\")\n",
    "graph_coherence = eval_coherence_clusters(graph_clusters, iters, \"graph\", repo_name, subdir=\"graph\")\n",
    "cgraph_coherence = eval_coherence_clusters(cgraph_clusters, iters, \"cgraph\", repo_name, subdir=\"cgraph\")\n",
    "random_coherence = eval_coherence_clusters(random_clusters, iters, \"random\", repo_name, subdir=\"random\")\n",
    "summary_coherence = eval_coherence_clusters(summary_clusters, iters, \"summary\", repo_name, subdir=\"summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm.evals.eval_cluster import eval_coherence_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full code coherence:  4.395833333333333\n",
      "Graph coherence:  4.2\n",
      "Cgraph coherence:  3.9393939393939394\n",
      "Random coherence:  3.033333333333333\n",
      "Summary coherence:  4.344086021505376\n"
     ]
    }
   ],
   "source": [
    "print(\"Full code coherence: \", full_code_coherence)\n",
    "print(\"Graph coherence: \", graph_coherence)\n",
    "print(\"Cgraph coherence: \", cgraph_coherence)\n",
    "print(\"Random coherence: \", random_coherence)\n",
    "print(\"Summary coherence: \", summary_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cross_file_single(cluster: ClusteredTopic, f_const: float = 2.0) -> float:\n",
    "    # Calculate the number of unique files in the cluster\n",
    "    unique_files = set(chunk.filepath for chunk in cluster.chunks \n",
    "                       if chunk.filepath is not None)\n",
    "    num_files = len(unique_files)\n",
    "    num_chunks = len(cluster.chunks)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if num_chunks == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate the ratio of files to chunks\n",
    "    score = num_files ** f_const / num_chunks\n",
    "\n",
    "    return num_files, num_chunks, score\n",
    "    \n",
    "\n",
    "def eval_cross_file_cluster(clusters: List[ClusteredTopic], f_const: float = 2.0, min_chunks: int = 3) -> float:\n",
    "    cross_file_scores = [eval_cross_file_single(cluster, f_const = f_const)[2] for cluster in clusters \n",
    "                         if len(cluster.chunks) >= min_chunks]\n",
    "\n",
    "    # Calculate the average cross-file score\n",
    "    if len(cross_file_scores) > 0:\n",
    "        avg_cross_file_score = sum(cross_file_scores) / len(cross_file_scores)\n",
    "    else:\n",
    "        avg_cross_file_score = 0.0\n",
    "\n",
    "    return avg_cross_file_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for f_const:  1.1\n",
      "1.Full Code:  7.2830141697258535\n",
      "2.Graph:  7.241129182189325\n",
      "3.Cgraph:  7.234413237902033\n",
      "4.Summary:  7.216055801234271\n",
      "5.Random:  5.8773892985417255\n",
      "Results for f_const:  1.2\n",
      "1.Cgraph:  7.388836167380329\n",
      "2.Full Code:  7.307053000698173\n",
      "3.Graph:  7.302710162788448\n",
      "4.Summary:  7.2277164328422305\n",
      "5.Random:  6.018757361681725\n",
      "Results for f_const:  1.3\n",
      "1.Cgraph:  7.569770334663936\n",
      "2.Graph:  7.372867229768563\n",
      "3.Full Code:  7.335297378655064\n",
      "4.Summary:  7.240877320949681\n",
      "5.Random:  6.179880024334345\n",
      "Results for f_const:  1.4\n",
      "1.Cgraph:  7.7819138692136605\n",
      "2.Graph:  7.452980013709861\n",
      "3.Full Code:  7.368488327986757\n",
      "4.Summary:  7.255754332833053\n",
      "5.Random:  6.36354768146329\n"
     ]
    }
   ],
   "source": [
    "f_const_vals = [1.1, 1.2, 1.3, 1.4]\n",
    "cohere_scores = [full_code_coherence, graph_coherence, cgraph_coherence, random_coherence, summary_coherence]\n",
    "clusters = [full_code_clusters, graph_clusters, cgraph_clusters, random_clusters, summary_clusters]\n",
    "labels = [\"Full Code\", \"Graph\", \"Cgraph\", \"Random\", \"Summary\"]\n",
    "\n",
    "for f_const in f_const_vals:\n",
    "    cross_file_scores = [\n",
    "        eval_cross_file_cluster(cluster, f_const=f_const) for cluster in clusters\n",
    "    ] \n",
    "    final_eval = [( 1.6 * cohere_sore + cross_score, label) for cohere_sore, cross_score, label\n",
    "                   in zip(cohere_scores, cross_file_scores, labels)]\n",
    "    final_eval = sorted(final_eval, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    print(\"Results for f_const: \", f_const)\n",
    "\n",
    "    print(f\"1.{final_eval[0][1]}: \", final_eval[0][0])\n",
    "    print(f\"2.{final_eval[1][1]}: \", final_eval[1][0])\n",
    "    print(f\"3.{final_eval[2][1]}: \", final_eval[2][0])\n",
    "    print(f\"4.{final_eval[3][1]}: \", final_eval[3][0])\n",
    "    print(f\"5.{final_eval[4][1]}: \", final_eval[4][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for f_const:  1.1\n",
      "1.Cgraph:  3.66908428888863\n",
      "2.Random:  3.1063030944654573\n",
      "3.Graph:  2.1887425651951635\n",
      "4.Summary:  1.153433756963121\n",
      "5.Full Code:  1.0975553433087866\n",
      "Results for f_const:  1.2\n",
      "1.Cgraph:  4.277417041378886\n",
      "2.Random:  3.5351195526567887\n",
      "3.Graph:  2.4473826837114787\n",
      "4.Summary:  1.204088543733182\n",
      "5.Full Code:  1.203226037791274\n",
      "Results for f_const:  1.3\n",
      "1.Cgraph:  4.990188003405217\n",
      "2.Random:  4.023858296036403\n",
      "3.Graph:  2.742042365027961\n",
      "4.Full Code:  1.3273836158934431\n",
      "5.Summary:  1.2612605737913545\n",
      "Results for f_const:  1.4\n",
      "1.Cgraph:  5.8259049576920106\n",
      "2.Random:  4.580983522660869\n",
      "3.Graph:  3.0785160575814134\n",
      "4.Full Code:  1.4732854973306773\n",
      "5.Summary:  1.325887593155681\n"
     ]
    }
   ],
   "source": [
    "f_const_vals = [1.1, 1.2, 1.3, 1.4]\n",
    "cohere_scores = [full_code_coherence, graph_coherence, cgraph_coherence, random_coherence, summary_coherence]\n",
    "clusters = [full_code_clusters, graph_clusters, cgraph_clusters, random_clusters, summary_clusters]\n",
    "labels = [\"Full Code\", \"Graph\", \"Cgraph\", \"Random\", \"Summary\"]\n",
    "\n",
    "for f_const in f_const_vals:\n",
    "    cross_file_scores = [\n",
    "        eval_cross_file_cluster(cluster, f_const=f_const) for cluster in clusters\n",
    "    ]\n",
    "    final_eval = [(cohere_sore * cross_score, label) for cohere_sore, cross_score, label\n",
    "                   in zip(cohere_scores, cross_file_scores, labels)]\n",
    "    final_eval = sorted(final_eval, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    print(\"Results for f_const: \", f_const)\n",
    "\n",
    "    print(f\"1.{final_eval[0][1]}: \", final_eval[0][0])\n",
    "    print(f\"2.{final_eval[1][1]}: \", final_eval[1][0])\n",
    "    print(f\"3.{final_eval[2][1]}: \", final_eval[2][0])\n",
    "    print(f\"4.{final_eval[3][1]}: \", final_eval[3][0])\n",
    "    print(f\"5.{final_eval[4][1]}: \", final_eval[4][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-Time API and Client Management | Conversation APIs and Real-time Communication | 7\n",
      "Web Application and Server Setup | Interactive CLI with Visual Representation | 6\n",
      "Factorial Calculation | Interactive CLI with Visual Representation | 5\n",
      "Store and Database Management | SQL Store and Query Operations | 5\n",
      "Store Management and Configuration | Configuration and Initialization | 4\n",
      "User Input Validation and Formatting | OpenAI and LLM Capabilities | 3\n",
      "Reinforcement Learning Environment Setup and Evaluation | RL Training Using Gym | 3\n",
      "Data Collection and Processing in Reinforcement Learning | CBPO Reinforcement Learning Algorithm | 3\n",
      "Model Registration and Handling | Ell Language Modeling | 3\n",
      "Language Model and Prompt Handling | Language Model Decorator Utilities | 3\n",
      "Main Application and Serving | Studio Command-Line Interface | 3\n",
      "String and Data Manipulation | Basic Classes and Methods | 3\n",
      "Real-time Audio Handling | Real-time Client and Event Handling | 3\n",
      "Content and Message Processing | Model of Language Models | 3\n",
      "Logging and Debugging | Environment Settings and Commands | 3\n",
      "Real-Time Client and Communication | Conversation APIs and Real-time Communication | 3\n",
      "Policy Networks in Reinforcement Learning | CBPO Reinforcement Learning Algorithm | 2\n",
      "String and Factorial Operations | Basic Classes and Methods | 2\n",
      "Data Processing and Serialization | Persistent Storage and Serialization | 2\n",
      "Client and Provider Management | Basic APIs and Util Functions | 2\n",
      "Function Tracking and Invocation Logging | Invocation and Tracking | 2\n",
      "API Interactions and Middleware | Anthropic Interaction Implementation | 2\n",
      "Real-time Communication and Event Handling | Real-time API Client | 2\n",
      "Event Callback Management | Real-time Event Dispatch and Utilities | 2\n",
      "Data Serialization and Transformation | Persistent Storage and Serialization | 2\n",
      "Model and Invocation Management | Basic Classes and Methods | 2\n",
      "Custom Data Structures and String Operations | Processing and Mapping Functions | 2\n",
      "Configuration and Dependency Management | Lexical Closure and Dependency Management | 2\n",
      "Image and Graphics Processing | Plotting ASCII Art | 2\n",
      "NPM Operations and Testing | Build Process and Commands | 1\n",
      "OpenAI API Interaction | Real-time API Client | 1\n",
      "Abstract Syntax Tree (AST) Manipulation | Context Versioning Using AST | 1\n",
      "Code Cloning and Updation | Configuration and Initialization | 1\n",
      "Testing and Validation | Basic APIs and Util Functions | 1\n",
      "Network Connections and Logging | Markov Decision Process | 1\n",
      "Configuration Management | Interactive CLI with Visual Representation | 1\n",
      "Content Transformation and Serialization | Anthropic Interaction Implementation | 1\n",
      "Tool and Invocation Management | Basic Classes and Methods | 1\n"
     ]
    }
   ],
   "source": [
    "# full_code_ids contain the superset of all code chunks\n",
    "id_map = {chunk_id: i for i, chunk_id in enumerate(full_code_ids)}\n",
    "\n",
    "# match clusters to find ones with the most shared chunks\n",
    "def compare_clusters(cluster_a: List[ClusteredTopic], \n",
    "                 cluster_b: List[ClusteredTopic]) -> List[Tuple[ClusteredTopic, ClusteredTopic, int]]:\n",
    "    \"\"\"\n",
    "    Loops through all clusters to find the best match for each cluster in the other set.\n",
    "    \"\"\"\n",
    "    seen = []\n",
    "    matched_clusters = []\n",
    "    for i, a in enumerate(cluster_a):\n",
    "        best_match = None\n",
    "        best_score = -1\n",
    "        for b in cluster_b:\n",
    "            # if b.name in seen:\n",
    "            #     continue\n",
    "            \n",
    "            a_chunk_ids = [id_map[chunk.id] for chunk in a.chunks]\n",
    "            b_chunk_ids = [id_map[chunk.id] for chunk in b.chunks]\n",
    "            score = len(set(a_chunk_ids) & set(b_chunk_ids))\n",
    "\n",
    "            # if i == 12:\n",
    "            #     print(\"a chunks: \", [id_map[chunk.id] for chunk in a.chunks])\n",
    "            #     print(\"b chunks: \", [id_map[chunk.id] for chunk in b.chunks])\n",
    "            #     print(score)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = b\n",
    "        \n",
    "        if best_match: \n",
    "            matched_clusters.append((a, best_match, best_score))\n",
    "            seen.append(best_match.name)\n",
    "\n",
    "    return matched_clusters\n",
    "\n",
    "matched_clusters = compare_clusters(summary_clusters, full_code_clusters)\n",
    "for c1, c2, score in sorted(matched_clusters, key=lambda x: x[2], reverse=True):\n",
    "    print(f\"{c1.name} | {c2.name} | {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
