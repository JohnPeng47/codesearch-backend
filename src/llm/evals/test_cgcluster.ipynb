{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Engine created:  Engine(sqlite:///logdir\\ell.db)\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\cowboy-server\n",
      "[Chunker]: 255 chunks used\n",
      "Chunk fp:  alembic\\versions\\13e10adf8e27_added_repo_id.py\n",
      "Chunk fp:  alembic\\versions\\2bc1adebc872_added_repoid_to_test_results_model.py\n",
      "Chunk fp:  alembic\\versions\\2bc1adebc872_added_repoid_to_test_results_model.py\n",
      "Chunk fp:  alembic\\versions\\2bc1adebc872_added_repoid_to_test_results_model.py\n",
      "Chunk fp:  alembic\\versions\\2ee3ecc05c40_added_filepath_to_nodemodel_and_name_to_.py\n",
      "Chunk fp:  alembic\\versions\\3c7b1822da96_added_repostats.py\n",
      "Chunk fp:  alembic\\versions\\3e1fd387befe_fixed_some_relations.py\n",
      "Chunk fp:  alembic\\versions\\3e1fd387befe_fixed_some_relations.py\n",
      "Chunk fp:  alembic\\versions\\3e1fd387befe_fixed_some_relations.py\n",
      "Chunk fp:  alembic\\versions\\4182504b989a_renamed_feedback_to_decide.py\n",
      "Chunk fp:  alembic\\versions\\486ee62cfdb2_added_main_and_remote_to_repoconfig.py\n",
      "Chunk fp:  alembic\\versions\\4b2308ac52fa_finally_working.py\n",
      "Chunk fp:  alembic\\versions\\4b2308ac52fa_finally_working.py\n",
      "Chunk fp:  alembic\\versions\\4be689188190_added_auto_gen_to_test_modules.py\n",
      "Chunk fp:  alembic\\versions\\4bf808259948_added_session_id_to_testresults.py\n",
      "Chunk fp:  alembic\\versions\\50c4f4a1bd1a_changed_cowboyusers_table.py\n",
      "Chunk fp:  alembic\\versions\\53cfcb81976f_repair_add_cloned_folders_deleteforked_.py\n",
      "Chunk fp:  alembic\\versions\\5a9b1afb76e3_added_repo_model.py\n",
      "Chunk fp:  alembic\\versions\\5a9b1afb76e3_added_repo_model.py\n",
      "Chunk fp:  alembic\\versions\\68e7bf63522d_updated_range_on_targecodemodel.py\n",
      "Chunk fp:  alembic\\versions\\7ab0269403a8_added_exp_id_to_repoconfig.py\n",
      "Chunk fp:  alembic\\versions\\7f8df6212797_added_augmenttestresult_models.py\n",
      "Chunk fp:  alembic\\versions\\7f8df6212797_added_augmenttestresult_models.py\n",
      "Chunk fp:  alembic\\versions\\8a518189662f_added_repostats_actually.py\n",
      "Chunk fp:  alembic\\versions\\8cbcddf29c9e_addde_language_field_to_repoconfig_model.py\n",
      "Chunk fp:  alembic\\versions\\91a43f59cf7d_removed_cloned_folders_from_repoconfig.py\n",
      "Chunk fp:  alembic\\versions\\94e8e80b2fc8_mistake.py\n",
      "Chunk fp:  alembic\\versions\\b4b0ffad026d_added_testmodules.py\n",
      "Chunk fp:  alembic\\versions\\b4b0ffad026d_added_testmodules.py\n",
      "Chunk fp:  alembic\\versions\\b9671daa1ec7_added_coverage_table.py\n",
      "Chunk fp:  alembic\\versions\\b9671daa1ec7_added_coverage_table.py\n",
      "Chunk fp:  alembic\\versions\\baca7f3acb5c_added_repoid_to_testmodulemodel.py\n",
      "Chunk fp:  alembic\\versions\\baca7f3acb5c_added_repoid_to_testmodulemodel.py\n",
      "Chunk fp:  alembic\\versions\\bf5d46d450f5_added_coverage_test_result_relation.py\n",
      "Chunk fp:  alembic\\versions\\d0c885df6cfa_change_targetcode_tables_from_varchar_.py\n",
      "Chunk fp:  alembic\\versions\\d428a766b0ef_added_backref_from_tmmodel_to_nodes.py\n",
      "Chunk fp:  alembic\\versions\\df1d20e2c832_added_cascade_delete_from_repo_to_tms_.py\n",
      "Chunk fp:  alembic\\versions\\e3cae2f0fb3e_added_repoid_to_coverage.py\n",
      "Chunk fp:  alembic\\versions\\e635c218de2a_added_repo_model_and_added_relation_.py\n",
      "Chunk fp:  alembic\\versions\\e635c218de2a_test2.py\n",
      "Chunk fp:  alembic\\versions\\e63f0f3ddd8b_added_initial_tables.py\n",
      "Chunk fp:  alembic\\versions\\f8709f78a9a4_updated_base.py\n",
      "Chunk fp:  cowboy-lib\\api\\runner\\shared.py\n",
      "Chunk fp:  cowboy-lib\\api\\runner\\shared.py\n",
      "Chunk fp:  cowboy-lib\\api\\runner\\shared.py\n",
      "Chunk fp:  cowboy-lib\\api\\runner\\shared.py\n",
      "Chunk fp:  cowboy-lib\\ast\\__init__.py\n",
      "Chunk fp:  cowboy-lib\\ast\\code.py\n",
      "Chunk fp:  cowboy-lib\\ast\\code.py\n",
      "Chunk fp:  cowboy-lib\\ast\\code.py\n",
      "Chunk fp:  cowboy-lib\\ast\\python\\ast.py\n",
      "Chunk fp:  cowboy-lib\\ast\\python\\ast.py\n",
      "Chunk fp:  cowboy-lib\\ast\\python\\ast.py\n",
      "Chunk fp:  cowboy-lib\\ast\\python\\ast.py\n",
      "Chunk fp:  cowboy-lib\\ast\\python\\ast.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\coverage.py\n",
      "Chunk fp:  cowboy-lib\\llm\\__init__.py\n",
      "Chunk fp:  cowboy-lib\\llm\\invoke_llm.py\n",
      "Chunk fp:  cowboy-lib\\llm\\models.py\n",
      "Chunk fp:  cowboy-lib\\llm\\models.py\n",
      "Chunk fp:  cowboy-lib\\llm\\models.py\n",
      "Chunk fp:  cowboy-lib\\llm\\models.py\n",
      "Chunk fp:  cowboy-lib\\llm\\models.py\n",
      "Chunk fp:  cowboy-lib\\llm\\models.py\n",
      "Chunk fp:  cowboy-lib\\llm\\models.py\n",
      "Chunk fp:  cowboy-lib\\llm\\utils.py\n",
      "Chunk fp:  cowboy-lib\\repo\\__init__.py\n",
      "Chunk fp:  cowboy-lib\\repo\\diff.py\n",
      "Chunk fp:  cowboy-lib\\repo\\diff.py\n",
      "Chunk fp:  cowboy-lib\\repo\\diff.py\n",
      "Chunk fp:  cowboy-lib\\repo\\diff.py\n",
      "Chunk fp:  cowboy-lib\\repo\\diff.py\n",
      "Chunk fp:  cowboy-lib\\repo\\diff.py\n",
      "Chunk fp:  cowboy-lib\\repo\\diff.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\repository.py\n",
      "Chunk fp:  cowboy-lib\\repo\\runner.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_file.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_file.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_file.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_file.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_file.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_file.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_file.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_repo.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_repo.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_repo.py\n",
      "Chunk fp:  cowboy-lib\\repo\\source_repo.py\n",
      "Chunk fp:  cowboy-lib\\test_modules\\__init__.py\n",
      "Chunk fp:  cowboy-lib\\test_modules\\target_code.py\n",
      "Chunk fp:  cowboy-lib\\test_modules\\test_module.py\n",
      "Chunk fp:  cowboy-lib\\test_modules\\test_module.py\n",
      "Chunk fp:  cowboy-lib\\test_modules\\test_module.py\n",
      "Chunk fp:  cowboy-lib\\test_modules\\test_module.py\n",
      "Chunk fp:  cowboy-lib\\test_modules\\test_module.py\n",
      "Chunk fp:  cowboy-lib\\utils.py\n",
      "Chunk fp:  cowboy-lib\\utils.py\n",
      "Chunk fp:  cowboy-lib\\utils.py\n",
      "Chunk fp:  main.py\n",
      "Chunk fp:  main.py\n",
      "Chunk fp:  main.py\n",
      "Chunk fp:  main.py\n",
      "Chunk fp:  src\\ast\\models.py\n",
      "Chunk fp:  src\\ast\\service.py\n",
      "Chunk fp:  src\\ast\\service.py\n",
      "Chunk fp:  src\\auth\\models.py\n",
      "Chunk fp:  src\\auth\\models.py\n",
      "Chunk fp:  src\\auth\\models.py\n",
      "Chunk fp:  src\\auth\\permissions.py\n",
      "Chunk fp:  src\\auth\\permissions.py\n",
      "Chunk fp:  src\\auth\\service.py\n",
      "Chunk fp:  src\\auth\\service.py\n",
      "Chunk fp:  src\\auth\\service.py\n",
      "Chunk fp:  src\\auth\\service.py\n",
      "Chunk fp:  src\\auth\\sm.py\n",
      "Chunk fp:  src\\auth\\sm.py\n",
      "Chunk fp:  src\\auth\\sm.py\n",
      "Chunk fp:  src\\auth\\views.py\n",
      "Chunk fp:  src\\auth\\views.py\n",
      "Chunk fp:  src\\config.py\n",
      "Chunk fp:  src\\coverage\\__init__.py\n",
      "Chunk fp:  src\\coverage\\models.py\n",
      "Chunk fp:  src\\coverage\\service.py\n",
      "Chunk fp:  src\\coverage\\service.py\n",
      "Chunk fp:  src\\coverage\\stats.py\n",
      "Chunk fp:  src\\database\\core.py\n",
      "Chunk fp:  src\\database\\core.py\n",
      "Chunk fp:  src\\database\\core.py\n",
      "Chunk fp:  src\\database\\manage.py\n",
      "Chunk fp:  src\\database\\manage.py\n",
      "Chunk fp:  src\\database\\manage.py\n",
      "Chunk fp:  src\\exceptions.py\n",
      "Chunk fp:  src\\experiments\\augment_test.py\n",
      "Chunk fp:  src\\experiments\\augment_test.py\n",
      "Chunk fp:  src\\experiments\\augment_test.py\n",
      "Chunk fp:  src\\experiments\\augment_test.py\n",
      "Chunk fp:  src\\experiments\\augment_test.py\n",
      "Chunk fp:  src\\experiments\\models.py\n",
      "Chunk fp:  src\\experiments\\views.py\n",
      "Chunk fp:  src\\logger.py\n",
      "Chunk fp:  src\\models.py\n",
      "Chunk fp:  src\\queue\\core.py\n",
      "Chunk fp:  src\\queue\\core.py\n",
      "Chunk fp:  src\\queue\\models.py\n",
      "Chunk fp:  src\\queue\\permissions.py\n",
      "Chunk fp:  src\\queue\\service.py\n",
      "Chunk fp:  src\\queue\\views.py\n",
      "Chunk fp:  src\\queue\\views.py\n",
      "Chunk fp:  src\\queue\\views.py\n",
      "Chunk fp:  src\\repo\\models.py\n",
      "Chunk fp:  src\\repo\\models.py\n",
      "Chunk fp:  src\\repo\\service.py\n",
      "Chunk fp:  src\\repo\\service.py\n",
      "Chunk fp:  src\\repo\\service.py\n",
      "Chunk fp:  src\\repo\\service.py\n",
      "Chunk fp:  src\\repo\\service.py\n",
      "Chunk fp:  src\\repo\\service.py\n",
      "Chunk fp:  src\\repo\\views.py\n",
      "Chunk fp:  src\\repo\\views.py\n",
      "Chunk fp:  src\\repo\\views.py\n",
      "Chunk fp:  src\\repo\\views.py\n",
      "Chunk fp:  src\\runner\\models.py\n",
      "Chunk fp:  src\\runner\\service.py\n",
      "Chunk fp:  src\\scripts\\neuter_repo.py\n",
      "Chunk fp:  src\\scripts\\neuter_repo.py\n",
      "Chunk fp:  src\\scripts\\show_tables.py\n",
      "Chunk fp:  src\\stats\\models.py\n",
      "Chunk fp:  src\\stats\\service.py\n",
      "Chunk fp:  src\\target_code\\models.py\n",
      "Chunk fp:  src\\target_code\\service.py\n",
      "Chunk fp:  src\\target_code\\views.py\n",
      "Chunk fp:  src\\tasks\\__init__.py\n",
      "Chunk fp:  src\\tasks\\create_tgt_coverage.py\n",
      "Chunk fp:  src\\tasks\\create_tgt_coverage.py\n",
      "Chunk fp:  src\\tasks\\create_tgt_coverage.py\n",
      "Chunk fp:  src\\tasks\\fork_repo.py\n",
      "Chunk fp:  src\\tasks\\fork_repo.py\n",
      "Chunk fp:  src\\tasks\\get_baseline.py\n",
      "Chunk fp:  src\\tasks\\get_baseline.py\n",
      "Chunk fp:  src\\tasks\\get_baseline_parallel.py\n",
      "Chunk fp:  src\\tasks\\get_baseline_parallel.py\n",
      "Chunk fp:  src\\test_gen\\augment.py\n",
      "Chunk fp:  src\\test_gen\\augment.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\base_strat.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\composer.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\composer.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\composer.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\composer.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\evaluators\\__init__.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\evaluators\\augment_additive.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\evaluators\\augment_additive.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\evaluators\\augment_parallel.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\evaluators\\augment_parallel.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\evaluators\\eval_base.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\evaluators\\eval_base.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\evaluators\\eval_base.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\evaluators\\types.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\__init__.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\augment_base.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\augment_base.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\augment_strat.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\augment_with_ctxt_file.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\augment_with_missing.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\prompt.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\prompt.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\prompt.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\strats\\types.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\types.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\types.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\utils.py\n",
      "Chunk fp:  src\\test_gen\\augment_test\\utils.py\n",
      "Chunk fp:  src\\test_gen\\models.py\n",
      "Chunk fp:  src\\test_gen\\models.py\n",
      "Chunk fp:  src\\test_gen\\models.py\n",
      "Chunk fp:  src\\test_gen\\service.py\n",
      "Chunk fp:  src\\test_gen\\service.py\n",
      "Chunk fp:  src\\test_gen\\utils.py\n",
      "Chunk fp:  src\\test_gen\\views.py\n",
      "Chunk fp:  src\\test_gen\\views.py\n",
      "Chunk fp:  src\\test_gen\\views.py\n",
      "Chunk fp:  src\\test_gen\\views.py\n",
      "Chunk fp:  src\\test_modules\\iter_tms.py\n",
      "Chunk fp:  src\\test_modules\\models.py\n",
      "Chunk fp:  src\\test_modules\\models.py\n",
      "Chunk fp:  src\\test_modules\\service.py\n",
      "Chunk fp:  src\\test_modules\\service.py\n",
      "Chunk fp:  src\\test_modules\\service.py\n",
      "Chunk fp:  src\\test_modules\\service.py\n",
      "Chunk fp:  src\\test_modules\\service.py\n",
      "Chunk fp:  src\\test_modules\\views.py\n",
      "Chunk fp:  src\\utils.py\n",
      "Chunk fp:  test.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from pathlib import Path\n",
    "from rtfs.chunk_resolution.chunk_graph import ChunkGraph\n",
    "from src.chunk.chunk import chunk_repo, ChunkStrat\n",
    "\n",
    "\n",
    "import ell\n",
    "ell.init(store=\"logdir\")\n",
    "\n",
    "repo_name = \"cowboy-server\"\n",
    "repo_path = Path(r\"C:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\cluster\\repos\") / repo_name\n",
    "\n",
    "chunks = chunk_repo(repo_path, ChunkStrat.VANILLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "Adding edge:  50 48\n",
      "Adding edge:  51 48\n",
      "Adding edge:  52 49\n",
      "Adding edge:  72 75\n",
      "Adding edge:  117 149\n",
      "Adding edge:  119 160\n",
      "Adding edge:  119 157\n",
      "Adding edge:  124 158\n",
      "Adding edge:  125 158\n",
      "Adding edge:  131 145\n",
      "Adding edge:  135 145\n",
      "Adding edge:  135 149\n",
      "Adding edge:  136 145\n",
      "Adding edge:  136 158\n",
      "Adding edge:  153 167\n",
      "Adding edge:  154 167\n",
      "Adding edge:  156 145\n",
      "Adding edge:  156 131\n",
      "Adding edge:  156 172\n",
      "Adding edge:  156 244\n",
      "Adding edge:  156 153\n",
      "Adding edge:  162 131\n",
      "Adding edge:  163 160\n",
      "Adding edge:  164 131\n",
      "Adding edge:  165 131\n",
      "Adding edge:  166 131\n",
      "Adding edge:  168 158\n",
      "Adding edge:  169 124\n",
      "Adding edge:  170 124\n",
      "Adding edge:  171 124\n",
      "Adding edge:  172 124\n",
      "Adding edge:  173 124\n",
      "Adding edge:  173 248\n",
      "Adding edge:  173 242\n",
      "Adding edge:  174 124\n",
      "Adding edge:  175 145\n",
      "Adding edge:  175 131\n",
      "Adding edge:  175 149\n",
      "Adding edge:  176 145\n",
      "Adding edge:  176 131\n",
      "Adding edge:  176 149\n",
      "Adding edge:  176 158\n",
      "Adding edge:  177 145\n",
      "Adding edge:  177 131\n",
      "Adding edge:  177 149\n",
      "Adding edge:  177 158\n",
      "Adding edge:  178 145\n",
      "Adding edge:  178 131\n",
      "Adding edge:  178 149\n",
      "Adding edge:  180 160\n",
      "Adding edge:  180 163\n",
      "Adding edge:  182 239\n",
      "Adding edge:  185 167\n",
      "Adding edge:  186 120\n",
      "Adding edge:  187 240\n",
      "Adding edge:  187 139\n",
      "Adding edge:  187 186\n",
      "Adding edge:  188 131\n",
      "Adding edge:  188 145\n",
      "Adding edge:  188 170\n",
      "Adding edge:  188 244\n",
      "Adding edge:  191 240\n",
      "Adding edge:  191 122\n",
      "Adding edge:  191 187\n",
      "Adding edge:  191 141\n",
      "Adding edge:  192 160\n",
      "Adding edge:  192 124\n",
      "Adding edge:  192 167\n",
      "Adding edge:  192 180\n",
      "Adding edge:  192 140\n",
      "Adding edge:  192 244\n",
      "Adding edge:  192 245\n",
      "Adding edge:  196 180\n",
      "Adding edge:  198 180\n",
      "Adding edge:  200 160\n",
      "Adding edge:  200 167\n",
      "Adding edge:  200 240\n",
      "Adding edge:  200 124\n",
      "Adding edge:  200 180\n",
      "Adding edge:  200 203\n",
      "Adding edge:  200 131\n",
      "Adding edge:  200 185\n",
      "Adding edge:  203 201\n",
      "Adding edge:  203 180\n",
      "Adding edge:  203 212\n",
      "Adding edge:  205 207\n",
      "Adding edge:  205 149\n",
      "Adding edge:  205 225\n",
      "Adding edge:  207 212\n",
      "Adding edge:  208 180\n",
      "Adding edge:  209 212\n",
      "Adding edge:  212 180\n",
      "Adding edge:  216 201\n",
      "Adding edge:  218 216\n",
      "Adding edge:  218 113\n",
      "Adding edge:  218 221\n",
      "Adding edge:  219 216\n",
      "Adding edge:  219 222\n",
      "Adding edge:  219 113\n",
      "Adding edge:  219 225\n",
      "Adding edge:  220 216\n",
      "Adding edge:  220 223\n",
      "Adding edge:  220 113\n",
      "Adding edge:  220 225\n",
      "Adding edge:  221 225\n",
      "Adding edge:  232 140\n",
      "Adding edge:  233 248\n",
      "Adding edge:  236 145\n",
      "Adding edge:  236 131\n",
      "Adding edge:  236 160\n",
      "Adding edge:  236 170\n",
      "Adding edge:  236 246\n",
      "Adding edge:  236 245\n",
      "Adding edge:  236 244\n",
      "Adding edge:  236 200\n",
      "Adding edge:  237 145\n",
      "Adding edge:  238 131\n",
      "Adding edge:  238 145\n",
      "Adding edge:  238 171\n",
      "Adding edge:  238 185\n",
      "Adding edge:  240 120\n",
      "Adding edge:  240 186\n",
      "Adding edge:  242 167\n",
      "Adding edge:  242 239\n",
      "Adding edge:  243 121\n",
      "Adding edge:  247 145\n",
      "Adding edge:  247 131\n",
      "Adding edge:  247 160\n",
      "Adding edge:  247 170\n",
      "Adding edge:  247 192\n",
      "Adding edge:  247 158\n",
      "{'ast\\\\code.py::2': 11, 'ast\\\\code.py::3': 13, 'python\\\\ast.py::1': 11, 'python\\\\ast.py::2': 11, 'python\\\\ast.py::3': 13, 'llm\\\\invoke_llm.py::1': 14, 'llm\\\\models.py::3': 14, 'cowboy-lib\\\\utils.py::1': 3, 'main.py::2': 1, 'main.py::4': 12, 'ast\\\\models.py::1': 5, 'ast\\\\service.py::1': 15, 'ast\\\\service.py::2': 5, 'auth\\\\models.py::2': 8, 'auth\\\\models.py::3': 1, 'auth\\\\service.py::4': 1, 'auth\\\\views.py::1': 1, 'auth\\\\views.py::2': 1, 'coverage\\\\models.py::1': 5, 'coverage\\\\service.py::1': 2, 'coverage\\\\service.py::2': 5, 'database\\\\core.py::3': 1, 'src\\\\exceptions.py::1': 1, 'experiments\\\\augment_test.py::4': 10, 'experiments\\\\augment_test.py::5': 6, 'experiments\\\\views.py::1': 10, 'src\\\\logger.py::1': 12, 'src\\\\models.py::1': 1, 'queue\\\\core.py::2': 2, 'queue\\\\permissions.py::1': 1, 'queue\\\\service.py::1': 2, 'queue\\\\views.py::1': 1, 'queue\\\\views.py::2': 1, 'queue\\\\views.py::3': 1, 'repo\\\\models.py::1': 6, 'repo\\\\models.py::2': 1, 'repo\\\\service.py::1': 8, 'repo\\\\service.py::2': 4, 'repo\\\\service.py::3': 8, 'repo\\\\service.py::4': 10, 'repo\\\\service.py::5': 9, 'repo\\\\service.py::6': 8, 'repo\\\\views.py::1': 1, 'repo\\\\views.py::2': 1, 'repo\\\\views.py::3': 1, 'repo\\\\views.py::4': 1, 'runner\\\\service.py::1': 2, 'scripts\\\\neuter_repo.py::2': 9, 'stats\\\\service.py::1': 6, 'target_code\\\\models.py::1': 5, 'target_code\\\\service.py::1': 5, 'target_code\\\\views.py::1': 4, 'tasks\\\\create_tgt_coverage.py::2': 5, 'tasks\\\\create_tgt_coverage.py::3': 2, 'tasks\\\\get_baseline.py::2': 2, 'tasks\\\\get_baseline_parallel.py::2': 2, 'test_gen\\\\augment.py::2': 6, 'augment_test\\\\base_strat.py::1': 7, 'augment_test\\\\composer.py::2': 7, 'augment_test\\\\composer.py::4': 7, 'evaluators\\\\augment_additive.py::1': 7, 'evaluators\\\\augment_additive.py::2': 2, 'evaluators\\\\augment_parallel.py::1': 7, 'evaluators\\\\eval_base.py::2': 7, 'strats\\\\augment_base.py::1': 3, 'strats\\\\augment_strat.py::1': 3, 'strats\\\\augment_with_ctxt_file.py::1': 3, 'strats\\\\augment_with_missing.py::1': 3, 'strats\\\\prompt.py::1': 3, 'strats\\\\prompt.py::2': 3, 'strats\\\\prompt.py::3': 3, 'augment_test\\\\types.py::1': 3, 'test_gen\\\\service.py::1': 2, 'test_gen\\\\service.py::2': 9, 'test_gen\\\\views.py::2': 4, 'test_gen\\\\views.py::3': 1, 'test_gen\\\\views.py::4': 1, 'test_modules\\\\iter_tms.py::1': 9, 'test_modules\\\\models.py::1': 5, 'test_modules\\\\service.py::1': 9, 'test_modules\\\\service.py::2': 15, 'test_modules\\\\service.py::3': 4, 'test_modules\\\\service.py::4': 4, 'test_modules\\\\service.py::5': 4, 'test_modules\\\\views.py::1': 1, 'src\\\\utils.py::1': 9}\n",
      "Adding edge:  ast\\code.py::2 11\n",
      "Adding edge:  ast\\code.py::3 13\n",
      "Adding edge:  python\\ast.py::1 11\n",
      "Adding edge:  python\\ast.py::2 11\n",
      "Adding edge:  python\\ast.py::3 13\n",
      "Adding edge:  llm\\invoke_llm.py::1 14\n",
      "Adding edge:  llm\\models.py::3 14\n",
      "Adding edge:  cowboy-lib\\utils.py::1 3\n",
      "Adding edge:  main.py::2 1\n",
      "Adding edge:  main.py::4 12\n",
      "Adding edge:  ast\\models.py::1 5\n",
      "Adding edge:  ast\\service.py::1 15\n",
      "Adding edge:  ast\\service.py::2 5\n",
      "Adding edge:  auth\\models.py::2 8\n",
      "Adding edge:  auth\\models.py::3 1\n",
      "Adding edge:  auth\\service.py::4 1\n",
      "Adding edge:  auth\\views.py::1 1\n",
      "Adding edge:  auth\\views.py::2 1\n",
      "Adding edge:  coverage\\models.py::1 5\n",
      "Adding edge:  coverage\\service.py::1 2\n",
      "Adding edge:  coverage\\service.py::2 5\n",
      "Adding edge:  database\\core.py::3 1\n",
      "Adding edge:  src\\exceptions.py::1 1\n",
      "Adding edge:  experiments\\augment_test.py::4 10\n",
      "Adding edge:  experiments\\augment_test.py::5 6\n",
      "Adding edge:  experiments\\views.py::1 10\n",
      "Adding edge:  src\\logger.py::1 12\n",
      "Adding edge:  src\\models.py::1 1\n",
      "Adding edge:  queue\\core.py::2 2\n",
      "Adding edge:  queue\\permissions.py::1 1\n",
      "Adding edge:  queue\\service.py::1 2\n",
      "Adding edge:  queue\\views.py::1 1\n",
      "Adding edge:  queue\\views.py::2 1\n",
      "Adding edge:  queue\\views.py::3 1\n",
      "Adding edge:  repo\\models.py::1 6\n",
      "Adding edge:  repo\\models.py::2 1\n",
      "Adding edge:  repo\\service.py::1 8\n",
      "Adding edge:  repo\\service.py::2 4\n",
      "Adding edge:  repo\\service.py::3 8\n",
      "Adding edge:  repo\\service.py::4 10\n",
      "Adding edge:  repo\\service.py::5 9\n",
      "Adding edge:  repo\\service.py::6 8\n",
      "Adding edge:  repo\\views.py::1 1\n",
      "Adding edge:  repo\\views.py::2 1\n",
      "Adding edge:  repo\\views.py::3 1\n",
      "Adding edge:  repo\\views.py::4 1\n",
      "Adding edge:  runner\\service.py::1 2\n",
      "Adding edge:  scripts\\neuter_repo.py::2 9\n",
      "Adding edge:  stats\\service.py::1 6\n",
      "Adding edge:  target_code\\models.py::1 5\n",
      "Adding edge:  target_code\\service.py::1 5\n",
      "Adding edge:  target_code\\views.py::1 4\n",
      "Adding edge:  tasks\\create_tgt_coverage.py::2 5\n",
      "Adding edge:  tasks\\create_tgt_coverage.py::3 2\n",
      "Adding edge:  tasks\\get_baseline.py::2 2\n",
      "Adding edge:  tasks\\get_baseline_parallel.py::2 2\n",
      "Adding edge:  test_gen\\augment.py::2 6\n",
      "Adding edge:  augment_test\\base_strat.py::1 7\n",
      "Adding edge:  augment_test\\composer.py::2 7\n",
      "Adding edge:  augment_test\\composer.py::4 7\n",
      "Adding edge:  evaluators\\augment_additive.py::1 7\n",
      "Adding edge:  evaluators\\augment_additive.py::2 2\n",
      "Adding edge:  evaluators\\augment_parallel.py::1 7\n",
      "Adding edge:  evaluators\\eval_base.py::2 7\n",
      "Adding edge:  strats\\augment_base.py::1 3\n",
      "Adding edge:  strats\\augment_strat.py::1 3\n",
      "Adding edge:  strats\\augment_with_ctxt_file.py::1 3\n",
      "Adding edge:  strats\\augment_with_missing.py::1 3\n",
      "Adding edge:  strats\\prompt.py::1 3\n",
      "Adding edge:  strats\\prompt.py::2 3\n",
      "Adding edge:  strats\\prompt.py::3 3\n",
      "Adding edge:  augment_test\\types.py::1 3\n",
      "Adding edge:  test_gen\\service.py::1 2\n",
      "Adding edge:  test_gen\\service.py::2 9\n",
      "Adding edge:  test_gen\\views.py::2 4\n",
      "Adding edge:  test_gen\\views.py::3 1\n",
      "Adding edge:  test_gen\\views.py::4 1\n",
      "Adding edge:  test_modules\\iter_tms.py::1 9\n",
      "Adding edge:  test_modules\\models.py::1 5\n",
      "Adding edge:  test_modules\\service.py::1 9\n",
      "Adding edge:  test_modules\\service.py::2 15\n",
      "Adding edge:  test_modules\\service.py::3 4\n",
      "Adding edge:  test_modules\\service.py::4 4\n",
      "Adding edge:  test_modules\\service.py::5 4\n",
      "Adding edge:  test_modules\\views.py::1 1\n",
      "Adding edge:  src\\utils.py::1 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ast\\\\code.py::2': 11,\n",
       " 'ast\\\\code.py::3': 13,\n",
       " 'python\\\\ast.py::1': 11,\n",
       " 'python\\\\ast.py::2': 11,\n",
       " 'python\\\\ast.py::3': 13,\n",
       " 'llm\\\\invoke_llm.py::1': 14,\n",
       " 'llm\\\\models.py::3': 14,\n",
       " 'cowboy-lib\\\\utils.py::1': 3,\n",
       " 'main.py::2': 1,\n",
       " 'main.py::4': 12,\n",
       " 'ast\\\\models.py::1': 5,\n",
       " 'ast\\\\service.py::1': 15,\n",
       " 'ast\\\\service.py::2': 5,\n",
       " 'auth\\\\models.py::2': 8,\n",
       " 'auth\\\\models.py::3': 1,\n",
       " 'auth\\\\service.py::4': 1,\n",
       " 'auth\\\\views.py::1': 1,\n",
       " 'auth\\\\views.py::2': 1,\n",
       " 'coverage\\\\models.py::1': 5,\n",
       " 'coverage\\\\service.py::1': 2,\n",
       " 'coverage\\\\service.py::2': 5,\n",
       " 'database\\\\core.py::3': 1,\n",
       " 'src\\\\exceptions.py::1': 1,\n",
       " 'experiments\\\\augment_test.py::4': 10,\n",
       " 'experiments\\\\augment_test.py::5': 6,\n",
       " 'experiments\\\\views.py::1': 10,\n",
       " 'src\\\\logger.py::1': 12,\n",
       " 'src\\\\models.py::1': 1,\n",
       " 'queue\\\\core.py::2': 2,\n",
       " 'queue\\\\permissions.py::1': 1,\n",
       " 'queue\\\\service.py::1': 2,\n",
       " 'queue\\\\views.py::1': 1,\n",
       " 'queue\\\\views.py::2': 1,\n",
       " 'queue\\\\views.py::3': 1,\n",
       " 'repo\\\\models.py::1': 6,\n",
       " 'repo\\\\models.py::2': 1,\n",
       " 'repo\\\\service.py::1': 8,\n",
       " 'repo\\\\service.py::2': 4,\n",
       " 'repo\\\\service.py::3': 8,\n",
       " 'repo\\\\service.py::4': 10,\n",
       " 'repo\\\\service.py::5': 9,\n",
       " 'repo\\\\service.py::6': 8,\n",
       " 'repo\\\\views.py::1': 1,\n",
       " 'repo\\\\views.py::2': 1,\n",
       " 'repo\\\\views.py::3': 1,\n",
       " 'repo\\\\views.py::4': 1,\n",
       " 'runner\\\\service.py::1': 2,\n",
       " 'scripts\\\\neuter_repo.py::2': 9,\n",
       " 'stats\\\\service.py::1': 6,\n",
       " 'target_code\\\\models.py::1': 5,\n",
       " 'target_code\\\\service.py::1': 5,\n",
       " 'target_code\\\\views.py::1': 4,\n",
       " 'tasks\\\\create_tgt_coverage.py::2': 5,\n",
       " 'tasks\\\\create_tgt_coverage.py::3': 2,\n",
       " 'tasks\\\\get_baseline.py::2': 2,\n",
       " 'tasks\\\\get_baseline_parallel.py::2': 2,\n",
       " 'test_gen\\\\augment.py::2': 6,\n",
       " 'augment_test\\\\base_strat.py::1': 7,\n",
       " 'augment_test\\\\composer.py::2': 7,\n",
       " 'augment_test\\\\composer.py::4': 7,\n",
       " 'evaluators\\\\augment_additive.py::1': 7,\n",
       " 'evaluators\\\\augment_additive.py::2': 2,\n",
       " 'evaluators\\\\augment_parallel.py::1': 7,\n",
       " 'evaluators\\\\eval_base.py::2': 7,\n",
       " 'strats\\\\augment_base.py::1': 3,\n",
       " 'strats\\\\augment_strat.py::1': 3,\n",
       " 'strats\\\\augment_with_ctxt_file.py::1': 3,\n",
       " 'strats\\\\augment_with_missing.py::1': 3,\n",
       " 'strats\\\\prompt.py::1': 3,\n",
       " 'strats\\\\prompt.py::2': 3,\n",
       " 'strats\\\\prompt.py::3': 3,\n",
       " 'augment_test\\\\types.py::1': 3,\n",
       " 'test_gen\\\\service.py::1': 2,\n",
       " 'test_gen\\\\service.py::2': 9,\n",
       " 'test_gen\\\\views.py::2': 4,\n",
       " 'test_gen\\\\views.py::3': 1,\n",
       " 'test_gen\\\\views.py::4': 1,\n",
       " 'test_modules\\\\iter_tms.py::1': 9,\n",
       " 'test_modules\\\\models.py::1': 5,\n",
       " 'test_modules\\\\service.py::1': 9,\n",
       " 'test_modules\\\\service.py::2': 15,\n",
       " 'test_modules\\\\service.py::3': 4,\n",
       " 'test_modules\\\\service.py::4': 4,\n",
       " 'test_modules\\\\service.py::5': 4,\n",
       " 'test_modules\\\\views.py::1': 1,\n",
       " 'src\\\\utils.py::1': 9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rtfs.transforms.cluster import cluster\n",
    "\n",
    "cg = ChunkGraph.from_chunks(repo_path, chunks)\n",
    "cluster(cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CodeChunk(id='ast\\\\code.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='@dataclass\\nclass Decorator(ASTNode):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n\\nclass Class(ASTNode):\\n    def __init__(self, *args, **kwargs):\\n        self.functions: List[Function] = kwargs.pop(\"functions\", [])\\n\\n        super().__init__(*args, **kwargs)\\n\\n    def add_func(self, func: \"Function\"):\\n        self.functions.append(func)\\n\\n    # def __str__(self):\\n    #     funcs_str = \"\\\\n\".join([f\"{func.__str__()}\" for func in self.functions])\\n    #     return f\"Class: {self._name} \\\\n{funcs_str}\"\\n\\n    def __eq__(self, other: \"Class\"):\\n        return self._name == other._name\\n\\n    # def __repr__(self):\\n    #     funcs_str = \"\\\\n\".join([f\"{func}\" for func in self.functions])\\n    #     return f\"Class: {self._name} \\\\n{funcs_str}\"\\n\\n    @property\\n    def name(self):\\n        return self._name', summary='', filepath='cowboy-lib\\\\ast\\\\code.py', metadata=None, node_id=''), CodeChunk(id='python\\\\ast.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from typing import List, Tuple\\nimport ast\\n\\nfrom ..code import Function, Class, Decorator, Argument, ASTNode, NodeType\\n\\nfrom logging import getLogger\\n\\nlogger = getLogger(\"test_results\")\\nAST_FUNCTIONS = (ast.FunctionDef, ast.AsyncFunctionDef)\\n\\n\\n# REFACTOR-AST: this whole function needs to be replaced by the Bloop/tree-sitter\\n# implementation\\nclass PythonAST:\\n    def __init__(self, code: str):\\n        assert isinstance(code, str)\\n\\n        self.classes: List[ASTNode] = []\\n        self.functions: List[ASTNode] = []\\n\\n        self._code = code\\n\\n    # Thank you GPT!\\n    def _parse_decorators(self, node):\\n        decorators = []\\n        for d in node.decorator_list:\\n            # Initialize d_name as None\\n            d_name = None\\n\\n            # Handle direct attribute access or function call with attribute access\\n            if isinstance(d, ast.Attribute) or (\\n                isinstance(d, ast.Call) and isinstance(d.func, ast.Attribute)\\n            ):\\n                d_name_parts = []\\n\\n                # If it\\'s a Call, we start with d.func to get the Attribute node\\n                current_node = d.func if isinstance(d, ast.Call) else d\\n\\n                # Traverse the Attribute nodes\\n                while isinstance(current_node, ast.Attribute):\\n                    d_name_parts.append(current_node.attr)\\n                    current_node = current_node.value\\n\\n                # The loop ends at an ast.Name node, which gives us the root name\\n                if isinstance(current_node, ast.Name):\\n                    d_name_parts.append(current_node.id)\\n\\n                # Combine the parts to form the full decorator name\\n                d_name = \".\".join(reversed(d_name_parts))\\n\\n            elif isinstance(d, ast.Name):\\n                d_name = d.id\\n\\n            # Append the decorator name to the list if it was found\\n            if d_name:\\n                range = self._get_range(d)\\n                decorators.append(\\n                    Decorator(\\n                        d_name,\\n                        # bring range to 0-based indexing\\n                        (range[0], getattr(d, \"end_lineno\", range[0]) - 1),\\n                        None,\\n                        [],\\n                        self._get_lines(*range),\\n                        d,\\n                    )\\n                )\\n\\n        return decorators', summary='', filepath='cowboy-lib\\\\ast\\\\python\\\\ast.py', metadata=None, node_id=''), CodeChunk(id='python\\\\ast.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class PythonAST:\\n\\n    def _parse_classes(self, child: ast.AST, parent: ast.AST):\\n        classes = []\\n        if isinstance(child, ast.ClassDef):\\n            range = self._get_range(child)\\n            decorators = self._parse_decorators(child)\\n\\n            scope = None\\n            if isinstance(parent, AST_FUNCTIONS) and isinstance(parent, ast.ClassDef):\\n                scope = self.find_node(parent)\\n\\n            classes.append(\\n                Class(\\n                    child.name, range, scope, decorators, self._get_lines(*range), child\\n                )\\n            )\\n        return classes', summary='', filepath='cowboy-lib\\\\ast\\\\python\\\\ast.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='ast\\\\code.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class Function(ASTNode):\\n    def __init__(self, *args, **kwargs):\\n        # should replace this with ... a prototype that contains .name property\\n        # self.scope: Class = kwargs.pop(\"scope\", [])\\n        self.arguments = kwargs.pop(\"arguments\", [])\\n\\n        super().__init__(*args, **kwargs)\\n\\n        self.is_test = True if self._name.startswith(\"test\") else False\\n\\n    def is_meth(self):\\n        return bool(self.scope)\\n\\n    def __str__(self):\\n        return f\"{self._name}({\\', \\'.join([arg.__str__() for arg in self.arguments])})\"\\n\\n    def is_method(self):\\n        return self.scope is not None\\n\\n    def func_name(self):\\n        return self._name.split(\".\")[-1]\\n\\n    @property\\n    def name(self):\\n        scope_prefix = f\"{self.scope.name}.\" if self.scope else \"\"\\n        return f\"{scope_prefix}{self._name}\"\\n\\n\\nclass NodeType(Enum):\\n    Function = Function.__name__\\n    Class = Class.__name__\\n    Decorator = Decorator.__name__', summary='', filepath='cowboy-lib\\\\ast\\\\code.py', metadata=None, node_id=''), CodeChunk(id='python\\\\ast.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class PythonAST:\\n\\n    def _parse_functions(self, child: ast.AST, parent: ast.AST):\\n        functions = []\\n        if isinstance(child, AST_FUNCTIONS):\\n            func_name = child.name\\n            child_range = self._get_range(child)\\n            decorators = self._parse_decorators(child)\\n\\n            args = [\\n                Argument(arg.arg, getattr(arg.annotation, \"id\", None))\\n                for arg in child.args.args\\n            ]\\n\\n            # rethink this loop when not hungover...\\n            # assign funcs and classes as parents\\n            scope = None\\n            if isinstance(parent, AST_FUNCTIONS) or isinstance(parent, ast.ClassDef):\\n                scope = self.find_node(parent)\\n\\n            func = Function(\\n                func_name,\\n                child_range,\\n                scope,\\n                decorators,\\n                self._get_lines(*child_range),\\n                child,\\n                arguments=args,\\n            )\\n\\n            # if a parent class contains a test function, then parent class is a test class\\n            if scope and isinstance(parent, ast.ClassDef):\\n                scope.add_func(func)\\n                if func.is_test:\\n                    scope.set_is_test(True)\\n\\n            functions.append(func)\\n\\n        return functions', summary='', filepath='cowboy-lib\\\\ast\\\\python\\\\ast.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='llm\\\\invoke_llm.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from typing import List, Tuple\\nimport asyncio\\n\\nfrom .models import BaseModel\\n\\nfrom .utils import (\\n    # TurboModel,\\n    extract_python_code,\\n    extract_yaml_code,\\n    extract_json_code,\\n)\\n\\n\\nasync def invoke_llm_async(\\n    prompt: str,\\n    model: BaseModel,\\n    n_times: int,\\n    output: str = \"str\",\\n) -> List[str]:\\n    output = []\\n\\n    coroutines = []\\n    for _ in range(n_times):\\n        coroutines.append(model.query(prompt))\\n\\n    llm_outputs = await asyncio.gather(*coroutines)\\n\\n    # should add the other methods here\\n    for out in llm_outputs:\\n        if output == \"yaml\":\\n            out = extract_yaml_code(out)\\n        elif output == \"json\":\\n            out = extract_json_code(out)\\n        elif output == \"code\":\\n            out = extract_python_code(out)\\n\\n    return llm_outputs', summary='', filepath='cowboy-lib\\\\llm\\\\invoke_llm.py', metadata=None, node_id=''), CodeChunk(id='llm\\\\models.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class BaseModel:\\n    MODELS = {}\\n    SHORTCUTS = {}\\n\\n    def __init__(self, args: ModelArguments):\\n        self.args = args\\n        self.model_metadata = {}\\n        self.stats = APIStats()\\n\\n        # Map `model_name` to API-compatible name `api_model`\\n        self.api_model = (\\n            self.SHORTCUTS[self.args.model_name]\\n            if self.args.model_name in self.SHORTCUTS\\n            else self.args.model_name\\n        )\\n\\n        # Map model name to metadata (cost, context info)\\n        MODELS = {\\n            **{dest: self.MODELS[src] for dest, src in self.SHORTCUTS.items()},\\n            **self.MODELS,\\n        }\\n        if args.model_name in MODELS:\\n            self.model_metadata = MODELS[args.model_name]\\n        else:\\n            raise ValueError(\\n                f\"Unregistered model ({args.model_name}). Add model name to MODELS metadata to {self.__class__}\"\\n            )\\n\\n    def reset_stats(self, other: APIStats = None):\\n        if other is None:\\n            self.stats = APIStats(total_cost=self.stats.total_cost)\\n            logger.info(\"Resetting model stats\")\\n        else:\\n            self.stats = other', summary='', filepath='cowboy-lib\\\\llm\\\\models.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='cowboy-lib\\\\utils.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from pathlib import Path\\nimport subprocess\\nimport os\\nimport uuid\\nimport random\\nimport string\\nimport shutil\\n\\n\\ndef get_current_git_commit(repo_path: Path) -> str:\\n    \"\"\"\\n    Uses subprocess to get the current git commit hash.\\n\\n    Returns:\\n        str: The current git commit hash.\\n    \"\"\"\\n    try:\\n        commit_hash = (\\n            subprocess.check_output(\\n                [\"cd\", str(repo_path.resolve()), \"&&\", \"git\", \"rev-parse\", \"HEAD\"],\\n                shell=True,\\n            )\\n            .strip()\\n            .decode(\"utf-8\")\\n        )\\n        return commit_hash\\n    except subprocess.CalledProcessError as e:\\n        print(f\"Error getting current git commit: {e}\")\\n        return \"\"', summary='', filepath='cowboy-lib\\\\utils.py', metadata=None, node_id=''), CodeChunk(id='strats\\\\augment_base.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from ..base_strat import BaseStrategy\\n\\nfrom dataclasses import dataclass\\nfrom typing import List, TYPE_CHECKING\\n\\nif TYPE_CHECKING:\\n    from cowboy_lib.test_modules import TestModule\\n\\nfrom logging import getLogger\\n\\nlogger = getLogger(\"test_results\")\\n\\n\\n@dataclass\\nclass LLMResAppend:\\n    start: int\\n    lines: List[str]\\n\\n    def __post_init__(self):\\n        self.start = int(self.start)\\n\\n    def __repr__(self):\\n        return \"\\\\n\".join(self.lines)\\n\\n\\nclass AugmentTestStrategy(BaseStrategy):\\n    \"\"\"\\n    Generates a test case for a test module\\n    \"\"\"\\n\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n        # \"cast\" TestCaseInput to TestModule\\n        self.test_module: \"TestModule\" = self.test_input\\n\\n        self.test_file = self.src_repo.find_file(\\n            self.test_module.test_file.path,\\n        )\\n        # this is weird .. we should just make it an arg on BaseStrategy\\n        self.target_cov = kwargs.get(\"target_cov\", None)\\n        self.failed_index = 0', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\strats\\\\augment_base.py', metadata=None, node_id=''), CodeChunk(id='strats\\\\augment_strat.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from .augment_base import AugmentTestStrategy\\nfrom .prompt import AugmentTestPrompt\\nfrom ..utils import gen_enumerated_code_str, get_current_git_commit\\n\\n\\nfrom logging import getLogger\\n\\n\\nlogger = getLogger(\"test_results\")\\n\\n\\nclass AugmentClassStrat(AugmentTestStrategy):\\n    \"\"\"\\n    Just simply executes the LLM prompt without providing additional\\n    context\\n    \"\"\"\\n\\n    def build_prompt(self) -> str:\\n        curr_commit = get_current_git_commit(self.src_repo.repo_path)\\n\\n        prompt = AugmentTestPrompt()\\n\\n        test_code = self.test_module.get_test_code(curr_commit)\\n        # test_file = self.repo_ctxt.src_repo.get_file(self.test_module.test_file.path)\\n        # test_code = self.get_test_code(test_file, self.test_module.nodes)\\n\\n        logger.info(f\"ADDITIVE TEST CODE: {test_code}\")\\n\\n        test_code = gen_enumerated_code_str(test_code.split(\"\\\\n\"))\\n        prompt.insert_line(\"test_code\", test_code)\\n\\n        return prompt.get_prompt()\\n\\n    def get_test_code(self, test_file, nodes):\\n        test_code = \"\"\\n        for node in nodes:\\n            try:\\n                test_code += test_file.find_by_nodetype(\\n                    node.name, node_type=node.node_type\\n                ).to_code()\\n            except Exception as e:\\n                logger.error(f\"Error: {e}\")\\n                continue\\n\\n        return test_code', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\strats\\\\augment_strat.py', metadata=None, node_id=''), CodeChunk(id='strats\\\\augment_with_ctxt_file.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from .augment_base import AugmentTestStrategy\\nfrom .prompt import AugmentTestPromptWithCtxt\\nfrom ..types import CtxtWindowExceeded\\nfrom ..utils import get_current_git_commit\\n\\nfrom typing import TYPE_CHECKING\\n\\nfrom src.logger import testgen_logger\\n\\n\\nclass AugmentClassWithCtxtStrat(AugmentTestStrategy):\\n    \"\"\"\\n    Provides the src code context from the source code file targeted by the\\n    test_module\\n    \"\"\"\\n\\n    def build_prompt(self) -> AugmentTestPromptWithCtxt:\\n        prompt = AugmentTestPromptWithCtxt()\\n        curr_commit = get_current_git_commit(self.src_repo.repo_path)\\n\\n        test_code = self.test_module.get_test_code(curr_commit)\\n        test_fit = prompt.insert_line(\"test_code\", test_code)\\n        if not test_fit:\\n            raise CtxtWindowExceeded(\"Test code too large to fit in prompt\")\\n\\n        # TODO: how to narrow the scope of this to class or even function\\n        # have ref to func/class node in TargetCode\\n        for fp in self.test_module.targeted_files():\\n            testgen_logger.info(f\"Src file ctxt of {self.test_module.name}: {fp}\")\\n            file = self.src_repo.find_file(fp)\\n            code_fit = prompt.insert_line(\"file_contents\", file.to_code())\\n\\n            if not code_fit:\\n                testgen_logger.warn(f\"File {fp} too large to fit in prompt\")\\n                continue\\n\\n        return prompt.get_prompt()\\n\\n    def get_test_code(self, test_file, nodes):\\n        test_code = \"\"\\n        for node in nodes:\\n            try:\\n                test_code += test_file.find_by_nodetype(\\n                    node.name, node_type=node.node_type\\n                ).to_code()\\n            except Exception as e:\\n                testgen_logger.error(f\"Error: {e}\")\\n                continue\\n\\n        return test_code', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\strats\\\\augment_with_ctxt_file.py', metadata=None, node_id=''), CodeChunk(id='strats\\\\augment_with_missing.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from .augment_base import AugmentTestStrategy\\nfrom .prompt import AugmentTestPromptMiss\\n\\nfrom ..types import CtxtWindowExceeded\\nfrom ..utils import gen_enumerated_code_str, get_current_git_commit\\n\\nfrom logging import getLogger\\n\\nlogger = getLogger(\"test_results\")\\n\\n\\nclass AugmentModuleMissing(AugmentTestStrategy):\\n    \"\"\"\\n    Augment test by focusing on missing/uncovered lines\\n    \"\"\"\\n\\n    def build_prompt(self) -> AugmentTestPromptMiss:\\n        if not self.target_cov:\\n            raise ValueError(\"Target coverage not set\")\\n\\n        prompt = AugmentTestPromptMiss()\\n\\n        curr_commit = get_current_git_commit(self.src_repo.repo_path)\\n        test_code = self.test_module.get_test_code(curr_commit).split(\"\\\\n\")\\n        test_code = gen_enumerated_code_str(test_code)\\n        missing_lines = self.target_cov.print_lines(line_type=\"missing\")\\n\\n        logger.info(f\"Missing lines: {missing_lines}\")\\n\\n        test_fit = prompt.insert_line(\"test_code\", test_code)\\n        if not test_fit:\\n            raise CtxtWindowExceeded(\"Test code too large to fit in prompt\")\\n\\n        for fp in self.test_module.targeted_files():\\n            file = self.src_repo.find_file(fp)\\n            code_fit = prompt.insert_line(\"file_contents\", file.to_code())\\n            if not code_fit:\\n                logger.warn(f\"File {fp} too large to fit in prompt\")\\n                continue\\n\\n        prompt.insert_line(\"missing_lines\", missing_lines)\\n        return prompt.get_prompt()', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\strats\\\\augment_with_missing.py', metadata=None, node_id=''), CodeChunk(id='strats\\\\prompt.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from ..types import Prompt, LMModelSpec\\nfrom typing import List\\n\\n\\ngpt4_spec = LMModelSpec(\"gpt-4\", 0.01, 8192)\\n\\n\\nclass AugmentTestPrompt(Prompt):\\n    def __init__(self):\\n        prompt = \"\"\"The following is a unit test case written for a code feature\\n{{test_code}}\\n\\nExtend the unit test class to increase coverage, by appending your new code into the existing code above.\\nHere is the structure of your output.\\nOn first line, write the insertion line that your new code is going to be appended to\\nThen on the following lines, generate the code to be inserted. Keep the indentation consistent\\nFor example, given the following input\\n\\n0. class FakeClass:\\n1.    def fake_method(self):\\n2.       pass\\n\\nThe following output was generated\\n2\\n\\n    def test_fake_method(self):\\n        pass\\n\\nNow, generate your response\\n\"\"\"\\n\\n        super().__init__(prompt, gpt4_spec, [\"test_code\"])', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\strats\\\\prompt.py', metadata=None, node_id=''), CodeChunk(id='strats\\\\prompt.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class AugmentTestPromptWithCtxt(Prompt):\\n    def __init__(self):\\n        prompt = \"\"\"The following is a unit test case written for a code feature. \\n{{test_code}}\\n\\n{% if file_contents %}\\nHere is the source code file that the test intends to cover:\\n{{file_contents}}\\n{% endif %}\\n\\nExtend the unit test class to increase coverage, by appending your new code into the existing code above.\\nHere is the structure of your output.\\nOn first line, write the insertion line that your new code is going to be appended to\\nThen on the following lines, generate the code to be inserted. Keep the indentation consistent\\nFor example, given the following input\\n\\n0. class FakeClass:\\n1.    def fake_method(self):\\n2.       pass\\n\\nThe following output was generated\\n2\\n\\n    def test_fake_method(self):\\n        pass\\n\\nNow, generate your response\\n\"\"\"\\n        super().__init__(prompt, gpt4_spec, [\"test_code\", \"file_contents\"])', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\strats\\\\prompt.py', metadata=None, node_id=''), CodeChunk(id='strats\\\\prompt.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class AugmentTestPromptMiss(Prompt):\\n    def __init__(self):\\n        prompt = \"\"\"The following is a unit test case written for a code feature. \\n{{test_code}}\\n\\n{% if file_contents %}\\nHere is the source code file that the test intends to cover:\\n{{file_contents}}\\n{% endif %}\\n\\n{% if missing_lines %}\\nHere are the missing lines that the augmented test should cover:\\n{{missing_lines}}\\n{% endif %}\\n\\nExtend the unit test class to increase coverage, by appending your new code into the existing code above.\\nHere is the structure of your output.\\nOn first line, write the insertion line that your new code is going to be appended to\\nThen on the following lines, generate the code to be inserted. Keep the indentation consistent\\nFor example, given the following input\\n\\n0. class FakeClass:\\n1.    def fake_method(self):\\n2.       pass\\n\\nThe following output was generated\\n2\\n\\n    def test_fake_method(self):\\n        pass\\n\\nNow, generate your response\\n\"\"\"\\n        super().__init__(\\n            prompt, gpt4_spec, [\"test_code\", \"file_contents\", \"missing_lines\"]\\n        )', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\strats\\\\prompt.py', metadata=None, node_id=''), CodeChunk(id='augment_test\\\\types.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from dataclasses import dataclass\\nfrom collections import defaultdict\\nfrom jinja2 import Template\\n\\nfrom typing import List, NamedTuple\\nimport tiktoken\\n\\nfrom enum import Enum, auto\\n\\n\\nclass StratResult(NamedTuple):\\n    contents: str\\n    test_path: str\\n\\n\\nclass StrategyInitError(Exception):\\n    pass\\n\\n\\nclass StrategyMode(Enum):\\n    GEN_NEW_TEST = auto()\\n    AUGMENT_TEST = auto()\\n\\n\\n@dataclass\\nclass LMModelSpec:\\n    model: str\\n    cost: float\\n    ctxt_window: int\\n\\n\\nclass CtxtWindowExceeded(Exception):\\n    pass\\n\\n\\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\\n    encoding = tiktoken.get_encoding(encoding_name)\\n    num_tokens = len(encoding.encode(string))\\n    return num_tokens', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\types.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='main.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class ExceptionMiddleware(BaseHTTPMiddleware):\\n    async def dispatch(\\n        self, request: Request, call_next: RequestResponseEndpoint\\n    ) -> StreamingResponse:\\n        try:\\n            response = await call_next(request)\\n        except ValidationError as e:\\n            log.exception(e)\\n            response = JSONResponse(\\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\\n                content={\"detail\": e.errors(), \"error\": True},\\n            )\\n        except ValueError as e:\\n            log.exception(e)\\n            response = JSONResponse(\\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\\n                content={\\n                    \"detail\": [\\n                        {\"msg\": \"Unknown\", \"loc\": [\"Unknown\"], \"type\": \"Unknown\"}\\n                    ],\\n                    \"error\": True,\\n                },\\n            )\\n        except CowboyRunTimeException as e:\\n            log.exception(e)\\n            response = JSONResponse(\\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\\n                content={\\n                    \"detail\": [{\"msg\": f\"Runtime error: {e.message}\"}],\\n                    \"error\": True,\\n                },\\n            )\\n        except Exception as e:\\n            log.exception(e)\\n            response = JSONResponse(\\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\\n                content={\\n                    \"detail\": [\\n                        {\"msg\": \"Unknown\", \"loc\": [\"Unknown\"], \"type\": \"Unknown\"}\\n                    ],\\n                    \"error\": True,\\n                },\\n            )\\n\\n        return response\\n\\n\\ntoken_registry = []', summary='', filepath='main.py', metadata=None, node_id=''), CodeChunk(id='auth\\\\models.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class UserBase(CowboyBase):\\n    email: EmailStr\\n\\n    @validator(\"email\")\\n    def email_required(cls, v):\\n        if not v:\\n            raise ValueError(\"Must not be empty string and must be a email\")\\n        return v\\n\\n\\nclass UserLogin(UserBase):\\n    password: str\\n\\n    @validator(\"password\")\\n    def password_required(cls, v):\\n        if not v:\\n            raise ValueError(\"Must not be empty string\")\\n        return v\\n\\n\\nclass UserRegister(UserLogin):\\n    openai_api_key: str\\n    password: Optional[str] = Field(None, nullable=True)\\n\\n    @validator(\"password\", pre=True, always=True)\\n    def password_required(cls, v):\\n        # we generate a password for those that don\\'t have one\\n        password = v or generate_password()\\n        return hash_password(password)\\n\\n\\nclass UserLoginResponse(CowboyBase):\\n    token: Optional[str] = Field(None, nullable=True)\\n\\n\\nclass UserRead(UserBase):\\n    id: PrimaryKey\\n    role: Optional[str] = Field(None, nullable=True)\\n    experimental_features: Optional[bool]\\n\\n\\nclass UserUpdate(CowboyBase):\\n    id: PrimaryKey\\n    password: Optional[str] = Field(None, nullable=True)\\n\\n    @validator(\"password\", pre=True)\\n    def hash(cls, v):\\n        return hash_password(str(v))\\n\\n\\nclass UserCreate(CowboyBase):\\n    email: EmailStr\\n    password: Optional[str] = Field(None, nullable=True)\\n\\n    @validator(\"password\", pre=True)\\n    def hash(cls, v):\\n        return hash_password(str(v))\\n\\n\\nclass UserRegisterResponse(CowboyBase):\\n    token: Optional[str] = Field(None, nullable=True)\\n\\n\\nclass UpdateOAIKey(BaseModel):\\n    openai_api_key: str\\n\\n\\n# Errors\\nclass UserExistsError(BaseModel):\\n    error = \"User already exists\"', summary='', filepath='src\\\\auth\\\\models.py', metadata=None, node_id=''), CodeChunk(id='auth\\\\service.py::4', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='# def get_current_user(request: Request) -> CowboyUser:\\n#     user_email = extract_user_email_jwt(request=request)\\n\\n#     if not user_email:\\n#         log.exception(f\"Failed to extract user email\")\\n#         raise InvalidCredentialException\\n\\n#     # kinda of strange ... if user exists, we generate a random password\\n#     # for the user here ...\\n#     return get_or_create(\\n#         db_session=request.state.db,\\n#         user_in=UserRegister(email=user_email),\\n#     )\\n\\n\\ndef get_current_user(request: Request) -> CowboyUser:\\n    user_email = extract_user_email_jwt(request=request)\\n    print(user_email)\\n\\n    if not user_email:\\n        log.exception(f\"Failed to extract user email\")\\n        raise InvalidCredentialException\\n\\n    # kinda of strange ... if user exists, we generate a random password\\n    # for the user here ...\\n    try:\\n        user = get_by_email(\\n            db_session=get_db(request),\\n            email=user_email,\\n        )\\n    # this is special case for requests polling the /task/get endpoint\\n    # where we are not passed a db session, and we want to proceed with the rest\\n    # of endpoint logic\\n    except DBNotSetException:\\n        print(\"No db set\")\\n        return None\\n\\n    # generic case for user not existing\\n    if not user:\\n        print(\"No user\")\\n        raise HTTPException(\\n            status_code=HTTP_401_UNAUTHORIZED, detail=[{\"msg\": \"User not found\"}]\\n        )\\n\\n    return user\\n\\n\\ndef store_oai_key(api_key, user_id):\\n    sm = SecretManager()\\n    sm.store_parameter(\"OAI_KEY_\" + str(user_id), api_key)\\n\\n\\ndef retrieve_oai_key(user_id):\\n    sm = SecretManager()\\n    return sm.retrieve_parameter(\"OAI_KEY_\" + str(user_id))', summary='', filepath='src\\\\auth\\\\service.py', metadata=None, node_id=''), CodeChunk(id='auth\\\\views.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from src.database.core import get_db\\nfrom src.auth.models import CowboyUser\\nfrom src.models import HTTPSuccess\\nfrom src.exceptions import InvalidConfigurationError\\n\\nfrom .service import get_current_user, get, get_by_email, create, store_oai_key\\nfrom .models import UserLoginResponse, UserRegister, UpdateOAIKey\\n\\nfrom fastapi import APIRouter, Depends, HTTPException, status\\nfrom pydantic.error_wrappers import ErrorWrapper, ValidationError\\nfrom sqlalchemy.orm import Session\\n\\n\\nauth_router = APIRouter()\\n\\n\\n@auth_router.post(\"/user/register\", response_model=UserLoginResponse)\\ndef register_user(\\n    user_in: UserRegister,\\n    db_session: Session = Depends(get_db),\\n):\\n    user = get_by_email(db_session=db_session, email=user_in.email)\\n    if user:\\n        raise ValidationError(\\n            [\\n                ErrorWrapper(\\n                    InvalidConfigurationError(\\n                        msg=\"A user with this email already exists.\"\\n                    ),\\n                    loc=\"email\",\\n                )\\n            ],\\n            model=UserRegister,\\n        )\\n\\n    user = create(db_session=db_session, user_in=user_in)\\n    return user', summary='', filepath='src\\\\auth\\\\views.py', metadata=None, node_id=''), CodeChunk(id='auth\\\\views.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='@auth_router.get(\"/user/delete\")\\ndef delete_user(\\n    curr_user: CowboyUser = Depends(get_current_user),\\n    db_session: Session = Depends(get_db),\\n):\\n    user = get(db_session=db_session, user_id=curr_user.id)\\n    if not user:\\n        raise HTTPException(\\n            status_code=status.HTTP_404_NOT_FOUND,\\n            detail=\"User not found\",\\n        )\\n\\n    db_session.delete(user)\\n    db_session.commit()\\n\\n    return HTTPSuccess()\\n\\n\\n@auth_router.post(\"/user/update/openai-key\")\\ndef update_oai_key(\\n    request: UpdateOAIKey,\\n    curr_user: CowboyUser = Depends(get_current_user),\\n    db_session: Session = Depends(get_db),\\n):\\n    user = get(db_session=db_session, user_id=curr_user.id)\\n    if not user:\\n        raise HTTPException(\\n            status_code=status.HTTP_404_NOT_FOUND,\\n            detail=\"User not found\",\\n        )\\n\\n    store_oai_key(user_id=user.id, api_key=request.openai_api_key)\\n\\n    return HTTPSuccess()', summary='', filepath='src\\\\auth\\\\views.py', metadata=None, node_id=''), CodeChunk(id='database\\\\core.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class CustomBase:\\n\\n    @property\\n    def _repr_attrs_str(self):\\n        max_length = self.__repr_max_length__\\n\\n        values = []\\n        single = len(self.__repr_attrs__) == 1\\n        for key in self.__repr_attrs__:\\n            if not hasattr(self, key):\\n                raise KeyError(\\n                    \"{} has incorrect attribute \\'{}\\' in \"\\n                    \"__repr__attrs__\".format(self.__class__, key)\\n                )\\n            value = getattr(self, key)\\n            wrap_in_quote = isinstance(value, str)\\n\\n            value = str(value)\\n            if len(value) > max_length:\\n                value = value[:max_length] + \"...\"\\n\\n            if wrap_in_quote:\\n                value = \"\\'{}\\'\".format(value)\\n            values.append(value if single else \"{}:{}\".format(key, value))\\n\\n        return \" \".join(values)\\n\\n    def __repr__(self):\\n        # get id like \\'#123\\'\\n        id_str = (\"#\" + self._id_str) if self._id_str else \"\"\\n        # join class name, id and repr_attrs\\n        return \"<{} {}{}>\".format(\\n            self.__class__.__name__,\\n            id_str,\\n            \" \" + self._repr_attrs_str if self._repr_attrs_str else \"\",\\n        )\\n\\n\\nBase = declarative_base(cls=CustomBase)\\n\\n\\nclass DBNotSetException(Exception):\\n    pass\\n\\n\\ndef get_db(request: Request):\\n    try:\\n        return request.state.db\\n    except AttributeError:\\n        raise DBNotSetException(\"Database not set on request.\")\\n\\n\\n# Triggers initial response field validation error\\n# DbSession = Annotated[Session, Depends(get_db)]\\r', summary='', filepath='src\\\\database\\\\core.py', metadata=None, node_id=''), CodeChunk(id='src\\\\exceptions.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from pydantic.errors import PydanticValueError\\n\\n\\nclass InvalidConfigurationError(PydanticValueError):\\n    code = \"invalid.configuration\"\\n    msg_template = \"{msg}\"\\n\\n\\nclass CowboyRunTimeException(Exception):\\n    def __init__(self, message: str):\\n        self.message = message\\n        super().__init__(message)', summary='', filepath='src\\\\exceptions.py', metadata=None, node_id=''), CodeChunk(id='src\\\\models.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from datetime import datetime\\nfrom sqlalchemy import Column, DateTime, event\\nfrom pydantic import BaseModel, Field\\nfrom pydantic.types import SecretStr\\n\\nfrom typing import Annotated\\n\\nPrimaryKey = Annotated[int, Field(gt=0, lt=2147483647)]\\nNameStr = Annotated[\\n    str, Field(regex=r\"^(?!\\\\s*$).+\", strip_whitespace=True, min_length=3)\\n]\\n\\n\\nclass TimeStampMixin(object):\\n    \"\"\"Timestamping mixin\"\"\"\\n\\n    created_at = Column(DateTime, default=datetime.utcnow)\\n    created_at._creation_order = 9998\\n    updated_at = Column(DateTime, default=datetime.utcnow)\\n    updated_at._creation_order = 9998\\n\\n    @staticmethod\\n    def _updated_at(mapper, connection, target):\\n        target.updated_at = datetime.utcnow()\\n\\n    @classmethod\\n    def __declare_last__(cls):\\n        event.listen(cls, \"before_update\", cls._updated_at)\\n\\n\\nclass CowboyBase(BaseModel):\\n    class Config:\\n        orm_mode = True\\n        validate_assignment = True\\n        arbitrary_types_allowed = True\\n        anystr_strip_whitespace = True\\n\\n        json_encoders = {\\n            # custom output conversion for datetime\\n            datetime: lambda v: v.strftime(\"%Y-%m-%dT%H:%M:%SZ\") if v else None,\\n            SecretStr: lambda v: v.get_secret_value() if v else None,\\n        }\\n\\n\\nclass HTTPSuccess(BaseModel):\\n    msg: str = \"Success\"', summary='', filepath='src\\\\models.py', metadata=None, node_id=''), CodeChunk(id='queue\\\\permissions.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from src.auth.permissions import BasePermission\\nfrom src.auth.service import get_current_user\\n\\nfrom fastapi import HTTPException\\n\\nfrom starlette.requests import Request\\nfrom starlette.responses import Response\\n\\n\\nclass TaskGetPermissions(BasePermission):\\n    def __init__(self, request: Request):\\n        try:\\n            user = get_current_user(request=request)\\n            if not user:\\n                raise HTTPException(\\n                    status_code=self.user_error_code, detail=self.user_error_msg\\n                )\\n\\n        # this happens when the db is not set\\n        except AttributeError:\\n            pass', summary='', filepath='src\\\\queue\\\\permissions.py', metadata=None, node_id=''), CodeChunk(id='queue\\\\views.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.api.runner.shared import Task\\n\\nfrom .service import list_tasks, dequeue_task, complete_task\\nfrom .models import CompleteTaskRequest\\nfrom .core import TaskQueue, get_queue, get_token_registry, get_token\\n\\nfrom fastapi import APIRouter, Depends, HTTPException, Response\\n\\nfrom src.database.core import get_db\\nfrom src.auth.service import get_current_user\\nfrom src.auth.models import CowboyUser\\n\\nfrom typing import List\\n\\ntask_queue_router = APIRouter()\\n\\n\\n@task_queue_router.get(\"/task/list\", response_model=List[Task])\\ndef list(\\n    task_queue: TaskQueue = Depends(get_queue),\\n    curr_user: CowboyUser = Depends(get_current_user),\\n):\\n    tasks = list_tasks(task_queue=task_queue, user_id=curr_user.id, n=3)\\n    return tasks', summary='', filepath='src\\\\queue\\\\views.py', metadata=None, node_id=''), CodeChunk(id='queue\\\\views.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='# LAUREN 6: This is where the client gets the task from the queue, and as you can see,\\n# some jank here\\n\\n\\n# incredibly hacky, basically, to prevent db connections from being used up\\n# we exclude db connections for this endpoint, we do the following:\\n# 1. First request actually does get a db sess, which we use to auth the user\\n# 2. Grab user id and add it into a in-mem token_registry list\\n# 3. Return user id as \"set-x-task-auth\" header\\n# 4. When the client puts user id into x-task-auth header\\n# 5. Our DBMiddleware will check the header, and if token is in registry, will not\\n# add a db session to the request\\n@task_queue_router.get(\"/task/get\", response_model=List[Task])\\ndef get(\\n    response: Response,\\n    task_queue: TaskQueue = Depends(get_queue),\\n    curr_user: CowboyUser = Depends(get_current_user),\\n    token_registry: List = Depends(get_token_registry),\\n    user_token: str = Depends(get_token),\\n    # perms: str = Depends(PermissionsDependency([TaskGetPermissions])),\\n):\\n    # at this point we have passed db user auth; test\\n    # catches if user sets random token\\n    if user_token and user_token not in token_registry:\\n        raise HTTPException(\\n            status_code=401,\\n            detail=\"Token not in registry, cannot proceed. \\\\\\n            Are you sure you are logged in on the client?\",\\n        )\\n    # issue token if it does not exist\\n    elif not user_token:\\n        print(\"Setting new token ..\")\\n        response.headers[\"set-x-task-auth\"] = str(curr_user.id)\\n        token_registry.append(str(curr_user.id))\\n\\n    tasks = dequeue_task(\\n        task_queue=task_queue, user_id=curr_user.id if curr_user else int(user_token)\\n    )\\n    return tasks', summary='', filepath='src\\\\queue\\\\views.py', metadata=None, node_id=''), CodeChunk(id='queue\\\\views.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='# LAUREN 7: Task is completed by the client\\n@task_queue_router.post(\"/task/complete\", response_model=CompleteTaskRequest)\\ndef complete(\\n    task: CompleteTaskRequest,\\n    task_queue: TaskQueue = Depends(get_queue),\\n    curr_user: CowboyUser = Depends(get_current_user),\\n):\\n\\n    task_queue = complete_task(\\n        task_queue=task_queue,\\n        user_id=curr_user.id,\\n        task_id=task.task_id,\\n        result=task.result,\\n    )\\n    return task', summary='', filepath='src\\\\queue\\\\views.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\models.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class LangConf(CowboyBase):\\n    \"\"\"\\n    Holds the language/framework specific settings\\n    for a repo\\n    \"\"\"\\n\\n    # currently I expect only an interpreter/compiler path that points\\n    # to the runtime for the targeted repo\\n    interp: str\\n\\n\\nclass PythonConf(LangConf):\\n    language: str = \"python\"\\n    cov_folders: List[str]\\n    test_folder: str\\n    interp: str\\n    pythonpath: str\\n\\n    def get(self, __name: str, default: Any = None) -> Any:\\n        return self.dict().get(__name, default)\\n\\n\\nclass RepoConfigBase(CowboyBase):\\n    repo_name: str\\n    url: str\\n    source_folder: str\\n    cloned_folders: List[str]\\n    language: Optional[Language]\\n    is_experiment: Optional[bool]\\n\\n    # REFACTOR-AST: ideally this would be type\\n    # LangConf but not sure how to implement validator\\n    # for all the LangConf subclasses\\n    python_conf: PythonConf\\n\\n    remote: str = Field(default=\"origin\")\\n    main: str = Field(default=\"main\")\\n\\n\\nclass RepoConfigGet(RepoConfigBase):\\n    pass\\n\\n\\nclass RepoConfigCreate(RepoConfigBase):\\n    repo_name: str\\n\\n\\nclass RepoConfigList(CowboyBase):\\n    repo_list: List[RepoConfigBase]\\n\\n\\n# class RepoConfigDelete(BaseModel):\\n#     repo_name: str\\r', summary='', filepath='src\\\\repo\\\\models.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\views.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from src.database.core import get_db\\nfrom src.exceptions import InvalidConfigurationError\\nfrom src.models import HTTPSuccess\\nfrom src.auth.service import get_current_user, CowboyUser\\n\\nfrom .service import create_or_update, get, delete, list, clean\\nfrom .models import RepoConfigCreate, RepoConfigList, RepoConfigGet\\n\\nfrom fastapi import APIRouter, Depends\\nfrom sqlalchemy.orm import Session\\nfrom pydantic.error_wrappers import ErrorWrapper, ValidationError\\n\\n\\nrepo_router = APIRouter()\\n\\n\\n# LAUREN 1: This endpoint is where the user creates the repo\\n@repo_router.post(\"/repo/create\", response_model=RepoConfigCreate)\\ndef create_repo(\\n    repo_in: RepoConfigCreate,\\n    db_session: Session = Depends(get_db),\\n    current_user: CowboyUser = Depends(get_current_user),\\n):\\n    repo = get(\\n        db_session=db_session, repo_name=repo_in.repo_name, curr_user=current_user\\n    )\\n    if repo:\\n        raise ValidationError(\\n            [\\n                ErrorWrapper(\\n                    InvalidConfigurationError(\\n                        msg=\"A repo with this name already exists.\"\\n                    ),\\n                    loc=\"repo_name\",\\n                )\\n            ],\\n            model=RepoConfigCreate,\\n        )\\n\\n    repo_config = create_or_update(\\n        db_session=db_session, repo_in=repo_in, curr_user=current_user\\n    )\\n    # need as_dict to convert cloned_folders to list\\n    return repo_config.to_dict()', summary='', filepath='src\\\\repo\\\\views.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\views.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='@repo_router.delete(\"/repo/delete/{repo_name}\", response_model=HTTPSuccess)\\ndef delete_repo(\\n    repo_name: str,\\n    db_session: Session = Depends(get_db),\\n    current_user: CowboyUser = Depends(get_current_user),\\n):\\n    deleted = delete(db_session=db_session, repo_name=repo_name, curr_user=current_user)\\n\\n    if not deleted:\\n        raise ValidationError(\\n            [\\n                ErrorWrapper(\\n                    InvalidConfigurationError(\\n                        msg=\"A repo with this name does not exist.\"\\n                    ),\\n                    loc=\"repo_name\",\\n                )\\n            ],\\n            model=RepoConfigCreate,\\n        )\\n    return HTTPSuccess()', summary='', filepath='src\\\\repo\\\\views.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\views.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='@repo_router.delete(\"/repo/clean/{repo_name}\", response_model=HTTPSuccess)\\ndef clean_repo(\\n    repo_name: str,\\n    db_session: Session = Depends(get_db),\\n    current_user: CowboyUser = Depends(get_current_user),\\n):\\n    cleaned = clean(db_session=db_session, repo_name=repo_name, curr_user=current_user)\\n\\n    if not cleaned:\\n        raise ValidationError(\\n            [\\n                ErrorWrapper(\\n                    InvalidConfigurationError(\\n                        msg=\"A repo with this name does not exist.\"\\n                    ),\\n                    loc=\"repo_name\",\\n                )\\n            ],\\n            model=RepoConfigCreate,\\n        )\\n    return HTTPSuccess()', summary='', filepath='src\\\\repo\\\\views.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\views.py::4', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='@repo_router.get(\"/repo/get/{repo_name}\", response_model=RepoConfigGet)\\ndef get_repo(\\n    repo_name: str,\\n    db_session: Session = Depends(get_db),\\n    current_user: CowboyUser = Depends(get_current_user),\\n):\\n    repo = get(db_session=db_session, repo_name=repo_name, curr_user=current_user)\\n    if not repo:\\n        raise ValidationError(\\n            [\\n                ErrorWrapper(\\n                    InvalidConfigurationError(\\n                        msg=\"A repo with this name does not exist.\"\\n                    ),\\n                    loc=\"repo_name\",\\n                )\\n            ],\\n            model=RepoConfigGet,\\n        )\\n\\n    return repo.to_dict()\\n\\n\\n@repo_router.get(\"/repo/list\", response_model=RepoConfigList)\\ndef list_repos(\\n    db_session: Session = Depends(get_db),\\n    current_user: CowboyUser = Depends(get_current_user),\\n):\\n    repos = list(db_session=db_session, curr_user=current_user)\\n    return RepoConfigList(repo_list=repos)', summary='', filepath='src\\\\repo\\\\views.py', metadata=None, node_id=''), CodeChunk(id='test_gen\\\\views.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='@test_gen_router.get(\"/test-gen/results/{session_id}\")\\ndef get_results(\\n    session_id: str,\\n    db_session: Session = Depends(get_db),\\n):\\n    trs = get_test_results_by_sessionid(db_session=db_session, session_id=session_id)\\n    return [\\n        TestResultResponse(\\n            id=tr.id,\\n            name=tr.name,\\n            test_case=tr.test_case,\\n            test_file=tr.testfile,\\n            cov_improved=tr.coverage_improve(),\\n            decided=tr.decide,\\n        )\\n        for tr in trs\\n    ]', summary='', filepath='src\\\\test_gen\\\\views.py', metadata=None, node_id=''), CodeChunk(id='test_gen\\\\views.py::4', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='@test_gen_router.post(\"/test-gen/results/decide/{sesssion_id}\")\\ndef accept_user_decision(\\n    request: UserDecisionRequest,\\n    curr_user=Depends(get_current_user),\\n    db_session=Depends(get_db),\\n):\\n    \"\"\"\\n    Takes the result of the selected tests and appends all of the selected\\n    tests to TestModule (testfile/test class). Then check out a new branch against\\n    the remote repo with the changed files\\n    \"\"\"\\n\\n    repo_id = get_test_result_by_id_or_raise(\\n        db_session=db_session, test_id=request.user_decision[0].id\\n    ).repo_id\\n    repo = get_by_id_or_raise(\\n        db_session=db_session, curr_user=curr_user, repo_id=repo_id\\n    )\\n    src_repo = SourceRepo(Path(repo.source_folder))\\n    git_repo = GitRepo(Path(repo.source_folder))\\n\\n    # NOTE: LintExceptions at this step should not happen because they would have occurred\\n    # earlier during the Evaluation phase\\n    changed_files = set()\\n    accepted_results = []\\n    for decision in request.user_decision:\\n        tr = get_test_result_by_id_or_raise(db_session=db_session, test_id=decision.id)\\n        test_file = src_repo.find_file(tr.testfile)\\n        if decision.decision:\\n            test_file.append(tr.test_case, class_name=tr.classname)\\n            src_repo.write_file(test_file.path)\\n            changed_files.add(str(test_file.path))\\n            accepted_results.append(tr)\\n\\n    # update stats\\n    with update_repo_stats(db_session=db_session, repo=repo) as repo_stats:\\n        repo_stats.accepted_tests += len(accepted_results)\\n        repo_stats.rejected_tests += len(request.user_decision) - len(accepted_results)\\n\\n    msg = gen_commit_msg(accepted_results)\\n    compare_url = git_repo.checkout_and_push(\\n        \"cowboy-augmented-tests\", msg, list(changed_files)\\n    )\\n    print(compare_url)\\n\\n    return UserDecisionResponse(compare_url=compare_url)', summary='', filepath='src\\\\test_gen\\\\views.py', metadata=None, node_id=''), CodeChunk(id='test_modules\\\\views.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from fastapi import APIRouter, Depends, BackgroundTasks\\n\\nfrom src.database.core import get_db\\nfrom src.auth.service import get_current_user, CowboyUser\\nfrom src.queue.core import TaskQueue, get_queue\\nfrom src.models import HTTPSuccess\\nfrom src.repo.service import get_or_raise\\nfrom src.tasks.create_tgt_coverage import create_tgt_coverage\\nfrom src.runner.models import ClientRunnerException, RunnerExceptionResponse\\n\\nfrom .models import GetTargetCovRequest\\n\\nfrom sqlalchemy.orm import Session\\nimport asyncio\\n\\ntm_router = APIRouter()\\n\\n\\n@tm_router.post(\"/tm/baseline\")\\nasync def get_tm_target_coverage(\\n    request: GetTargetCovRequest,\\n    background_tasks: BackgroundTasks,\\n    db_session: Session = Depends(get_db),\\n    current_user: CowboyUser = Depends(get_current_user),\\n    task_queue: TaskQueue = Depends(get_queue),\\n):\\n    print(\"Running baseline\")\\n\\n    repo_conf = get_or_raise(\\n        db_session=db_session, curr_user=current_user, repo_name=request.repo_name\\n    )\\n    # When ran like this, task execution does not continue execuing the await block\\n    # inside get_tgt_coverage after a task has been updated by the client\\n    # asyncio.create_task(\\n    #   create_tgt_coverage(\\n    #       get_tgt_coverage,\\n    #       task_queue=task_queue,\\n    #       curr_user=current_user,\\n    #       repo_config=repo_conf,\\n    #       tm_names=request.test_modules,\\n    #   )\\n    # )\\n\\n    # NOTE: don\\'t need to await here because we dont need to return the result right away\\n\\n    # Stopped here, deciding what error to return to /augment and /baseline\\n    # endpoints\\n\\n    try:\\n        # LAUREN 3: Once test modules have been created, we call this method from the client\\n        # to build that mapping from test_file to the target source file\\n        await create_tgt_coverage(\\n            db_session=db_session,\\n            task_queue=task_queue,\\n            curr_user=current_user,\\n            repo_config=repo_conf,\\n            tm_names=request.test_modules,\\n        )\\n\\n        return HTTPSuccess()\\n\\n    except ClientRunnerException as e:\\n        raise e', summary='', filepath='src\\\\test_modules\\\\views.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='main.py::4', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class AddTaskQueueMiddleware(BaseHTTPMiddleware):\\n    async def dispatch(\\n        self, request: Request, call_next: RequestResponseEndpoint\\n    ) -> Response:\\n        request.state.task_queue = TaskQueue()\\n        response = await call_next(request)\\n        return response\\n\\n\\napp.add_middleware(ExceptionMiddleware)\\napp.add_middleware(DBMiddleware)\\napp.add_middleware(AddTaskQueueMiddleware)\\n\\napp.include_router(auth_router)\\napp.include_router(repo_router)\\napp.include_router(tm_router)\\napp.include_router(task_queue_router)\\napp.include_router(test_gen_router)\\napp.include_router(tgtcode_router)\\napp.include_router(exp_router)\\n\\n# logfire.configure(console=False)\\n# logfire.instrument_fastapi(app)\\n\\nif __name__ == \"__main__\":\\n    uvicorn_version = uvicorn.__version__\\n\\n    # doesnt work ??\\n    configure_uvicorn_logger()\\n\\n    with open(\"uvicorn.yaml\", \"r\") as f:\\n        config = yaml.safe_load(f)\\n\\n    uvicorn.run(\\n        \"main:app\",\\n        host=\"0.0.0.0\",\\n        port=3000,\\n        # reload=True,\\n        reload_excludes=[\"./repos\"],\\n        # log_config=config,\\n    )', summary='', filepath='main.py', metadata=None, node_id=''), CodeChunk(id='src\\\\logger.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from src.config import LOG_DIR\\n\\nimport logging\\nimport os\\nfrom datetime import datetime\\nimport pytz\\n\\n\\ndef converter(timestamp):\\n    dt = datetime.fromtimestamp(timestamp, tz=pytz.utc)\\n    return dt.astimezone(pytz.timezone(\"US/Eastern\")).timetuple()\\n\\n\\nformatter = logging.Formatter(\\n    \"%(asctime)s - %(name)s:%(levelname)s: %(filename)s:%(lineno)s - %(message)s\",\\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\\n)\\nformatter.converter = converter\\n\\n\\ndef get_file_handler(log_dir=LOG_DIR):\\n    \"\"\"\\n    Returns a file handler for logging.\\n    \"\"\"\\n    os.makedirs(log_dir, exist_ok=True)\\n    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\\n    file_name = f\"runner_{timestamp}.log\"\\n    file_handler = logging.FileHandler(os.path.join(log_dir, file_name))\\n    file_handler.setFormatter(formatter)\\n    return file_handler\\n\\n\\ndef get_console_handler():\\n    \"\"\"\\n    Returns a console handler for logging.\\n    \"\"\"\\n    console_handler = logging.StreamHandler()\\n    console_handler.setFormatter(formatter)\\n    return console_handler\\n\\n\\ntestgen_logger = logging.getLogger(\"testgen_logger\")\\ntestgen_logger.setLevel(logging.INFO)\\ntestgen_logger.addHandler(get_file_handler())\\ntestgen_logger.addHandler(get_console_handler())\\n\\n\\nloggers = [testgen_logger]\\n\\n\\ndef set_log_level(level=logging.INFO):\\n    \"\"\"\\n    Sets the logging level for all defined loggers.\\n    \"\"\"\\n    for logger in loggers:\\n        logger.setLevel(level)\\n        for handler in logger.handlers:\\n            handler.setLevel(level)\\n\\n\\ndef configure_uvicorn_logger():\\n    uvicorn_error_logger = logging.getLogger(\"uvicorn.error\")\\n    uvicorn_error_logger.addHandler(get_file_handler())\\n    uvicorn_error_logger.addHandler(get_console_handler())', summary='', filepath='src\\\\logger.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='ast\\\\models.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from sqlalchemy import Column, Integer, String, DateTime, ForeignKey, Boolean\\n\\nfrom cowboy_lib.ast.code import ASTNode\\nfrom cowboy_lib.repo.source_repo import SourceRepo\\nfrom src.database.core import Base\\n\\n\\nclass NodeModel(Base):\\n    \"\"\"\\n    An AST node, either a class or a function, that holds a single or a group of unit tests\\n    \"\"\"\\n\\n    __tablename__ = \"nodes\"\\n    id = Column(Integer, primary_key=True)\\n    name = Column(String)\\n    node_type = Column(String)\\n    testfilepath = Column(String)\\n\\n    # start_line = Column(Integer)\\n    # end_line = Column(Integer)\\n    # lines = Column(String)\\n    # is_test = Column(Boolean)\\n    # scope_id = Column(Integer, ForeignKey(\"nodes.id\"))\\n    # children = relationship(\\n    #     \"NodeModel\",\\n    #     backref=backref(\"parent\", remote_side=[id]),\\n    #     cascade=\"all, delete-orphan\",\\n    # )\\n\\n    # # decorators = Column\\n\\n    repo_id = Column(Integer, ForeignKey(\"repo_config.id\", ondelete=\"CASCADE\"))\\n    # Node is either related to test_modules or target_code\\n    test_module_id = Column(\\n        Integer,\\n        ForeignKey(\"test_modules.id\", ondelete=\"CASCADE\"),\\n        nullable=True,\\n    )\\n    target_code_id = Column(\\n        Integer,\\n        ForeignKey(\"target_code.id\", ondelete=\"CASCADE\"),\\n        nullable=True,\\n    )\\n\\n    # chunks = relationship(\"TargetCodeModel\", backref=\"nodes\")\\n\\n    def to_astnode(self, source_repo: SourceRepo):\\n        node = source_repo.find_node(self.name, self.testfilepath, self.node_type)\\n        return node\\n\\n    @classmethod\\n    def from_astnode(cls, node: ASTNode) -> \"NodeModel\":\\n        return cls(\\n            name=node.name,\\n            node_type=node.node_type,\\n        )', summary='', filepath='src\\\\ast\\\\models.py', metadata=None, node_id=''), CodeChunk(id='ast\\\\service.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def create_or_update_node(\\n    *, db_session: Session, repo_id: str, node: ASTNode, filepath: str\\n):\\n    old_node = get_node(\\n        db_session=db_session,\\n        node_name=node.name,\\n        repo_id=repo_id,\\n        node_type=node.node_type.value,\\n        filepath=filepath,\\n    )\\n\\n    if old_node:\\n        # NOTE: there is actually no point in updating node right now\\n        # because none of the node attributes should change ..\\n        print(\"Node exists: \", node.name)\\n        # node_model = (\\n        #     db_session.query(NodeModel)\\n        #     .filter(\\n        #         NodeModel.name == node.name\\n        #         and NodeModel.repo_id == repo_id\\n        #         and NodeModel.node_type == node.node_type\\n        #         and NodeModel.testfilepath == filepath\\n        #     )\\n        #     .update(node)\\n        # )\\n        return old_node\\n    else:\\n        node_model = create_node(\\n            db_session=db_session, node=node, repo_id=repo_id, filepath=filepath\\n        )\\n\\n    return node_model', summary='', filepath='src\\\\ast\\\\service.py', metadata=None, node_id=''), CodeChunk(id='coverage\\\\models.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.coverage import Coverage, TestCoverage\\n\\nfrom sqlalchemy import Column, Integer, String, ForeignKey\\nfrom sqlalchemy.orm import relationship\\nfrom src.database.core import Base\\nfrom typing import List\\n\\n\\nclass CoverageModel(Base):\\n    __tablename__ = \"coverage\"\\n\\n    id = Column(Integer, primary_key=True)\\n    filename = Column(String, nullable=False)\\n    covered_lines = Column(String, nullable=False)\\n    missing_lines = Column(String, nullable=False)\\n\\n    stmts = Column(Integer, nullable=False)\\n    misses = Column(Integer, nullable=False)\\n    covered = Column(Integer, nullable=False)\\n\\n    # test_coverage_id = Column(Integer, ForeignKey(\"test_coverage.id\"))\\n    target_code_list = relationship(\\n        \"TargetCodeModel\", back_populates=\"coverage\", cascade=\"all, delete-orphan\"\\n    )\\n    repo_id = Column(Integer, ForeignKey(\"repo_config.id\"))\\n    test_result_id = Column(Integer, ForeignKey(\"augment_test_results.id\"))\\n\\n    def deserialize(self) -> Coverage:\\n        return Coverage(\\n            filename=self.filename,\\n            covered_lines=[int(l) for l in self.covered_lines.split(\",\") if l],\\n            missing_lines=[int(l) for l in self.missing_lines.split(\",\") if l],\\n        )', summary='', filepath='src\\\\coverage\\\\models.py', metadata=None, node_id=''), CodeChunk(id='coverage\\\\service.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def get_cov_by_filename(\\n    *, db_session: Session, repo_id: int, filename: str\\n) -> CoverageModel:\\n    \"\"\"Get a coverage model by filename.\"\"\"\\n\\n    return (\\n        db_session.query(CoverageModel)\\n        .filter(CoverageModel.filename == filename, CoverageModel.repo_id == repo_id)\\n        .one_or_none()\\n    )\\n\\n\\ndef get_cov_by_id(*, db_session: Session, repo_id: int, id: int) -> CoverageModel:\\n    \"\"\"Get a coverage model by id.\"\"\"\\n\\n    return (\\n        db_session.query(CoverageModel)\\n        .filter(CoverageModel.id == id, CoverageModel.repo_id == repo_id)\\n        .one_or_none()\\n    )', summary='', filepath='src\\\\coverage\\\\service.py', metadata=None, node_id=''), CodeChunk(id='target_code\\\\models.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from sqlalchemy import Column, Integer, String, DateTime, ForeignKey\\nfrom sqlalchemy.orm import relationship\\nfrom pydantic import BaseModel\\nfrom pathlib import Path\\nfrom typing import List\\n\\nfrom cowboy_lib.test_modules.test_module import TestModule\\nfrom cowboy_lib.test_modules.target_code import TargetCode\\nfrom cowboy_lib.repo.source_repo import SourceRepo\\nfrom src.database.core import Base\\nfrom src.ast.models import NodeModel\\n\\n\\nclass TargetCodeModel(Base):\\n    \"\"\"\\n    A chunk of code that is covered by the lines in a TestModule\\n    \"\"\"\\n\\n    __tablename__ = \"target_code\"\\n    id = Column(Integer, primary_key=True)\\n    start = Column(Integer)\\n    end = Column(Integer)\\n    lines = Column(String)\\n    filepath = Column(String)\\n\\n    func_scope = relationship(\\n        \"NodeModel\",\\n        foreign_keys=[NodeModel.target_code_id],\\n        cascade=\"all, delete\",\\n        uselist=False,\\n        single_parent=True,\\n    )\\n    class_scope = relationship(\\n        \"NodeModel\",\\n        foreign_keys=[NodeModel.target_code_id],\\n        cascade=\"all, delete\",\\n        uselist=False,\\n        single_parent=True,\\n    )\\n    test_module_id = Column(Integer, ForeignKey(\"test_modules.id\", ondelete=\"CASCADE\"))\\n    coverage_id = Column(Integer, ForeignKey(\"coverage.id\", ondelete=\"CASCADE\"))\\n    coverage = relationship(\"CoverageModel\")\\n\\n    def __init__(\\n        self,\\n        start,\\n        end,\\n        lines,\\n        filepath,\\n        func_scope,\\n        class_scope,\\n        test_module_id,\\n        coverage_id,\\n    ):\\n        self.start = start\\n        self.end = end\\n        self.lines = \"\\\\n\".join(lines)\\n        self.filepath = str(filepath)\\n        self.func_scope = func_scope\\n        self.class_scope = class_scope\\n        self.test_module_id = test_module_id\\n        self.coverage_id = coverage_id\\n\\n    def get_lines(self) -> List[str]:\\n        return self.lines.split(\"\\\\n\")\\n\\n    def serialize(self, src_repo: SourceRepo):\\n        return TargetCode(\\n            range=(self.start, self.end),\\n            lines=self.get_lines(),\\n            filepath=Path(self.filepath),\\n            func_scope=(\\n                self.func_scope.to_astnode(src_repo) if self.func_scope else None\\n            ),\\n            class_scope=(\\n                self.class_scope.to_astnode(src_repo) if self.class_scope else None\\n            ),\\n        )\\n\\n\\nclass TgtCodeDeleteRequest(BaseModel):\\n    repo_name: str\\n    tm_name: str', summary='', filepath='src\\\\target_code\\\\models.py', metadata=None, node_id=''), CodeChunk(id='target_code\\\\service.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from src.ast.models import NodeModel\\nfrom src.target_code.models import TargetCode, TargetCodeModel\\nfrom src.test_modules.models import TestModuleModel\\nfrom src.coverage.models import CoverageModel\\n\\nfrom sqlalchemy.orm import Session\\n\\n\\ndef create_target_code(\\n    db_session: Session,\\n    tm_model: TestModuleModel,\\n    chunk: TargetCode,\\n    cov_model: CoverageModel,\\n    func_scope: NodeModel = None,\\n    class_scope: NodeModel = None,\\n):\\n    \"\"\"Create a target code chunk for a test module.\"\"\"\\n\\n    print(\"Creating tgtcode: \", tm_model.id, tm_model.name)\\n\\n    target_code = TargetCodeModel(\\n        lines=chunk.lines,\\n        start=chunk.range[0],\\n        end=chunk.range[1],\\n        filepath=chunk.filepath,\\n        func_scope=func_scope,\\n        class_scope=class_scope,\\n        test_module_id=tm_model.id,\\n        coverage_id=cov_model.id,\\n    )\\n\\n    db_session.add(target_code)\\n    db_session.commit()\\n\\n    return target_code\\n\\n\\ndef delete_target_code(db_session: Session, tm_id: int):\\n    \"\"\"Delete all target code for a test module.\"\"\"\\n\\n    deleted = (\\n        db_session.query(TargetCodeModel)\\n        .filter(TargetCodeModel.test_module_id == tm_id)\\n        .delete()\\n    )\\n\\n    db_session.commit()\\n\\n    return deleted', summary='', filepath='src\\\\target_code\\\\service.py', metadata=None, node_id=''), CodeChunk(id='tasks\\\\create_tgt_coverage.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def create_tgt_code_models(\\n    tgt_code_chunks: List[TargetCode],\\n    db_session: Session,\\n    repo_id: int,\\n    tm_model: TestModuleModel,\\n) -> List[TargetCode]:\\n    \"\"\"\\n    Create target code models\\n    \"\"\"\\n    target_chunks = []\\n    for tgt in tgt_code_chunks:\\n        func_scope = (\\n            create_or_update_node(\\n                db_session=db_session,\\n                node=tgt.func_scope,\\n                repo_id=repo_id,\\n                filepath=str(tgt.filepath),\\n            )\\n            if tgt.func_scope\\n            else None\\n        )\\n        class_scope = (\\n            create_or_update_node(\\n                db_session=db_session,\\n                node=tgt.class_scope,\\n                repo_id=repo_id,\\n                filepath=str(tgt.filepath),\\n            )\\n            if tgt.class_scope\\n            else None\\n        )\\n\\n        target_chunks.append(\\n            create_target_code(\\n                db_session=db_session,\\n                tm_model=tm_model,\\n                chunk=tgt,\\n                cov_model=get_cov_by_filename(\\n                    db_session=db_session,\\n                    repo_id=repo_id,\\n                    filename=str(tgt.filepath),\\n                ),\\n                func_scope=func_scope,\\n                class_scope=class_scope,\\n            )\\n        )\\n\\n    return target_chunks', summary='', filepath='src\\\\tasks\\\\create_tgt_coverage.py', metadata=None, node_id=''), CodeChunk(id='test_modules\\\\models.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from sqlalchemy import Column, Integer, String, DateTime, ForeignKey, Boolean\\nfrom sqlalchemy.orm import relationship\\nfrom pydantic import BaseModel\\nfrom pathlib import Path\\nfrom typing import List\\n\\nfrom cowboy_lib.test_modules.test_module import TestModule\\nfrom cowboy_lib.repo.source_repo import SourceRepo\\n\\nfrom src.database.core import Base\\nfrom src.ast.models import NodeModel\\nfrom src.target_code.models import TargetCodeModel\\n\\n\\nclass IncompatibleCommit(Exception):\\n    pass\\n\\n\\nclass TestModuleModel(Base):\\n    __tablename__ = \"test_modules\"\\n    id = Column(Integer, primary_key=True)\\n    name = Column(String)\\n    testfilepath = Column(String)\\n    commit_sha = Column(String)\\n    experiment_id = Column(String, nullable=True)\\n    # use this flag to track test_modules that have already gone through\\n    # auto-test augmentation\\n    auto_gen = Column(Boolean, default=False)\\n\\n    repo_id = Column(Integer, ForeignKey(\"repo_config.id\"))\\n    nodes = relationship(\\n        \"NodeModel\",\\n        backref=\"test_module\",\\n        foreign_keys=[NodeModel.test_module_id],\\n        cascade=\"all, delete-orphan\",\\n    )\\n    target_chunks = relationship(\\n        \"TargetCodeModel\",\\n        backref=\"test_module\",\\n        foreign_keys=[TargetCodeModel.test_module_id],\\n        cascade=\"all, delete-orphan\",\\n    )\\n\\n    def serialize(self, src_repo: SourceRepo) -> TestModule:\\n        \"\"\"\\n        Convert model back to TestModule\\n        \"\"\"\\n        return TestModule(\\n            test_file=src_repo.find_file(Path(self.testfilepath)),\\n            commit_sha=self.commit_sha,\\n            nodes=[NodeModel.to_astnode(n, src_repo) for n in self.nodes],\\n            chunks=[c.serialize(src_repo) for c in self.target_chunks],\\n        )\\n\\n    def get_covered_files(self) -> List[str]:\\n        \"\"\"\\n        Returns the source files that are covered by this test module\\n        \"\"\"\\n\\n        # there must be a better way of doing this ...\\n        return list(set([chunk.filepath for chunk in self.target_chunks]))', summary='', filepath='src\\\\test_modules\\\\models.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='ast\\\\service.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.ast.code import ASTNode\\nfrom cowboy_lib.test_modules.test_module import TestModule\\n\\nfrom src.test_modules.models import TestModuleModel\\n\\nfrom sqlalchemy.orm import Session\\nfrom .models import NodeModel\\n\\n\\ndef get_node(\\n    *, db_session: Session, node_name: str, repo_id: int, node_type: str, filepath: str\\n):\\n    return (\\n        db_session.query(NodeModel)\\n        .filter(\\n            NodeModel.name == node_name,\\n            NodeModel.repo_id == repo_id,\\n            NodeModel.node_type == node_type,\\n            NodeModel.testfilepath == filepath,\\n        )\\n        .one_or_none()\\n    )\\n\\n\\ndef create_node(\\n    *,\\n    db_session: Session,\\n    node: ASTNode,\\n    repo_id: int,\\n    filepath: str,\\n    test_module_id: str = None,\\n):\\n    node = NodeModel(\\n        name=node.name,\\n        node_type=node.node_type.value,\\n        repo_id=repo_id,\\n        test_module_id=test_module_id,\\n        testfilepath=filepath,\\n    )\\n\\n    db_session.add(node)\\n    db_session.commit()\\n\\n    return node', summary='', filepath='src\\\\ast\\\\service.py', metadata=None, node_id=''), CodeChunk(id='test_modules\\\\service.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def create_tm(*, db_session: Session, repo_id: str, tm: TestModule):\\n    \"\"\"Create a test module and the nodes\"\"\"\\n\\n    tm_model = TestModuleModel(\\n        name=tm.name,\\n        testfilepath=str(tm.test_file.path),\\n        commit_sha=tm.commit_sha,\\n        repo_id=repo_id,\\n    )\\n\\n    # need to commit before so node has access to tm_model.id\\n    db_session.add(tm_model)\\n    db_session.commit()\\n\\n    for node in tm.nodes:\\n        create_node(\\n            db_session=db_session,\\n            node=node,\\n            repo_id=repo_id,\\n            filepath=tm_model.testfilepath,\\n            test_module_id=tm_model.id,\\n        )\\n\\n    return tm_model', summary='', filepath='src\\\\test_modules\\\\service.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='auth\\\\models.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class CowboyUser(Base, TimeStampMixin):\\n    __tablename__ = \"cowboy_user\"\\n\\n    id = Column(Integer, primary_key=True)\\n    email = Column(String, unique=True)\\n    password = Column(LargeBinary, nullable=False)\\n    last_mfa_time = Column(DateTime, nullable=True)\\n    experimental_features = Column(Boolean, default=False)\\n\\n    # repos = relationship(\"Repo\", backref=\"cowboy_user\")\\n\\n    # search_vector = Column(\\n    #     TSVectorType(\"email\", regconfig=\"pg_catalog.simple\", weights={\"email\": \"A\"})\\n    # )\\n\\n    def check_password(self, password):\\n        return bcrypt.checkpw(password.encode(\"utf-8\"), self.password)\\n\\n    @property\\n    def token(self):\\n        now = datetime.utcnow()\\n        exp = (now + timedelta(seconds=COWBOY_JWT_EXP)).timestamp()\\n        data = {\\n            \"exp\": exp,\\n            \"email\": self.email,\\n        }\\n        return jwt.encode(data, COWBOY_JWT_SECRET, algorithm=COWBOY_JWT_ALG)', summary='', filepath='src\\\\auth\\\\models.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\service.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.repo import GitRepo, SourceRepo\\n\\nfrom src.utils import gen_random_name\\nfrom src.auth.models import CowboyUser\\nfrom src.test_modules.service import create_all_tms\\nfrom src.config import REPOS_ROOT\\n\\nfrom .models import RepoConfig, RepoConfigCreate\\n\\nfrom pathlib import Path\\nfrom logging import getLogger\\nfrom fastapi import HTTPException\\n\\n\\nlogger = getLogger(__name__)\\n\\n\\ndef get(*, db_session, curr_user: CowboyUser, repo_name: str) -> RepoConfig:\\n    \"\"\"Returns a repo based on the given repo name.\"\"\"\\n    return (\\n        db_session.query(RepoConfig)\\n        .filter(RepoConfig.repo_name == repo_name, RepoConfig.user_id == curr_user.id)\\n        .one_or_none()\\n    )', summary='', filepath='src\\\\repo\\\\service.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\service.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def get_by_id_or_raise(\\n    *, db_session, curr_user: CowboyUser, repo_id: int\\n) -> RepoConfig:\\n    \"\"\"Returns a repo based on the given repo id.\"\"\"\\n    repo = (\\n        db_session.query(RepoConfig)\\n        .filter(RepoConfig.id == repo_id, RepoConfig.user_id == curr_user.id)\\n        .one_or_none()\\n    )\\n    if not repo:\\n        raise HTTPException(status_code=400, detail=f\"Repo {repo_id} not found\")\\n\\n    return repo', summary='', filepath='src\\\\repo\\\\service.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\service.py::6', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def update(\\n    *, db_session, curr_user: CowboyUser, repo_name: int, repo_in: RepoConfigCreate\\n) -> RepoConfig:\\n    \"\"\"Updates a repo.\"\"\"\\n\\n    repo = get(db_session=db_session, curr_user=curr_user, repo_name=repo_name)\\n    if not repo:\\n        return None\\n\\n    repo.update(repo_in)\\n    db_session.commit()\\n\\n    return repo\\n\\n\\ndef create_or_update(\\n    *, db_session, curr_user: CowboyUser, repo_in: RepoConfigCreate\\n) -> RepoConfig:\\n    \"\"\"Create or update a repo\"\"\"\\n    print(\"Creating: \", repo_in)\\n    repo_conf = get(\\n        db_session=db_session, curr_user=curr_user, repo_name=repo_in.repo_name\\n    )\\n\\n    if not repo_conf:\\n        return create(db_session=db_session, curr_user=curr_user, repo_in=repo_in)\\n\\n    return update(\\n        db_session=db_session,\\n        curr_user=curr_user,\\n        repo_name=repo_in.repo_name,\\n        repo_in=repo_in,\\n    )\\n\\n\\ndef list(*, db_session, curr_user: CowboyUser) -> RepoConfig:\\n    \"\"\"Lists all repos for a user.\"\"\"\\n\\n    return db_session.query(RepoConfig).filter(RepoConfig.user_id == curr_user.id).all()', summary='', filepath='src\\\\repo\\\\service.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='coverage\\\\service.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.coverage import Coverage\\nfrom src.test_gen.models import AugmentTestResult\\n\\nfrom .models import CoverageModel\\n\\nfrom sqlalchemy.orm import Session\\n\\n\\ndef create_or_update_cov(\\n    *, db_session: Session, repo_id: int, coverage: Coverage, test_result_id: int = None\\n):\\n    \"\"\"Create or update a coverage model from a coverage object.\"\"\"\\n    cov_model = get_cov_by_filename(\\n        db_session=db_session, repo_id=repo_id, filename=coverage.filename\\n    )\\n\\n    if cov_model:\\n        # if it exists update\\n        cov_model.covered_lines = \",\".join(map(str, coverage.covered_lines))\\n        cov_model.missing_lines = \",\".join(map(str, coverage.missing_lines))\\n        cov_model.stmts = coverage.stmts\\n        cov_model.misses = coverage.misses\\n        cov_model.covered = coverage.covered\\n        cov_model.test_result_id = test_result_id\\n    else:\\n        # if it does not exist, create\\n        cov_model = CoverageModel(\\n            filename=coverage.filename,\\n            covered_lines=\",\".join(map(str, coverage.covered_lines)),\\n            missing_lines=\",\".join(map(str, coverage.missing_lines)),\\n            stmts=coverage.stmts,\\n            misses=coverage.misses,\\n            covered=coverage.covered,\\n            repo_id=repo_id,\\n            test_result_id=test_result_id,\\n        )\\n        db_session.add(cov_model)\\n\\n    db_session.commit()\\n\\n    return cov_model', summary='', filepath='src\\\\coverage\\\\service.py', metadata=None, node_id=''), CodeChunk(id='queue\\\\core.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class TaskQueue:\\n    \"\"\"\\n    A set of queues separated by user_id\\n    \"\"\"\\n\\n    _instance = None\\n\\n    def __new__(cls, *args, **kwargs):\\n        if not isinstance(cls._instance, cls):\\n            print(\"Creating new TaskQueue instance\")\\n            cls._instance = super(TaskQueue, cls).__new__(cls, *args, **kwargs)\\n            cls._instance._initialized = False\\n\\n        return cls._instance\\n\\n    def __init__(self):\\n        if not self._initialized:\\n            # Initialize instance variables only once\\n            self.queue: Dict[str, List[TaskEvent]] = defaultdict(list)\\n            self.locks = defaultdict(list)\\n            self._initialized = True  # Mark as initialized\\n\\n    def _acquire_lock(self, user_id: int):\\n        if self.locks.get(user_id, None) is None:\\n            self.locks[user_id] = Lock()\\n        return self.locks.get(user_id)\\n\\n    def put(self, user_id: int, task: str) -> TaskEvent:\\n        with self._acquire_lock(user_id):\\n            t = TaskEvent(task)\\n            self.queue[user_id].append(t)\\n\\n            return t\\n\\n    def complete(self, user_id: int, task_id: str, res):\\n        with self._acquire_lock(user_id):\\n            for i in range(len(self.queue[user_id])):\\n                if self.queue[user_id][i].task_id == task_id:\\n                    t = self.queue[user_id].pop(i)\\n                    t.complete(res)\\n                    break\\n\\n    # def get(self, user_id: int) -> Task:\\n    #     \"\"\"\\n    #     Returns the first PENDING task and changes its status to STARTED\\n    #     \"\"\"\\n    #     with self._acquire_lock(user_id):\\n    #         if len(self.queue[user_id]) == 0:\\n    #             return None\\n\\n    #         return self.queue[user_id].pop()\\n\\n    def get_all(self, user_id: int) -> List[Task]:\\n        with self._acquire_lock(user_id):\\n            if len(self.queue[user_id]) == 0:\\n                return []\\n\\n            tasks = []\\n            for t in filter(\\n                lambda t: t.task.status == TaskStatus.PENDING.value, self.queue[user_id]\\n            ):\\n                t.task.status = TaskStatus.STARTED.value\\n                tasks.append(t.task)\\n\\n            return tasks\\n\\n    def peak(self, user_id: int, n: int) -> List[Task]:\\n        \"\"\"\\n        Get the first n tasks in queue without removing\\n        \"\"\"\\n        with self._acquire_lock(user_id):\\n            if len(self.queue[user_id]) == 0:\\n                return []\\n\\n            return [t.task for t in self.queue[user_id][:n]]\\n\\n\\ndef get_queue(request: Request):\\n    return request.state.task_queue\\n\\n\\ndef get_token_registry(request: Request):\\n    from main import token_registry\\n\\n    return token_registry\\n\\n\\ndef get_token(request: Request):\\n    \"\"\"\\n    Returns the user id\\n    \"\"\"\\n    token = request.headers.get(\"x-task-auth\", None)\\n    # need this or else we end up converting None to \"None\" **shakes fist @ python moment\"\\n    return str(token) if token else None', summary='', filepath='src\\\\queue\\\\core.py', metadata=None, node_id=''), CodeChunk(id='queue\\\\service.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.api.runner.shared import Task\\n\\nfrom typing import Optional, List, Dict\\n\\nfrom src.queue.core import TaskQueue\\n\\n\\ndef list_tasks(*, task_queue: TaskQueue, user_id: int, n: int) -> Optional[List[Task]]:\\n    \"\"\"List all tasks in the queue.\"\"\"\\n\\n    return task_queue.peak(user_id, n)\\n\\n\\ndef dequeue_task(*, task_queue: TaskQueue, user_id: int) -> Optional[List[Task]]:\\n    \"\"\"Dequeue the first task in the queue: retrieve and delete it.\"\"\"\\n\\n    return task_queue.get_all(user_id)\\n\\n\\ndef complete_task(\\n    *, task_queue: TaskQueue, user_id: int, task_id: str, result: Dict\\n) -> None:\\n    \"\"\"Mark a task as completed.\"\"\"\\n\\n    task_queue.complete(user_id, task_id, result)\\n\\n\\ndef enqueue_task_and_wait(*, task_queue: TaskQueue, task: Task, user_id: int):\\n    \"\"\"Enqueue a task to the specified queue.\"\"\"\\n\\n    # LAUREN 5: This is where the task is put to the queue\\n    f = task_queue.put(user_id, task)\\n    print(len(task_queue.queue))\\n    return f', summary='', filepath='src\\\\queue\\\\service.py', metadata=None, node_id=''), CodeChunk(id='runner\\\\service.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.repo.repository import PatchFile\\nfrom cowboy_lib.coverage import CoverageResult\\nfrom cowboy_lib.ast.code import Function\\n\\nfrom src.queue.service import enqueue_task_and_wait\\nfrom src.queue.core import TaskQueue\\n\\nfrom .models import json_to_coverage_result, RunTestTask\\n\\nfrom typing import List, Tuple\\nfrom dataclasses import dataclass\\n\\n\\n@dataclass\\nclass RunServiceArgs:\\n    user_id: int\\n    repo_name: str\\n    task_queue: TaskQueue\\n\\n\\nasync def run_test(\\n    service_args: RunServiceArgs,\\n    exclude_tests: List[Tuple[Function, str]] = [],\\n    include_tests: List[str] = [],\\n    patch_file: PatchFile = None,\\n) -> CoverageResult:\\n\\n    task = RunTestTask(\\n        repo_name=service_args.repo_name,\\n        exclude_tests=exclude_tests,\\n        include_tests=include_tests,\\n        patch_file=patch_file,\\n    )\\n\\n    future = enqueue_task_and_wait(\\n        task_queue=service_args.task_queue, user_id=service_args.user_id, task=task\\n    )\\n    res = await future.wait()\\n\\n    cov_res = json_to_coverage_result(res)\\n    return cov_res', summary='', filepath='src\\\\runner\\\\service.py', metadata=None, node_id=''), CodeChunk(id='tasks\\\\create_tgt_coverage.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='# TODO:\\n@async_timed\\nasync def create_tgt_coverage(\\n    *,\\n    db_session: Session,\\n    task_queue: TaskQueue,\\n    curr_user: CowboyUser,\\n    repo_config: RepoConfig,\\n    tm_names: List[str],\\n    overwrite: bool = True\\n):\\n    \"\"\"\\n    Important function that sets up relationships between TestModule, TargetCode and\\n    Coverage\\n    \"\"\"\\n    src_repo = SourceRepo(Path(repo_config.source_folder))\\n    run_args = RunServiceArgs(curr_user.id, repo_config.repo_name, task_queue)\\n\\n    base_cov = await run_test(run_args)\\n    for cov in base_cov.coverage.cov_list:\\n        create_or_update_cov(\\n            db_session=db_session, repo_id=repo_config.id, coverage=cov\\n        )\\n\\n    # TODO: we should combine TMModel and TM into single object instead of serializing\\n    # and deserializing it\\n    tm_models = get_tms_by_names(\\n        db_session=db_session, repo_id=repo_config.id, tm_names=tm_names\\n    )\\n    tms = [tm_model.serialize(src_repo) for tm_model in tm_models]\\n\\n    if not overwrite:\\n        # we only want to baseline TMs that dont already have target code coverage\\n        tm_models = [tm for tm in tm_models if not tm.target_chunks]\\n        tms = [tm_model.serialize(src_repo) for tm_model in tm_models]\\n\\n    for tm_model, tm in zip(tm_models, tms):\\n        tm, targets = await build_tm_src_mapping(src_repo, tm, base_cov, run_args)\\n        for t in targets:\\n            print(\"Target code: \", t.filepath)\\n\\n        # store chunks and their nodes\\n        tgt_code_chunks = create_tgt_code_models(\\n            targets, db_session, repo_config.id, tm_model\\n        )\\n\\n        tm_model.target_chunks = tgt_code_chunks\\n        update_tm(db_session=db_session, tm_model=tm_model)', summary='', filepath='src\\\\tasks\\\\create_tgt_coverage.py', metadata=None, node_id=''), CodeChunk(id='tasks\\\\get_baseline.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='async def get_tm_target_coverage(\\n    src_repo: SourceRepo,\\n    tm: TestModule,\\n    base_cov: CoverageResult,\\n    run_args: RunServiceArgs,\\n) -> Tuple[TestModule, List[TargetCode]]:\\n    \"\"\"\\n    Test augmenting existing test classes by deleting random test methods, and then\\n    having LLM strategy generate them. Coverage is taken:\\n    1. After the deletion\\n    2. After the deletion with newly generated LLM testcases\\n\\n    The diff measures how well we are able to supplant the coverage of the deleted methods\\n    \"\"\"\\n\\n    if testfiles_in_coverage(base_cov.coverage, src_repo):\\n        raise TestInCoverageException\\n\\n    # First loop we find the total coverage of each test by itself\\n    only_module = [tm.name]\\n    # coverage with ONLY the current test module turned on\\n    print(\"Running initial test ... \", tm.name)\\n\\n    module_cov = await run_test(\\n        run_args,\\n        include_tests=only_module,\\n    )\\n\\n    module_diff = base_cov.coverage - module_cov.coverage\\n    total_cov_diff = module_diff.total_cov.covered\\n    if total_cov_diff > 0:\\n        # part 2:\\n        single_covs = []\\n        for test in tm.tests:\\n            print(\"Running test ... \", test.name)\\n            # tm.test_file.delete(test.name, node_type=test.type)\\n            # deleted_file = PatchFile(tm.test_file.path, tm.test_file.to_code())\\n            # with PatchFileContext(repo_ctxt.git_repo, deleted_file):\\n\\n            # exclude_test = get_exclude_path(test, tm.test_file.path)\\n            single_cov = await run_test(\\n                run_args,\\n                exclude_tests=[(test, tm.test_file.path)],\\n                include_tests=only_module,\\n            )\\n            print(\"Test results: \", single_cov.coverage.total_cov.covered)\\n\\n            print(\\n                f\"Module cov: {module_cov.coverage.total_cov.covered}, Single cov: {single_cov.coverage.total_cov.covered}\"\\n            )\\n\\n            single_diff = (module_cov.coverage - single_cov.coverage).cov_list\\n            for c in single_diff:\\n                logger.info(\\n                    f\"Changed coverage from deleting {test.name}:\\\\n {c.__str__()}\"\\n                )\\n\\n            # dont think we actually need this here .. confirm\\n            tm.test_file = src_repo.find_file(tm.test_file.path)\\n            single_covs.extend(single_diff)\\n\\n        # re-init the chunks according to the aggregated individual test coverages\\n        tm.set_chunks(\\n            single_covs,\\n            source_repo=src_repo,\\n            base_path=src_repo.repo_path,\\n        )\\n\\n        print(f\"Chunks: \\\\n{tm.print_chunks()}\")\\n    # Find out what\\'s the reason for the missed tests\\n    else:\\n        logger.info(f\"No coverage difference found for {tm.name}\")\\n\\n    return tm, tm.chunks', summary='', filepath='src\\\\tasks\\\\get_baseline.py', metadata=None, node_id=''), CodeChunk(id='tasks\\\\get_baseline_parallel.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='async def build_tm_src_mapping(\\n    src_repo: SourceRepo,\\n    tm: TestModule,\\n    base_cov: CoverageResult,\\n    run_args: RunServiceArgs,\\n) -> Tuple[TestModule, List[TargetCode]]:\\n    \"\"\"\\n    Builds the mapping from each test inside a test module to\\n    a chunk of a source code file. The algorithm works by diffing the new coverage\\n    of each individual test with the coverage of the test module as a whole. The diff\\n    in lines covered are the lines that the test is responsible for covering\\n    \"\"\"\\n\\n    if testfiles_in_coverage(base_cov.coverage, src_repo):\\n        raise TestInCoverageException\\n\\n    # First loop we find the total coverage of each test by itself\\n    only_module = [tm.name]\\n    # coverage with ONLY the current test module turned on\\n    print(\"Running initial test ... \", tm.name)\\n\\n    # LAUREN 4: First we collect the coverage metric for the entire test module\\n    # via the run_test method, which puts a run_test task on a queue that is\\n    # regularly poll\\'d by the client (remember all unit test execution has to be\\n    # done in the client environment where their environment is installed)\\n    module_cov = await run_test(\\n        run_args,\\n        include_tests=only_module,\\n    )\\n\\n    # LAUREN 8: Once run_test is finished, proceed to calculating the diff\\n    # between the coverage. Note that CoverageA - CoverageB is a __sub__ method\\n    # that just takes the set difference of Set(A) - Set(B), which is all of\\n    # the covered line in A but not B is\\n    module_diff = base_cov.coverage - module_cov.coverage\\n    total_cov_diff = module_diff.total_cov.covered\\n    if total_cov_diff > 0:\\n        chg_cov = []\\n        coroutines = []\\n\\n        for test in tm.tests:\\n            print(\"Running test ... \", test.name)\\n            # get coverage for test with the current test turned off\\n            task = run_test(\\n                run_args,\\n                exclude_tests=[(test, tm.test_file.path)],\\n                include_tests=only_module,\\n            )\\n            coroutines.append(task)\\n\\n        # do this to run tasks in paralell on the client\\n        cov_res = await asyncio.gather(*[t for t in coroutines])\\n        for test, cov_miss in zip(tm.tests, cov_res):\\n            print(\"Test results: \", cov_miss.coverage.total_cov.covered)\\n            print(\\n                f\"Module cov: {module_cov.coverage.total_cov.covered}, Single cov: {cov_miss.coverage.total_cov.covered}\"\\n            )\\n\\n            # get the coverage diff between the total test module coverage and the coverage\\n            # when a test is excluded\\n            single_diff = (module_cov.coverage - cov_miss.coverage).cov_list\\n            for c in single_diff:\\n                logger.info(\\n                    f\"Changed coverage from deleting {test.name}:\\\\n {c.__str__()}\"\\n                )\\n\\n            # dont think we actually need this here .. confirm\\n            chg_cov.extend(single_diff)\\n\\n        # re-init the chunks according to the aggregated individual test coverages\\n        tm.set_chunks(\\n            chg_cov,\\n            source_repo=src_repo,\\n            base_path=src_repo.repo_path,\\n        )\\n\\n        print(f\"Chunks: \\\\n{tm.print_chunks()}\")\\n    # Find out what\\'s the reason for the missed tests\\n    else:\\n        logger.info(f\"No coverage difference found for {tm.name}\")\\n\\n    return tm, tm.chunks', summary='', filepath='src\\\\tasks\\\\get_baseline_parallel.py', metadata=None, node_id=''), CodeChunk(id='evaluators\\\\augment_additive.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class AugmentAdditiveEvaluator(Evaluator):\\n    async def process_test_results(\\n        self,\\n        test_results: List[Tuple[CoverageResult, str]],\\n        tm: \"TestModule\",\\n        base_cov: TestCoverage,\\n    ) -> Tuple[\\n        List[Tuple[Function, TestCoverage]],\\n        List[Tuple[Function, TestError]],\\n        List[Function],\\n    ]:\\n        \"\"\"\\n        Sequentially build a set of coverage improving testcases, discarding any\\n        generated tests that dont contribute coverage improvements\\n        \"\"\"\\n        improved_tests: List[Tuple[Function, TestCoverage]] = []\\n        failed_tests: List[Tuple[Function, TestError]] = []\\n        noimprov_tests: List[Function] = []\\n\\n        for cov_res, cov_diff, test_file in test_results:\\n            if cov_diff:\\n                new_funcs = self.get_new_funcs(test_file, tm.path)\\n                # iterate each generated function and measure if it has coverage\\n                # improvement against the base\\n                for func in new_funcs:\\n                    test_error = cov_res.get_failed(func.name)\\n                    if test_error:\\n                        testgen_logger.info(f\"[FAILED] Generated Func: {func.name}\")\\n                        testgen_logger.info(f\"Code: {func.to_code()}\")\\n\\n                        failed_tests.append((func, test_error))\\n                        continue\\n\\n                    # TODO: make sure that this works for filename TMs as well\\n                    og_testfile = self.src_repo.find_file(tm.path).clone()\\n                    og_testfile.append(\\n                        func.to_code(), class_name=func.scope.name if func.scope else \"\"\\n                    )\\n\\n                    patch_file = PatchFile(str(tm.path), og_testfile.to_code())\\n                    indvtest_cov = await run_test(\\n                        service_args=self.run_args, patch_file=patch_file\\n                    )\\n\\n                    indv_improve = indvtest_cov.coverage - base_cov\\n                    if indv_improve.total_cov.covered > 0:\\n                        testgen_logger.info(f\"[IMPROVE] Generated Func: {func.name}\")\\n                        testgen_logger.info(f\"Code: {func.to_code()}\")\\n\\n                        improved_tests.append((func, indv_improve))\\n                    else:\\n                        testgen_logger.info(f\"[NOIMPROVE] Generated Func: {func.name}\")\\n                        testgen_logger.info(f\"Code: {func.to_code()}\")\\n\\n                        noimprov_tests.append((func, TestCoverage([])))\\n\\n        return improved_tests, failed_tests, noimprov_tests', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\evaluators\\\\augment_additive.py', metadata=None, node_id=''), CodeChunk(id='test_gen\\\\service.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from .models import AugmentTestResult, UserDecision\\n\\nfrom src.coverage.service import create_or_update_cov\\n\\nfrom typing import List\\nfrom src.utils import generate_id\\n\\n\\ndef save_all(*, db_session, test_results: List[AugmentTestResult]):\\n    for tr in test_results:\\n        db_session.add(tr)\\n    db_session.commit()\\n\\n\\ndef create_test_result(\\n    *,\\n    db_session,\\n    repo_id,\\n    name,\\n    test_case,\\n    cov_list,\\n    tm_id,\\n    commit_hash,\\n    testfile,\\n    session_id,\\n    classname=None\\n):\\n    tr_model = AugmentTestResult(\\n        name=name,\\n        test_case=test_case,\\n        test_module_id=tm_id,\\n        commit_hash=commit_hash,\\n        testfile=testfile,\\n        classname=classname,\\n        session_id=session_id,\\n        repo_id=repo_id,\\n    )\\n\\n    for cov in cov_list:\\n        create_or_update_cov(\\n            db_session=db_session,\\n            repo_id=repo_id,\\n            coverage=cov,\\n            test_result_id=tr_model.id,\\n        )\\n\\n    db_session.add(tr_model)\\n    db_session.commit()\\n\\n    return tr_model', summary='', filepath='src\\\\test_gen\\\\service.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='experiments\\\\augment_test.py::4', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def run_experiment(\\n    repo: RepoConfig,\\n    test_modules: List[TestModuleModel],\\n    to_keep: int = 0,\\n    to_delete: int = 0,\\n):\\n    src_repo = SourceRepo(Path(repo.source_folder))\\n    git_repo = GitRepo(Path(repo.source_folder))\\n    tms = [tm.serialize(src_repo) for tm in test_modules]\\n\\n    base_cov = repo.base_cov()\\n    nuked_branch = nuke_name_br(to_keep, to_delete)\\n\\n    if not git_repo.branch_exists(nuked_branch):\\n        print(\"Creating new branch\")\\n        create_nuked_branch(tms, src_repo, git_repo, nuked_branch, to_keep, to_delete)\\n\\n    git_repo.checkout(nuked_branch)\\n\\n    print(f\"Finished modifying repo: {src_repo.repo_path}\")\\n\\n    # for tm in test_modules:\\n    #     # we need to update fs so we can collect the coverage on the deleted tests\\n    #     # to compare it\\n    #     del_res, *_ = self.repo_ctxt.runner.run_test(exclude_tests=to_exclude)\\n    #     del_cov = base_cov.coverage - del_res.coverage\\n    #     logger.info(f\"Deleted coverage: {del_cov.total_cov.covered}\")\\n\\n    #     if del_cov.total_cov.covered <= 0:\\n    #         logger.info(\\n    #             f\"Skipping {tm.name}, no coverage difference: {del_cov.total_cov.misses}\"\\n    #         )\\n    #         zero_improvement_tests += f\"{tm.name}\\\\n\"\\n    #         continue\\n\\n    #     # TODO: we need to use base_path from RepoPath here\\n    #     # we may have just deleted the targeted coverage from abvoe\\n    #     tgt_file = tm.targeted_files()[0]\\n    #     tgt_filecov_before = del_cov.get_file_cov(\\n    #         tgt_file, self.repo_ctxt.repo_path\\n    #     )\\n    #     tgt_filecov_before.read_line_contents(self.repo_ctxt.repo_path)\\n    #     logger.info(\\n    #         f\"Target file coverage: {tgt_filecov_before} : {tgt_filecov_before.filename}\"\\n    #     )\\n\\n    #     total_coverage += del_cov.total_cov.covered\\n\\n    #     evaluator_inst: Evaluator = evaluator(\\n    #         self.repo_ctxt.runner,\\n    #         self.repo_ctxt.git_repo,\\n    #         self.repo_ctxt.src_repo,\\n    #     )\\n    #     strat_instance: BaseTestStrategy = strategy(\\n    #         tm,\\n    #         self.repo_ctxt,\\n    #         evaluator_inst,\\n    #         del_res,\\n    #         tgt_filecov_before,\\n    #     )\\n    #     improved, failed, no_improve = await strat_instance.generate_test(\\n    #         n_times=1\\n    #     )\\n\\n    #     print(\"Improved: \")\\n    #     for imp in improved:\\n    #         print(imp)\\n\\n    #     self.save_results(tm, del_cov, improved, failed, no_improve)\\n    #     num_improve = len(improved)\\n    #     num_failed = len(failed)\\n    #     num_noimprove = len(no_improve)\\n\\n    #     logger.info(\\n    #         f\"Results for TM: {tm.name} => Improve: {num_improve}, Failed: {num_failed}, NoImprove: {num_noimprove}\"\\n    #     )\\n\\n    #     total_improvement += sum(\\n    #         [test[1].total_cov.covered for test in improved]\\n    #     )\\n\\n    # except LintException as e:\\n    #     logger.info(f\"Linting error: {e} on {tm.path}\")\\r', summary='', filepath='src\\\\experiments\\\\augment_test.py', metadata=None, node_id=''), CodeChunk(id='experiments\\\\views.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from src.database.core import get_db\\nfrom src.exceptions import InvalidConfigurationError\\nfrom src.auth.service import get_current_user, CowboyUser\\nfrom src.repo.service import get_experiment\\nfrom src.test_modules.service import get_tms_by_names\\n\\nfrom .models import ExperimentRequest\\nfrom .augment_test import run_experiment\\n\\nfrom fastapi import APIRouter, Depends\\nfrom sqlalchemy.orm import Session\\nfrom pydantic.error_wrappers import ErrorWrapper, ValidationError\\n\\n\\nexp_router = APIRouter()\\n\\n\\n@exp_router.post(\"/experiment/create\")\\ndef create_experiment(\\n    exp_config: ExperimentRequest,\\n    db_session: Session = Depends(get_db),\\n    current_user: CowboyUser = Depends(get_current_user),\\n):\\n    repo = get_experiment(\\n        db_session=db_session, curr_user=current_user, repo_name=exp_config.repo_name\\n    )\\n    tm_models = get_tms_by_names(\\n        db_session=db_session, repo_id=repo.id, tm_names=exp_config.tms\\n    )\\n\\n    run_experiment(\\n        repo=repo,\\n        test_modules=tm_models,\\n        to_keep=int(exp_config.to_keep),\\n        to_delete=int(exp_config.to_delete),\\n    )', summary='', filepath='src\\\\experiments\\\\views.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\service.py::4', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def get_experiment(*, db_session, curr_user: CowboyUser, repo_name: str) -> RepoConfig:\\n    \"\"\"Returns a repo based on the given repo name.\"\"\"\\n\\n    return (\\n        db_session.query(RepoConfig)\\n        .filter(\\n            RepoConfig.repo_name == repo_name,\\n            RepoConfig.user_id == curr_user.id,\\n            RepoConfig.is_experiment == True,\\n        )\\n        .one_or_none()\\n    )\\n\\n\\ndef delete(*, db_session, curr_user: CowboyUser, repo_name: str) -> RepoConfig:\\n    \"\"\"Deletes a repo based on the given repo name.\"\"\"\\n\\n    repo = get(db_session=db_session, curr_user=curr_user, repo_name=repo_name)\\n    if repo:\\n        db_session.delete(repo)\\n        db_session.commit()\\n\\n        GitRepo.delete_repo(Path(repo.source_folder))\\n        return repo\\n\\n    return None\\n\\n\\ndef clean(*, db_session, curr_user: CowboyUser, repo_name: str) -> RepoConfig:\\n    \"\"\"Cleans repo branches.\"\"\"\\n\\n    repo = get(db_session=db_session, curr_user=curr_user, repo_name=repo_name)\\n    if repo:\\n        GitRepo.clean_branches(Path(repo.source_folder))\\n        return repo\\n\\n    return None', summary='', filepath='src\\\\repo\\\\service.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='experiments\\\\augment_test.py::5', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def run_experiment(\\n    repo: RepoConfig,\\n    test_modules: List[TestModuleModel],\\n    to_keep: int = 0,\\n    to_delete: int = 0,\\n):\\n    #     continue\\n\\n    # except SameNodeException as e:\\n    #     logger.info(f\"SameNodeException on class: {tm.name}\")\\n    #     continue\\n\\n    # except Exception as e:\\n    #     logger.info(\\n    #         f\"Exception on class: {tm.name} in {tm.path}\",\\n    #         exc_info=True,\\n    #     )\\n\\n    # logger.info(f\"Final Results: {results}\")\\n    # logger.info(f\"Total improvement: {total_improvement}/{total_coverage}\")\\n    # logger.info(f\"Zero improvement tests: {zero_improvement_tests}\")\\n    # logger.info(f\"Run complete for {self.repo_ctxt.exp_id} \")\\r', summary='', filepath='src\\\\experiments\\\\augment_test.py', metadata=None, node_id=''), CodeChunk(id='repo\\\\models.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.coverage import TestCoverage\\n\\nfrom sqlalchemy import Column, Integer, String, JSON, ForeignKey, Boolean\\nfrom sqlalchemy.orm import relationship\\nfrom pydantic import Field\\n\\nfrom src.models import CowboyBase\\nfrom src.database.core import Base\\nfrom src.config import Language\\n\\nfrom typing import List, Any, Dict, Optional\\n\\n\\nclass RepoConfig(Base):\\n    \"\"\"\\n    Stores configuration for a repository\\n    \"\"\"\\n\\n    __tablename__ = \"repo_config\"\\n\\n    id = Column(Integer, primary_key=True)\\n    repo_name = Column(String)\\n    url = Column(String)\\n    source_folder = Column(String)\\n    cloned_folders = Column(String)\\n    # git remote and git main branch (to merge into)\\n    remote = Column(String)\\n    main = Column(String)\\n    language = Column(String)\\n\\n    # keep this argument fluid, may change\\n    python_conf = Column(JSON)\\n    user_id = Column(Integer, ForeignKey(\"cowboy_user.id\"))\\n    is_experiment = Column(Boolean)\\n\\n    # relations\\n    test_modules = relationship(\\n        \"TestModuleModel\", backref=\"repo_config\", cascade=\"all, delete-orphan\"\\n    )\\n    nodes = relationship(\\n        \"NodeModel\", backref=\"repo_config\", cascade=\"all, delete-orphan\"\\n    )\\n    cov_list = relationship(\\n        \"CoverageModel\", backref=\"repo_config\", cascade=\"all, delete-orphan\"\\n    )\\n    stats = relationship(\"RepoStats\", uselist=False, cascade=\"all, delete-orphan\")\\n\\n    def __init__(\\n        self,\\n        repo_name,\\n        url,\\n        source_folder,\\n        cloned_folders,\\n        python_conf,\\n        user_id,\\n        remote,  # origin\\n        main,\\n        language,\\n        is_experiment=False,\\n    ):\\n        self.repo_name = repo_name\\n        self.url = url\\n        self.source_folder = source_folder\\n        self.cloned_folders = \",\".join(cloned_folders)\\n        self.python_conf = python_conf\\n        self.user_id = user_id\\n        self.remote = remote\\n        self.main = main\\n        self.language = language\\n        self.is_experiment = is_experiment\\n\\n    def to_dict(self):\\n        return {\\n            \"repo_name\": self.repo_name,\\n            \"url\": self.url,\\n            \"source_folder\": self.source_folder,\\n            \"cloned_folders\": self.cloned_folders.split(\",\"),\\n            \"python_conf\": self.python_conf,\\n            \"user_id\": self.user_id,\\n            \"remote\": self.remote,\\n            \"main\": self.main,\\n            \"language\": self.language,\\n            \"is_experiment\": self.is_experiment,\\n        }\\n\\n    def base_cov(self) -> TestCoverage:\\n        return TestCoverage([cov.deserialize() for cov in self.cov_list])', summary='', filepath='src\\\\repo\\\\models.py', metadata=None, node_id=''), CodeChunk(id='stats\\\\service.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from src.repo.models import RepoConfig\\nfrom .models import RepoStats\\n\\nfrom contextlib import contextmanager\\nfrom sqlalchemy.orm import Session\\n\\n\\n@contextmanager\\ndef update_repo_stats(*, db_session: Session, repo: RepoConfig):\\n    try:\\n        stats = db_session.query(RepoStats).filter_by(repo_id=repo.id).one_or_none()\\n        if not stats:\\n            stats = RepoStats(\\n                repo_id=repo.id, total_tests=0, accepted_tests=0, rejected_tests=0\\n            )\\n            db_session.add(stats)\\n        yield stats\\n\\n        db_session.commit()\\n    except Exception as e:\\n        db_session.rollback()\\n        raise e', summary='', filepath='src\\\\stats\\\\service.py', metadata=None, node_id=''), CodeChunk(id='test_gen\\\\augment.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='async def augment_test(\\n    *,\\n    db_session: Session,\\n    task_queue: TaskQueue,\\n    repo: RepoConfig,\\n    tm_model: TestModuleModel,\\n    curr_user: CowboyUser,\\n    session_id: str\\n) -> List[AugmentTestResult]:\\n    \"\"\"\\n    Generate test cases for the given test module using the specified strategy and evaluator\\n    \"\"\"\\n    src_repo = SourceRepo(Path(repo.source_folder))\\n    git_repo = GitRepo(Path(repo.source_folder), remote=repo.remote, main=repo.main)\\n    tm = tm_model.serialize(src_repo)\\n    run_args = RunServiceArgs(\\n        user_id=curr_user.id, repo_name=repo.repo_name, task_queue=task_queue\\n    )\\n\\n    base_cov = repo.base_cov()\\n    composer = Composer(\\n        strat=\"WITH_CTXT\",\\n        evaluator=\"ADDITIVE\",\\n        src_repo=src_repo,\\n        test_input=tm,\\n        run_args=run_args,\\n        base_cov=base_cov,\\n        api_key=retrieve_oai_key(curr_user.id),\\n    )\\n\\n    improved_tests, failed_tests, no_improve_tests = await composer.generate_test(\\n        n_times=1\\n    )\\n\\n    # write all improved test to source file and check out merge on repo\\n    # serialize tm first\\n    test_results = []\\n    test_file = tm.test_file\\n    for improved, cov in improved_tests:\\n        test_result = create_test_result(\\n            db_session=db_session,\\n            repo_id=repo.id,\\n            name=improved.name,\\n            test_case=improved.to_code(),\\n            cov_list=cov.cov_list,\\n            tm_id=tm_model.id,\\n            commit_hash=git_repo.get_curr_commit(),\\n            testfile=str(test_file.path),\\n            session_id=session_id,\\n            classname=None,\\n        )\\n        test_results.append(test_result)\\n\\n    # update repo stats\\n    with update_repo_stats(db_session=db_session, repo=repo) as repo_stats:\\n        repo_stats.total_tests += (\\n            len(improved_tests) + len(failed_tests) + len(no_improve_tests)\\n        )\\n\\n    return test_results', summary='', filepath='src\\\\test_gen\\\\augment.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='repo\\\\service.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def get_or_raise(*, db_session, curr_user: CowboyUser, repo_name: str) -> RepoConfig:\\n    \"\"\"Returns a repo based on the given repo name.\"\"\"\\n    repo = (\\n        db_session.query(RepoConfig)\\n        .filter(\\n            RepoConfig.repo_name == repo_name,\\n            RepoConfig.user_id == curr_user.id,\\n        )\\n        .one_or_none()\\n    )\\n    # TODO: consider raising pydantic Validation error here instead\\n    # seems to be what dispatch does\\n    if not repo:\\n        raise HTTPException(status_code=400, detail=f\"Repo {repo_name} not found\")\\n\\n    return repo', summary='', filepath='src\\\\repo\\\\service.py', metadata=None, node_id=''), CodeChunk(id='target_code\\\\views.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from fastapi import APIRouter, Depends, HTTPException\\nfrom src.database.core import get_db\\nfrom src.repo.service import get_or_raise\\nfrom src.auth.service import get_current_user\\n\\nfrom src.test_modules.service import get_tm_by_name\\n\\nfrom .models import TgtCodeDeleteRequest\\nfrom .service import delete_target_code\\n\\n\\ntgtcode_router = APIRouter()\\n\\n\\n@tgtcode_router.post(\"/tgt_code/delete/\")\\nasync def delete_tgt_code(\\n    tgt_delete_req: TgtCodeDeleteRequest,\\n    user=Depends(get_current_user),\\n    db=Depends(get_db),\\n):\\n    \"\"\"\\n    Delete all target code for a tesst module\\n    \"\"\"\\n    try:\\n        repo = get_or_raise(\\n            db_session=db, curr_user=user, repo_name=tgt_delete_req.repo_name\\n        )\\n        tm_model = get_tm_by_name(\\n            db_session=db, repo_id=repo.id, tm_name=tgt_delete_req.tm_name\\n        )\\n        deleted = delete_target_code(db_session=db, tm_id=tm_model.id)\\n\\n    except Exception as e:\\n        raise HTTPException(status_code=500, detail=str(e))\\n\\n    return {\"detail\": \"Target code deleted\"}', summary='', filepath='src\\\\target_code\\\\views.py', metadata=None, node_id=''), CodeChunk(id='test_gen\\\\views.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='@test_gen_router.post(\"/test-gen/augment\", response_model=AugmentTestResponse)\\nasync def augment_test_route(\\n    request: AugmentTestRequest,\\n    db_session=Depends(get_db),\\n    curr_user=Depends(get_current_user),\\n    task_queue=Depends(get_queue),\\n    session_id=Depends(get_session_id),\\n):\\n    \"\"\"\\n    Augment tests for a test module\\n    \"\"\"\\n    repo = get_or_raise(\\n        db_session=db_session, curr_user=curr_user, repo_name=request.repo_name\\n    )\\n    src_repo = SourceRepo(Path(repo.source_folder))\\n\\n    coroutines = []\\n    if request.mode == AugmentTestMode.AUTO.value:\\n        tm_models = get_all_tms_sorted(\\n            db_session=db_session, src_repo=src_repo, repo_id=repo.id, n=AUTO_GRP_SIZE\\n        )\\n    elif request.mode == AugmentTestMode.FILE.value:\\n        tm_models = get_tms_by_filename(\\n            db_session=db_session, repo_id=repo.id, src_file=request.src_file\\n        )\\n    elif request.mode == AugmentTestMode.TM.value:\\n        tm_models = get_tms_by_names(\\n            db_session=db_session, repo_id=repo.id, tm_names=request.tms\\n        )\\n    elif request.mode == AugmentTestMode.ALL.value:\\n        tm_models = get_tms_by_names(\\n            db_session=db_session, repo_id=repo.id, tm_names=[]\\n        )\\n\\n    for tm_model in tm_models:\\n        coroutine = augment_test(\\n            db_session=db_session,\\n            task_queue=task_queue,\\n            repo=repo,\\n            tm_model=tm_model,\\n            curr_user=curr_user,\\n            session_id=session_id,\\n        )\\n        coroutines.append(coroutine)\\n\\n    test_results = await asyncio.gather(*coroutines)\\n    test_results = reduce(lambda x, y: x + y, test_results)\\n\\n    # we save here after async ops have finished running\\n    save_all(db_session=db_session, test_results=test_results)\\n\\n    return AugmentTestResponse(session_id=session_id)', summary='', filepath='src\\\\test_gen\\\\views.py', metadata=None, node_id=''), CodeChunk(id='test_modules\\\\service.py::3', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def get_tm_by_name(\\n    *, db_session: Session, repo_id: str, tm_name: str\\n) -> TestModuleModel:\\n    \"\"\"\\n    Query by name and return all if no names are provided\\n    \"\"\"\\n\\n    query = db_session.query(TestModuleModel).filter(TestModuleModel.repo_id == repo_id)\\n    if tm_name:\\n        query = query.filter(TestModuleModel.name == tm_name)\\n\\n    return query.one_or_none()\\n\\n\\ndef get_tms_by_names(\\n    *, db_session: Session, repo_id: str, tm_names: List[str]\\n) -> List[TestModuleModel]:\\n    \"\"\"\\n    Query by name and return all if no names are provided\\n    \"\"\"\\n    if tm_names == []:\\n        return get_all_tms(db_session=db_session, repo_id=repo_id)\\n\\n    query = db_session.query(TestModuleModel).filter(TestModuleModel.repo_id == repo_id)\\n    if tm_names:\\n        query = query.filter(TestModuleModel.name.in_(tm_names))\\n\\n    return query.all()', summary='', filepath='src\\\\test_modules\\\\service.py', metadata=None, node_id=''), CodeChunk(id='test_modules\\\\service.py::4', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def update_tm(*, db_session: Session, tm_model: TestModuleModel):\\n    \"\"\"\\n    Updates an existing TM\\n    \"\"\"\\n    db_session.merge(tm_model)\\n    db_session.commit()\\n\\n    return tm_model\\n\\n\\ndef get_all_tms(*, db_session: Session, repo_id: str) -> List[TestModuleModel]:\\n    \"\"\"\\n    Query all TMs for a repo\\n    \"\"\"\\n    return (\\n        db_session.query(TestModuleModel)\\n        .filter(TestModuleModel.repo_id == repo_id)\\n        .all()\\n    )\\n\\n\\ndef get_tms_by_filename(\\n    *, db_session: Session, repo_id: str, src_file: str\\n) -> List[TestModuleModel]:\\n    \"\"\"\\n    Query all TMs for a repo\\n    \"\"\"\\n    all_tms = get_all_tms(db_session=db_session, repo_id=repo_id)\\n    return [tm for tm in all_tms if src_file in tm.get_covered_files()]', summary='', filepath='src\\\\test_modules\\\\service.py', metadata=None, node_id=''), CodeChunk(id='test_modules\\\\service.py::5', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def get_all_tms_sorted(\\n    *, db_session: Session, repo_id: str, src_repo: SourceRepo, n: int = 2\\n) -> List[TestModuleModel]:\\n    \"\"\"\\n    Query all TMs for a repo\\n    \"\"\"\\n    all_tms = get_all_tms(db_session=db_session, repo_id=repo_id)\\n    sorted_tms = sorted(all_tms, key=lambda tm: tm.agg_score(src_repo), reverse=True)\\n    select_tms = sorted_tms[:n]\\n\\n    # update auto_gen flag on TMs\\n    for tm in select_tms:\\n        tm.auto_gen = True\\n        update_tm(db_session=db_session, tm_model=tm)\\n\\n    return select_tms', summary='', filepath='src\\\\test_modules\\\\service.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='repo\\\\service.py::5', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='# CONSIDER: do we want to isolate create TM from create repo\\ndef create(\\n    *, db_session, curr_user: CowboyUser, repo_in: RepoConfigCreate\\n) -> RepoConfig:\\n    \"\"\"Creates a new repo.\"\"\"\\n\\n    repo_dst = None\\n    try:\\n        repo_conf = RepoConfig(\\n            **repo_in.dict(),\\n            user_id=curr_user.id,\\n        )\\n\\n        repo_dst = Path(REPOS_ROOT) / repo_conf.repo_name / gen_random_name()\\n        GitRepo.clone_repo(repo_dst, repo_conf.url)\\n\\n        src_repo = SourceRepo(repo_dst)\\n\\n        repo_conf.source_folder = str(repo_dst)\\n\\n        db_session.add(repo_conf)\\n        db_session.flush()\\n\\n        # LAUREN 2: Create TestModules (containers for unit tests, either a class or a source file)\\n        create_all_tms(db_session=db_session, repo_conf=repo_conf, src_repo=src_repo)\\n\\n        db_session.commit()\\n        return repo_conf\\n\\n    except Exception as e:\\n        db_session.rollback()\\n        if repo_dst:\\n            GitRepo.delete_repo(repo_dst)\\n\\n        logger.error(f\"Failed to create repo configuration: {e}\")\\n        raise', summary='', filepath='src\\\\repo\\\\service.py', metadata=None, node_id=''), CodeChunk(id='scripts\\\\neuter_repo.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def neuter_tests(\\n    test_modules: List[TestModule], src_repo: SourceRepo, to_keep, to_delete=0\\n):\\n    total_deleted = 0\\n    failed_mod = 0\\n    for tm in test_modules:\\n        try:\\n            print(\"Deleting tm: \", tm.name)\\n            to_exclude = []\\n            # BUG: tm.tests gets modified somehow\\n            num_to_del = num_delete(tm, to_keep=to_keep, to_delete=to_delete)\\n            total_tests = len(tm.tests)\\n\\n            for func in tm.tests[:num_to_del]:\\n                to_exclude.append((func, tm.test_file.path))\\n                # CARE: this operation has changes state of src_repo,\\n                # which is then propagated to strategy below\\n                src_repo.find_file(tm.path).delete(\\n                    func.name, node_type=NodeType.Function\\n                )\\n                # tm.test_file.delete(func.name, node_type=NodeType.Function)\\n\\n                with open(src_repo.repo_path / tm.test_file.path, \"w\") as f:\\n                    # print(tm.test_file.to_code())\\n                    f.write(src_repo.find_file(tm.path).to_code())\\n\\n                total_deleted += 1\\n        except Exception as e:\\n            failed_mod += 1\\n\\n    print(\"Total failed:\", failed_mod)\\n\\n\\nif __name__ == \"__main__\":\\n    repo = Path(sys.argv[1])\\n    if not repo.exists():\\n        print(\"Repo does not exist\")\\n        sys.exit()\\n\\n    src_repo = SourceRepo(repo)\\n    test_modules = iter_test_modules(src_repo)\\n\\n    neuter_tests(test_modules, src_repo, to_keep=2, to_delete=0)', summary='', filepath='src\\\\scripts\\\\neuter_repo.py', metadata=None, node_id=''), CodeChunk(id='test_gen\\\\service.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='def get_test_result_by_id_or_raise(*, db_session, test_id) -> AugmentTestResult:\\n    return (\\n        db_session.query(AugmentTestResult)\\n        .filter(AugmentTestResult.id == test_id)\\n        .one_or_none()\\n    )\\n\\n\\ndef get_test_results_by_sessionid(*, db_session, session_id) -> AugmentTestResult:\\n    return (\\n        db_session.query(AugmentTestResult)\\n        .filter(AugmentTestResult.session_id == session_id)\\n        .all()\\n    )\\n\\n\\ndef delete_test_results_by_sessionid(*, db_session, session_id):\\n    db_session.query(AugmentTestResult).filter(\\n        AugmentTestResult.session_id == session_id\\n    ).delete()\\n\\n    db_session.commit()\\n\\n\\ndef get_session_id():\\n    return generate_id()', summary='', filepath='src\\\\test_gen\\\\service.py', metadata=None, node_id=''), CodeChunk(id='test_modules\\\\iter_tms.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.utils import get_current_git_commit\\nfrom cowboy_lib.repo.source_repo import SourceRepo\\n\\nfrom src.test_modules.models import TestModule\\n\\nfrom typing import List\\n\\n\\ndef iter_test_modules(src_repo: SourceRepo) -> List[TestModule]:\\n    \"\"\"\\n    Generator for TestModules\\n    TestModules can be either be:\\n    1. All the individual functions inside a TestFile\\n    2. All the functions inside a class inside a TestFile\\n    3. Some of the individual functions inside a TestFile\\n    4. Some of the functions inside a class inside a TestFile\\n    \"\"\"\\n    test_modules: List[TestModule] = []\\n    for test_file in src_repo.test_files:\\n        ind_funcs = [f for f in test_file.test_funcs() if not f.scope]\\n        if ind_funcs:\\n            func_module = TestModule(\\n                test_file, ind_funcs, get_current_git_commit(src_repo.repo_path)\\n            )\\n            test_modules.append(func_module)\\n\\n        for test_class in test_file.test_classes():\\n            class_module = TestModule(\\n                test_file, [test_class], get_current_git_commit(src_repo.repo_path)\\n            )\\n            test_modules.append(class_module)\\n\\n    # NOTE: literally dont work and literally dont need\\n    # name_counter = Counter([tm.name for tm in test_modules])\\n    # while any([count > 1 for count in name_counter.values()]):\\n    #     for tm in test_modules:\\n    #         if name_counter[tm.name] > 1:\\n    #             tm.name = get_new_name(tm.name, tm.test_file.path)\\n    #             name_counter = Counter([tm.name for tm in test_modules])\\n\\n    return test_modules', summary='', filepath='src\\\\test_modules\\\\iter_tms.py', metadata=None, node_id=''), CodeChunk(id='test_modules\\\\service.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.repo import SourceRepo\\n\\nfrom src.repo.models import RepoConfig\\nfrom src.ast.service import create_node\\n\\nfrom sqlalchemy.orm import Session\\n\\n\\nfrom .models import TestModuleModel, TestModule\\nfrom .iter_tms import iter_test_modules\\n\\nfrom typing import List\\n\\n\\ndef create_all_tms(*, db_session: Session, repo_conf: RepoConfig, src_repo: SourceRepo):\\n    \"\"\"Create all test modules for a repo.\"\"\"\\n    test_modules = iter_test_modules(src_repo)\\n\\n    for tm in test_modules:\\n        create_tm(db_session=db_session, repo_id=repo_conf.id, tm=tm)', summary='', filepath='src\\\\test_modules\\\\service.py', metadata=None, node_id=''), CodeChunk(id='src\\\\utils.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='import functools\\nimport random\\nimport string\\nimport uuid\\nimport time\\nimport functools\\n\\nfrom src.logger import testgen_logger\\n\\n\\n# nested level get() function\\ndef resolve_attr(obj, attr, default=None):\\n    \"\"\"Attempts to access attr via dotted notation, returns none if attr does not exist.\"\"\"\\n    try:\\n        return functools.reduce(getattr, attr.split(\".\"), obj)\\n    except AttributeError:\\n        return default\\n\\n\\ndef gen_random_name():\\n    \"\"\"\\n    Generates a random name using ASCII, 8 characters in length\\n    \"\"\"\\n\\n    return \"\".join(random.choices(string.ascii_lowercase, k=8))\\n\\n\\ndef generate_id():\\n    \"\"\"\\n    Generates a random UUID\\n    \"\"\"\\n    return str(uuid.uuid4())\\n\\n\\ndef async_timed(func):\\n    @functools.wraps(func)\\n    async def wrapper(*args, **kwargs):\\n        start_time = time.time()\\n        result = await func(*args, **kwargs)\\n        end_time = time.time()\\n        testgen_logger.info(\\n            f\"[PARALLEL] Function {func.__name__} took {end_time - start_time:.4f} seconds\"\\n        )\\n        return result\\n\\n    return wrapper', summary='', filepath='src\\\\utils.py', metadata=None, node_id='')]\n",
      "[CodeChunk(id='augment_test\\\\base_strat.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.repo.source_repo import SourceRepo\\n\\nfrom abc import ABC, abstractmethod\\nfrom pathlib import Path\\nfrom dataclasses import dataclass\\n\\nfrom src.runner.service import RunServiceArgs\\n\\n\\n@dataclass\\nclass TestCaseInput(ABC):\\n\\n    @property\\n    @abstractmethod\\n    def path(self) -> Path:\\n        raise NotImplementedError\\n\\n\\nclass BaseStrategy(ABC):\\n    def __init__(self, src_repo: SourceRepo, test_input: TestCaseInput):\\n        self.src_repo = src_repo\\n        self.test_input = test_input\\n\\n    @abstractmethod\\n    def build_prompt(self) -> str:\\n        \"\"\"\\n        Builds the base prompt according to the strategy\\n        \"\"\"\\n\\n        raise NotImplementedError\\n\\n    @abstractmethod\\n    def parse_llm_res(self):\\n        \"\"\"\\n        Parses the LLM response to get the generated code\\n        \"\"\"\\n        raise NotImplementedError', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\base_strat.py', metadata=None, node_id=''), CodeChunk(id='augment_test\\\\composer.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class Composer:\\n    \"\"\"\\n    Used to instantiate different combinations of strategies for generating test cases\\n    \"\"\"\\n\\n    def __init__(\\n        self,\\n        strat: AugmentStratType,\\n        evaluator: EvaluatorType,\\n        src_repo: SourceRepo,\\n        test_input: TestCaseInput,\\n        run_args: RunServiceArgs,\\n        base_cov: TestCoverage,\\n        api_key: str,\\n        verify: bool = False,\\n    ):\\n        self.src_repo = src_repo\\n        self.test_input = test_input\\n        self.verify = verify\\n        self.base_cov = base_cov\\n        self.run_args = run_args\\n\\n        self.strat: BaseStrategy = AUGMENT_STRATS[strat](self.src_repo, self.test_input)\\n        self.evaluator: Evaluator = AUGMENT_EVALS[evaluator](\\n            self.src_repo, self.run_args\\n        )\\n\\n        model_name = \"gpt4\"\\n        self.model = OpenAIModel(ModelArguments(model_name=model_name, api_key=api_key))\\n\\n    def get_strat_name(self) -> str:\\n        return self.__class__.__name__\\n\\n    def filter_overlap_improvements(\\n        self, tests: List[Tuple[Function, TestCoverage]]\\n    ) -> List[Tuple[Function, TestCoverage]]:\\n        no_overlap = []\\n        overlap_cov = self.base_cov\\n        for test, cov in tests:\\n            new_cov = overlap_cov + cov\\n            if new_cov.total_cov.covered > overlap_cov.total_cov.covered:\\n                no_overlap.append((test, cov))\\n                overlap_cov = new_cov\\n\\n        return no_overlap\\n\\n    # TODO: this function name is a lie, we should parallelize this\\r', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\composer.py', metadata=None, node_id=''), CodeChunk(id='augment_test\\\\composer.py::4', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class Composer:\\n\\n    async def gen_test_serial_additive(self, n_times: int) -> Tuple[\\n        List[Tuple[Function, TestCoverage]],\\n        List[Tuple[Function, TestError]],\\n        List[Function],\\n    ]:\\n        if not isinstance(self.evaluator, AugmentAdditiveEvaluator):\\n            raise Exception(\\n                f\"Expected AugmentAdditiveEvaluator, got {self.evaluator.__class__}\"\\n            )\\n\\n        improved_tests = []\\n        failed_tests = []\\n        no_improve_tests = []\\n        prompt = self.strat.build_prompt()\\n\\n        print(\"Prompt: \", prompt)\\n\\n        for _ in range(n_times):\\n            retries = LLM_RETRIES\\n            src_file = None\\n            while retries > 0 and not src_file:\\n                try:\\n                    llm_res = await invoke_llm_async(\\n                        prompt,\\n                        model=self.model,\\n                        n_times=1,\\n                    )\\n                    src_file = self.strat.parse_llm_res(llm_res[0])\\n                except SyntaxError:\\n                    testgen_logger.info(f\"LLM syntax error ... {retries} left\")\\n                    retries -= 1\\n                    continue\\n                except ValueError:\\n                    testgen_logger.info(f\"LLM parsing format error ... {retries} left\")\\n                    retries -= 1\\n                    continue\\n\\n            if not src_file:\\n                raise CowboyRunTimeException(\\n                    f\"LLM generation failed for {self.test_input}\"\\n                )\\n\\n            test_result = [StratResult(src_file, self.test_input.path)]\\n\\n            # continue here\\n            # be careful about the fs state as represented by the test module\\n            # and that represented by the source repo\\n            # although i think if we limit our modifications to the test_input\\n            # we should be fine\\n            improved, failed, no_improve = await self.evaluator(\\n                test_result, self.test_input, self.base_cov, n_times=n_times\\n            )\\n            improved_tests.extend(improved)\\n            filtered_improved = self.filter_overlap_improvements(improved_tests)\\n            improved_tests = filtered_improved\\n\\n            # update test input with new functions that improved coverage\\n            for new_func in [\\n                func\\n                for func, _ in improved\\n                if func in [f[0] for f in filtered_improved]\\n            ]:\\n                self.test_input.test_file.append(\\n                    new_func.to_code(),\\n                    # wrong too, we need to check the\\n                    class_name=new_func.scope.name if new_func.scope else \"\",\\n                )\\n\\n            failed_tests.extend(failed)\\n            no_improve_tests.extend(no_improve)\\n\\n        return improved_tests, failed_tests, no_improve_tests\\n\\n    async def generate_test(self, n_times: int) -> Tuple[\\n        List[Tuple[Function, TestCoverage]],\\n        List[Tuple[Function, TestError]],\\n        List[Function],\\n    ]:\\n        if isinstance(self.evaluator, AugmentAdditiveEvaluator):\\n            return await self.gen_test_serial_additive(n_times)\\n        elif isinstance(self.evaluator, AugmentParallelEvaluator):\\n            return await self.gen_test_parallel(n_times)', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\composer.py', metadata=None, node_id=''), CodeChunk(id='evaluators\\\\augment_additive.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.coverage import CoverageResult, TestError, TestCoverage\\nfrom cowboy_lib.repo.repository import PatchFile\\nfrom cowboy_lib.repo.source_file import Function\\n\\nfrom typing import Tuple, List, TYPE_CHECKING\\n\\nif TYPE_CHECKING:\\n    from test_gen.augment_test.types import StratResult\\n    from cowboy_lib.test_modules import TestModule\\n\\nfrom .eval_base import Evaluator\\n\\nfrom src.runner.service import run_test\\nfrom src.logger import testgen_logger\\n\\n\\nclass AugmentAdditiveEvaluator(Evaluator):\\n    \"\"\"\\n    Iteratively evals test results and re-prompts with partially successful\\n    test file to **attempt** to get additive coverage\\n    \"\"\"\\n\\n    async def __call__(\\n        self,\\n        llm_results: List[\"StratResult\"],\\n        tm: \"TestModule\",\\n        base_cov: TestCoverage,\\n        n_times: int = 1,\\n    ) -> Tuple[\\n        List[Tuple[Function, TestCoverage]],\\n        List[Tuple[Function, TestError]],\\n        List[Function],\\n        \"TestModule\",\\n    ]:\\n        \"\"\"\\n        Main eval method, accepts a list of results from the strategy and the\\n        targeted test module, and a baseline coverage to compare against\\n        \"\"\"\\n        test_fp = tm.test_file.path\\n        test_results = await self.gen_test_and_diff_coverage(\\n            llm_results, base_cov, test_fp, n_times\\n        )\\n        improved, failed, no_improve = await self.process_test_results(\\n            test_results, tm, base_cov\\n        )\\n\\n        return improved, failed, no_improve\\n\\n    # questionable decision to make non-existent func Functions ..\\r', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\evaluators\\\\augment_additive.py', metadata=None, node_id=''), CodeChunk(id='evaluators\\\\augment_parallel.py::1', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='from cowboy_lib.coverage import CoverageResult, TestError, TestCoverage\\nfrom cowboy_lib.repo.repository import PatchFile\\nfrom cowboy_lib.repo.source_file import Function\\n\\nfrom typing import Tuple, List, TYPE_CHECKING\\n\\nif TYPE_CHECKING:\\n    from test_gen.augment_test.types import StratResult\\n    from cowboy_lib.test_modules import TestModule\\n\\nfrom .eval_base import Evaluator\\n\\nfrom concurrent.futures import ThreadPoolExecutor\\nfrom src.logger import testgen_logger\\n\\n\\nclass AugmentParallelEvaluator(Evaluator):\\n    \"\"\"\\n    Used to evaluate the results of a test strategy\\n    \"\"\"\\n\\n    async def __call__(\\n        self,\\n        llm_results: List[\"StratResult\"],\\n        tm: \"TestModule\",\\n        base_cov: CoverageResult,\\n        n_times: int = 1,\\n    ) -> Tuple[\\n        List[Tuple[Function, TestCoverage]],\\n        List[Tuple[Function, TestError]],\\n        List[Function],\\n    ]:\\n        \"\"\"\\n        Main eval method, accepts a list of results from the strategy and the\\n        targeted test module, and a baseline coverage to compare against\\n        \"\"\"\\n        test_fp = tm.test_file.path\\n        test_results = await self.gen_test_and_diff_coverage(\\n            llm_results, base_cov, test_fp, n_times\\n        )\\n        improved, failed, no_improve = await self.process_test_results(\\n            test_results, tm, base_cov\\n        )\\n\\n        return improved, failed, no_improve\\n\\n    # questionable decision to make non-existent func Functions ..\\r', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\evaluators\\\\augment_parallel.py', metadata=None, node_id=''), CodeChunk(id='evaluators\\\\eval_base.py::2', input_type=<ClusterInputType.CHUNK: 'chunk'>, content='class Evaluator(ABC):\\n    def __init__(self, src_repo: \"SourceRepo\", run_args: RunServiceArgs):\\n        self.src_repo = src_repo\\n        self.run_args = run_args\\n\\n    async def gen_test_and_diff_coverage(\\n        self,\\n        strat_results: List[\"StratResult\"],\\n        base_cov: TestCoverage,\\n        test_fp: Path,\\n        n_times: int = 1,\\n    ) -> List[Tuple[CoverageResult, TestCoverage]]:\\n        \"\"\"\\n        Does two runs:\\n        1. Run to get coverage baseline\\n        2. Run with generated test case\\n        Return diff in coverage, and generated test case\\n        \"\"\"\\n        test_results = []\\n        total_cost = 0\\n\\n        # WARNING: for some reason failures here are not recorded fully for every new individual\\n        # that is generate. Possibly due to failures cascading?\\n        for i, (test_file, test_funcs) in enumerate(strat_results, start=1):\\n            patch_file = PatchFile(path=test_fp, patch=test_file)\\n            cov_ptched = await run_test(\\n                service_args=self.run_args, patch_file=patch_file\\n            )\\n            cov_diff = cov_ptched.coverage - base_cov\\n            testgen_logger.info(\\n                f\"New coverage from generated tests: {cov_diff.total_cov.covered}\"\\n            )\\n            test_results.append((cov_ptched, cov_diff, test_file))\\n\\n        return test_results', summary='', filepath='src\\\\test_gen\\\\augment_test\\\\evaluators\\\\eval_base.py', metadata=None, node_id='')]\n"
     ]
    }
   ],
   "source": [
    "from src.chunk.adapter import convert_rtfs_clusters \n",
    "\n",
    "clusters = cg.get_clusters()\n",
    "adapter = convert_rtfs_clusters(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
