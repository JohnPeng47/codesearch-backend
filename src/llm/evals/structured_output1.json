Build Automation:

-> Chunk: build.py::2


import os
import subprocess
import shutil
import toml
import sys

def run_command(command, cwd=None):
    result = subprocess.run(command, shell=True, cwd=cwd, check=True)
    return result

def npm_install():
    print("Running npm install")
    run_command("npm install", cwd="ell-studio")


def npm_build():
    print("Running npm build")
    run_command("npm run build", cwd="ell-studio")
    print("Copying static files")
    source_dir = os.path.join("ell-studio", "build")
    target_dir = os.path.join("src", "ell", "studio", "static")
    shutil.rmtree(target_dir, ignore_errors=True)
    shutil.copytree(source_dir, target_dir)
    print(f"Copied static files from {source_dir} to {target_dir}")


def get_ell_version():
    pyproject_path = "pyproject.toml"
    pyproject_data = toml.load(pyproject_path)
    return pyproject_data["tool"]["poetry"]["version"]


def run_pytest():
    print("Running pytest")
    try:
        run_command("pytest", cwd="tests")
    except subprocess.CalledProcessError:
        print("Pytest failed. Aborting build.")
        sys.exit(1)


def run_all_examples():
    print("Running all examples")
    try:
        run_command("python run_all_examples.py -w 16", cwd="tests")
    except subprocess.CalledProcessError:
        print("Some examples failed. Please review the output above.")
        user_input = input("Do you want to continue with the build? (y/n): ").lower()
        if user_input != 'y':
            print("Aborting build.")
            sys.exit(1)


def main():
    ell_version = get_ell_version()
    os.environ['REACT_APP_ELL_VERSION'] = ell_version
    npm_install()
    npm_build()
    run_pytest()
    run_all_examples()
    print("Build completed successfully.")


if __name__ == "__main__":
    main()
====================================================================


Chatbot and Completions:

-> Chunk: 0.1.0\autostreamprevention.py::1


import openai
import os

# Define the function to stream the response
def stream_openai_response(prompt):
    try:
        # Make the API call
        response = openai.chat.completions.create(
            model="o1-mini",  # Specify the model
            messages=[{"role": "user", "content": prompt}],
            stream=True  # Enable streaming
        )

        # Stream the response
        for chunk in response:
            if chunk.choices[0].delta.get("content"):
                print(chunk.choices[0].delta.content, end="", flush=True)

        print()  # Print a newline at the end

    except Exception as e:
        print(f"An error occurred: {e}")

# Example usage
prompt = "Tell me a short joke."
stream_openai_response(prompt)
====================================================================


CartPole Evolutionary Algorithms:

-> Chunk: 0.1.0\cem.py::1


import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from gym.vector import AsyncVectorEnv
import random

# Set random seeds for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# Hyperparameters
NUM_ENVIRONMENTS = 4           # Reduced for simplicity
NUM_ITERATIONS = 50            # Number of training iterations
TRAJECTORIES_PER_ITER = 100    # Total number of trajectories per iteration
ELITE_PERCENT = 10             # Top k% trajectories to select
LEARNING_RATE = 1e-3
BATCH_SIZE = 64
MAX_STEPS = 500                # Max steps per trajectory
ENV_NAME = 'CartPole-v1'
====================================================================
-> Chunk: 0.1.0\cem.py::2


       # Gym environment

# Define the Policy Network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state):
        logits = self.fc(state)
        return logits

    def get_action(self, state):
        logits = self.forward(state)
        action_probs = torch.softmax(logits, dim=-1)
        action = torch.multinomial(action_probs, num_samples=1)
        return action.squeeze(-1)

# Function to create multiple environments
def make_env(env_name, seed):
    def _init():
        env = gym.make(env_name)
        return env
    return _init
====================================================================
-> Chunk: 0.1.0\cem.py::3


def collect_trajectories(envs, policy, num_trajectories, max_steps):
    trajectories = []
    num_envs = envs.num_envs

    # Handle the return type of reset()
    reset_output = envs.reset()
    if isinstance(reset_output, tuple) or isinstance(reset_output, list):
        obs = reset_output[0]  # Extract observations
    else:
        obs = reset_output

    done_envs = [False] * num_envs
    steps = 0

    # Initialize storage for states, actions, and rewards per environment
    env_states = [[] for _ in range(num_envs)]
    env_actions = [[] for _ in range(num_envs)]
    env_rewards = [0.0 for _ in range(num_envs)]
    total_collected = 0
    # ... other code
====================================================================
-> Chunk: 0.1.0\cem.py::4


def collect_trajectories(envs, policy, num_trajectories, max_steps):
    # ... other code

    while total_collected < num_trajectories and steps < max_steps:
        # Convert observations to tensor efficiently
        try:
            # Ensure 'obs' is a NumPy array
            if not isinstance(obs, np.ndarray):
                print(f"Unexpected type for observations: {type(obs)}")
                raise ValueError("Observations are not a NumPy array.")

            # Convert observations to tensor using from_numpy for efficiency
            obs_tensor = torch.from_numpy(obs).float()
            # Ensure the observation dimension matches expected
            assert obs_tensor.shape[1] == 4, f"Expected observation dimension 4, got {obs_tensor.shape[1]}"
        except Exception as e:
            print(f"Error converting observations to tensor at step {steps}: {e}")
            print(f"Observations: {obs}")
            raise e

        with torch.no_grad():
            actions = policy.get_action(obs_tensor).cpu().numpy()

        # Unpack step based on Gym version
        try:
            # For Gym versions >=0.26, step returns five values
            next_obs, rewards, dones, truncs, infos = envs.step(actions)
        except ValueError:
            # For older Gym versions, step returns four values
            next_obs, rewards, dones, infos = envs.step(actions)
            truncs = [False] * len(dones)  # Assume no truncations if not provided

        # Handle the reset output of step()
        if isinstance(next_obs, tuple) or isinstance(next_obs, list):
            next_obs = next_obs[0]  # Extract observations

        # Ensure infos is a list
        if not isinstance(infos, list):
            infos = [{} for _ in range(num_envs)]  # Default to empty dicts

        for i in range(num_envs):
            if not done_envs[i]:
                # Check if obs[i] has the correct shape
                if len(obs[i]) != 4:
                    print(f"Unexpected observation shape for env {i}: {obs[i]}")
                    continue  # Skip this step for the problematic environment

                env_states[i].append(obs[i])
                env_actions[i].append(actions[i])
                env_rewards[i] += rewards[i]
                if dones[i] or truncs[i]:
                    # Extract reward from infos
                    if isinstance(infos[i], dict):
                        episode_info = infos[i].get('episode', {})
                        traj_reward = episode_info.get('r') if 'r' in episode_info else env_rewards[i]
                    else:
                        # Handle cases where infos[i] is not a dict
                        traj_reward = env_rewards[i]
                        print(f"Warning: infos[{i}] is not a dict. Received type: {type(infos[i])}")

                    trajectories.append({
                        'states': env_states[i],
                        'actions': env_actions[i],
                        'reward': traj_reward
                    })
                    total_collected += 1
                    env_states[i] = []
                    env_actions[i] = []
                    env_rewards[i] = 0.0
                    done_envs[i] = True

        obs = next_obs
        steps += 1

        # Reset environments that are done
        if any(done_envs):
            indices = [i for i, done in enumerate(done_envs) if done]
            if total_collected < num_trajectories:
                for i in indices:
                    try:
                        # Directly reset the environment
                        reset_output = envs.envs[i].reset()
                        if isinstance(reset_output, tuple) or isinstance(reset_output, list):
                            # For Gym versions where reset returns (obs, info)
                            obs[i] = reset_output[0]
                        else:
                            # For Gym versions where reset returns only obs
                            obs[i] = reset_output
                        done_envs[i] = False
                    except Exception as e:
                        print(f"Error resetting environment {i}: {e}")
                        # Optionally, handle the failure (e.g., retry, terminate the environment)
                        done_envs[i] = False  # Prevent infinite loop

    return trajectories
====================================================================
-> Chunk: 0.1.0\cem.py::5


def select_elite(trajectories, percentile=ELITE_PERCENT):
    rewards = [traj['reward'] for traj in trajectories]
    if not rewards:
        return []
    reward_threshold = np.percentile(rewards, 100 - percentile)
    elite_trajectories = [traj for traj in trajectories if traj['reward'] >= reward_threshold]
    return elite_trajectories

# Function to create training dataset from elite trajectories
def create_training_data(elite_trajectories):
    states = []
    actions = []
    for traj in elite_trajectories:
        states.extend(traj['states'])
        actions.extend(traj['actions'])
    if not states or not actions:
        return None, None
    # Convert lists to NumPy arrays first for efficiency
    states = np.array(states, dtype=np.float32)
    actions = np.array(actions, dtype=np.int64)
    # Convert to PyTorch tensors
    states = torch.from_numpy(states)
    actions = torch.from_numpy(actions)
    return states, actions
====================================================================


Context Management and AST Processing:

-> Chunk: 0.1.0\context_versioning.py::1


import inspect
import ast
from contextlib import contextmanager

@contextmanager
def context():
    # Get the current frame
    frame = inspect.currentframe()
    try:
        # Get the caller's frame
        caller_frame = frame.f_back.f_back
        # Get the filename and line number where the context manager is called
        filename = caller_frame.f_code.co_filename
        lineno = caller_frame.f_lineno

        # Read the source code from the file
        with open(filename, 'r') as f:
            source = f.read()

        # Parse the source code into an AST
        parsed = ast.parse(source, filename)
        # print(source)
        # Find the 'with' statement at the given line number
        class WithVisitor(ast.NodeVisitor):
            def __init__(self, target_lineno):
                self.target_lineno = target_lineno
                self.with_node = None

            def visit_With(self, node):
                if node.lineno <= self.target_lineno <= node.end_lineno:
                    self.with_node = node
                self.generic_visit(node)

        visitor = WithVisitor(lineno)
        visitor.visit(parsed)

        # print(parsed, source)
        if visitor.with_node:
            # Extract the source code of the block inside 'with'
            start = visitor.with_node.body[0].lineno
            end = visitor.with_node.body[-1].end_lineno
            block_source = '\n'.join(source.splitlines()[start-1:end])
            print("Source code inside 'with' block:")
            print(block_source)
        else:
            print("Could not find the 'with' block.")

        # Yield control to the block inside 'with'
        yield
    finally:
        # Any cleanup can be done here
        pass

from context_versioning import context
# Example usage
if __name__ == "__main__":
    with context():
        x = 10
        y = x * 2
        print(y)
====================================================================


Policy Testing:

-> Chunk: 0.1.0\cem.py::6


# Main execution code
if __name__ == '__main__':
    # Initialize environments
    env_fns = [make_env(ENV_NAME, SEED + i) for i in range(NUM_ENVIRONMENTS)]
    envs = AsyncVectorEnv(env_fns)

    # Get environment details
    dummy_env = gym.make(ENV_NAME)
    state_dim = dummy_env.observation_space.shape[0]
    action_dim = dummy_env.action_space.n
    dummy_env.close()

    # Initialize policy network and optimizer
    policy = PolicyNetwork(state_dim, action_dim)
    optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    # Training Loop
    for iteration in range(1, NUM_ITERATIONS + 1):
        try:
            # Step 1: Collect Trajectories
            trajectories = collect_trajectories(envs, policy, TRAJECTORIES_PER_ITER, MAX_STEPS)
        except Exception as e:
            print(f"Error during trajectory collection at iteration {iteration}: {e}")
            break

        # Step 2: Select Elite Trajectories
        elite_trajectories = select_elite(trajectories, ELITE_PERCENT)

        if len(elite_trajectories) == 0:
            print(f"Iteration {iteration}: No elite trajectories found. Skipping update.")
            continue

        # Step 3: Create Training Data
        states, actions = create_training_data(elite_trajectories)

        if states is None or actions is None:
            print(f"Iteration {iteration}: No training data available. Skipping update.")
            continue

        # Step 4: Behavioral Cloning (Policy Update)
        dataset_size = states.size(0)
        indices = np.arange(dataset_size)
        np.random.shuffle(indices)

        for start in range(0, dataset_size, BATCH_SIZE):
            end = start + BATCH_SIZE
            batch_indices = indices[start:end]
            batch_states = states[batch_indices]
            batch_actions = actions[batch_indices]

            optimizer.zero_grad()
            logits = policy(batch_states)
            loss = criterion(logits, batch_actions)
            loss.backward()
            optimizer.step()

        # Step 5: Evaluate Current Policy
        avg_reward = np.mean([traj['reward'] for traj in elite_trajectories])
        print(f"Iteration {iteration}: Elite Trajectories: {len(elite_trajectories)}, Average Reward: {avg_reward:.2f}")

    # Close environments
    envs.close()

    # Testing the Trained Policy
    def test_policy(policy, env_name=ENV_NAME, episodes=5, max_steps=500):
        env = gym.make(env_name)
        total_rewards = []
        for episode in range(episodes):
            obs, _ = env.reset()
            done = False
            episode_reward = 0
            for _ in range(max_steps):
                obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)
                with torch.no_grad():
                    action = policy.get_action(obs_tensor).item()
                obs, reward, done, info, _ = env.step(action)
                episode_reward += reward
                if done:
                    break
            total_rewards.append(episode_reward)
            print(f"Test Episode {episode + 1}: Reward: {episode_reward}")
        env.close()
        print(f"Average Test Reward over {episodes} episodes: {np.mean(total_rewards):.2f}")

    # Run the test
    test_policy(policy)
====================================================================


Simple Policy Networks:

-> Chunk: 0.1.0\cpbo.py::1


import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import namedtuple
from torch.utils.data import DataLoader, TensorDataset

# Define a simple policy network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)  # Output action probabilities
        )

    def forward(self, x):
        return self.network(x)
====================================================================
-> Chunk: 0.1.0\cpbo.py::2


# Function to collect trajectories
def collect_trajectories(env, policy, num_episodes, device):
    trajectories = []
    Episode = namedtuple('Episode', ['states', 'actions', 'rewards'])

    for episode_num in range(num_episodes):
        states = []
        actions = []
        rewards = []
        # Handle Gym's updated reset() API
        state, info = env.reset(seed=42 + episode_num)  # Optional: set seed for reproducibility
        done = False

        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                action_probs = policy(state_tensor)
            action_dist = torch.distributions.Categorical(action_probs)
            action = action_dist.sample().item()

            # Handle Gym's updated step() API
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            states.append(state)
            actions.append(action)
            rewards.append(reward)

            state = next_state

        trajectories.append(Episode(states, actions, rewards))

    return trajectories

# Function to compute returns
def compute_returns(trajectories, gamma=0.99):
    all_returns = []
    for episode in trajectories:
        returns = []
        G = 0
        for reward in reversed(episode.rewards):
            G = reward + gamma * G
            returns.insert(0, G)
        all_returns.extend(returns)
    return all_returns
====================================================================
-> Chunk: 0.1.0\cpbo.py::3


# Function to create labeled dataset
def create_labeled_dataset(trajectories, gamma=0.99, device='cpu'):
    states = []
    actions = []
    labels = []

    all_returns = compute_returns(trajectories, gamma)
    all_returns = np.array(all_returns)
    median_return = np.median(all_returns)

    for episode in trajectories:
        for t in range(len(episode.rewards)):
            # Compute return from timestep t
            G = sum([gamma**k * episode.rewards[t + k] for k in range(len(episode.rewards) - t)])
            label = 1 if G >= median_return else 0
            states.append(episode.states[t])
            actions.append(episode.actions[t])
            labels.append(label)

    # Convert lists to NumPy arrays first for efficiency
    states = np.array(states)
    actions = np.array(actions)
    labels = np.array(labels)

    # Convert to PyTorch tensors
    states = torch.FloatTensor(states).to(device)
    actions = torch.LongTensor(actions).to(device)
    labels = torch.FloatTensor(labels).to(device)

    return states, actions, labels
====================================================================
-> Chunk: 0.1.0\cpbo.py::4


# Function to perform behavioral cloning update
def behavioral_cloning_update(policy, optimizer, dataloader, device):
    criterion = nn.BCELoss()
    policy.train()

    for states, actions, labels in dataloader:
        optimizer.zero_grad()
        action_probs = policy(states)
        # Gather the probability of the taken action
        selected_probs = action_probs.gather(1, actions.unsqueeze(1)).squeeze(1)
        # Labels are 1 for good actions, 0 for bad actions
        loss = criterion(selected_probs, labels)
        loss.backward()
        optimizer.step()
====================================================================
-> Chunk: 0.1.0\cpbo.py::5


# Evaluation function
def evaluate_policy(env, policy, device, episodes=5):
    policy.eval()
    total_rewards = []
    for _ in range(episodes):
        state, info = env.reset()
        done = False
        ep_reward = 0
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                action_probs = policy(state_tensor)
            action = torch.argmax(action_probs, dim=1).item()
            # Handle Gym's updated step() API
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            ep_reward += reward
            state = next_state
        total_rewards.append(ep_reward)
    average_reward = np.mean(total_rewards)
    return average_reward
====================================================================


CartPole Policy Execution:

-> Chunk: 0.1.0\cpbo.py::6


# Main CBPO algorithm
def CBPO(env_name='CartPole-v1', num_epochs=10, num_episodes_per_epoch=100, gamma=0.99, 
         batch_size=64, learning_rate=1e-3, device='cpu'):

    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")

        # 1. Collect trajectories
        trajectories = collect_trajectories(env, policy, num_episodes_per_epoch, device)

        # 2. Create labeled dataset
        states, actions, labels = create_labeled_dataset(trajectories, gamma, device)

        # 3. Create DataLoader
        dataset = TensorDataset(states, actions, labels)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        # 4. Behavioral Cloning Update
        behavioral_cloning_update(policy, optimizer, dataloader, device)

        # 5. Evaluate current policy
        avg_reward = evaluate_policy(env, policy, device)
        print(f"Average Reward: {avg_reward}")

        # Early stopping if solved
        if avg_reward >= env.spec.reward_threshold:
            print(f"Environment solved in {epoch+1} epochs!")
            break

    env.close()
    return policy
====================================================================
-> Chunk: 0.1.0\cpbo.py::7


if __name__ == "__main__":
    # Check if GPU is available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Run CBPO
    trained_policy = CBPO(
        env_name='CartPole-v1',
        num_epochs=50,
        num_episodes_per_epoch=500,
        gamma=0.99,
        batch_size=64,
        learning_rate=1e-3,
        device=device
    )

    # Final Evaluation
    env = gym.make('CartPole-v1')
    final_avg_reward = evaluate_policy(env, trained_policy, device, episodes=20)
    print(f"Final Average Reward over 20 episodes: {final_avg_reward}")
    env.close()

    # Save the trained policy
    torch.save(trained_policy.state_dict(), "trained_cartpole_policy.pth")
    print("Trained policy saved to trained_cartpole_policy.pth")

    # Demo the trained policy with rendering
    env = gym.make('CartPole-v1', render_mode='human')
    state, _ = env.reset()
    done = False
    total_reward = 0

    while not done:
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        action = trained_policy(state_tensor).argmax().item()
        state, reward, terminated, truncated, _ = env.step(action)
        total_reward += reward
        done = terminated or truncated
        env.render()

    print(f"Demo episode finished with total reward: {total_reward}")
    env.close()
====================================================================


Metaprompting with Torch:

-> Chunk: 0.1.0\metapromptingtorch.py::1


import torch as th


weights = th.nn.Parameter(th.randn(10))


def forward(x):
    return x * weights


x = th.randn(10)

print(forward(x))
print(weights)

# OOOH WAHT IF WE DID MANY TYPES OF LEARNABLES in
====================================================================


Type Testing with TypedDict:

-> Chunk: 0.1.0\mypytest.py::1


from typing import TypedDict


class Test(TypedDict):
    name: str
    age: int


def test(**t: Test):
    print(t)

# no type hinting like ts thats unfortunate.
test( )
====================================================================


Callable Testing and Decorators:

-> Chunk: 0.1.0\test.py::1


from typing import Callable

# The follwoing works...



def decorator(fn : Callable):
    def wrapper(*args, **kwargs):
        print("before")
        result = fn(*args, **kwargs)
        print("after")
        return result
    return wrapper


class TestCallable:
    def __init__(self, fn : Callable):
        self.fn = fn

    def __call__(self, *args, **kwargs):
        return self.fn(*args, **kwargs)

def convert_to_test_callable(fn : Callable):
    return TestCallable(fn)

x = TestCallable(lambda : 1)

@decorator
@convert_to_test_callable
def test():
    print("test")

@decorator
class MyCallable:
    def __init__(self, fn : Callable):
        self.fn = fn

    def __call__(self, *args, **kwargs):
        return self.fn(*args, **kwargs)

# Oh so now ell.simples can actually be used as decorators on classes
====================================================================


Documentation Configuration:

-> Chunk: src\conf.py::1


# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

project = 'ell'
copyright = '2024, William Guss'
author = 'William Guss'

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.napoleon', 'sphinxawesome_theme', 'sphinxcontrib.autodoc_pydantic']

templates_path = ['_templates']
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

html_theme = "sphinxawesome_theme"


# Favicon configuration
html_favicon = '_static/favicon.ico'

# Configure syntax highlighting for Awesome Sphinx Theme
pygments_style = "default"
pygments_style_dark = "dracula"

# Additional theme configuration
====================================================================
-> Chunk: src\conf.py::2


html_theme_options = {
    "show_prev_next": True,
    "show_scrolltop": True,
    "main_nav_links": {
        "Docs": "index",
        "API Reference": "reference/index",
        "AI Jobs Board": "https://jobs.ell.so",
    },
    "extra_header_link_icons": {
        "Discord": {
        "link": "https://discord.gg/vWntgU52Xb",
            "icon": """<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" height="18" fill="currentColor"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M524.5 69.8a1.5 1.5 0 0 0 -.8-.7A485.1 485.1 0 0 0 404.1 32a1.8 1.8 0 0 0 -1.9 .9 337.5 337.5 0 0 0 -14.9 30.6 447.8 447.8 0 0 0 -134.4 0 309.5 309.5 0 0 0 -15.1-30.6 1.9 1.9 0 0 0 -1.9-.9A483.7 483.7 0 0 0 116.1 69.1a1.7 1.7 0 0 0 -.8 .7C39.1 183.7 18.2 294.7 28.4 404.4a2 2 0 0 0 .8 1.4A487.7 487.7 0 0 0 176 479.9a1.9 1.9 0 0 0 2.1-.7A348.2 348.2 0 0 0 208.1 430.4a1.9 1.9 0 0 0 -1-2.6 321.2 321.2 0 0 1 -45.9-21.9 1.9 1.9 0 0 1 -.2-3.1c3.1-2.3 6.2-4.7 9.1-7.1a1.8 1.8 0 0 1 1.9-.3c96.2 43.9 200.4 43.9 295.5 0a1.8 1.8 0 0 1 1.9 .2c2.9 2.4 6 4.9 9.1 7.2a1.9 1.9 0 0 1 -.2 3.1 301.4 301.4 0 0 1 -45.9 21.8 1.9 1.9 0 0 0 -1 2.6 391.1 391.1 0 0 0 30 48.8 1.9 1.9 0 0 0 2.1 .7A486 486 0 0 0 610.7 405.7a1.9 1.9 0 0 0 .8-1.4C623.7 277.6 590.9 167.5 524.5 69.8zM222.5 337.6c-29 0-52.8-26.6-52.8-59.2S193.1 219.1 222.5 219.1c29.7 0 53.3 26.8 52.8 59.2C275.3 311 251.9 337.6 222.5 337.6zm195.4 0c-29 0-52.8-26.6-52.8-59.2S388.4 219.1 417.9 219.1c29.7 0 53.3 26.8 52.8 59.2C470.7 311 447.5 337.6 417.9 337.6z"/></svg>""",
            "type": "font-awesome",
            "name": "Discord",
        },
    },

    "logo_light": "_static/ell-wide-light.png",
    "logo_dark": "_static/ell-wide-dark.png",
    
}

html_static_path = ['_static']



templates_path = ['_templates']
====================================================================


Ell LMP Initialization:

-> Chunk: ell\__init__.py::1


"""
ell is a Python library for language model programming (LMP). It provides a simple
and intuitive interface for working with large language models.
"""


from ell.lmp.simple import simple
from ell.lmp.tool import tool
from ell.lmp.complex import complex
from ell.types.message import system, user, assistant, Message, ContentBlock
from ell.__version__ import __version__

# Import all models
import ell.providers
import ell.models


# Import everything from configurator
from ell.configurator import *
====================================================================


Version Management:

-> Chunk: ell\__version__.py::1


try:
    from importlib.metadata import version
except ImportError:
    from importlib_metadata import version

__version__ = version("ell-ai")
====================================================================


Configuration Handling:

-> Chunk: ell\configurator.py::1


from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)
====================================================================
-> Chunk: ell\configurator.py::2


class Config(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    registry: Dict[str, _Model] = Field(default_factory=dict, description="A dictionary mapping model names to their configurations.")
    verbose: bool = Field(default=False, description="If True, enables verbose logging.")
    wrapped_logging: bool = Field(default=True, description="If True, enables wrapped logging for better readability.")
    override_wrapped_logging_width: Optional[int] = Field(default=None, description="If set, overrides the default width for wrapped logging.")
    store: Optional[Store] = Field(default=None, description="An optional Store instance for persistence.")
    autocommit: bool = Field(default=False, description="If True, enables automatic committing of changes to the store.")
    lazy_versioning: bool = Field(default=True, description="If True, enables lazy versioning for improved performance.")
    default_api_params: Dict[str, Any] = Field(default_factory=dict, description="Default parameters for language models.")
    default_client: Optional[openai.Client] = Field(default=None, description="The default OpenAI client used when a specific model client is not found.")
    autocommit_model: str = Field(default="gpt-4o-mini", description="When set, changes the default autocommit model from GPT 4o mini.")
    providers: Dict[Type, Provider] = Field(default_factory=dict, description="A dictionary mapping client types to provider classes.")
    def __init__(self, **data):
        super().__init__(**data)
        self._lock = threading.Lock()
        self._local = threading.local()
====================================================================
-> Chunk: ell\configurator.py::3


class Config(BaseModel):


    def register_model(
        self, 
        name: str,
        default_client: Optional[Union[openai.Client, Any]] = None,
        supports_streaming: Optional[bool] = None
    ) -> None:
        """
        Register a model with its configuration.
        """
        with self._lock:
            # XXX: Will be deprecated in 0.1.0
            self.registry[name] = _Model(
                name=name,
                default_client=default_client,
                supports_streaming=supports_streaming
            )
====================================================================
-> Chunk: ell\configurator.py::4


class Config(BaseModel):



    @contextmanager
    def model_registry_override(self, overrides: Dict[str, _Model]):
        """
        Temporarily override the model registry with new model configurations.

        :param overrides: A dictionary of model names to ModelConfig instances to override.
        :type overrides: Dict[str, ModelConfig]
        """
        if not hasattr(self._local, 'stack'):
            self._local.stack = []

        with self._lock:
            current_registry = self._local.stack[-1] if self._local.stack else self.registry
            new_registry = current_registry.copy()
            new_registry.update(overrides)

        self._local.stack.append(new_registry)
        try:
            yield
        finally:
            self._local.stack.pop()
====================================================================
-> Chunk: ell\configurator.py::5


class Config(BaseModel):

    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:
        """
        Get the OpenAI client for a specific model name.

        :param model_name: The name of the model to get the client for.
        :type model_name: str
        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.
        :rtype: Tuple[Optional[openai.Client], bool]
        """
        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry
        model_config = current_registry.get(model_name)
        fallback = False
        if not model_config:
            warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."
            if self.verbose:
                from colorama import Fore, Style
                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")
            else:
                _config_logger.debug(warning_message)
            client = self.default_client
            fallback = True
        else:
            client = model_config.default_client
        return client, fallback
====================================================================
-> Chunk: ell\configurator.py::6


class Config(BaseModel):

    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:
        """
        Register a provider class for a specific client type.

        :param provider_class: The provider class to register.
        :type provider_class: Type[Provider]
        """
        assert isinstance(client_type, type), "client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))"
        with self._lock:
            self.providers[client_type] = provider
====================================================================
-> Chunk: ell\configurator.py::7


class Config(BaseModel):

    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:
        """
        Get the provider instance for a specific client instance.

        :param client: The client instance to get the provider for.
        :type client: Any
        :return: The provider instance for the specified client, or None if not found.
        :rtype: Optional[Provider]
        """

        client_type = type(client) if not isinstance(client, type) else client
        for provider_type, provider in self.providers.items():
            if issubclass(client_type, provider_type) or client_type == provider_type:
                return provider
        return None

# Single* instance
# XXX: Make a singleton
config = Config()
====================================================================
-> Chunk: ell\configurator.py::8


def init(
    store: Optional[Union[Store, str]] = None,
    verbose: bool = False,
    autocommit: bool = True,
    lazy_versioning: bool = True,
    default_api_params: Optional[Dict[str, Any]] = None,
    default_client: Optional[Any] = None,
    autocommit_model: str = "gpt-4o-mini"
) -> None:
    """
    Initialize the ELL configuration with various settings.

    :param verbose: Set verbosity of ELL operations.
    :type verbose: bool
    :param store: Set the store for ELL. Can be a Store instance or a string path for SQLiteStore.
    :type store: Union[Store, str], optional
    :param autocommit: Set autocommit for the store operations.
    :type autocommit: bool
    :param lazy_versioning: Enable or disable lazy versioning.
    :type lazy_versioning: bool
    :param default_api_params: Set default parameters for language models.
    :type default_api_params: Dict[str, Any], optional
    :param default_openai_client: Set the default OpenAI client.
    :type default_openai_client: openai.Client, optional
    :param autocommit_model: Set the model used for autocommitting.
    :type autocommit_model: str
    """
    # XXX: prevent double init
    config.verbose = verbose
    config.lazy_versioning = lazy_versioning

    if isinstance(store, str):
        from ell.stores.sql import SQLiteStore
        config.store = SQLiteStore(store)
    else:
        config.store = store
    config.autocommit = autocommit or config.autocommit

    if default_api_params is not None:
        config.default_api_params.update(default_api_params)

    if default_client is not None:
        config.default_client = default_client

    if autocommit_model is not None:
        config.autocommit_model = autocommit_model
====================================================================
-> Chunk: ell\configurator.py::9


# Existing helper functions
def get_store() -> Union[Store, None]:
    return config.store

# Will be deprecated at 0.1.0 

# You can add more helper functions here if needed
def register_provider(provider: Provider, client_type: Type[Any]) -> None:
    return config.register_provider(provider, client_type)

# Deprecated now (remove at 0.1.0)
def set_store(*args, **kwargs) -> None:
    raise DeprecationWarning("The set_store function is deprecated and will be removed in a future version. Use ell.init(store=...) instead.")
====================================================================


Basic LM Setup:

-> Chunk: lmp\__init__.py::1


from ell.lmp.simple import simple
from ell.lmp.complex import complex
====================================================================
-> Chunk: lmp\_track.py::1


import json
import logging
import threading
from ell.types import SerializedLMP, Invocation, InvocationTrace, InvocationContents
from ell.types.studio import LMPType, utc_now
from ell.util._warnings import _autocommit_warning
import ell.util.closure
from ell.configurator import config
from ell.types._lstr import _lstr

import inspect

import secrets
import time
from datetime import datetime
from functools import wraps
from typing import Any, Callable, Dict, Iterable, Optional, OrderedDict, Tuple

from ell.util.serialization import get_immutable_vars
from ell.util.serialization import compute_state_cache_key
from ell.util.serialization import prepare_invocation_params

logger = logging.getLogger(__name__)

# Thread-local storage for the invocation stack
_invocation_stack = threading.local()

def get_current_invocation() -> Optional[str]:
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    return _invocation_stack.stack[-1] if _invocation_stack.stack else None

def push_invocation(invocation_id: str):
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    _invocation_stack.stack.append(invocation_id)

def pop_invocation():
    if hasattr(_invocation_stack, 'stack') and _invocation_stack.stack:
        _invocation_stack.stack.pop()
====================================================================
-> Chunk: lmp\_track.py::2


def _track(func_to_track: Callable, *, forced_dependencies: Optional[Dict[str, Any]] = None) -> Callable:

    lmp_type = getattr(func_to_track, "__ell_type__", LMPType.OTHER)


    # see if it exists
    if not hasattr(func_to_track, "_has_serialized_lmp"):
        func_to_track._has_serialized_lmp = False

    if not hasattr(func_to_track, "__ell_hash__") and not config.lazy_versioning:
        ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)


    @wraps(func_to_track)
    def tracked_func(*fn_args, _get_invocation_id=False, **fn_kwargs) -> str:
        # XXX: Cache keys and global variable binding is not thread safe.
        # Compute the invocation id and hash the inputs for serialization.
        invocation_id = "invocation-" + secrets.token_hex(16)

        state_cache_key : str = None
        if not config.store:
            return func_to_track(*fn_args, **fn_kwargs, _invocation_origin=invocation_id)[0]

        parent_invocation_id = get_current_invocation()
        try:
            push_invocation(invocation_id)

            # Convert all positional arguments to named keyword arguments
            sig = inspect.signature(func_to_track)
            # Filter out kwargs that are not in the function signature
            filtered_kwargs = {k: v for k, v in fn_kwargs.items() if k in sig.parameters}

            bound_args = sig.bind(*fn_args, **filtered_kwargs)
            bound_args.apply_defaults()
            all_kwargs = dict(bound_args.arguments)

            # Get the list of consumed lmps and clean the invocation params for serialization.
            cleaned_invocation_params, ipstr, consumes = prepare_invocation_params( all_kwargs)

            try_use_cache = hasattr(func_to_track.__wrapper__, "__ell_use_cache__")

            if  try_use_cache:
                # Todo: add nice logging if verbose for when using a cahced invocaiton. IN a different color with thar args..
                if not hasattr(func_to_track, "__ell_hash__")  and config.lazy_versioning:
                    fn_closure, _ = ell.util.closure.lexically_closured_source(func_to_track)

                # compute the state cachekey
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

                cache_store = func_to_track.__wrapper__.__ell_use_cache__
                cached_invocations = cache_store.get_cached_invocations(func_to_track.__ell_hash__, state_cache_key)


                if len(cached_invocations) > 0:
                    # XXX: Fix caching.
                    results =  [d.deserialize() for  d in cached_invocations[0].results]

                    logger.info(f"Using cached result for {func_to_track.__qualname__} with state cache key: {state_cache_key}")
                    if len(results) == 1:
                        return results[0]
                    else:
                        return results
                    # Todo: Unfiy this with the non-cached case. We should go through the same code pathway.
                else:
                    logger.info(f"Attempted to use cache on {func_to_track.__qualname__} but it was not cached, or did not exist in the store. Refreshing cache...")


            _start_time = utc_now()

            # XXX: thread saftey note, if I prevent yielding right here and get the global context I should be fine re: cache key problem

            # get the prompt
            (result, invocation_api_params, metadata) = (
                (func_to_track(*fn_args, **fn_kwargs), {}, {})
                if lmp_type == LMPType.OTHER
                else func_to_track(*fn_args, _invocation_origin=invocation_id, **fn_kwargs, )
                )
            latency_ms = (utc_now() - _start_time).total_seconds() * 1000
            usage = metadata.get("usage", {"prompt_tokens": 0, "completion_tokens": 0})
            prompt_tokens= usage.get("prompt_tokens", 0) if usage else 0
            completion_tokens= usage.get("completion_tokens", 0) if usage else 0


            #XXX: cattrs add invocation origin here recursively on all pirmitive types within a message.
            #XXX: This will allow all objects to be traced automatically irrespective origin rather than relying on the API to do it, it will of vourse be expensive but unify track.
            #XXX: No other code will need to consider tracking after this point.

            if not hasattr(func_to_track, "__ell_hash__") and config.lazy_versioning:
                ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)
            _serialize_lmp(func_to_track)

            if not state_cache_key:
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

            _write_invocation(func_to_track, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                            state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id)

            if _get_invocation_id:
                return result, invocation_id
            else:
                return result
        finally:
            pop_invocation()


    func_to_track.__wrapper__  = tracked_func
    if hasattr(func_to_track, "__ell_api_params__"):
        tracked_func.__ell_api_params__ = func_to_track.__ell_api_params__
    if hasattr(func_to_track, "__ell_params_model__"):
        tracked_func.__ell_params_model__ = func_to_track.__ell_params_model__
    tracked_func.__ell_func__ = func_to_track
    tracked_func.__ell_track = True

    return tracked_func
====================================================================
-> Chunk: lmp\_track.py::3


def _serialize_lmp(func):
    # Serialize deptjh first all fo the used lmps.
    for f in func.__ell_uses__:
        _serialize_lmp(f)

    if getattr(func, "_has_serialized_lmp", False):
        return
    func._has_serialized_lmp = False
    fn_closure = func.__ell_closure__
    lmp_type = func.__ell_type__
    name = func.__qualname__
    api_params = getattr(func, "__ell_api_params__", None)

    lmps = config.store.get_versions_by_fqn(fqn=name)
    version = 0
    already_in_store = any(lmp.lmp_id == func.__ell_hash__ for lmp in lmps)

    if not already_in_store:
        commit = None
        if lmps:
            latest_lmp = max(lmps, key=lambda x: x.created_at)
            version = latest_lmp.version_number + 1
            if config.autocommit:
                # XXX: Move this out to autocommit itself.
                if not _autocommit_warning():
                    from ell.util.differ import write_commit_message_for_diff
                    commit = str(write_commit_message_for_diff(
                    f"{latest_lmp.dependencies}\n\n{latest_lmp.source}", 
                        f"{fn_closure[1]}\n\n{fn_closure[0]}")[0])

        serialized_lmp = SerializedLMP(
            lmp_id=func.__ell_hash__,
            name=name,
            created_at=utc_now(),
            source=fn_closure[0],
            dependencies=fn_closure[1],
            commit_message=commit,
            initial_global_vars=get_immutable_vars(fn_closure[2]),
            initial_free_vars=get_immutable_vars(fn_closure[3]),
            lmp_type=lmp_type,
            api_params=api_params if api_params else None,
            version_number=version,
        )
        config.store.write_lmp(serialized_lmp, [f.__ell_hash__ for f in func.__ell_uses__])
    func._has_serialized_lmp = True
====================================================================
-> Chunk: lmp\_track.py::4


def _write_invocation(func, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                     state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id):

    invocation_contents = InvocationContents(
        invocation_id=invocation_id,
        params=cleaned_invocation_params,
        results=result,
        invocation_api_params=invocation_api_params,
        global_vars=get_immutable_vars(func.__ell_closure__[2]),
        free_vars=get_immutable_vars(func.__ell_closure__[3])
    )

    if invocation_contents.should_externalize and config.store.has_blob_storage:
        invocation_contents.is_external = True

        # Write to the blob store 
        blob_id = config.store.blob_store.store_blob(
            json.dumps(invocation_contents.model_dump(
            ), default=str, ensure_ascii=False).encode('utf-8'),
            invocation_id
        )
        invocation_contents = InvocationContents(
            invocation_id=invocation_id,
            is_external=True,
        )

    invocation = Invocation(
        id=invocation_id,
        lmp_id=func.__ell_hash__,
        created_at=utc_now(),
        latency_ms=latency_ms,
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        state_cache_key=state_cache_key,
        used_by_id=parent_invocation_id,
        contents=invocation_contents
    )

    config.store.write_invocation(invocation, consumes)
====================================================================


Advanced LM Setup:

-> Chunk: lmp\complex.py::1


from ell.configurator import config
from ell.lmp._track import _track
from ell.provider import EllCallParams
from ell.types._lstr import _lstr
from ell.types import Message, ContentBlock
from ell.types.message import LMP, InvocableLM, LMPParams, MessageOrDict, _lstr_generic
from ell.types.studio import LMPType
from ell.util._warnings import _no_api_key_warning, _warnings
from ell.util.verbosity import compute_color, model_usage_logger_pre

from ell.util.verbosity import model_usage_logger_post_end, model_usage_logger_post_intermediate, model_usage_logger_post_start

from functools import wraps
from typing import Any, Dict, Optional, List, Callable, Tuple, Union
====================================================================
-> Chunk: lmp\complex.py::2


def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    default_client_from_decorator = client
    default_model_from_decorator = model
    default_api_params_from_decorator = api_params
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        _warnings(model, prompt, default_client_from_decorator)

        @wraps(prompt)
        def model_call(
            *prompt_args,
            _invocation_origin : Optional[str] = None,
            client: Optional[Any] = None,
            api_params: Optional[Dict[str, Any]] = None,
            lm_params: Optional[DeprecationWarning] = None,
            **prompt_kwargs,
        ) -> Tuple[Any, Any, Any]:
            # XXX: Deprecation in 0.1.0
            if lm_params:
                raise DeprecationWarning("lm_params is deprecated. Use api_params instead.")

            # promt -> str
            res = prompt(*prompt_args, **prompt_kwargs)
            # Convert prompt into ell messages
            messages = _get_messages(res, prompt)

            # XXX: move should log to a logger.
            should_log = not exempt_from_tracking and config.verbose
            # Cute verbose logging.
            if should_log: model_usage_logger_pre(prompt, prompt_args, prompt_kwargs, "[]", messages) #type: ignore

            # Call the model.
            # Merge API params
            merged_api_params = {**config.default_api_params, **default_api_params_from_decorator, **(api_params or {})}
            n = merged_api_params.get("n", 1)
            # Merge client overrides & client registry
            merged_client = _client_for_model(model, client or default_client_from_decorator)
            ell_call = EllCallParams(
                # XXX: Could change behaviour of overriding ell params for dyanmic tool calls.
                model=merged_api_params.pop("model", default_model_from_decorator),
                messages=messages,
                client = merged_client,
                api_params=merged_api_params,
                tools=tools or [],
            )
            # Get the provider for the model
            provider = config.get_provider_for(ell_call.client)
            assert provider is not None, f"No provider found for client {ell_call.client}."

            if should_log: model_usage_logger_post_start(n)
            with model_usage_logger_post_intermediate(n) as _logger:
                (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)
                if isinstance(result, list) and len(result) == 1:
                    result = result[0]

            result = post_callback(result) if post_callback else result
            if should_log:
                model_usage_logger_post_end()
            #
            #  These get sent to track. This is wack.           
            return result, final_api_params, metadata
        # ... other code
    # ... other code
====================================================================
-> Chunk: lmp\complex.py::3


def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        # ... other code



        model_call.__ell_api_params__ = default_api_params_from_decorator #type: ignore
        model_call.__ell_func__ = prompt #type: ignore
        model_call.__ell_type__ = LMPType.LM #type: ignore
        model_call.__ell_exempt_from_tracking = exempt_from_tracking #type: ignore


        if exempt_from_tracking:
            return model_call
        else:
            # XXX: Analyze decorators with AST instead.
            return _track(model_call, forced_dependencies=dict(tools=tools, response_format=api_params.get("response_format", {})))
    return parameterized_lm_decorator
====================================================================
-> Chunk: lmp\complex.py::4


def _get_messages(prompt_ret: Union[str, list[MessageOrDict]], prompt: LMP) -> list[Message]:
    """
    Helper function to convert the output of an LMP into a list of Messages.
    """
    if isinstance(prompt_ret, str):
        has_system_prompt = prompt.__doc__ is not None and prompt.__doc__.strip() != ""
        messages =     [Message(role="system", content=[ContentBlock(text=_lstr(prompt.__doc__ ) )])] if has_system_prompt else []
        return messages + [
            Message(role="user", content=[ContentBlock(text=prompt_ret)])
        ]
    else:
        assert isinstance(
            prompt_ret, list
        ), "Need to pass a list of Messages to the language model"
        return prompt_ret
====================================================================
-> Chunk: lmp\complex.py::5


def _client_for_model(
    model: str,
    client: Optional[Any] = None,
    _name: Optional[str] = None,
) -> Any:
    # XXX: Move to config to centralize api keys etc.
    if not client:
        client, was_fallback = config.get_client_for(model)

        # XXX: Wrong.
        if not client and not was_fallback:
            raise RuntimeError(_no_api_key_warning(model, _name, '', long=True, error=True))

    if client is None:
        raise ValueError(f"No client found for model '{model}'. Ensure the model is registered using 'register_model' in 'config.py' or specify a client directly using the 'client' argument in the decorator or function call.")
    return client


complex.__doc__ =
 # ... other code
====================================================================


Ell Simple Decorator:

-> Chunk: lmp\simple.py::1


from functools import wraps
from typing import Any, Optional

from ell.lmp.complex import complex


def simple(model: str, client: Optional[Any] = None,  exempt_from_tracking=False, **api_params):
    assert 'tools' not in api_params, "tools are not supported in lm decorator, use multimodal decorator instead"
    assert 'tool_choice' not in api_params, "tool_choice is not supported in lm decorator, use multimodal decorator instead"
    assert 'response_format' not in api_params or isinstance(api_params.get('response_format', None), dict), "response_format is not supported in lm decorator, use multimodal decorator instead"

    def convert_multimodal_response_to_lstr(response):
        return [x.content[0].text for x in response] if isinstance(response, list) else response.content[0].text
    return complex(model, client,  exempt_from_tracking=exempt_from_tracking, **api_params, post_callback=convert_multimodal_response_to_lstr)
====================================================================
-> Chunk: lmp\simple.py::2


simple.__doc__ = """The fundamental unit of language model programming in ell.

  This decorator simplifies the process of creating Language Model Programs (LMPs) 
  that return text-only outputs from language models, while supporting multimodal inputs.
  It wraps the more complex 'complex' decorator, providing a streamlined interface for common use cases.

  :param model: The name or identifier of the language model to use.
  :type model: str
  :param client: An optional OpenAI client instance. If not provided, a default client will be used.
  :type client: Optional[openai.Client]
  :param exempt_from_tracking: If True, the LMP usage won't be tracked. Default is False.
  :type exempt_from_tracking: bool
  :param api_params: Additional keyword arguments to pass to the underlying API call.
  :type api_params: Any

  Usage:
  The decorated function can return either a single prompt or a list of ell.Message objects:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def summarize_text(text: str) -> str:
          '''You are an expert at summarizing text.''' # System prompt
          return f"Please summarize the following text:\\n\\n{text}" # User prompt


      @ell.simple(model="gpt-4", temperature=0.7)
      def describe_image(image : PIL.Image.Image) -> List[ell.Message]:
          '''Describe the contents of an image.''' # unused because we're returning a list of Messages
          return [
              # helper function for ell.Message(text="...", role="system")
              ell.system("You are an AI trained to describe images."),
              # helper function for ell.Message(content="...", role="user")
              ell.user(["Describe this image in detail.", image]),
          ]


      image_description = describe_image(PIL.Image.open("https://example.com/image.jpg"))
      print(image_description) 
      # Output will be a string text-only description of the image

      summary = summarize_text("Long text to summarize...")
      print(summary)
      # Output will be a text-only summary

  Notes:

  - This decorator is designed for text-only model outputs, but supports multimodal inputs.
  - It simplifies complex responses from language models to text-only format, regardless of 
    the model's capability for structured outputs, function calling, or multimodal outputs.
  - For preserving complex model outputs (e.g., structured data, function calls, or multimodal 
    outputs), use the @ell.complex decorator instead. @ell.complex returns a Message object (role='assistant')
  - The decorated function can return a string or a list of ell.Message objects for more 
    complex prompts, including multimodal inputs.
  - If called with n > 1 in api_params, the wrapped LMP will return a list of strings for the n parallel outputs
    of the model instead of just one string. Otherwise, it will return a single string.
  - You can pass LM API parameters either in the decorator or when calling the decorated function.
    Parameters passed during the function call will override those set in the decorator.

  Example of passing LM API params:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def generate_story(prompt: str) -> str:
          return f"Write a short story based on this prompt: {prompt}"

      # Using default parameters
      story1 = generate_story("A day in the life of a time traveler")

      # Overriding parameters during function call
      story2 = generate_story("An AI's first day of consciousness", api_params={"temperature": 0.9, "max_tokens": 500})

  See Also:

  - :func:`ell.complex`: For LMPs that preserve full structure of model responses, including multimodal outputs.
  - :func:`ell.tool`: For defining tools that can be used within complex LMPs.
  - :mod:`ell.studio`: For visualizing and analyzing LMP executions.
    """
====================================================================


