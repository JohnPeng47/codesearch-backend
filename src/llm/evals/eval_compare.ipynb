{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\starlette\\config.py:66: UserWarning: Config file '.env' not found.\n",
      "  warnings.warn(f\"Config file '{env_file}' not found.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n",
      "Summary tokens: 8459,         Code tokens: 60918,         Ratio: 0.13885879378837127\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from src.chunk.chunk import ChunkStrat\n",
    "from pathlib import Path\n",
    "\n",
    "from src.cluster.cluster import ClusterStrategy\n",
    "from src.cluster.lmp.cluster_v4 import generate_clusters\n",
    "from src.chunk.chunk import chunk_repo, ChunkStrat\n",
    "\n",
    "\n",
    "repo_name = \"ell\"\n",
    "repo_path = Path(\"src/cluster/repos\") / repo_name\n",
    "\n",
    "chunks = chunk_repo(repo_path, ChunkStrat.VANILLA)\n",
    "chunks_summarized = chunk_repo(repo_path, ChunkStrat.SUMMARY)\n",
    "chunks_random = chunk_repo(repo_path, ChunkStrat.RANDOM)\n",
    "\n",
    "chunk_strat = ClusterStrategy(chunks, \n",
    "                              cluster_op=generate_clusters)\n",
    "summarized_strat = ClusterStrategy(chunks_summarized,\n",
    "                                   cluster_op=generate_clusters)\n",
    "random_strat = ClusterStrategy(chunks_random,\n",
    "                                 cluster_op=generate_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine created:  Engine(sqlite:///logdir\\ell.db)\n"
     ]
    }
   ],
   "source": [
    "import ell\n",
    "ell.init(store=\"logdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello???\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for generate_clusters_lmp: False\n",
      "Return_metadata False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for generate_clusters_lmp: False\n",
      "Return_metadata False\n",
      "Perplexity: 1.4001706349436076e+20\n"
     ]
    }
   ],
   "source": [
    "from src.cluster.lmp.cluster_v4 import generate_clusters\n",
    "\n",
    "clusters, perp = generate_clusters(chunks)\n",
    "print(f\"Perplexity: {perp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# TODO: write a function that calculates how close together the clustered chunks\n",
    "print(len(newnew_matched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  0.7\n",
      "Real-Time Client and Interaction Handlers:\n",
      "openai_realtime\\__init__.py::1\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\client.py::7\n",
      "\n",
      "\n",
      "B:  0.7\n",
      "Cluster 1: Real-time API Functionality:\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\conversation.py::1\n",
      "openai_realtime\\conversation.py::2\n",
      "\n",
      "\n",
      "A:  0.8571428571428571\n",
      "Cross-Entropy Method Implementation:\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cem.py::6\n",
      "lmp\\complex.py::5\n",
      "\n",
      "\n",
      "B:  0.6\n",
      "Cluster 2: Reinforcement Learning with CEM:\n",
      "0.1.0\\cem.py::6\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::6\n",
      "0.1.0\\cpbo.py::7\n",
      "\n",
      "\n",
      "A:  0.8\n",
      "Configuration and Initialization:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "models\\__init__.py::1\n",
      "models\\openai.py::1\n",
      "models\\openai.py::2\n",
      "\n",
      "\n",
      "B:  0.8\n",
      "Cluster 3: Model Configuration and Interaction:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "models\\openai.py::1\n",
      "\n",
      "\n",
      "A:  0.7\n",
      "Provider Abstraction and Implementation:\n",
      "ell\\provider.py::3\n",
      "providers\\openai.py::1\n",
      "providers\\anthropic.py::1\n",
      "providers\\bedrock.py::1\n",
      "ell\\provider.py::2\n",
      "ell\\provider.py::6\n",
      "providers\\openai.py::2\n",
      "providers\\anthropic.py::2\n",
      "providers\\bedrock.py::2\n",
      "providers\\groq.py::1\n",
      "\n",
      "\n",
      "B:  0.7\n",
      "Cluster 5: Providers and Service Integrations:\n",
      "providers\\anthropic.py::1\n",
      "providers\\anthropic.py::2\n",
      "providers\\anthropic.py::3\n",
      "providers\\openai.py::1\n",
      "providers\\openai.py::2\n",
      "models\\openai.py::2\n",
      "providers\\bedrock.py::1\n",
      "providers\\bedrock.py::2\n",
      "providers\\groq.py::1\n",
      "ell\\configurator.py::5\n",
      "\n",
      "\n",
      "A:  1.0\n",
      "Message and Content Handling:\n",
      "types\\message.py::1\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::7\n",
      "types\\message.py::8\n",
      "types\\message.py::9\n",
      "types\\message.py::10\n",
      "\n",
      "\n",
      "B:  0.6666666666666666\n",
      "Message and Content Handling:\n",
      "types\\message.py::1\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::7\n",
      "types\\message.py::8\n",
      "types\\message.py::9\n",
      "types\\message.py::10\n",
      "types\\message.py::11\n",
      "types\\message.py::12\n",
      "types\\message.py::13\n",
      "types\\message.py::14\n",
      "types\\message.py::15\n",
      "\n",
      "\n",
      "A:  0.5\n",
      "Utilities and Lexical Closure Management:\n",
      "util\\closure.py::11\n",
      "util\\closure.py::12\n",
      "util\\closure.py::13\n",
      "util\\closure.py::14\n",
      "util\\closure.py::15\n",
      "util\\closure.py::16\n",
      "util\\closure.py::17\n",
      "util\\closure.py::18\n",
      "util\\closure_util.py::1\n",
      "util\\closure_util.py::3\n",
      "\n",
      "\n",
      "B:  0.5\n",
      "Closure and Dependency Handling:\n",
      "util\\closure.py::15\n",
      "util\\closure.py::1\n",
      "util\\closure.py::3\n",
      "util\\closure.py::6\n",
      "util\\closure.py::7\n",
      "util\\closure.py::10\n",
      "util\\closure.py::11\n",
      "util\\closure.py::13\n",
      "util\\closure_util.py::1\n",
      "util\\closure_util.py::3\n",
      "\n",
      "\n",
      "A:  0.5\n",
      "Provider Integration:\n",
      "ell\\provider.py::1\n",
      "ell\\provider.py::2\n",
      "ell\\provider.py::3\n",
      "ell\\provider.py::4\n",
      "ell\\provider.py::5\n",
      "ell\\provider.py::6\n",
      "providers\\openai.py::1\n",
      "providers\\openai.py::2\n",
      "providers\\anthropic.py::1\n",
      "providers\\bedrock.py::1\n",
      "\n",
      "\n",
      "B:  0.4166666666666667\n",
      "API Call Parameter Management:\n",
      "ell\\provider.py::2\n",
      "ell\\provider.py::3\n",
      "ell\\provider.py::4\n",
      "ell\\provider.py::5\n",
      "ell\\provider.py::6\n",
      "openai_realtime\\event_handler.py::1\n",
      "openai_realtime\\utils.py::1\n",
      "models\\__init__.py::1\n",
      "models\\anthropic.py::1\n",
      "models\\bedrock.py::1\n",
      "models\\groq.py::1\n",
      "models\\ollama.py::1\n",
      "\n",
      "\n",
      "A:  0.7\n",
      "Configuration Management:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "stores\\sql.py::10\n",
      "\n",
      "\n",
      "B:  0.7\n",
      "Model Configuration and API Interaction:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\__init__.py::1\n",
      "models\\openai.py::1\n",
      "models\\openai.py::2\n",
      "\n",
      "\n",
      "A:  0.7\n",
      "Provider API Integration:\n",
      "ell\\provider.py::1\n",
      "ell\\provider.py::2\n",
      "ell\\provider.py::3\n",
      "ell\\provider.py::4\n",
      "ell\\provider.py::5\n",
      "ell\\provider.py::6\n",
      "providers\\openai.py::1\n",
      "providers\\openai.py::2\n",
      "models\\openai.py::1\n",
      "models\\openai.py::2\n",
      "\n",
      "\n",
      "B:  0.7\n",
      "OpenAI API Integration:\n",
      "providers\\openai.py::1\n",
      "providers\\openai.py::2\n",
      "0.1.0\\autostreamprevention.py::1\n",
      "ell\\provider.py::2\n",
      "ell\\provider.py::3\n",
      "ell\\provider.py::4\n",
      "ell\\provider.py::5\n",
      "providers\\anthropic.py::1\n",
      "providers\\bedrock.py::2\n",
      "models\\openai.py::2\n",
      "\n",
      "\n",
      "A:  0.6\n",
      "Message Handling and Types:\n",
      "types\\message.py::1\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::7\n",
      "types\\message.py::8\n",
      "types\\message.py::9\n",
      "types\\message.py::10\n",
      "\n",
      "\n",
      "B:  0.5454545454545454\n",
      "Message and Content Handling:\n",
      "types\\message.py::6\n",
      "types\\message.py::9\n",
      "types\\message.py::10\n",
      "types\\message.py::11\n",
      "types\\message.py::12\n",
      "types\\message.py::13\n",
      "types\\message.py::14\n",
      "types\\message.py::15\n",
      "types\\message.py::1\n",
      "types\\message.py::7\n",
      "types\\message.py::8\n",
      "\n",
      "\n",
      "A:  0.5\n",
      "Complex Language Model Programs (LMP):\n",
      "lmp\\complex.py::1\n",
      "lmp\\complex.py::2\n",
      "lmp\\complex.py::3\n",
      "lmp\\complex.py::4\n",
      "lmp\\complex.py::5\n",
      "lmp\\tool.py::1\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::3\n",
      "lmp\\tool.py::4\n",
      "lmp\\simple.py::1\n",
      "\n",
      "\n",
      "B:  0.5\n",
      "Reinforcement Learning and Policy Management:\n",
      "0.1.0\\cpbo.py::6\n",
      "0.1.0\\cpbo.py::5\n",
      "0.1.0\\cpbo.py::7\n",
      "lmp\\complex.py::1\n",
      "lmp\\complex.py::3\n",
      "lmp\\complex.py::4\n",
      "lmp\\complex.py::5\n",
      "lmp\\tool.py::1\n",
      "util\\serialization.py::1\n",
      "util\\should_import.py::1\n",
      "\n",
      "\n",
      "A:  0.8\n",
      "SQL Storage and Persistence:\n",
      "stores\\sql.py::1\n",
      "stores\\sql.py::2\n",
      "stores\\sql.py::3\n",
      "stores\\sql.py::4\n",
      "stores\\sql.py::5\n",
      "stores\\sql.py::6\n",
      "stores\\sql.py::7\n",
      "stores\\sql.py::8\n",
      "stores\\sql.py::9\n",
      "ell\\store.py::2\n",
      "\n",
      "\n",
      "B:  0.8\n",
      "Blob Storage and Data Management:\n",
      "ell\\store.py::1\n",
      "ell\\store.py::2\n",
      "ell\\store.py::3\n",
      "stores\\sql.py::2\n",
      "stores\\sql.py::3\n",
      "stores\\sql.py::4\n",
      "stores\\sql.py::5\n",
      "stores\\sql.py::6\n",
      "stores\\sql.py::7\n",
      "stores\\sql.py::8\n",
      "\n",
      "\n",
      "A:  0.7\n",
      "Real-Time Client Interaction:\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\client.py::6\n",
      "openai_realtime\\client.py::7\n",
      "openai_realtime\\client.py::8\n",
      "openai_realtime\\client.py::9\n",
      "openai_realtime\\api.py::1\n",
      "\n",
      "\n",
      "B:  0.7\n",
      "Real-time Processing with WebSocket and API:\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\client.py::6\n",
      "\n",
      "\n",
      "A:  0.7\n",
      "Configuration and Initialization:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "models\\openai.py::1\n",
      "providers\\openai.py::1\n",
      "ell\\provider.py::6\n",
      "models\\anthropic.py::1\n",
      "\n",
      "\n",
      "B:  0.7\n",
      "Model Configuration and API Interaction:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\__init__.py::1\n",
      "models\\openai.py::1\n",
      "models\\openai.py::2\n",
      "\n",
      "\n",
      "A:  0.7\n",
      "Persistency and Storage:\n",
      "ell\\store.py::2\n",
      "stores\\sql.py::1\n",
      "stores\\sql.py::2\n",
      "stores\\sql.py::3\n",
      "stores\\sql.py::4\n",
      "stores\\sql.py::5\n",
      "stores\\sql.py::6\n",
      "stores\\sql.py::7\n",
      "stores\\sql.py::10\n",
      "stores\\sql.py::11\n",
      "\n",
      "\n",
      "B:  0.7\n",
      "Blob Storage and Data Management:\n",
      "ell\\store.py::1\n",
      "ell\\store.py::2\n",
      "ell\\store.py::3\n",
      "stores\\sql.py::2\n",
      "stores\\sql.py::3\n",
      "stores\\sql.py::4\n",
      "stores\\sql.py::5\n",
      "stores\\sql.py::6\n",
      "stores\\sql.py::7\n",
      "stores\\sql.py::8\n",
      "\n",
      "\n",
      "A:  0.8888888888888888\n",
      "Reinforcement Learning with Cross-Entropy Methods (CEM):\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cem.py::6\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::5\n",
      "\n",
      "\n",
      "B:  0.8\n",
      "Reinforcement Learning with Policy Networks:\n",
      "0.1.0\\cem.py::6\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::4\n",
      "\n",
      "\n",
      "A:  0.7\n",
      "Blob Storage Implementation:\n",
      "ell\\store.py::1\n",
      "ell\\store.py::3\n",
      "studio\\server.py::1\n",
      "studio\\server.py::5\n",
      "studio\\datamodels.py::1\n",
      "studio\\config.py::1\n",
      "studio\\connection_manager.py::1\n",
      "studio\\server.py::2\n",
      "studio\\server.py::4\n",
      "studio\\server.py::6\n",
      "\n",
      "\n",
      "B:  0.7\n",
      "Server-side Operations and Configuration:\n",
      "studio\\server.py::2\n",
      "studio\\server.py::3\n",
      "studio\\server.py::4\n",
      "studio\\server.py::5\n",
      "studio\\server.py::6\n",
      "studio\\server.py::7\n",
      "studio\\config.py::1\n",
      "studio\\connection_manager.py::1\n",
      "studio\\datamodels.py::1\n",
      "studio\\__main__.py::2\n",
      "\n",
      "\n",
      "A:  0.5\n",
      "Closure and Code Analysis:\n",
      "util\\closure.py::2\n",
      "util\\closure.py::3\n",
      "util\\closure.py::4\n",
      "util\\closure.py::5\n",
      "util\\closure.py::6\n",
      "util\\closure.py::8\n",
      "util\\closure.py::9\n",
      "util\\closure.py::10\n",
      "util\\closure.py::13\n",
      "util\\closure.py::15\n",
      "\n",
      "\n",
      "B:  0.5\n",
      "Lexical Closures and Introspection:\n",
      "util\\closure.py::1\n",
      "util\\closure.py::2\n",
      "util\\closure.py::3\n",
      "util\\closure.py::4\n",
      "util\\closure.py::5\n",
      "util\\closure.py::6\n",
      "util\\closure_util.py::1\n",
      "util\\closure_util.py::3\n",
      "util\\closure_util.py::4\n",
      "util\\differ.py::1\n",
      "\n",
      "\n",
      "A:  0.5\n",
      "Real-Time API Communication:\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\conversation.py::1\n",
      "openai_realtime\\conversation.py::2\n",
      "openai_realtime\\conversation.py::5\n",
      "openai_realtime\\conversation.py::7\n",
      "openai_realtime\\event_handler.py::1\n",
      "openai_realtime\\utils.py::1\n",
      "studio\\connection_manager.py::1\n",
      "\n",
      "\n",
      "B:  0.4166666666666667\n",
      "Real-Time Conversation Management:\n",
      "openai_realtime\\conversation.py::1\n",
      "openai_realtime\\conversation.py::2\n",
      "openai_realtime\\conversation.py::3\n",
      "openai_realtime\\conversation.py::4\n",
      "openai_realtime\\conversation.py::5\n",
      "openai_realtime\\conversation.py::6\n",
      "openai_realtime\\conversation.py::7\n",
      "openai_realtime\\conversation.py::8\n",
      "openai_realtime\\conversation.py::9\n",
      "openai_realtime\\event_handler.py::1\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\client.py::1\n",
      "\n",
      "\n",
      "A:  0.9090909090909091\n",
      "Cross-Entropy Method (CEM) Reinforcement Learning:\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::6\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::4\n",
      "0.1.0\\cpbo.py::6\n",
      "\n",
      "\n",
      "B:  1.0\n",
      "Reinforcement Learning with Policy Networks:\n",
      "0.1.0\\cem.py::6\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::4\n",
      "\n",
      "\n",
      "A:  0.4166666666666667\n",
      "Lexical Closure and Code Introspection:\n",
      "util\\closure.py::2\n",
      "util\\closure.py::3\n",
      "util\\closure.py::4\n",
      "util\\closure.py::5\n",
      "util\\closure.py::6\n",
      "util\\closure.py::7\n",
      "util\\closure.py::8\n",
      "util\\closure.py::9\n",
      "util\\closure.py::10\n",
      "util\\closure.py::11\n",
      "util\\closure.py::12\n",
      "util\\closure.py::15\n",
      "\n",
      "\n",
      "B:  0.5\n",
      "Lexical Closures and Introspection:\n",
      "util\\closure.py::1\n",
      "util\\closure.py::2\n",
      "util\\closure.py::3\n",
      "util\\closure.py::4\n",
      "util\\closure.py::5\n",
      "util\\closure.py::6\n",
      "util\\closure_util.py::1\n",
      "util\\closure_util.py::3\n",
      "util\\closure_util.py::4\n",
      "util\\differ.py::1\n",
      "\n",
      "\n",
      "A:  0.9090909090909091\n",
      "FastAPI Server Implementation for Studio:\n",
      "studio\\server.py::2\n",
      "studio\\server.py::3\n",
      "studio\\server.py::4\n",
      "studio\\server.py::5\n",
      "studio\\server.py::6\n",
      "studio\\server.py::7\n",
      "studio\\connection_manager.py::1\n",
      "studio\\config.py::1\n",
      "ell\\store.py::1\n",
      "studio\\datamodels.py::1\n",
      "studio\\__main__.py::2\n",
      "\n",
      "\n",
      "B:  1.0\n",
      "Server-side Operations and Configuration:\n",
      "studio\\server.py::2\n",
      "studio\\server.py::3\n",
      "studio\\server.py::4\n",
      "studio\\server.py::5\n",
      "studio\\server.py::6\n",
      "studio\\server.py::7\n",
      "studio\\config.py::1\n",
      "studio\\connection_manager.py::1\n",
      "studio\\datamodels.py::1\n",
      "studio\\__main__.py::2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a, b, perc_a, perc_b in newnew_matched:\n",
    "    print(\"A: \", perc_a)\n",
    "    print(a)\n",
    "    print(\"B: \", perc_b)\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vanilla_clusters), len(summary_clusters))\n",
    "\n",
    "matching = match_clusters(vanilla_clusters, summary_clusters, min_match=2)\n",
    "vanilla_matching = [m[0] for m in matching]\n",
    "for a, b, perc_a, perc_b in matching:\n",
    "    print(\"A: \", perc_a)\n",
    "    print(a)\n",
    "    print(\"B: \", perc_b)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "vanilla_matching = [m[0] for m in matching]\n",
    "unqiue_vanilla = [c for c in vanilla_clusters if c not in vanilla_matching]\n",
    "print(len(unqiue_vanilla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration: 1\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "Chunk Name: build.py::4 hallucinated, skipping...\n",
      "Chunk Name: build.py::5 hallucinated, skipping...\n",
      "Chunk Name: build.py::6 hallucinated, skipping...\n",
      "Chunk Name: providers\\openai.py::3 hallucinated, skipping...\n",
      "Chunk Name: providers\\openai.py::4 hallucinated, skipping...\n",
      "Found clusters: 95/212\n",
      "New inputs: 133\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "Chunk Name: lmp\\track.py::1 hallucinated, skipping...\n",
      "Chunk Name: lmp\\track.py::2 hallucinated, skipping...\n",
      "Chunk Name: lmp\\track.py::3 hallucinated, skipping...\n",
      "Chunk Name: lmp\\track.py::4 hallucinated, skipping...\n",
      "Found clusters: 85/212\n",
      "New inputs: 72\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "Chunk Name: ellis\\configurator.py::1 hallucinated, skipping...\n",
      "Chunk Name: ellis\\configurator.py::3 hallucinated, skipping...\n",
      "Chunk Name: ellis\\configurator.py::4 hallucinated, skipping...\n",
      "Chunk Name: ellis\\configurator.py::5 hallucinated, skipping...\n",
      "Chunk Name: ellis\\__init__.py::1 hallucinated, skipping...\n",
      "Chunk Name: ellis\\__version__.py::1 hallucinated, skipping...\n",
      "Chunk Name: ellis\\configurator.py::6 hallucinated, skipping...\n",
      "Chunk Name: ellis\\configurator.py::7 hallucinated, skipping...\n",
      "Chunk Name: ellis\\configurator.py::8 hallucinated, skipping...\n",
      "Found clusters: 111/212\n",
      "New inputs: 125\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "Chunk Name: store\\sql.py::5 hallucinated, skipping...\n",
      "Chunk Name: store\\sql.py::3 hallucinated, skipping...\n",
      "Chunk Name: store\\sql.py::4 hallucinated, skipping...\n",
      "Chunk Name: store\\sql.py::7 hallucinated, skipping...\n",
      "Chunk Name: store\\sql.py::8 hallucinated, skipping...\n",
      "Chunk Name: store\\sql.py::9 hallucinated, skipping...\n",
      "Chunk Name: store\\sql.py::10 hallucinated, skipping...\n",
      "Found clusters: 83/212\n",
      "New inputs: 53\n",
      "Total length:  0\n",
      "Running iteration: 1\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "Chunk Name: lmp_track.py::1 hallucinated, skipping...\n",
      "Chunk Name: lmp_track.py::2 hallucinated, skipping...\n",
      "Chunk Name: lmp_complex.py::2 hallucinated, skipping...\n",
      "Found clusters: 97/212\n",
      "New inputs: 144\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "Chunk Name: 0.1.0\\cem.py::7 hallucinated, skipping...\n",
      "Chunk Name: providers\\openai_realtime\\api.py::2 hallucinated, skipping...\n",
      "Chunk Name: providers\\openai_realtime\\api.py::3 hallucinated, skipping...\n",
      "Chunk Name: providers\\openai_realtime\\api.py::4 hallucinated, skipping...\n",
      "Chunk Name: providers\\openai_realtime\\conversation.py::1 hallucinated, skipping...\n",
      "Chunk Name: providers\\openai_realtime\\conversation.py::3 hallucinated, skipping...\n",
      "Found clusters: 99/212\n",
      "New inputs: 79\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "Found clusters: 94/212\n",
      "New inputs: 128\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "Return_metadata False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Return_metadata False\n",
      "Chunk Name: lang_model.py::1 hallucinated, skipping...\n",
      "Chunk Name: util\\warnings.py::1 hallucinated, skipping...\n",
      "Chunk Name: util\\warnings.py::2 hallucinated, skipping...\n",
      "Found clusters: 100/212\n",
      "New inputs: 52\n",
      "Total length:  0\n",
      "Running iteration: 1\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(additional_matches) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m13\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning iteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     vanilla_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_strat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     summary_clusters \u001b[38;5;241m=\u001b[39m summarized_strat\u001b[38;5;241m.\u001b[39mrun(iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      7\u001b[0m     additional_matches\u001b[38;5;241m.\u001b[39mextend(match_clusters(vanilla_clusters, summary_clusters, min_match\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\cluster\\cluster.py:40\u001b[0m, in \u001b[0;36mClusterStrategy.run\u001b[1;34m(self, remove_classified, iters)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Get clusters for current iteration\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m new_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cluster_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcluster_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname_to_cluster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_to_cluster\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# apply gather ops\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enrich_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menrich_ops:\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\cluster\\cluster.py:69\u001b[0m, in \u001b[0;36mClusterStrategy._cluster_iteration\u001b[1;34m(self, cluster_inputs, all_chunks, name_to_cluster)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cluster_iteration\u001b[39m(\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     61\u001b[0m     cluster_inputs: List[ClusterInput],\n\u001b[0;32m     62\u001b[0m     all_chunks: List[ClusterInput],\n\u001b[0;32m     63\u001b[0m     name_to_cluster: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]\n\u001b[0;32m     64\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[ClusteredTopic]:\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    Iterate through the output of the cluster operation and match the names of the LLM output\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    to actual CodeChunk objects and update the stats\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     clusters, perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     new_clusters \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     71\u001b[0m     unique_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\cluster\\lmp\\cluster_v4.py:131\u001b[0m, in \u001b[0;36mgenerate_clusters\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk_type \u001b[38;5;129;01min\u001b[39;00m ChunkType:\n\u001b[0;32m    130\u001b[0m     cluster_func \u001b[38;5;241m=\u001b[39m LogProbLMP(generate_clusters_raw)\n\u001b[1;32m--> 131\u001b[0m     llm_res \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rgx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     clusters: LMClusteredTopicList \u001b[38;5;241m=\u001b[39m format_clusters(llm_res)\u001b[38;5;241m.\u001b[39mparsed\n\u001b[0;32m    133\u001b[0m     all_clusters\u001b[38;5;241m.\u001b[39mextend(clusters\u001b[38;5;241m.\u001b[39mtopics)\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\llm\\lmp_base\\logprobs.py:39\u001b[0m, in \u001b[0;36mLogProbLMP.call\u001b[1;34m(self, output_rgx, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, output_rgx: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# if not self._check_lmp_returns_metadata():\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m#     raise ValueError(\"LMP must have return_metadata enabled\")\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     result, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lmp(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logprobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_logprobs(metadata, output_rgx)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_rgx:\n",
      "File \u001b[1;32m~\\Documents\\projects\\useful\\ell-forked\\src\\ell\\lmp\\_track.py:136\u001b[0m, in \u001b[0;36m_track.<locals>.tracked_func\u001b[1;34m(_get_invocation_id, *fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m _start_time \u001b[38;5;241m=\u001b[39m utc_now()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# XXX: thread saftey note, if I prevent yielding right here and get the global context I should be fine re: cache key problem\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# get the prompt\u001b[39;00m\n\u001b[0;32m    133\u001b[0m (result, invocation_api_params, metadata) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    134\u001b[0m     (func_to_track(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs), {}, {})\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lmp_type \u001b[38;5;241m==\u001b[39m LMPType\u001b[38;5;241m.\u001b[39mOTHER\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m func_to_track(\u001b[38;5;241m*\u001b[39mfn_args, _invocation_origin\u001b[38;5;241m=\u001b[39minvocation_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs, )\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m latency_ms \u001b[38;5;241m=\u001b[39m (utc_now() \u001b[38;5;241m-\u001b[39m _start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    139\u001b[0m usage \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n",
      "File \u001b[1;32m~\\Documents\\projects\\useful\\ell-forked\\src\\ell\\lmp\\complex.py:41\u001b[0m, in \u001b[0;36mcomplex.<locals>.parameterized_lm_decorator.<locals>.model_call\u001b[1;34m(_invocation_origin, client, api_params, lm_params, session_id, *prompt_args, **prompt_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_params is deprecated. Use api_params instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# promt -> str\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m res \u001b[38;5;241m=\u001b[39m prompt(\u001b[38;5;241m*\u001b[39mprompt_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_kwargs)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Set the session id if it's provided in the prompt_kwargs\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session_id:\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\cluster\\lmp\\cluster_v4.py:102\u001b[0m, in \u001b[0;36mgenerate_clusters_raw\u001b[1;34m(chunks, chunk_type)\u001b[0m\n\u001b[0;32m     61\u001b[0m         codebase \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mget_content() \u001b[38;5;241m+\u001b[39m DELIMETER\n\u001b[0;32m     63\u001b[0m     GEN_CLUSTERS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124mHere are chunks of code representing a repo:\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;132;01m{code}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124mNow generate the list of clusters according to the examples above. Each cluster should have at least 10 chunks\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GEN_CLUSTERS\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    101\u001b[0m         code\u001b[38;5;241m=\u001b[39mcodebase, \n\u001b[1;32m--> 102\u001b[0m         seed_chunks\u001b[38;5;241m=\u001b[39m\u001b[43midentify_key_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\projects\\useful\\ell-forked\\src\\ell\\lmp\\_track.py:136\u001b[0m, in \u001b[0;36m_track.<locals>.tracked_func\u001b[1;34m(_get_invocation_id, *fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m _start_time \u001b[38;5;241m=\u001b[39m utc_now()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# XXX: thread saftey note, if I prevent yielding right here and get the global context I should be fine re: cache key problem\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# get the prompt\u001b[39;00m\n\u001b[0;32m    133\u001b[0m (result, invocation_api_params, metadata) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    134\u001b[0m     (func_to_track(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs), {}, {})\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lmp_type \u001b[38;5;241m==\u001b[39m LMPType\u001b[38;5;241m.\u001b[39mOTHER\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m func_to_track(\u001b[38;5;241m*\u001b[39mfn_args, _invocation_origin\u001b[38;5;241m=\u001b[39minvocation_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs, )\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m latency_ms \u001b[38;5;241m=\u001b[39m (utc_now() \u001b[38;5;241m-\u001b[39m _start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    139\u001b[0m usage \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n",
      "File \u001b[1;32m~\\Documents\\projects\\useful\\ell-forked\\src\\ell\\lmp\\complex.py:75\u001b[0m, in \u001b[0;36mcomplex.<locals>.parameterized_lm_decorator.<locals>.model_call\u001b[1;34m(_invocation_origin, client, api_params, lm_params, session_id, *prompt_args, **prompt_kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_log: model_usage_logger_post_start(n)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model_usage_logger_post_intermediate(n) \u001b[38;5;28;01mas\u001b[39;00m _logger:\n\u001b[1;32m---> 75\u001b[0m     (result, final_api_params, metadata) \u001b[38;5;241m=\u001b[39m \u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mell_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_invocation_origin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_logger\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshould_log\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     77\u001b[0m         result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\projects\\useful\\ell-forked\\src\\ell\\provider.py:125\u001b[0m, in \u001b[0;36mProvider.call\u001b[1;34m(self, ell_call, origin_id, logger)\u001b[0m\n\u001b[0;32m    121\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider_call_function(ell_call\u001b[38;5;241m.\u001b[39mclient, final_api_call_params)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdangerous_disable_validation \u001b[38;5;129;01mor\u001b[39;00m _validate_provider_call_params(final_api_call_params, call)\n\u001b[1;32m--> 125\u001b[0m provider_resp \u001b[38;5;241m=\u001b[39m call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfinal_api_call_params)\n\u001b[0;32m    127\u001b[0m messages, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_from_provider(\n\u001b[0;32m    128\u001b[0m     provider_resp, ell_call, final_api_call_params, origin_id, logger\n\u001b[0;32m    129\u001b[0m )\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# assert \"choices\" not in metadata, \"choices should be in the metadata.\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\openai\\resources\\chat\\completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\openai\\_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1276\u001b[0m     )\n\u001b[1;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\openai\\_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\openai\\_base_client.py:990\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 990\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    991\u001b[0m         request,\n\u001b[0;32m    992\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    993\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    994\u001b[0m     )\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    996\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpx\\_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpx\\_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpx\\_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    989\u001b[0m     hook(request)\n\u001b[1;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpx\\_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1024\u001b[0m     )\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpx\\_transports\\default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    245\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    107\u001b[0m     (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m         trailing_data,\n\u001b[1;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m         http_version,\n\u001b[0;32m    116\u001b[0m         status,\n\u001b[0;32m    117\u001b[0m         reason_phrase,\n\u001b[0;32m    118\u001b[0m         headers,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpcore\\_sync\\http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1259\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1258\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "while len(additional_matches) < 13 and i < 5:\n",
    "    print(f\"Running iteration: {i+1}\")\n",
    "    vanilla_clusters = chunk_strat.run(iters=2)\n",
    "    summary_clusters = summarized_strat.run(iters=2)\n",
    "    \n",
    "    additional_matches.extend(match_clusters(vanilla_clusters, summary_clusters, min_match=5))\n",
    "    \n",
    "    print(\"Total length: \", len(additional_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(ClusteredTopic(name='Cluster 1: Real-time API Functionality', chunks=[CodeChunk(id='openai_realtime\\\\api.py::1', input_type=<ClusterInputType.FILE: 'file'>, content=\"import asyncio\\nimport json\\nimport websockets\\nfrom .event_handler import RealtimeEventHandler\\nfrom .utils import RealtimeUtils\\n\\nclass RealtimeAPI(RealtimeEventHandler):\\n    def __init__(self, url=None, api_key=None, dangerously_allow_api_key_in_browser=False, debug=False):\\n        super().__init__()\\n        self.default_url = 'wss://api.openai.com/v1/realtime'\\n        self.url = url or self.default_url\\n        self.api_key = api_key\\n        self.debug = debug\\n        self.ws = None\\n\\n    def is_connected(self):\\n        return self.ws is not None and self.ws.open\\n\\n    def log(self, *args):\\n        if self.debug:\\n            print(*args)\\n        return True\", filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\api.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\api.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeAPI(RealtimeEventHandler):\\n\\n    async def connect(self, model=\\'gpt-4o-realtime-preview-2024-10-01\\'):\\n        if self.is_connected():\\n            raise Exception(\"Already connected\")\\n\\n        headers = {\\n            \\'Authorization\\': f\\'Bearer {self.api_key}\\',\\n            \\'OpenAI-Beta\\': \\'realtime=v1\\'\\n        }\\n\\n        self.ws = await websockets.connect(f\"{self.url}?model={model}\", extra_headers=headers)\\n\\n        self.log(f\"Connected to {self.url}\")\\n\\n        asyncio.create_task(self._message_handler())\\n\\n        return True', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\api.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\api.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeAPI(RealtimeEventHandler):\\n\\n    async def _message_handler(self):\\n        try:\\n            async for message in self.ws:\\n                data = json.loads(message)\\n                self.receive(data[\\'type\\'], data)\\n        except websockets.exceptions.ConnectionClosed:\\n            self.disconnect()\\n            self.dispatch(\\'close\\', {\\'error\\': True})\\n\\n    def disconnect(self):\\n        if self.ws:\\n            asyncio.create_task(self.ws.close())\\n            self.ws = None\\n        return True\\n\\n    def receive(self, event_name, event):\\n        self.log(\"received:\", event_name, event)\\n        self.dispatch(f\"server.{event_name}\", event)\\n        self.dispatch(\"server.*\", event)\\n        return True', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\api.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\api.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeAPI(RealtimeEventHandler):\\n\\n    def send(self, event_name, data=None):\\n        if not self.is_connected():\\n            raise Exception(\"RealtimeAPI is not connected\")\\n\\n        data = data or {}\\n        if not isinstance(data, dict):\\n            raise ValueError(\"data must be a dictionary\")\\n\\n        event = {\\n            \"event_id\": RealtimeUtils.generate_id(\"evt_\"),\\n            \"type\": event_name,\\n            **data\\n        }\\n\\n        self.dispatch(f\"client.{event_name}\", event)\\n        self.dispatch(\"client.*\", event)\\n        self.log(\"sent:\", event_name, event)\\n\\n        asyncio.create_task(self.ws.send(json.dumps(event, ensure_ascii=False)))\\n        return True', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\api.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\client.py::1', input_type=<ClusterInputType.FILE: 'file'>, content=\"import asyncio\\nimport numpy as np\\nfrom .event_handler import RealtimeEventHandler\\nfrom .api import RealtimeAPI\\nfrom .conversation import RealtimeConversation\\nfrom .utils import RealtimeUtils\\nimport json\\n\\nclass RealtimeClient(RealtimeEventHandler):\\n    def __init__(self, url=None, api_key=None, instructions='', dangerously_allow_api_key_in_browser=False, debug=False):\\n        super().__init__()\\n        self.default_session_config = {\\n            'modalities': ['text', 'audio'],\\n            'instructions': instructions,\\n            'voice': 'alloy',\\n            'input_audio_format': 'pcm16',\\n            'output_audio_format': 'pcm16',\\n            'input_audio_transcription': None,\\n            'turn_detection': None,\\n            'tools': [],\\n            'tool_choice': 'auto',\\n            'temperature': 0.8,\\n            'max_response_output_tokens': 4096,\\n        }\\n        self.session_config = {}\\n        self.transcription_models = [{'model': 'whisper-1'}]\\n        self.default_server_vad_config = {\\n            'type': 'server_vad',\\n            'threshold': 0.5,\\n            'prefix_padding_ms': 300,\\n            'silence_duration_ms': 200,\\n        }\\n        self.realtime = RealtimeAPI(url, api_key, dangerously_allow_api_key_in_browser, debug)\\n        self.conversation = RealtimeConversation()\\n        self._reset_config()\\n        self._add_api_event_handlers()\\n\\n    def _reset_config(self):\\n        self.session_created = False\\n        self.tools = {}\\n        self.session_config = self.default_session_config.copy()\\n        self.input_audio_buffer = np.array([], dtype=np.int16)\\n        return True\", filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\client.py::2', input_type=<ClusterInputType.FILE: 'file'>, content=\"class RealtimeClient(RealtimeEventHandler):\\n\\n    def _add_api_event_handlers(self):\\n        self.realtime.on('client.*', lambda event: self.dispatch('realtime.event', {\\n            'time': RealtimeUtils.generate_id('time_'),\\n            'source': 'client',\\n            'event': event\\n        }))\\n        self.realtime.on('server.*', lambda event: self.dispatch('realtime.event', {\\n            'time': RealtimeUtils.generate_id('time_'),\\n            'source': 'server',\\n            'event': event\\n        }))\\n        self.realtime.on('server.session.created', lambda _: setattr(self, 'session_created', True))\\n\\n        def handle_conversation_event(event, *args):\\n            result = self.conversation.process_event(event, *args)\\n            if result['item']:\\n                self.dispatch('conversation.updated', result)\\n            return result\\n\\n        self.realtime.on('server.response.created', handle_conversation_event)\\n        self.realtime.on('server.response.output_item.added', handle_conversation_event)\\n        self.realtime.on('server.response.content_part.added', handle_conversation_event)\\n        self.realtime.on('server.input_audio_buffer.speech_started', lambda event: (\\n            handle_conversation_event(event),\\n            self.dispatch('conversation.interrupted', event)\\n        ))\\n        self.realtime.on('server.input_audio_buffer.speech_stopped', lambda event: \\n            handle_conversation_event(event, self.input_audio_buffer)\\n        )\\n        self.realtime.on('server.conversation.item.created', lambda event: (\\n            handle_conversation_event(event),\\n            self.dispatch('conversation.item.appended', {'item': event['item']})\\n        ))\\n        self.realtime.on('server.conversation.item.truncated', handle_conversation_event)\\n        self.realtime.on('server.conversation.item.deleted', handle_conversation_event)\\n        self.realtime.on('server.conversation.item.input_audio_transcription.completed', handle_conversation_event)\\n        self.realtime.on('server.response.audio_transcript.delta', handle_conversation_event)\\n        self.realtime.on('server.response.audio.delta', handle_conversation_event)\\n        self.realtime.on('server.response.text.delta', handle_conversation_event)\\n        self.realtime.on('server.response.function_call_arguments.delta', handle_conversation_event)\\n        def handle_output_item_done( event):\\n            handle_conversation_event(event)\\n            item = event.get('item', {})\\n\\n            if item.get('status') == 'completed':\\n                self.dispatch('conversation.item.completed', {'item': item})\\n\\n            formatted = item.get('formatted', {})\\n            tool = formatted.get('tool') if isinstance(formatted, dict) else None\\n\\n            if tool:\\n                asyncio.create_task(self._call_tool(tool))\\n        self.realtime.on('server.response.output_item.done', handle_output_item_done)\", filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\client.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeClient(RealtimeEventHandler):\\n\\n\\n\\n    def is_connected(self):\\n        return self.realtime.is_connected() and self.session_created\\n\\n    def reset(self):\\n        self.disconnect()\\n        self.clear_event_handlers()\\n        self.realtime.clear_event_handlers()\\n        self._reset_config()\\n        self._add_api_event_handlers()\\n        return True\\n\\n    async def connect(self):\\n        if self.is_connected():\\n            raise Exception(\"Already connected, use .disconnect() first\")\\n        await self.realtime.connect()\\n        self.update_session()\\n        return True\\n\\n    async def wait_for_session_created(self):\\n        if not self.realtime.is_connected():\\n            raise Exception(\"Not connected, use .connect() first\")\\n        while not self.session_created:\\n            await asyncio.sleep(0.001)\\n        return True\\n\\n    def disconnect(self):\\n        self.session_created = False\\n        self.conversation.clear()\\n        if self.realtime.is_connected():\\n            self.realtime.disconnect()\\n\\n    def get_turn_detection_type(self):\\n        turn_detection = self.session_config.get(\\'turn_detection\\')\\n        if isinstance(turn_detection, dict):\\n            return turn_detection.get(\\'type\\')\\n        return None', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\client.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeClient(RealtimeEventHandler):\\n\\n    def remove_tool(self, name):\\n        if name not in self.tools:\\n            raise ValueError(f\"Tool \\'{name}\\' does not exist, cannot be removed.\")\\n        del self.tools[name]\\n        return True\\n\\n    def delete_item(self, id):\\n        self.realtime.send(\\'conversation.item.delete\\', {\\'item_id\\': id})\\n        return True\\n\\n    def update_session(self, **kwargs):\\n        self.session_config.update(kwargs)\\n        use_tools = [\\n            {**tool.get(\\'definition\\', {}), \\'type\\': \\'function\\'}\\n            for tool in self.tools.values()\\n        ]\\n        session = {**self.session_config, \\'tools\\': use_tools}\\n        if self.realtime.is_connected():\\n            self.realtime.send(\\'session.update\\', {\\'session\\': session})\\n        return True', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\conversation.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='import numpy as np\\nimport json\\nfrom .utils import RealtimeUtils\\nimport copy\\n\\nclass RealtimeConversation:\\n    def __init__(self):\\n        self.default_frequency = 24000  # 24,000 Hz\\n        self.clear()\\n\\n    def clear(self):\\n        self.item_lookup = {}\\n        self.items = []\\n        self.response_lookup = {}\\n        self.responses = []\\n        self.queued_speech_items = {}\\n        self.queued_transcript_items = {}\\n        self.queued_input_audio = None\\n        return True\\n\\n    def queue_input_audio(self, input_audio):\\n        self.queued_input_audio = input_audio\\n        return input_audio\\n\\n    def process_event(self, event, *args):\\n        if \\'event_id\\' not in event:\\n            raise ValueError(\"Missing \\'event_id\\' on event\")\\n        if \\'type\\' not in event:\\n            raise ValueError(\"Missing \\'type\\' on event\")\\n\\n        event_processor = getattr(self, f\"_process_{event[\\'type\\'].replace(\\'.\\', \\'_\\')}\", None)\\n        if not event_processor:\\n            raise ValueError(f\"Missing conversation event processor for \\'{event[\\'type\\']}\\'\")\\n\\n        return event_processor(event, *args)\\n\\n    def get_item(self, id):\\n        return self.item_lookup.get(id)\\n\\n    def get_items(self):\\n        return self.items.copy()', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\conversation.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\conversation.py::2', input_type=<ClusterInputType.FILE: 'file'>, content=\"class RealtimeConversation:\\n\\n    def _process_conversation_item_created(self, event):\\n        item = event['item']\\n        new_item = copy.deepcopy(item)\\n        if new_item['id'] not in self.item_lookup:\\n            self.item_lookup[new_item['id']] = new_item\\n            self.items.append(new_item)\\n\\n        new_item['formatted'] = {\\n            'audio': np.array([], dtype=np.int16),\\n            'text': '',\\n            'transcript': ''\\n        }\\n\\n        if new_item['type'] == 'message':\\n            if new_item['role'] == 'user':\\n                new_item['status'] = 'completed'\\n                if self.queued_input_audio is not None:\\n                    new_item['formatted']['audio'] = self.queued_input_audio\\n                    self.queued_input_audio = None\\n            else:\\n                new_item['status'] = 'in_progress'\\n        elif new_item['type'] == 'function_call':\\n            new_item['formatted']['tool'] = {\\n                'type': 'function',\\n                'name': new_item['name'],\\n                'call_id': new_item['call_id'],\\n                'arguments': ''\\n            }\\n            new_item['status'] = 'in_progress'\\n        elif new_item['type'] == 'function_call_output':\\n            new_item['status'] = 'completed'\\n            new_item['formatted']['output'] = new_item['output']\\n\\n        if new_item.get('content'):\\n            text_content = [c for c in new_item['content'] if c['type'] in ['text', 'input_text']]\\n            for content in text_content:\\n                new_item['formatted']['text'] += content['text']\\n\\n        if new_item['id'] in self.queued_speech_items:\\n            new_item['formatted']['audio'] = self.queued_speech_items[new_item['id']]['audio']\\n            del self.queued_speech_items[new_item['id']]\\n\\n        if new_item['id'] in self.queued_transcript_items:\\n            new_item['formatted']['transcript'] = self.queued_transcript_items[new_item['id']]['transcript']\\n            del self.queued_transcript_items[new_item['id']]\\n\\n        return {'item': new_item, 'delta': None}\", filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\conversation.py', metadata=None, node_id='')]),\n",
       "  ClusteredTopic(name='Real-Time Client and Interaction Handlers', chunks=[CodeChunk(id='openai_realtime\\\\__init__.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from .client import RealtimeClient\\nfrom .api import RealtimeAPI\\nfrom .conversation import RealtimeConversation\\nfrom .event_handler import RealtimeEventHandler\\nfrom .utils import RealtimeUtils\\n\\n__all__ = [\\n    \"RealtimeClient\",\\n    \"RealtimeAPI\",\\n    \"RealtimeConversation\",\\n    \"RealtimeEventHandler\",\\n    \"RealtimeUtils\"\\n]', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\__init__.py', metadata=None, node_id='openai_realtime\\\\__init__.py::1'), CodeChunk(id='openai_realtime\\\\api.py::1', input_type=<ClusterInputType.FILE: 'file'>, content=\"import asyncio\\nimport json\\nimport websockets\\nfrom .event_handler import RealtimeEventHandler\\nfrom .utils import RealtimeUtils\\n\\nclass RealtimeAPI(RealtimeEventHandler):\\n    def __init__(self, url=None, api_key=None, dangerously_allow_api_key_in_browser=False, debug=False):\\n        super().__init__()\\n        self.default_url = 'wss://api.openai.com/v1/realtime'\\n        self.url = url or self.default_url\\n        self.api_key = api_key\\n        self.debug = debug\\n        self.ws = None\\n\\n    def is_connected(self):\\n        return self.ws is not None and self.ws.open\\n\\n    def log(self, *args):\\n        if self.debug:\\n            print(*args)\\n        return True\", filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\api.py', metadata=None, node_id='openai_realtime\\\\api.py::1'), CodeChunk(id='openai_realtime\\\\api.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeAPI(RealtimeEventHandler):\\n\\n    async def connect(self, model=\\'gpt-4o-realtime-preview-2024-10-01\\'):\\n        if self.is_connected():\\n            raise Exception(\"Already connected\")\\n\\n        headers = {\\n            \\'Authorization\\': f\\'Bearer {self.api_key}\\',\\n            \\'OpenAI-Beta\\': \\'realtime=v1\\'\\n        }\\n\\n        self.ws = await websockets.connect(f\"{self.url}?model={model}\", extra_headers=headers)\\n\\n        self.log(f\"Connected to {self.url}\")\\n\\n        asyncio.create_task(self._message_handler())\\n\\n        return True', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\api.py', metadata=None, node_id='openai_realtime\\\\api.py::2'), CodeChunk(id='openai_realtime\\\\api.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeAPI(RealtimeEventHandler):\\n\\n    async def _message_handler(self):\\n        try:\\n            async for message in self.ws:\\n                data = json.loads(message)\\n                self.receive(data[\\'type\\'], data)\\n        except websockets.exceptions.ConnectionClosed:\\n            self.disconnect()\\n            self.dispatch(\\'close\\', {\\'error\\': True})\\n\\n    def disconnect(self):\\n        if self.ws:\\n            asyncio.create_task(self.ws.close())\\n            self.ws = None\\n        return True\\n\\n    def receive(self, event_name, event):\\n        self.log(\"received:\", event_name, event)\\n        self.dispatch(f\"server.{event_name}\", event)\\n        self.dispatch(\"server.*\", event)\\n        return True', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\api.py', metadata=None, node_id='openai_realtime\\\\api.py::3'), CodeChunk(id='openai_realtime\\\\client.py::1', input_type=<ClusterInputType.FILE: 'file'>, content=\"import asyncio\\nimport numpy as np\\nfrom .event_handler import RealtimeEventHandler\\nfrom .api import RealtimeAPI\\nfrom .conversation import RealtimeConversation\\nfrom .utils import RealtimeUtils\\nimport json\\n\\nclass RealtimeClient(RealtimeEventHandler):\\n    def __init__(self, url=None, api_key=None, instructions='', dangerously_allow_api_key_in_browser=False, debug=False):\\n        super().__init__()\\n        self.default_session_config = {\\n            'modalities': ['text', 'audio'],\\n            'instructions': instructions,\\n            'voice': 'alloy',\\n            'input_audio_format': 'pcm16',\\n            'output_audio_format': 'pcm16',\\n            'input_audio_transcription': None,\\n            'turn_detection': None,\\n            'tools': [],\\n            'tool_choice': 'auto',\\n            'temperature': 0.8,\\n            'max_response_output_tokens': 4096,\\n        }\\n        self.session_config = {}\\n        self.transcription_models = [{'model': 'whisper-1'}]\\n        self.default_server_vad_config = {\\n            'type': 'server_vad',\\n            'threshold': 0.5,\\n            'prefix_padding_ms': 300,\\n            'silence_duration_ms': 200,\\n        }\\n        self.realtime = RealtimeAPI(url, api_key, dangerously_allow_api_key_in_browser, debug)\\n        self.conversation = RealtimeConversation()\\n        self._reset_config()\\n        self._add_api_event_handlers()\\n\\n    def _reset_config(self):\\n        self.session_created = False\\n        self.tools = {}\\n        self.session_config = self.default_session_config.copy()\\n        self.input_audio_buffer = np.array([], dtype=np.int16)\\n        return True\", filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id='openai_realtime\\\\client.py::1'), CodeChunk(id='openai_realtime\\\\client.py::2', input_type=<ClusterInputType.FILE: 'file'>, content=\"class RealtimeClient(RealtimeEventHandler):\\n\\n    def _add_api_event_handlers(self):\\n        self.realtime.on('client.*', lambda event: self.dispatch('realtime.event', {\\n            'time': RealtimeUtils.generate_id('time_'),\\n            'source': 'client',\\n            'event': event\\n        }))\\n        self.realtime.on('server.*', lambda event: self.dispatch('realtime.event', {\\n            'time': RealtimeUtils.generate_id('time_'),\\n            'source': 'server',\\n            'event': event\\n        }))\\n        self.realtime.on('server.session.created', lambda _: setattr(self, 'session_created', True))\\n\\n        def handle_conversation_event(event, *args):\\n            result = self.conversation.process_event(event, *args)\\n            if result['item']:\\n                self.dispatch('conversation.updated', result)\\n            return result\\n\\n        self.realtime.on('server.response.created', handle_conversation_event)\\n        self.realtime.on('server.response.output_item.added', handle_conversation_event)\\n        self.realtime.on('server.response.content_part.added', handle_conversation_event)\\n        self.realtime.on('server.input_audio_buffer.speech_started', lambda event: (\\n            handle_conversation_event(event),\\n            self.dispatch('conversation.interrupted', event)\\n        ))\\n        self.realtime.on('server.input_audio_buffer.speech_stopped', lambda event: \\n            handle_conversation_event(event, self.input_audio_buffer)\\n        )\\n        self.realtime.on('server.conversation.item.created', lambda event: (\\n            handle_conversation_event(event),\\n            self.dispatch('conversation.item.appended', {'item': event['item']})\\n        ))\\n        self.realtime.on('server.conversation.item.truncated', handle_conversation_event)\\n        self.realtime.on('server.conversation.item.deleted', handle_conversation_event)\\n        self.realtime.on('server.conversation.item.input_audio_transcription.completed', handle_conversation_event)\\n        self.realtime.on('server.response.audio_transcript.delta', handle_conversation_event)\\n        self.realtime.on('server.response.audio.delta', handle_conversation_event)\\n        self.realtime.on('server.response.text.delta', handle_conversation_event)\\n        self.realtime.on('server.response.function_call_arguments.delta', handle_conversation_event)\\n        def handle_output_item_done( event):\\n            handle_conversation_event(event)\\n            item = event.get('item', {})\\n\\n            if item.get('status') == 'completed':\\n                self.dispatch('conversation.item.completed', {'item': item})\\n\\n            formatted = item.get('formatted', {})\\n            tool = formatted.get('tool') if isinstance(formatted, dict) else None\\n\\n            if tool:\\n                asyncio.create_task(self._call_tool(tool))\\n        self.realtime.on('server.response.output_item.done', handle_output_item_done)\", filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id='openai_realtime\\\\client.py::2'), CodeChunk(id='openai_realtime\\\\client.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeClient(RealtimeEventHandler):\\n\\n\\n\\n    def is_connected(self):\\n        return self.realtime.is_connected() and self.session_created\\n\\n    def reset(self):\\n        self.disconnect()\\n        self.clear_event_handlers()\\n        self.realtime.clear_event_handlers()\\n        self._reset_config()\\n        self._add_api_event_handlers()\\n        return True\\n\\n    async def connect(self):\\n        if self.is_connected():\\n            raise Exception(\"Already connected, use .disconnect() first\")\\n        await self.realtime.connect()\\n        self.update_session()\\n        return True\\n\\n    async def wait_for_session_created(self):\\n        if not self.realtime.is_connected():\\n            raise Exception(\"Not connected, use .connect() first\")\\n        while not self.session_created:\\n            await asyncio.sleep(0.001)\\n        return True\\n\\n    def disconnect(self):\\n        self.session_created = False\\n        self.conversation.clear()\\n        if self.realtime.is_connected():\\n            self.realtime.disconnect()\\n\\n    def get_turn_detection_type(self):\\n        turn_detection = self.session_config.get(\\'turn_detection\\')\\n        if isinstance(turn_detection, dict):\\n            return turn_detection.get(\\'type\\')\\n        return None', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id='openai_realtime\\\\client.py::3'), CodeChunk(id='openai_realtime\\\\client.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeClient(RealtimeEventHandler):\\n\\n    def add_tool(self, definition, handler):\\n        if not definition.get(\\'name\\'):\\n            raise ValueError(\"Missing tool name in definition\")\\n        name = definition[\\'name\\']\\n        if name in self.tools:\\n            raise ValueError(f\"Tool \\'{name}\\' already added. Please use .remove_tool(\\'{name}\\') before trying to add again.\")\\n        if not callable(handler):\\n            raise ValueError(f\"Tool \\'{name}\\' handler must be a function\")\\n        self.tools[name] = {\\'definition\\': definition, \\'handler\\': handler}\\n        self.update_session()\\n        return self.tools[name]', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id='openai_realtime\\\\client.py::4'), CodeChunk(id='openai_realtime\\\\client.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='class RealtimeClient(RealtimeEventHandler):\\n\\n    def remove_tool(self, name):\\n        if name not in self.tools:\\n            raise ValueError(f\"Tool \\'{name}\\' does not exist, cannot be removed.\")\\n        del self.tools[name]\\n        return True\\n\\n    def delete_item(self, id):\\n        self.realtime.send(\\'conversation.item.delete\\', {\\'item_id\\': id})\\n        return True\\n\\n    def update_session(self, **kwargs):\\n        self.session_config.update(kwargs)\\n        use_tools = [\\n            {**tool.get(\\'definition\\', {}), \\'type\\': \\'function\\'}\\n            for tool in self.tools.values()\\n        ]\\n        session = {**self.session_config, \\'tools\\': use_tools}\\n        if self.realtime.is_connected():\\n            self.realtime.send(\\'session.update\\', {\\'session\\': session})\\n        return True', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id='openai_realtime\\\\client.py::5'), CodeChunk(id='openai_realtime\\\\client.py::7', input_type=<ClusterInputType.FILE: 'file'>, content=\"class RealtimeClient(RealtimeEventHandler):\\n\\n    def append_input_audio(self, array_buffer):\\n        if len(array_buffer) > 0:\\n            self.realtime.send('input_audio_buffer.append', {\\n                'audio': RealtimeUtils.array_buffer_to_base64(array_buffer)\\n            })\\n            self.input_audio_buffer = RealtimeUtils.merge_int16_arrays(\\n                self.input_audio_buffer,\\n                array_buffer\\n            )\\n        return True\\n\\n    def create_response(self):\\n        if self.get_turn_detection_type() is None and len(self.input_audio_buffer) > 0:\\n            self.realtime.send('input_audio_buffer.commit')\\n            self.conversation.queue_input_audio(self.input_audio_buffer)\\n            self.input_audio_buffer = np.array([], dtype=np.int16)\\n        self.realtime.send('response.create')\\n        return True\", filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id='openai_realtime\\\\client.py::7')]),\n",
       "  0.7,\n",
       "  0.7),\n",
       " (ClusteredTopic(name='Cluster 2: Reinforcement Learning with CEM', chunks=[CodeChunk(id='0.1.0\\\\cem.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='# Main execution code\\nif __name__ == \\'__main__\\':\\n    # Initialize environments\\n    env_fns = [make_env(ENV_NAME, SEED + i) for i in range(NUM_ENVIRONMENTS)]\\n    envs = AsyncVectorEnv(env_fns)\\n\\n    # Get environment details\\n    dummy_env = gym.make(ENV_NAME)\\n    state_dim = dummy_env.observation_space.shape[0]\\n    action_dim = dummy_env.action_space.n\\n    dummy_env.close()\\n\\n    # Initialize policy network and optimizer\\n    policy = PolicyNetwork(state_dim, action_dim)\\n    optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)\\n    criterion = nn.CrossEntropyLoss()\\n\\n    # Training Loop\\n    for iteration in range(1, NUM_ITERATIONS + 1):\\n        try:\\n            # Step 1: Collect Trajectories\\n            trajectories = collect_trajectories(envs, policy, TRAJECTORIES_PER_ITER, MAX_STEPS)\\n        except Exception as e:\\n            print(f\"Error during trajectory collection at iteration {iteration}: {e}\")\\n            break\\n\\n        # Step 2: Select Elite Trajectories\\n        elite_trajectories = select_elite(trajectories, ELITE_PERCENT)\\n\\n        if len(elite_trajectories) == 0:\\n            print(f\"Iteration {iteration}: No elite trajectories found. Skipping update.\")\\n            continue\\n\\n        # Step 3: Create Training Data\\n        states, actions = create_training_data(elite_trajectories)\\n\\n        if states is None or actions is None:\\n            print(f\"Iteration {iteration}: No training data available. Skipping update.\")\\n            continue\\n\\n        # Step 4: Behavioral Cloning (Policy Update)\\n        dataset_size = states.size(0)\\n        indices = np.arange(dataset_size)\\n        np.random.shuffle(indices)\\n\\n        for start in range(0, dataset_size, BATCH_SIZE):\\n            end = start + BATCH_SIZE\\n            batch_indices = indices[start:end]\\n            batch_states = states[batch_indices]\\n            batch_actions = actions[batch_indices]\\n\\n            optimizer.zero_grad()\\n            logits = policy(batch_states)\\n            loss = criterion(logits, batch_actions)\\n            loss.backward()\\n            optimizer.step()\\n\\n        # Step 5: Evaluate Current Policy\\n        avg_reward = np.mean([traj[\\'reward\\'] for traj in elite_trajectories])\\n        print(f\"Iteration {iteration}: Elite Trajectories: {len(elite_trajectories)}, Average Reward: {avg_reward:.2f}\")\\n\\n    # Close environments\\n    envs.close()\\n\\n    # Testing the Trained Policy\\n    def test_policy(policy, env_name=ENV_NAME, episodes=5, max_steps=500):\\n        env = gym.make(env_name)\\n        total_rewards = []\\n        for episode in range(episodes):\\n            obs, _ = env.reset()\\n            done = False\\n            episode_reward = 0\\n            for _ in range(max_steps):\\n                obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)\\n                with torch.no_grad():\\n                    action = policy.get_action(obs_tensor).item()\\n                obs, reward, done, info, _ = env.step(action)\\n                episode_reward += reward\\n                if done:\\n                    break\\n            total_rewards.append(episode_reward)\\n            print(f\"Test Episode {episode + 1}: Reward: {episode_reward}\")\\n        env.close()\\n        print(f\"Average Test Reward over {episodes} episodes: {np.mean(total_rewards):.2f}\")\\n\\n    # Run the test\\n    test_policy(policy)', filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id=''), CodeChunk(id='0.1.0\\\\cem.py::1', input_type=<ClusterInputType.FILE: 'file'>, content=\"import gym\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport numpy as np\\nfrom gym.vector import AsyncVectorEnv\\nimport random\\n\\n# Set random seeds for reproducibility\\nSEED = 42\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\n\\n# Hyperparameters\\nNUM_ENVIRONMENTS = 4           # Reduced for simplicity\\nNUM_ITERATIONS = 50            # Number of training iterations\\nTRAJECTORIES_PER_ITER = 100    # Total number of trajectories per iteration\\nELITE_PERCENT = 10             # Top k% trajectories to select\\nLEARNING_RATE = 1e-3\\nBATCH_SIZE = 64\\nMAX_STEPS = 500                # Max steps per trajectory\\nENV_NAME = 'CartPole-v1'\", filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id=''), CodeChunk(id='0.1.0\\\\cem.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='       # Gym environment\\n\\n# Define the Policy Network\\nclass PolicyNetwork(nn.Module):\\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\\n        super(PolicyNetwork, self).__init__()\\n        self.fc = nn.Sequential(\\n            nn.Linear(state_dim, hidden_dim),\\n            nn.ReLU(),\\n            nn.Linear(hidden_dim, hidden_dim),\\n            nn.ReLU(),\\n            nn.Linear(hidden_dim, action_dim)\\n        )\\n\\n    def forward(self, state):\\n        logits = self.fc(state)\\n        return logits\\n\\n    def get_action(self, state):\\n        logits = self.forward(state)\\n        action_probs = torch.softmax(logits, dim=-1)\\n        action = torch.multinomial(action_probs, num_samples=1)\\n        return action.squeeze(-1)\\n\\n# Function to create multiple environments\\ndef make_env(env_name, seed):\\n    def _init():\\n        env = gym.make(env_name)\\n        return env\\n    return _init', filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id=''), CodeChunk(id='0.1.0\\\\cem.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='def collect_trajectories(envs, policy, num_trajectories, max_steps):\\n    trajectories = []\\n    num_envs = envs.num_envs\\n\\n    # Handle the return type of reset()\\n    reset_output = envs.reset()\\n    if isinstance(reset_output, tuple) or isinstance(reset_output, list):\\n        obs = reset_output[0]  # Extract observations\\n    else:\\n        obs = reset_output\\n\\n    done_envs = [False] * num_envs\\n    steps = 0\\n\\n    # Initialize storage for states, actions, and rewards per environment\\n    env_states = [[] for _ in range(num_envs)]\\n    env_actions = [[] for _ in range(num_envs)]\\n    env_rewards = [0.0 for _ in range(num_envs)]\\n    total_collected = 0\\n    # ... other code', filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id=''), CodeChunk(id='0.1.0\\\\cem.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='def collect_trajectories(envs, policy, num_trajectories, max_steps):\\n    # ... other code\\n\\n    while total_collected < num_trajectories and steps < max_steps:\\n        # Convert observations to tensor efficiently\\n        try:\\n            # Ensure \\'obs\\' is a NumPy array\\n            if not isinstance(obs, np.ndarray):\\n                print(f\"Unexpected type for observations: {type(obs)}\")\\n                raise ValueError(\"Observations are not a NumPy array.\")\\n\\n            # Convert observations to tensor using from_numpy for efficiency\\n            obs_tensor = torch.from_numpy(obs).float()\\n            # Ensure the observation dimension matches expected\\n            assert obs_tensor.shape[1] == 4, f\"Expected observation dimension 4, got {obs_tensor.shape[1]}\"\\n        except Exception as e:\\n            print(f\"Error converting observations to tensor at step {steps}: {e}\")\\n            print(f\"Observations: {obs}\")\\n            raise e\\n\\n        with torch.no_grad():\\n            actions = policy.get_action(obs_tensor).cpu().numpy()\\n\\n        # Unpack step based on Gym version\\n        try:\\n            # For Gym versions >=0.26, step returns five values\\n            next_obs, rewards, dones, truncs, infos = envs.step(actions)\\n        except ValueError:\\n            # For older Gym versions, step returns four values\\n            next_obs, rewards, dones, infos = envs.step(actions)\\n            truncs = [False] * len(dones)  # Assume no truncations if not provided\\n\\n        # Handle the reset output of step()\\n        if isinstance(next_obs, tuple) or isinstance(next_obs, list):\\n            next_obs = next_obs[0]  # Extract observations\\n\\n        # Ensure infos is a list\\n        if not isinstance(infos, list):\\n            infos = [{} for _ in range(num_envs)]  # Default to empty dicts\\n\\n        for i in range(num_envs):\\n            if not done_envs[i]:\\n                # Check if obs[i] has the correct shape\\n                if len(obs[i]) != 4:\\n                    print(f\"Unexpected observation shape for env {i}: {obs[i]}\")\\n                    continue  # Skip this step for the problematic environment\\n\\n                env_states[i].append(obs[i])\\n                env_actions[i].append(actions[i])\\n                env_rewards[i] += rewards[i]\\n                if dones[i] or truncs[i]:\\n                    # Extract reward from infos\\n                    if isinstance(infos[i], dict):\\n                        episode_info = infos[i].get(\\'episode\\', {})\\n                        traj_reward = episode_info.get(\\'r\\') if \\'r\\' in episode_info else env_rewards[i]\\n                    else:\\n                        # Handle cases where infos[i] is not a dict\\n                        traj_reward = env_rewards[i]\\n                        print(f\"Warning: infos[{i}] is not a dict. Received type: {type(infos[i])}\")\\n\\n                    trajectories.append({\\n                        \\'states\\': env_states[i],\\n                        \\'actions\\': env_actions[i],\\n                        \\'reward\\': traj_reward\\n                    })\\n                    total_collected += 1\\n                    env_states[i] = []\\n                    env_actions[i] = []\\n                    env_rewards[i] = 0.0\\n                    done_envs[i] = True\\n\\n        obs = next_obs\\n        steps += 1\\n\\n        # Reset environments that are done\\n        if any(done_envs):\\n            indices = [i for i, done in enumerate(done_envs) if done]\\n            if total_collected < num_trajectories:\\n                for i in indices:\\n                    try:\\n                        # Directly reset the environment\\n                        reset_output = envs.envs[i].reset()\\n                        if isinstance(reset_output, tuple) or isinstance(reset_output, list):\\n                            # For Gym versions where reset returns (obs, info)\\n                            obs[i] = reset_output[0]\\n                        else:\\n                            # For Gym versions where reset returns only obs\\n                            obs[i] = reset_output\\n                        done_envs[i] = False\\n                    except Exception as e:\\n                        print(f\"Error resetting environment {i}: {e}\")\\n                        # Optionally, handle the failure (e.g., retry, terminate the environment)\\n                        done_envs[i] = False  # Prevent infinite loop\\n\\n    return trajectories', filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id=''), CodeChunk(id='0.1.0\\\\cem.py::5', input_type=<ClusterInputType.FILE: 'file'>, content=\"def select_elite(trajectories, percentile=ELITE_PERCENT):\\n    rewards = [traj['reward'] for traj in trajectories]\\n    if not rewards:\\n        return []\\n    reward_threshold = np.percentile(rewards, 100 - percentile)\\n    elite_trajectories = [traj for traj in trajectories if traj['reward'] >= reward_threshold]\\n    return elite_trajectories\\n\\n# Function to create training dataset from elite trajectories\\ndef create_training_data(elite_trajectories):\\n    states = []\\n    actions = []\\n    for traj in elite_trajectories:\\n        states.extend(traj['states'])\\n        actions.extend(traj['actions'])\\n    if not states or not actions:\\n        return None, None\\n    # Convert lists to NumPy arrays first for efficiency\\n    states = np.array(states, dtype=np.float32)\\n    actions = np.array(actions, dtype=np.int64)\\n    # Convert to PyTorch tensors\\n    states = torch.from_numpy(states)\\n    actions = torch.from_numpy(actions)\\n    return states, actions\", filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id=''), CodeChunk(id='0.1.0\\\\cpbo.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='import gym\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport numpy as np\\nfrom collections import namedtuple\\nfrom torch.utils.data import DataLoader, TensorDataset\\n\\n# Define a simple policy network\\nclass PolicyNetwork(nn.Module):\\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\\n        super(PolicyNetwork, self).__init__()\\n        self.network = nn.Sequential(\\n            nn.Linear(state_dim, hidden_dim),\\n            nn.ReLU(),\\n            nn.Linear(hidden_dim, hidden_dim),\\n            nn.ReLU(),\\n            nn.Linear(hidden_dim, action_dim),\\n            nn.Softmax(dim=-1)  # Output action probabilities\\n        )\\n\\n    def forward(self, x):\\n        return self.network(x)', filepath='docs\\\\ramblings\\\\0.1.0\\\\cpbo.py', metadata=None, node_id=''), CodeChunk(id='0.1.0\\\\cpbo.py::2', input_type=<ClusterInputType.FILE: 'file'>, content=\"# Function to collect trajectories\\ndef collect_trajectories(env, policy, num_episodes, device):\\n    trajectories = []\\n    Episode = namedtuple('Episode', ['states', 'actions', 'rewards'])\\n\\n    for episode_num in range(num_episodes):\\n        states = []\\n        actions = []\\n        rewards = []\\n        # Handle Gym's updated reset() API\\n        state, info = env.reset(seed=42 + episode_num)  # Optional: set seed for reproducibility\\n        done = False\\n\\n        while not done:\\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\\n            with torch.no_grad():\\n                action_probs = policy(state_tensor)\\n            action_dist = torch.distributions.Categorical(action_probs)\\n            action = action_dist.sample().item()\\n\\n            # Handle Gym's updated step() API\\n            next_state, reward, terminated, truncated, info = env.step(action)\\n            done = terminated or truncated\\n\\n            states.append(state)\\n            actions.append(action)\\n            rewards.append(reward)\\n\\n            state = next_state\\n\\n        trajectories.append(Episode(states, actions, rewards))\\n\\n    return trajectories\\n\\n# Function to compute returns\\ndef compute_returns(trajectories, gamma=0.99):\\n    all_returns = []\\n    for episode in trajectories:\\n        returns = []\\n        G = 0\\n        for reward in reversed(episode.rewards):\\n            G = reward + gamma * G\\n            returns.insert(0, G)\\n        all_returns.extend(returns)\\n    return all_returns\", filepath='docs\\\\ramblings\\\\0.1.0\\\\cpbo.py', metadata=None, node_id=''), CodeChunk(id='0.1.0\\\\cpbo.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='# Main CBPO algorithm\\ndef CBPO(env_name=\\'CartPole-v1\\', num_epochs=10, num_episodes_per_epoch=100, gamma=0.99, \\n         batch_size=64, learning_rate=1e-3, device=\\'cpu\\'):\\n\\n    env = gym.make(env_name)\\n    state_dim = env.observation_space.shape[0]\\n    action_dim = env.action_space.n\\n\\n    policy = PolicyNetwork(state_dim, action_dim).to(device)\\n    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\\n\\n    for epoch in range(num_epochs):\\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\\n\\n        # 1. Collect trajectories\\n        trajectories = collect_trajectories(env, policy, num_episodes_per_epoch, device)\\n\\n        # 2. Create labeled dataset\\n        states, actions, labels = create_labeled_dataset(trajectories, gamma, device)\\n\\n        # 3. Create DataLoader\\n        dataset = TensorDataset(states, actions, labels)\\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\\n\\n        # 4. Behavioral Cloning Update\\n        behavioral_cloning_update(policy, optimizer, dataloader, device)\\n\\n        # 5. Evaluate current policy\\n        avg_reward = evaluate_policy(env, policy, device)\\n        print(f\"Average Reward: {avg_reward}\")\\n\\n        # Early stopping if solved\\n        if avg_reward >= env.spec.reward_threshold:\\n            print(f\"Environment solved in {epoch+1} epochs!\")\\n            break\\n\\n    env.close()\\n    return policy', filepath='docs\\\\ramblings\\\\0.1.0\\\\cpbo.py', metadata=None, node_id=''), CodeChunk(id='0.1.0\\\\cpbo.py::7', input_type=<ClusterInputType.FILE: 'file'>, content='if __name__ == \"__main__\":\\n    # Check if GPU is available\\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n    print(f\"Using device: {device}\")\\n\\n    # Run CBPO\\n    trained_policy = CBPO(\\n        env_name=\\'CartPole-v1\\',\\n        num_epochs=50,\\n        num_episodes_per_epoch=500,\\n        gamma=0.99,\\n        batch_size=64,\\n        learning_rate=1e-3,\\n        device=device\\n    )\\n\\n    # Final Evaluation\\n    env = gym.make(\\'CartPole-v1\\')\\n    final_avg_reward = evaluate_policy(env, trained_policy, device, episodes=20)\\n    print(f\"Final Average Reward over 20 episodes: {final_avg_reward}\")\\n    env.close()\\n\\n    # Save the trained policy\\n    torch.save(trained_policy.state_dict(), \"trained_cartpole_policy.pth\")\\n    print(\"Trained policy saved to trained_cartpole_policy.pth\")\\n\\n    # Demo the trained policy with rendering\\n    env = gym.make(\\'CartPole-v1\\', render_mode=\\'human\\')\\n    state, _ = env.reset()\\n    done = False\\n    total_reward = 0\\n\\n    while not done:\\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\\n        action = trained_policy(state_tensor).argmax().item()\\n        state, reward, terminated, truncated, _ = env.step(action)\\n        total_reward += reward\\n        done = terminated or truncated\\n        env.render()\\n\\n    print(f\"Demo episode finished with total reward: {total_reward}\")\\n    env.close()', filepath='docs\\\\ramblings\\\\0.1.0\\\\cpbo.py', metadata=None, node_id='')]),\n",
       "  ClusteredTopic(name='Cross-Entropy Method Implementation', chunks=[CodeChunk(id='0.1.0\\\\cem.py::1', input_type=<ClusterInputType.FILE: 'file'>, content=\"import gym\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport numpy as np\\nfrom gym.vector import AsyncVectorEnv\\nimport random\\n\\n# Set random seeds for reproducibility\\nSEED = 42\\nrandom.seed(SEED)\\nnp.random.seed(SEED)\\ntorch.manual_seed(SEED)\\n\\n# Hyperparameters\\nNUM_ENVIRONMENTS = 4           # Reduced for simplicity\\nNUM_ITERATIONS = 50            # Number of training iterations\\nTRAJECTORIES_PER_ITER = 100    # Total number of trajectories per iteration\\nELITE_PERCENT = 10             # Top k% trajectories to select\\nLEARNING_RATE = 1e-3\\nBATCH_SIZE = 64\\nMAX_STEPS = 500                # Max steps per trajectory\\nENV_NAME = 'CartPole-v1'\", filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id='0.1.0\\\\cem.py::1'), CodeChunk(id='0.1.0\\\\cem.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='       # Gym environment\\n\\n# Define the Policy Network\\nclass PolicyNetwork(nn.Module):\\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\\n        super(PolicyNetwork, self).__init__()\\n        self.fc = nn.Sequential(\\n            nn.Linear(state_dim, hidden_dim),\\n            nn.ReLU(),\\n            nn.Linear(hidden_dim, hidden_dim),\\n            nn.ReLU(),\\n            nn.Linear(hidden_dim, action_dim)\\n        )\\n\\n    def forward(self, state):\\n        logits = self.fc(state)\\n        return logits\\n\\n    def get_action(self, state):\\n        logits = self.forward(state)\\n        action_probs = torch.softmax(logits, dim=-1)\\n        action = torch.multinomial(action_probs, num_samples=1)\\n        return action.squeeze(-1)\\n\\n# Function to create multiple environments\\ndef make_env(env_name, seed):\\n    def _init():\\n        env = gym.make(env_name)\\n        return env\\n    return _init', filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id='0.1.0\\\\cem.py::2'), CodeChunk(id='0.1.0\\\\cem.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='def collect_trajectories(envs, policy, num_trajectories, max_steps):\\n    trajectories = []\\n    num_envs = envs.num_envs\\n\\n    # Handle the return type of reset()\\n    reset_output = envs.reset()\\n    if isinstance(reset_output, tuple) or isinstance(reset_output, list):\\n        obs = reset_output[0]  # Extract observations\\n    else:\\n        obs = reset_output\\n\\n    done_envs = [False] * num_envs\\n    steps = 0\\n\\n    # Initialize storage for states, actions, and rewards per environment\\n    env_states = [[] for _ in range(num_envs)]\\n    env_actions = [[] for _ in range(num_envs)]\\n    env_rewards = [0.0 for _ in range(num_envs)]\\n    total_collected = 0\\n    # ... other code', filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id='0.1.0\\\\cem.py::3'), CodeChunk(id='0.1.0\\\\cem.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='def collect_trajectories(envs, policy, num_trajectories, max_steps):\\n    # ... other code\\n\\n    while total_collected < num_trajectories and steps < max_steps:\\n        # Convert observations to tensor efficiently\\n        try:\\n            # Ensure \\'obs\\' is a NumPy array\\n            if not isinstance(obs, np.ndarray):\\n                print(f\"Unexpected type for observations: {type(obs)}\")\\n                raise ValueError(\"Observations are not a NumPy array.\")\\n\\n            # Convert observations to tensor using from_numpy for efficiency\\n            obs_tensor = torch.from_numpy(obs).float()\\n            # Ensure the observation dimension matches expected\\n            assert obs_tensor.shape[1] == 4, f\"Expected observation dimension 4, got {obs_tensor.shape[1]}\"\\n        except Exception as e:\\n            print(f\"Error converting observations to tensor at step {steps}: {e}\")\\n            print(f\"Observations: {obs}\")\\n            raise e\\n\\n        with torch.no_grad():\\n            actions = policy.get_action(obs_tensor).cpu().numpy()\\n\\n        # Unpack step based on Gym version\\n        try:\\n            # For Gym versions >=0.26, step returns five values\\n            next_obs, rewards, dones, truncs, infos = envs.step(actions)\\n        except ValueError:\\n            # For older Gym versions, step returns four values\\n            next_obs, rewards, dones, infos = envs.step(actions)\\n            truncs = [False] * len(dones)  # Assume no truncations if not provided\\n\\n        # Handle the reset output of step()\\n        if isinstance(next_obs, tuple) or isinstance(next_obs, list):\\n            next_obs = next_obs[0]  # Extract observations\\n\\n        # Ensure infos is a list\\n        if not isinstance(infos, list):\\n            infos = [{} for _ in range(num_envs)]  # Default to empty dicts\\n\\n        for i in range(num_envs):\\n            if not done_envs[i]:\\n                # Check if obs[i] has the correct shape\\n                if len(obs[i]) != 4:\\n                    print(f\"Unexpected observation shape for env {i}: {obs[i]}\")\\n                    continue  # Skip this step for the problematic environment\\n\\n                env_states[i].append(obs[i])\\n                env_actions[i].append(actions[i])\\n                env_rewards[i] += rewards[i]\\n                if dones[i] or truncs[i]:\\n                    # Extract reward from infos\\n                    if isinstance(infos[i], dict):\\n                        episode_info = infos[i].get(\\'episode\\', {})\\n                        traj_reward = episode_info.get(\\'r\\') if \\'r\\' in episode_info else env_rewards[i]\\n                    else:\\n                        # Handle cases where infos[i] is not a dict\\n                        traj_reward = env_rewards[i]\\n                        print(f\"Warning: infos[{i}] is not a dict. Received type: {type(infos[i])}\")\\n\\n                    trajectories.append({\\n                        \\'states\\': env_states[i],\\n                        \\'actions\\': env_actions[i],\\n                        \\'reward\\': traj_reward\\n                    })\\n                    total_collected += 1\\n                    env_states[i] = []\\n                    env_actions[i] = []\\n                    env_rewards[i] = 0.0\\n                    done_envs[i] = True\\n\\n        obs = next_obs\\n        steps += 1\\n\\n        # Reset environments that are done\\n        if any(done_envs):\\n            indices = [i for i, done in enumerate(done_envs) if done]\\n            if total_collected < num_trajectories:\\n                for i in indices:\\n                    try:\\n                        # Directly reset the environment\\n                        reset_output = envs.envs[i].reset()\\n                        if isinstance(reset_output, tuple) or isinstance(reset_output, list):\\n                            # For Gym versions where reset returns (obs, info)\\n                            obs[i] = reset_output[0]\\n                        else:\\n                            # For Gym versions where reset returns only obs\\n                            obs[i] = reset_output\\n                        done_envs[i] = False\\n                    except Exception as e:\\n                        print(f\"Error resetting environment {i}: {e}\")\\n                        # Optionally, handle the failure (e.g., retry, terminate the environment)\\n                        done_envs[i] = False  # Prevent infinite loop\\n\\n    return trajectories', filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id='0.1.0\\\\cem.py::4'), CodeChunk(id='0.1.0\\\\cem.py::5', input_type=<ClusterInputType.FILE: 'file'>, content=\"def select_elite(trajectories, percentile=ELITE_PERCENT):\\n    rewards = [traj['reward'] for traj in trajectories]\\n    if not rewards:\\n        return []\\n    reward_threshold = np.percentile(rewards, 100 - percentile)\\n    elite_trajectories = [traj for traj in trajectories if traj['reward'] >= reward_threshold]\\n    return elite_trajectories\\n\\n# Function to create training dataset from elite trajectories\\ndef create_training_data(elite_trajectories):\\n    states = []\\n    actions = []\\n    for traj in elite_trajectories:\\n        states.extend(traj['states'])\\n        actions.extend(traj['actions'])\\n    if not states or not actions:\\n        return None, None\\n    # Convert lists to NumPy arrays first for efficiency\\n    states = np.array(states, dtype=np.float32)\\n    actions = np.array(actions, dtype=np.int64)\\n    # Convert to PyTorch tensors\\n    states = torch.from_numpy(states)\\n    actions = torch.from_numpy(actions)\\n    return states, actions\", filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id='0.1.0\\\\cem.py::5'), CodeChunk(id='0.1.0\\\\cem.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='# Main execution code\\nif __name__ == \\'__main__\\':\\n    # Initialize environments\\n    env_fns = [make_env(ENV_NAME, SEED + i) for i in range(NUM_ENVIRONMENTS)]\\n    envs = AsyncVectorEnv(env_fns)\\n\\n    # Get environment details\\n    dummy_env = gym.make(ENV_NAME)\\n    state_dim = dummy_env.observation_space.shape[0]\\n    action_dim = dummy_env.action_space.n\\n    dummy_env.close()\\n\\n    # Initialize policy network and optimizer\\n    policy = PolicyNetwork(state_dim, action_dim)\\n    optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)\\n    criterion = nn.CrossEntropyLoss()\\n\\n    # Training Loop\\n    for iteration in range(1, NUM_ITERATIONS + 1):\\n        try:\\n            # Step 1: Collect Trajectories\\n            trajectories = collect_trajectories(envs, policy, TRAJECTORIES_PER_ITER, MAX_STEPS)\\n        except Exception as e:\\n            print(f\"Error during trajectory collection at iteration {iteration}: {e}\")\\n            break\\n\\n        # Step 2: Select Elite Trajectories\\n        elite_trajectories = select_elite(trajectories, ELITE_PERCENT)\\n\\n        if len(elite_trajectories) == 0:\\n            print(f\"Iteration {iteration}: No elite trajectories found. Skipping update.\")\\n            continue\\n\\n        # Step 3: Create Training Data\\n        states, actions = create_training_data(elite_trajectories)\\n\\n        if states is None or actions is None:\\n            print(f\"Iteration {iteration}: No training data available. Skipping update.\")\\n            continue\\n\\n        # Step 4: Behavioral Cloning (Policy Update)\\n        dataset_size = states.size(0)\\n        indices = np.arange(dataset_size)\\n        np.random.shuffle(indices)\\n\\n        for start in range(0, dataset_size, BATCH_SIZE):\\n            end = start + BATCH_SIZE\\n            batch_indices = indices[start:end]\\n            batch_states = states[batch_indices]\\n            batch_actions = actions[batch_indices]\\n\\n            optimizer.zero_grad()\\n            logits = policy(batch_states)\\n            loss = criterion(logits, batch_actions)\\n            loss.backward()\\n            optimizer.step()\\n\\n        # Step 5: Evaluate Current Policy\\n        avg_reward = np.mean([traj[\\'reward\\'] for traj in elite_trajectories])\\n        print(f\"Iteration {iteration}: Elite Trajectories: {len(elite_trajectories)}, Average Reward: {avg_reward:.2f}\")\\n\\n    # Close environments\\n    envs.close()\\n\\n    # Testing the Trained Policy\\n    def test_policy(policy, env_name=ENV_NAME, episodes=5, max_steps=500):\\n        env = gym.make(env_name)\\n        total_rewards = []\\n        for episode in range(episodes):\\n            obs, _ = env.reset()\\n            done = False\\n            episode_reward = 0\\n            for _ in range(max_steps):\\n                obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)\\n                with torch.no_grad():\\n                    action = policy.get_action(obs_tensor).item()\\n                obs, reward, done, info, _ = env.step(action)\\n                episode_reward += reward\\n                if done:\\n                    break\\n            total_rewards.append(episode_reward)\\n            print(f\"Test Episode {episode + 1}: Reward: {episode_reward}\")\\n        env.close()\\n        print(f\"Average Test Reward over {episodes} episodes: {np.mean(total_rewards):.2f}\")\\n\\n    # Run the test\\n    test_policy(policy)', filepath='docs\\\\ramblings\\\\0.1.0\\\\cem.py', metadata=None, node_id='0.1.0\\\\cem.py::6'), CodeChunk(id='lmp\\\\complex.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='def _client_for_model(\\n    model: str,\\n    client: Optional[Any] = None,\\n    _name: Optional[str] = None,\\n) -> Any:\\n    # XXX: Move to config to centralize api keys etc.\\n    if not client:\\n        client, was_fallback = config.get_client_for(model)\\n\\n        # XXX: Wrong.\\n        if not client and not was_fallback:\\n            raise RuntimeError(_no_api_key_warning(model, _name, \\'\\', long=True, error=True))\\n\\n    if client is None:\\n        raise ValueError(f\"No client found for model \\'{model}\\'. Ensure the model is registered using \\'register_model\\' in \\'config.py\\' or specify a client directly using the \\'client\\' argument in the decorator or function call.\")\\n    return client\\n\\n\\ncomplex.__doc__ =\\n # ... other code', filepath='src\\\\ell\\\\lmp\\\\complex.py', metadata=None, node_id='lmp\\\\complex.py::5')]),\n",
       "  0.6,\n",
       "  0.8571428571428571),\n",
       " (ClusteredTopic(name='Cluster 3: Model Configuration and Interaction', chunks=[CodeChunk(id='ell\\\\configurator.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from functools import lru_cache, wraps\\nfrom typing import Dict, Any, Optional, Tuple, Union, Type\\nimport openai\\nimport logging\\nfrom contextlib import contextmanager\\nimport threading\\nfrom pydantic import BaseModel, ConfigDict, Field\\nfrom ell.store import Store\\nfrom ell.provider import Provider\\nfrom dataclasses import dataclass, field\\n\\n_config_logger = logging.getLogger(__name__)\\n\\n@dataclass(frozen=True)\\nclass _Model:\\n    name: str\\n    default_client: Optional[Union[openai.Client, Any]] = None\\n    #XXX: Deprecation in 0.1.0\\n    #XXX: We will depreciate this when streaming is implemented. \\n    # Currently we stream by default for the verbose renderer,\\n    # but in the future we will not support streaming by default \\n    # and stream=True must be passed which will then make API providers the\\n    # single source of truth for whether or not a model supports an api parameter.\\n    # This makes our implementation extremely light, only requiring us to provide\\n    # a list of model names in registration.\\n    supports_streaming : Optional[bool] = field(default=None)', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\configurator.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n    registry: Dict[str, _Model] = Field(default_factory=dict, description=\"A dictionary mapping model names to their configurations.\")\\n    verbose: bool = Field(default=False, description=\"If True, enables verbose logging.\")\\n    wrapped_logging: bool = Field(default=True, description=\"If True, enables wrapped logging for better readability.\")\\n    override_wrapped_logging_width: Optional[int] = Field(default=None, description=\"If set, overrides the default width for wrapped logging.\")\\n    store: Optional[Store] = Field(default=None, description=\"An optional Store instance for persistence.\")\\n    autocommit: bool = Field(default=False, description=\"If True, enables automatic committing of changes to the store.\")\\n    lazy_versioning: bool = Field(default=True, description=\"If True, enables lazy versioning for improved performance.\")\\n    default_api_params: Dict[str, Any] = Field(default_factory=dict, description=\"Default parameters for language models.\")\\n    default_client: Optional[openai.Client] = Field(default=None, description=\"The default OpenAI client used when a specific model client is not found.\")\\n    autocommit_model: str = Field(default=\"gpt-4o-mini\", description=\"When set, changes the default autocommit model from GPT 4o mini.\")\\n    providers: Dict[Type, Provider] = Field(default_factory=dict, description=\"A dictionary mapping client types to provider classes.\")\\n    def __init__(self, **data):\\n        super().__init__(**data)\\n        self._lock = threading.Lock()\\n        self._local = threading.local()', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\configurator.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n\\n    def register_model(\\n        self, \\n        name: str,\\n        default_client: Optional[Union[openai.Client, Any]] = None,\\n        supports_streaming: Optional[bool] = None\\n    ) -> None:\\n        \"\"\"\\n        Register a model with its configuration.\\n        \"\"\"\\n        with self._lock:\\n            # XXX: Will be deprecated in 0.1.0\\n            self.registry[name] = _Model(\\n                name=name,\\n                default_client=default_client,\\n                supports_streaming=supports_streaming\\n            )', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\configurator.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n\\n\\n    @contextmanager\\n    def model_registry_override(self, overrides: Dict[str, _Model]):\\n        \"\"\"\\n        Temporarily override the model registry with new model configurations.\\n\\n        :param overrides: A dictionary of model names to ModelConfig instances to override.\\n        :type overrides: Dict[str, ModelConfig]\\n        \"\"\"\\n        if not hasattr(self._local, \\'stack\\'):\\n            self._local.stack = []\\n\\n        with self._lock:\\n            current_registry = self._local.stack[-1] if self._local.stack else self.registry\\n            new_registry = current_registry.copy()\\n            new_registry.update(overrides)\\n\\n        self._local.stack.append(new_registry)\\n        try:\\n            yield\\n        finally:\\n            self._local.stack.pop()', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\configurator.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:\\n        \"\"\"\\n        Get the OpenAI client for a specific model name.\\n\\n        :param model_name: The name of the model to get the client for.\\n        :type model_name: str\\n        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.\\n        :rtype: Tuple[Optional[openai.Client], bool]\\n        \"\"\"\\n        current_registry = self._local.stack[-1] if hasattr(self._local, \\'stack\\') and self._local.stack else self.registry\\n        model_config = current_registry.get(model_name)\\n        fallback = False\\n        if not model_config:\\n            warning_message = f\"Warning: A default provider for model \\'{model_name}\\' could not be found. Falling back to default OpenAI client from environment variables.\"\\n            if self.verbose:\\n                from colorama import Fore, Style\\n                _config_logger.warning(f\"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}\")\\n            else:\\n                _config_logger.debug(warning_message)\\n            client = self.default_client\\n            fallback = True\\n        else:\\n            client = model_config.default_client\\n        return client, fallback', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\configurator.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:\\n        \"\"\"\\n        Register a provider class for a specific client type.\\n\\n        :param provider_class: The provider class to register.\\n        :type provider_class: Type[Provider]\\n        \"\"\"\\n        assert isinstance(client_type, type), \"client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))\"\\n        with self._lock:\\n            self.providers[client_type] = provider', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\configurator.py::7', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:\\n        \"\"\"\\n        Get the provider instance for a specific client instance.\\n\\n        :param client: The client instance to get the provider for.\\n        :type client: Any\\n        :return: The provider instance for the specified client, or None if not found.\\n        :rtype: Optional[Provider]\\n        \"\"\"\\n\\n        client_type = type(client) if not isinstance(client, type) else client\\n        for provider_type, provider in self.providers.items():\\n            if issubclass(client_type, provider_type) or client_type == provider_type:\\n                return provider\\n        return None\\n\\n# Single* instance\\n# XXX: Make a singleton\\nconfig = Config()', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\configurator.py::8', input_type=<ClusterInputType.FILE: 'file'>, content='def init(\\n    store: Optional[Union[Store, str]] = None,\\n    verbose: bool = False,\\n    autocommit: bool = True,\\n    lazy_versioning: bool = True,\\n    default_api_params: Optional[Dict[str, Any]] = None,\\n    default_client: Optional[Any] = None,\\n    autocommit_model: str = \"gpt-4o-mini\"\\n) -> None:\\n    \"\"\"\\n    Initialize the ELL configuration with various settings.\\n\\n    :param verbose: Set verbosity of ELL operations.\\n    :type verbose: bool\\n    :param store: Set the store for ELL. Can be a Store instance or a string path for SQLiteStore.\\n    :type store: Union[Store, str], optional\\n    :param autocommit: Set autocommit for the store operations.\\n    :type autocommit: bool\\n    :param lazy_versioning: Enable or disable lazy versioning.\\n    :type lazy_versioning: bool\\n    :param default_api_params: Set default parameters for language models.\\n    :type default_api_params: Dict[str, Any], optional\\n    :param default_openai_client: Set the default OpenAI client.\\n    :type default_openai_client: openai.Client, optional\\n    :param autocommit_model: Set the model used for autocommitting.\\n    :type autocommit_model: str\\n    \"\"\"\\n    # XXX: prevent double init\\n    config.verbose = verbose\\n    config.lazy_versioning = lazy_versioning\\n\\n    if isinstance(store, str):\\n        from ell.stores.sql import SQLiteStore\\n        config.store = SQLiteStore(store)\\n    else:\\n        config.store = store\\n    config.autocommit = autocommit or config.autocommit\\n\\n    if default_api_params is not None:\\n        config.default_api_params.update(default_api_params)\\n\\n    if default_client is not None:\\n        config.default_client = default_client\\n\\n    if autocommit_model is not None:\\n        config.autocommit_model = autocommit_model', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\configurator.py::9', input_type=<ClusterInputType.FILE: 'file'>, content='# Existing helper functions\\ndef get_store() -> Union[Store, None]:\\n    return config.store\\n\\n# Will be deprecated at 0.1.0 \\n\\n# You can add more helper functions here if needed\\ndef register_provider(provider: Provider, client_type: Type[Any]) -> None:\\n    return config.register_provider(provider, client_type)\\n\\n# Deprecated now (remove at 0.1.0)\\ndef set_store(*args, **kwargs) -> None:\\n    raise DeprecationWarning(\"The set_store function is deprecated and will be removed in a future version. Use ell.init(store=...) instead.\")', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id=''), CodeChunk(id='models\\\\openai.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='\"\"\"\\nThis module handles the registration of OpenAI models within the ell framework.\\n\\nIt provides functionality to register various OpenAI models with a given OpenAI client,\\nmaking them available for use throughout the system. The module also sets up a default\\nclient behavior for unregistered models.\\n\\nKey features:\\n1. Registration of specific OpenAI models with their respective types (system, openai, openai-internal).\\n2. Utilization of a default OpenAI client for any unregistered models,\\n\\nThe default client behavior ensures that even if a specific model is not explicitly\\nregistered, the system can still attempt to use it with the default OpenAI client.\\nThis fallback mechanism provides flexibility in model usage while maintaining a\\nstructured approach to model registration.\\n\\nNote: The actual model availability may depend on your OpenAI account\\'s access and the\\ncurrent offerings from OpenAI.\\n\\nAdditionally, due to the registration of default mdoels, the OpenAI client may be used for\\nanthropic, cohere, groq, etc. models if their clients are not registered or fail\\nto register due to an error (lack of API keys, rate limits, etc.)\\n\"\"\"\\n\\nfrom ell.configurator import config\\nimport openai\\n\\nimport logging\\nimport colorama\\n\\nlogger = logging.getLogger(__name__)', filepath='src\\\\ell\\\\models\\\\openai.py', metadata=None, node_id='')]),\n",
       "  ClusteredTopic(name='Configuration and Initialization', chunks=[CodeChunk(id='ell\\\\configurator.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from functools import lru_cache, wraps\\nfrom typing import Dict, Any, Optional, Tuple, Union, Type\\nimport openai\\nimport logging\\nfrom contextlib import contextmanager\\nimport threading\\nfrom pydantic import BaseModel, ConfigDict, Field\\nfrom ell.store import Store\\nfrom ell.provider import Provider\\nfrom dataclasses import dataclass, field\\n\\n_config_logger = logging.getLogger(__name__)\\n\\n@dataclass(frozen=True)\\nclass _Model:\\n    name: str\\n    default_client: Optional[Union[openai.Client, Any]] = None\\n    #XXX: Deprecation in 0.1.0\\n    #XXX: We will depreciate this when streaming is implemented. \\n    # Currently we stream by default for the verbose renderer,\\n    # but in the future we will not support streaming by default \\n    # and stream=True must be passed which will then make API providers the\\n    # single source of truth for whether or not a model supports an api parameter.\\n    # This makes our implementation extremely light, only requiring us to provide\\n    # a list of model names in registration.\\n    supports_streaming : Optional[bool] = field(default=None)', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id='ell\\\\configurator.py::1'), CodeChunk(id='ell\\\\configurator.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n    registry: Dict[str, _Model] = Field(default_factory=dict, description=\"A dictionary mapping model names to their configurations.\")\\n    verbose: bool = Field(default=False, description=\"If True, enables verbose logging.\")\\n    wrapped_logging: bool = Field(default=True, description=\"If True, enables wrapped logging for better readability.\")\\n    override_wrapped_logging_width: Optional[int] = Field(default=None, description=\"If set, overrides the default width for wrapped logging.\")\\n    store: Optional[Store] = Field(default=None, description=\"An optional Store instance for persistence.\")\\n    autocommit: bool = Field(default=False, description=\"If True, enables automatic committing of changes to the store.\")\\n    lazy_versioning: bool = Field(default=True, description=\"If True, enables lazy versioning for improved performance.\")\\n    default_api_params: Dict[str, Any] = Field(default_factory=dict, description=\"Default parameters for language models.\")\\n    default_client: Optional[openai.Client] = Field(default=None, description=\"The default OpenAI client used when a specific model client is not found.\")\\n    autocommit_model: str = Field(default=\"gpt-4o-mini\", description=\"When set, changes the default autocommit model from GPT 4o mini.\")\\n    providers: Dict[Type, Provider] = Field(default_factory=dict, description=\"A dictionary mapping client types to provider classes.\")\\n    def __init__(self, **data):\\n        super().__init__(**data)\\n        self._lock = threading.Lock()\\n        self._local = threading.local()', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id='ell\\\\configurator.py::2'), CodeChunk(id='ell\\\\configurator.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n\\n    def register_model(\\n        self, \\n        name: str,\\n        default_client: Optional[Union[openai.Client, Any]] = None,\\n        supports_streaming: Optional[bool] = None\\n    ) -> None:\\n        \"\"\"\\n        Register a model with its configuration.\\n        \"\"\"\\n        with self._lock:\\n            # XXX: Will be deprecated in 0.1.0\\n            self.registry[name] = _Model(\\n                name=name,\\n                default_client=default_client,\\n                supports_streaming=supports_streaming\\n            )', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id='ell\\\\configurator.py::3'), CodeChunk(id='ell\\\\configurator.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n\\n\\n    @contextmanager\\n    def model_registry_override(self, overrides: Dict[str, _Model]):\\n        \"\"\"\\n        Temporarily override the model registry with new model configurations.\\n\\n        :param overrides: A dictionary of model names to ModelConfig instances to override.\\n        :type overrides: Dict[str, ModelConfig]\\n        \"\"\"\\n        if not hasattr(self._local, \\'stack\\'):\\n            self._local.stack = []\\n\\n        with self._lock:\\n            current_registry = self._local.stack[-1] if self._local.stack else self.registry\\n            new_registry = current_registry.copy()\\n            new_registry.update(overrides)\\n\\n        self._local.stack.append(new_registry)\\n        try:\\n            yield\\n        finally:\\n            self._local.stack.pop()', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id='ell\\\\configurator.py::4'), CodeChunk(id='ell\\\\configurator.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:\\n        \"\"\"\\n        Get the OpenAI client for a specific model name.\\n\\n        :param model_name: The name of the model to get the client for.\\n        :type model_name: str\\n        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.\\n        :rtype: Tuple[Optional[openai.Client], bool]\\n        \"\"\"\\n        current_registry = self._local.stack[-1] if hasattr(self._local, \\'stack\\') and self._local.stack else self.registry\\n        model_config = current_registry.get(model_name)\\n        fallback = False\\n        if not model_config:\\n            warning_message = f\"Warning: A default provider for model \\'{model_name}\\' could not be found. Falling back to default OpenAI client from environment variables.\"\\n            if self.verbose:\\n                from colorama import Fore, Style\\n                _config_logger.warning(f\"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}\")\\n            else:\\n                _config_logger.debug(warning_message)\\n            client = self.default_client\\n            fallback = True\\n        else:\\n            client = model_config.default_client\\n        return client, fallback', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id='ell\\\\configurator.py::5'), CodeChunk(id='ell\\\\configurator.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:\\n        \"\"\"\\n        Register a provider class for a specific client type.\\n\\n        :param provider_class: The provider class to register.\\n        :type provider_class: Type[Provider]\\n        \"\"\"\\n        assert isinstance(client_type, type), \"client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))\"\\n        with self._lock:\\n            self.providers[client_type] = provider', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id='ell\\\\configurator.py::6'), CodeChunk(id='ell\\\\configurator.py::7', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:\\n        \"\"\"\\n        Get the provider instance for a specific client instance.\\n\\n        :param client: The client instance to get the provider for.\\n        :type client: Any\\n        :return: The provider instance for the specified client, or None if not found.\\n        :rtype: Optional[Provider]\\n        \"\"\"\\n\\n        client_type = type(client) if not isinstance(client, type) else client\\n        for provider_type, provider in self.providers.items():\\n            if issubclass(client_type, provider_type) or client_type == provider_type:\\n                return provider\\n        return None\\n\\n# Single* instance\\n# XXX: Make a singleton\\nconfig = Config()', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id='ell\\\\configurator.py::7'), CodeChunk(id='models\\\\__init__.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='\"\"\"\\nAttempts to registeres model names with their respective API client bindings. This allows for the creation of a unified interface for interacting with different LLM providers.\\n\\nFor example, to register an OpenAI model:\\n@ell.simple(model=\\'gpt-4o-mini\\') -> @ell.simple(model=\\'gpt-4o-mini\\', client=openai.OpenAI())\\n\\n\"\"\"\\n\\nimport ell.models.openai\\nimport ell.models.anthropic\\nimport ell.models.ollama\\nimport ell.models.groq\\nimport ell.models.bedrock', filepath='src\\\\ell\\\\models\\\\__init__.py', metadata=None, node_id='models\\\\__init__.py::1'), CodeChunk(id='models\\\\openai.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='\"\"\"\\nThis module handles the registration of OpenAI models within the ell framework.\\n\\nIt provides functionality to register various OpenAI models with a given OpenAI client,\\nmaking them available for use throughout the system. The module also sets up a default\\nclient behavior for unregistered models.\\n\\nKey features:\\n1. Registration of specific OpenAI models with their respective types (system, openai, openai-internal).\\n2. Utilization of a default OpenAI client for any unregistered models,\\n\\nThe default client behavior ensures that even if a specific model is not explicitly\\nregistered, the system can still attempt to use it with the default OpenAI client.\\nThis fallback mechanism provides flexibility in model usage while maintaining a\\nstructured approach to model registration.\\n\\nNote: The actual model availability may depend on your OpenAI account\\'s access and the\\ncurrent offerings from OpenAI.\\n\\nAdditionally, due to the registration of default mdoels, the OpenAI client may be used for\\nanthropic, cohere, groq, etc. models if their clients are not registered or fail\\nto register due to an error (lack of API keys, rate limits, etc.)\\n\"\"\"\\n\\nfrom ell.configurator import config\\nimport openai\\n\\nimport logging\\nimport colorama\\n\\nlogger = logging.getLogger(__name__)', filepath='src\\\\ell\\\\models\\\\openai.py', metadata=None, node_id='models\\\\openai.py::1'), CodeChunk(id='models\\\\openai.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='def register(client: openai.Client):\\n    \"\"\"\\n    Register OpenAI models with the provided client.\\n\\n    This function takes an OpenAI client and registers various OpenAI models\\n    with the global configuration. It allows the system to use these models\\n    for different AI tasks.\\n\\n    Args:\\n        client (openai.Client): An instance of the OpenAI client to be used\\n                                for model registration.\\n\\n    Note:\\n        The function doesn\\'t return anything but updates the global\\n        configuration with the registered models.\\n    \"\"\"\\n    #XXX: Deprecation in 0.1.0\\n    standard_models = [\\n        \\'gpt-4-1106-preview\\',\\n        \\'gpt-4-32k-0314\\',\\n        \\'text-embedding-3-large\\',\\n        \\'gpt-4-0125-preview\\',\\n        \\'babbage-002\\',\\n        \\'gpt-4-turbo-preview\\',\\n        \\'gpt-4o\\',\\n        \\'gpt-4o-2024-05-13\\',\\n        \\'gpt-4o-mini-2024-07-18\\',\\n        \\'gpt-4o-mini\\',\\n        \\'gpt-4o-2024-08-06\\',\\n        \\'gpt-3.5-turbo-0301\\',\\n        \\'gpt-3.5-turbo-0613\\',\\n        \\'tts-1\\',\\n        \\'gpt-3.5-turbo\\',\\n        \\'gpt-3.5-turbo-16k\\',\\n        \\'davinci-002\\',\\n        \\'gpt-3.5-turbo-16k-0613\\',\\n        \\'gpt-4-turbo-2024-04-09\\',\\n        \\'gpt-3.5-turbo-0125\\',\\n        \\'gpt-4-turbo\\',\\n        \\'gpt-3.5-turbo-1106\\',\\n        \\'gpt-3.5-turbo-instruct-0914\\',\\n        \\'gpt-3.5-turbo-instruct\\',\\n        \\'gpt-4-0613\\',\\n        \\'gpt-4\\',\\n        \\'gpt-4-0314\\',\\n        \\'gpt-4o-audio-preview\\',\\n        \\'gpt-4o-realtime\\',\\n    ]\\n    for model_id in standard_models:\\n        config.register_model(model_id, client)\\n\\n    #XXX: Deprecation in 0.1.0\\n    config.register_model(\\'o1-preview\\', client, supports_streaming=False)\\n    config.register_model(\\'o1-mini\\', client, supports_streaming=False)\\n\\ndefault_client = None\\ntry:\\n    default_client = openai.Client()\\nexcept openai.OpenAIError as e:\\n    pass\\n\\nregister(default_client)\\nconfig.default_client = default_client', filepath='src\\\\ell\\\\models\\\\openai.py', metadata=None, node_id='models\\\\openai.py::2')]),\n",
       "  0.8,\n",
       "  0.8),\n",
       " (ClusteredTopic(name='Cluster 5: Providers and Service Integrations', chunks=[CodeChunk(id='providers\\\\anthropic.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast\\nfrom ell.provider import  EllCallParams, Metadata, Provider\\nfrom ell.types import Message, ContentBlock, ToolCall, ImageContent\\n\\nfrom ell.types._lstr import _lstr\\nfrom ell.types.message import LMP\\nfrom ell.configurator import register_provider\\nfrom ell.util.serialization import serialize_image\\nimport base64\\nfrom io import BytesIO\\nimport json\\nimport requests\\nfrom PIL import Image as PILImage\\n\\ntry:\\n    import anthropic\\n    from anthropic import Anthropic\\n    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent\\n    from anthropic.types.message_create_params import MessageCreateParamsStreaming\\n    from anthropic._streaming import Stream\\n\\n    class AnthropicProvider(Provider):\\n        dangerous_disable_validation = True\\n\\n        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\\n            return client.messages.create\\n\\n        def translate_to_provider(self, ell_call : EllCallParams):\\n            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())\\n            # XXX: Helper, but should be depreicated due to ssot\\n            assert final_call_params.get(\"max_tokens\") is not None, f\"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{\\'max_tokens\\': your_max_tokens}})).\"\\n\\n            dirty_msgs = [\\n                MessageParam(\\n                    role=cast(Literal[\"user\", \"assistant\"], message.role), \\n                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]\\n            role_correct_msgs   : List[MessageParam] = []\\n            for msg in dirty_msgs:\\n                if (not len(role_correct_msgs) or role_correct_msgs[-1][\\'role\\'] != msg[\\'role\\']):\\n                    role_correct_msgs.append(msg)\\n                else: cast(List, role_correct_msgs[-1][\\'content\\']).extend(msg[\\'content\\'])\\n\\n            system_message = None\\n            if role_correct_msgs and role_correct_msgs[0][\"role\"] == \"system\":\\n                system_message = role_correct_msgs.pop(0)\\n\\n            if system_message:\\n                final_call_params[\"system\"] = system_message[\"content\"][0][\"text\"]\\n\\n\\n            final_call_params[\\'stream\\'] = True\\n            final_call_params[\"model\"] = ell_call.model\\n            final_call_params[\"messages\"] = role_correct_msgs\\n\\n            if ell_call.tools:\\n                final_call_params[\"tools\"] = [\\n                    #XXX: Cleaner with LMP\\'s as a class.\\n                    dict(\\n                        name=tool.__name__,\\n                        description=tool.__doc__,\\n                        input_schema=tool.__ell_params_model__.model_json_schema(),\\n                    )\\n                    for tool in ell_call.tools\\n                ]\\n\\n            # print(final_call_params)\\n            return final_call_params\\n\\n        def translate_from_provider(\\n            self,\\n            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],\\n            ell_call: EllCallParams,\\n            provider_call_params: Dict[str, Any],\\n            origin_id: Optional[str] = None,\\n            logger: Optional[Callable[..., None]] = None,\\n        ) -> Tuple[List[Message], Metadata]:\\n\\n            usage = {}\\n            tracked_results = []\\n            metadata = {}\\n\\n            #XXX: Support n > 0\\n\\n            if provider_call_params.get(\"stream\", False):\\n                content = []\\n                current_blocks: Dict[int, Dict[str, Any]] = {}\\n                message_metadata = {}\\n\\n                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:\\n                    for chunk in stream:\\n                        if chunk.type == \"message_start\":\\n                            message_metadata = chunk.message.model_dump()\\n                            message_metadata.pop(\"content\", None)  # Remove content as we\\'ll build it separately\\n\\n                        elif chunk.type == \"content_block_start\":\\n                            block = chunk.content_block.model_dump()\\n                            current_blocks[chunk.index] = block\\n                            if block[\"type\"] == \"tool_use\":\\n                                if logger: logger(f\" <tool_use: {block[\\'name\\']}(\")\\n                                block[\"input\"] = \"\" # force it to be a string, XXX: can implement partially parsed json later.\\n                        elif chunk.type == \"content_block_delta\":\\n                            if chunk.index in current_blocks:\\n                                block = current_blocks[chunk.index]\\n                                if (delta := chunk.delta).type == \"text_delta\":\\n                                    block[\"text\"] += delta.text\\n                                    if logger: logger(delta.text)\\n                                if delta.type == \"input_json_delta\":\\n                                    block[\"input\"] += delta.partial_json\\n                                    if logger: logger(delta.partial_json)\\n\\n                        elif chunk.type == \"content_block_stop\":\\n                            if chunk.index in current_blocks:\\n                                block = current_blocks.pop(chunk.index)\\n                                if block[\"type\"] == \"text\":\\n                                    content.append(ContentBlock(text=_lstr(block[\"text\"],origin_trace=origin_id)))\\n                                elif block[\"type\"] == \"tool_use\":\\n                                    try:\\n                                        matching_tool = ell_call.get_tool_by_name(block[\"name\"])\\n                                        if matching_tool:\\n                                            content.append(\\n                                                ContentBlock(\\n                                                    tool_call=ToolCall(\\n                                                        tool=matching_tool,\\n                                                        tool_call_id=_lstr(\\n                                                            block[\\'id\\'],origin_trace=origin_id\\n                                                        ),\\n                                                        params=json.loads(block[\\'input\\']) if block[\\'input\\'] else {},\\n                                                    )\\n                                                )\\n                                            )\\n                                    except json.JSONDecodeError:\\n                                        if logger: logger(f\" - FAILED TO PARSE JSON\")\\n                                        pass\\n                                    if logger: logger(f\")>\")\\n\\n                        elif chunk.type == \"message_delta\":\\n                            message_metadata.update(chunk.delta.model_dump())\\n                            if chunk.usage:\\n                                usage.update(chunk.usage.model_dump())\\n\\n                        elif chunk.type == \"message_stop\":\\n                            tracked_results.append(Message(role=\"assistant\", content=content))\\n\\n                        # print(chunk)\\n                metadata = message_metadata\\n\\n            # process metadata for ell\\n            # XXX: Unify an ell metadata format for ell studio.\\n            usage[\"prompt_tokens\"] = usage.get(\"input_tokens\", 0)\\n            usage[\"completion_tokens\"] = usage.get(\"output_tokens\", 0)\\n            usage[\"total_tokens\"] = usage[\\'prompt_tokens\\'] + usage[\\'completion_tokens\\']\\n\\n            metadata[\"usage\"] = usage\\n            return tracked_results, metadata\\n\\n    # XXX: Make a singleton.\\n    anthropic_provider = AnthropicProvider()\\n    register_provider(anthropic_provider, anthropic.Anthropic)\\n    register_provider(anthropic_provider, anthropic.AnthropicBedrock)\\n    register_provider(anthropic_provider, anthropic.AnthropicVertex)\\n\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\anthropic.py', metadata=None, node_id=''), CodeChunk(id='providers\\\\anthropic.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='def serialize_image_for_anthropic(img : ImageContent):\\n    if img.url:\\n        # Download the image from the URL\\n        response = requests.get(img.url)\\n        response.raise_for_status()  # Raise an exception for bad responses\\n        pil_image = PILImage.open(BytesIO(response.content))\\n    elif img.image:\\n        pil_image = img.image\\n    else:\\n        raise ValueError(\"Image object has neither url nor image data.\")\\n    buffer = BytesIO()\\n    pil_image.save(buffer, format=\"PNG\")\\n    base64_image =  base64.b64encode(buffer.getvalue()).decode()\\n    return dict(\\n        type=\"image\",\\n        source=dict(\\n            type=\"base64\",\\n            media_type=\"image/png\",\\n            data=base64_image\\n        )\\n    )', filepath='src\\\\ell\\\\providers\\\\anthropic.py', metadata=None, node_id=''), CodeChunk(id='providers\\\\anthropic.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='def _content_block_to_anthropic_format(content_block: ContentBlock):\\n        if (image := content_block.image): return serialize_image_for_anthropic(image)\\n        elif ((text := content_block.text) is not None): return dict(type=\"text\", text=text)\\n        elif (parsed := content_block.parsed):\\n            return dict(type=\"text\", text=json.dumps(parsed.model_dump(), ensure_ascii=False))\\n        elif (tool_call := content_block.tool_call):\\n            return dict(\\n                type=\"tool_use\",\\n                id=tool_call.tool_call_id,\\n                name=tool_call.tool.__name__,\\n                input=tool_call.params.model_dump()\\n            )\\n        elif (tool_result := content_block.tool_result):\\n            return dict(\\n                type=\"tool_result\",\\n                tool_use_id=tool_result.tool_call_id,\\n                content=[_content_block_to_anthropic_format(c) for c in tool_result.result]\\n            )\\n        else:\\n            raise ValueError(\"Content block is not supported by anthropic\")', filepath='src\\\\ell\\\\providers\\\\anthropic.py', metadata=None, node_id=''), CodeChunk(id='providers\\\\openai.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from abc import ABC, abstractmethod\\nfrom collections import defaultdict\\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\\n\\nfrom pydantic import BaseModel\\nfrom ell.provider import  EllCallParams, Metadata, Provider\\nfrom ell.types import Message, ContentBlock, ToolCall\\nfrom ell.types._lstr import _lstr\\nimport json\\nfrom ell.configurator import _Model, config, register_provider\\nfrom ell.types.message import LMP\\nfrom ell.util.serialization import serialize_image\\n\\ntry:\\n    # XXX: Could genericize.\\n    import openai\\n    from openai._streaming import Stream\\n    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam\\n\\n    class OpenAIProvider(Provider):\\n        dangerous_disable_validation = True\\n\\n        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\\n            if api_call_params and (isinstance(fmt := api_call_params.get(\"response_format\"), type)) and issubclass(fmt, BaseModel):\\n                return client.beta.chat.completions.parse\\n            else:\\n                return client.chat.completions.create\\n\\n        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:\\n            final_call_params = ell_call.api_params.copy()\\n            final_call_params[\"model\"] = ell_call.model\\n            # Stream by default for verbose logging.\\n            final_call_params[\"stream\"] = True\\n            final_call_params[\"stream_options\"] = {\"include_usage\": True}\\n\\n            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.\\n            if ell_call.tools or final_call_params.get(\"response_format\") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:\\n                final_call_params.pop(\"stream\", None)\\n                final_call_params.pop(\"stream_options\", None)\\n            if ell_call.tools:\\n                final_call_params.update(\\n                    tool_choice=final_call_params.get(\"tool_choice\", \"auto\"),\\n                    tools=[  \\n                        dict(\\n                            type=\"function\",\\n                            function=dict(\\n                                name=tool.__name__,\\n                                description=tool.__doc__,\\n                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore\\n                            )\\n                        ) for tool in ell_call.tools\\n                    ]\\n                )\\n            # messages\\n            openai_messages : List[ChatCompletionMessageParam] = []\\n            for message in ell_call.messages:\\n                if (tool_calls := message.tool_calls):\\n                    assert message.role == \"assistant\", \"Tool calls must be from the assistant.\"\\n                    assert all(t.tool_call_id for t in tool_calls), \"Tool calls must have tool call ids.\"\\n                    openai_messages.append(dict(\\n                        tool_calls=[\\n                            dict(\\n                                id=cast(str, tool_call.tool_call_id),\\n                                type=\"function\",\\n                                function=dict(\\n                                    name=tool_call.tool.__name__,\\n                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)\\n                                )\\n                            ) for tool_call in tool_calls ],\\n                        role=\"assistant\",\\n                        content=None,\\n                    ))\\n                elif (tool_results := message.tool_results):\\n                    for tool_result in tool_results:\\n                        assert all(cb.type == \"text\" for cb in tool_result.result), \"Tool result does not match expected content blocks.\"\\n                        openai_messages.append(dict(\\n                            role=\"tool\",\\n                            tool_call_id=tool_result.tool_call_id,\\n                            content=tool_result.text_only, \\n                        ))\\n                else:\\n                    openai_messages.append(cast(ChatCompletionMessageParam, dict(\\n                        role=message.role,\\n                        content=[_content_block_to_openai_format(c) for c in message.content] \\n                             if message.role != \"system\" \\n                             else message.text_only\\n                    )))\\n\\n            final_call_params[\"messages\"] = openai_messages\\n\\n            return final_call_params\\n\\n        def translate_from_provider(\\n            self,\\n            provider_response: Union[\\n                ChatCompletion, \\n                ParsedChatCompletion,\\n                Stream[ChatCompletionChunk], Any],\\n            ell_call: EllCallParams,\\n            provider_call_params: Dict[str, Any],\\n            origin_id: Optional[str] = None,\\n            logger: Optional[Callable[..., None]] = None,\\n        ) -> Tuple[List[Message], Metadata]:\\n\\n            metadata : Metadata = {}\\n            messages : List[Message] = []\\n            did_stream = provider_call_params.get(\"stream\", False)\\n\\n\\n            if did_stream:\\n                stream = cast(Stream[ChatCompletionChunk], provider_response)\\n                message_streams = defaultdict(list)\\n                role : Optional[str] = None\\n                for chunk in stream:\\n                    metadata.update(chunk.model_dump(exclude={\"choices\"}))\\n\\n                    for chat_compl_chunk in chunk.choices:\\n                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)\\n                        delta = chat_compl_chunk.delta\\n                        role = role or delta.role\\n                        if  chat_compl_chunk.index == 0 and logger:\\n                            logger(delta.content, is_refusal=hasattr(delta, \"refusal\") and delta.refusal)\\n                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):\\n                    text = \"\".join((choice.delta.content or \"\") for choice in message_stream)\\n                    messages.append(\\n                        Message(role=role, \\n                                content=_lstr(content=text,origin_trace=origin_id)))\\n                    #XXX: Support streaming other types.\\n            else:\\n                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)\\n                metadata = chat_completion.model_dump(exclude={\"choices\"})\\n                for oai_choice in chat_completion.choices:\\n                    role = oai_choice.message.role\\n                    content_blocks = []\\n                    if (hasattr(message := oai_choice.message, \"refusal\") and (refusal := message.refusal)):\\n                        raise ValueError(refusal)\\n                    if hasattr(message, \"parsed\"):\\n                        if (parsed := message.parsed):\\n                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing\\n                            if logger: logger(parsed.model_dump_json())\\n                    else:\\n                        if (content := message.content):\\n                            content_blocks.append(\\n                                ContentBlock(\\n                                    text=_lstr(content=content,origin_trace=origin_id)))\\n                            if logger: logger(content)\\n                        if (tool_calls := message.tool_calls):\\n                            for tool_call in tool_calls:\\n                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)\\n                                assert matching_tool, \"Model called tool not found in provided toolset.\"\\n                                content_blocks.append(\\n                                    ContentBlock(\\n                                        tool_call=ToolCall(\\n                                            tool=matching_tool,\\n                                            tool_call_id=_lstr(\\n                                                tool_call.id, origin_trace= origin_id),\\n                                            params=json.loads(tool_call.function.arguments),\\n                                        )\\n                                    )\\n                                )\\n                                if logger: logger(repr(tool_call))\\n                    messages.append(Message(role=role, content=content_blocks))\\n            return messages, metadata\\n\\n\\n    # xx: singleton needed\\n    openai_provider = OpenAIProvider()\\n    register_provider(openai_provider, openai.Client)\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\openai.py', metadata=None, node_id=''), CodeChunk(id='providers\\\\openai.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='def _content_block_to_openai_format(content_block: ContentBlock) -> Dict[str, Any]:\\n    if (image := content_block.image):\\n        image_url = dict(url=serialize_image(image.image) if image.image else image.url)\\n        # XXX: Solve per content params better\\n        if image.detail: image_url[\"detail\"] = image.detail\\n        return {\\n            \"type\": \"image_url\",\\n            \"image_url\": image_url\\n        }\\n    elif ((text := content_block.text) is not None): return dict(type=\"text\", text=text)\\n    elif (parsed := content_block.parsed): return dict(type=\"text\", text=parsed.model_dump_json())\\n    else:\\n        raise ValueError(f\"Unsupported content block type for openai: {content_block}\")', filepath='src\\\\ell\\\\providers\\\\openai.py', metadata=None, node_id=''), CodeChunk(id='models\\\\openai.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='def register(client: openai.Client):\\n    \"\"\"\\n    Register OpenAI models with the provided client.\\n\\n    This function takes an OpenAI client and registers various OpenAI models\\n    with the global configuration. It allows the system to use these models\\n    for different AI tasks.\\n\\n    Args:\\n        client (openai.Client): An instance of the OpenAI client to be used\\n                                for model registration.\\n\\n    Note:\\n        The function doesn\\'t return anything but updates the global\\n        configuration with the registered models.\\n    \"\"\"\\n    #XXX: Deprecation in 0.1.0\\n    standard_models = [\\n        \\'gpt-4-1106-preview\\',\\n        \\'gpt-4-32k-0314\\',\\n        \\'text-embedding-3-large\\',\\n        \\'gpt-4-0125-preview\\',\\n        \\'babbage-002\\',\\n        \\'gpt-4-turbo-preview\\',\\n        \\'gpt-4o\\',\\n        \\'gpt-4o-2024-05-13\\',\\n        \\'gpt-4o-mini-2024-07-18\\',\\n        \\'gpt-4o-mini\\',\\n        \\'gpt-4o-2024-08-06\\',\\n        \\'gpt-3.5-turbo-0301\\',\\n        \\'gpt-3.5-turbo-0613\\',\\n        \\'tts-1\\',\\n        \\'gpt-3.5-turbo\\',\\n        \\'gpt-3.5-turbo-16k\\',\\n        \\'davinci-002\\',\\n        \\'gpt-3.5-turbo-16k-0613\\',\\n        \\'gpt-4-turbo-2024-04-09\\',\\n        \\'gpt-3.5-turbo-0125\\',\\n        \\'gpt-4-turbo\\',\\n        \\'gpt-3.5-turbo-1106\\',\\n        \\'gpt-3.5-turbo-instruct-0914\\',\\n        \\'gpt-3.5-turbo-instruct\\',\\n        \\'gpt-4-0613\\',\\n        \\'gpt-4\\',\\n        \\'gpt-4-0314\\',\\n        \\'gpt-4o-audio-preview\\',\\n        \\'gpt-4o-realtime\\',\\n    ]\\n    for model_id in standard_models:\\n        config.register_model(model_id, client)\\n\\n    #XXX: Deprecation in 0.1.0\\n    config.register_model(\\'o1-preview\\', client, supports_streaming=False)\\n    config.register_model(\\'o1-mini\\', client, supports_streaming=False)\\n\\ndefault_client = None\\ntry:\\n    default_client = openai.Client()\\nexcept openai.OpenAIError as e:\\n    pass\\n\\nregister(default_client)\\nconfig.default_client = default_client', filepath='src\\\\ell\\\\models\\\\openai.py', metadata=None, node_id=''), CodeChunk(id='providers\\\\bedrock.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from abc import ABC, abstractmethod\\nfrom collections import defaultdict\\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\\nfrom ell.provider import  EllCallParams, Metadata, Provider\\nfrom ell.types import Message, ContentBlock, ToolCall, ImageContent\\nfrom ell.types._lstr import _lstr\\nimport json\\nfrom ell.configurator import config, register_provider\\nfrom ell.types.message import LMP\\nfrom ell.util.serialization import serialize_image\\nfrom io import BytesIO\\nimport requests\\nfrom PIL import Image as PILImage', filepath='src\\\\ell\\\\providers\\\\bedrock.py', metadata=None, node_id=''), CodeChunk(id='providers\\\\bedrock.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='try:\\n    from botocore.client import BaseClient\\n    from botocore.eventstream import (EventStream)\\n    class BedrockProvider(Provider):\\n        dangerous_disable_validation = True\\n\\n        def provider_call_function(self, client : Any, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\\n            if api_call_params and api_call_params.get(\"stream\", False):\\n                api_call_params.pop(\\'stream\\')\\n                return client.converse_stream\\n            else:\\n                return client.converse\\n\\n        def translate_to_provider(self, ell_call : EllCallParams):\\n            final_call_params = {}\\n\\n            if ell_call.api_params.get(\\'api_params\\',{}).get(\\'stream\\', False):\\n                final_call_params[\\'stream\\'] = ell_call.api_params.get(\\'api_params\\',{}).get(\\'stream\\', False)\\n\\n            bedrock_converse_messages = [message_to_bedrock_message_format(message) for message in ell_call.messages]\\n\\n            system_message = None\\n            if bedrock_converse_messages and bedrock_converse_messages[0][\"role\"] == \"system\":\\n                system_message = bedrock_converse_messages.pop(0)\\n\\n            if system_message:\\n                final_call_params[\"system\"] = [{\\'text\\':system_message[\"content\"][0][\"text\"]}]\\n\\n            final_call_params[\"modelId\"] = ell_call.model\\n            final_call_params[\"messages\"] = bedrock_converse_messages\\n\\n            if ell_call.tools:\\n                tools = [\\n                    #XXX: Cleaner with LMP\\'s as a class.\\n                    dict(\\n                        toolSpec = dict(\\n                            name=tool.__name__,\\n                            description=tool.__doc__,\\n                            inputSchema=dict(\\n                                json=tool.__ell_params_model__.model_json_schema(),\\n                            )\\n                        )\\n                    )\\n                    for tool in ell_call.tools\\n                ]\\n                final_call_params[\"toolConfig\"] = {\\'tools\\':tools}\\n\\n            return final_call_params\\n\\n        def translate_from_provider(\\n                self,\\n                provider_response: Union[EventStream, Any],\\n                ell_call: EllCallParams,\\n                provider_call_params: Dict[str, Any],\\n                origin_id: Optional[str] = None,\\n                logger: Optional[Callable[..., None]] = None,\\n            ) -> Tuple[List[Message], Metadata]:\\n\\n            usage = {}\\n            metadata : Metadata = {}\\n\\n            metadata : Metadata = {}\\n            tracked_results : List[Message] = []\\n            did_stream = ell_call.api_params.get(\"api_params\", {}).get(\\'stream\\')\\n\\n            if did_stream:\\n                content = []\\n                current_block: Optional[Dict[str, Any]] = {}\\n                message_metadata = {}\\n                for chunk in provider_response.get(\\'stream\\'):\\n\\n                    if \"messageStart\" in chunk:\\n                        current_block[\\'content\\'] = \\'\\'\\n                        pass\\n                    elif \"contentBlockStart\" in chunk:\\n                        pass\\n                    elif \"contentBlockDelta\" in chunk:\\n                        delta = chunk.get(\"contentBlockDelta\", {}).get(\"delta\", {})\\n                        if \"text\" in delta:\\n                            current_block[\\'type\\'] = \\'text\\'\\n                            current_block[\\'content\\'] += delta.get(\"text\")\\n                            if logger:\\n                                logger(delta.get(\"text\"))\\n                        else:\\n                            pass\\n                    elif \"contentBlockStop\" in chunk:\\n                        if current_block is not None:\\n                            if current_block[\"type\"] == \"text\":\\n                                content.append(ContentBlock(text=_lstr(content=content, origin_trace=origin_id)))\\n\\n                    elif \"messageStop\" in chunk:\\n                        tracked_results.append(Message(role=\"assistant\", content=content))\\n\\n                    elif \"metadata\" in chunk:\\n                        if \"usage\" in chunk[\"metadata\"]:\\n                            usage[\"prompt_tokens\"] = chunk[\"metadata\"].get(\\'usage\\').get(\"inputTokens\", 0)\\n                            usage[\"completion_tokens\"] = chunk[\"metadata\"].get(\\'usage\\').get(\"outputTokens\", 0)\\n                            usage[\"total_tokens\"] = usage[\\'prompt_tokens\\'] + usage[\\'completion_tokens\\']\\n                            message_metadata[\"usage\"] = usage\\n                    else:\\n                        pass\\n\\n\\n                metadata = message_metadata\\n            else:\\n                # Non-streaming response processing (unchanged)\\n                cbs = []\\n                for content_block in provider_response.get(\\'output\\', {}).get(\\'message\\', {}).get(\\'content\\', []):\\n                    if \\'text\\' in content_block:\\n                        cbs.append(ContentBlock(text=_lstr(content_block.get(\\'text\\'), origin_trace=origin_id)))\\n                    elif \\'toolUse\\' in content_block:\\n                        assert ell_call.tools is not None, \"Tools were not provided to the model when calling it and yet bedrock returned a tool use.\"\\n                        try:\\n                            toolUse = content_block[\\'toolUse\\']\\n                            matching_tool = ell_call.get_tool_by_name(toolUse[\"name\"])\\n                            if matching_tool:\\n                                cbs.append(\\n                                    ContentBlock(\\n                                        tool_call=ToolCall(\\n                                            tool=matching_tool,\\n                                            tool_call_id=_lstr(\\n                                                toolUse[\\'toolUseId\\'],origin_trace=origin_id\\n                                            ),\\n                                            params=toolUse[\\'input\\'],\\n                                        )\\n                                    )\\n                                )\\n                        except json.JSONDecodeError:\\n                            if logger: logger(f\" - FAILED TO PARSE JSON\")\\n                            pass\\n                tracked_results.append(Message(role=\"assistant\", content=cbs))\\n                if logger:\\n                    logger(tracked_results[0].text)\\n\\n\\n                # usage = call_result.response.usage.dict() if call_result.response.get(\\'usage\\') else {}\\n                # metadata = call_result.response.model_dump()\\n                # del metadata[\"content\"]\\n\\n            # process metadata for ell\\n            # XXX: Unify an ell metadata format for ell studio.\\n            usage[\"prompt_tokens\"] = usage.get(\"inputTokens\", 0)\\n            usage[\"completion_tokens\"] = usage.get(\"outputTokens\", 0)\\n            usage[\"total_tokens\"] = usage[\\'prompt_tokens\\'] + usage[\\'completion_tokens\\']\\n\\n            metadata[\"usage\"] = usage\\n            return tracked_results, metadata\\n\\n\\n    # XXX: Make a singleton.\\n    register_provider(BedrockProvider(), BaseClient)\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\bedrock.py', metadata=None, node_id=''), CodeChunk(id='providers\\\\groq.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='\"\"\"\\nGroq provider.\\n\"\"\"\\n\\nfrom ell.providers.openai import OpenAIProvider\\nfrom ell.configurator import register_provider\\n\\n\\ntry:\\n    import groq\\n    class GroqProvider(OpenAIProvider):\\n        dangerous_disable_validation = True\\n        def translate_to_provider(self, *args, **kwargs):\\n            params = super().translate_to_provider(*args, **kwargs)\\n            params.pop(\\'stream_options\\', None)\\n            return params\\n\\n        def translate_from_provider(self, *args, **kwargs):\\n            res, meta = super().translate_from_provider(*args, **kwargs)\\n            if not meta[\\'usage\\']:\\n                meta[\\'usage\\'] = meta[\\'x_groq\\'][\\'usage\\']\\n            return res, meta\\n    register_provider(GroqProvider(), groq.Client)\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\groq.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\configurator.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='class Config(BaseModel):\\n\\n    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:\\n        \"\"\"\\n        Get the OpenAI client for a specific model name.\\n\\n        :param model_name: The name of the model to get the client for.\\n        :type model_name: str\\n        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.\\n        :rtype: Tuple[Optional[openai.Client], bool]\\n        \"\"\"\\n        current_registry = self._local.stack[-1] if hasattr(self._local, \\'stack\\') and self._local.stack else self.registry\\n        model_config = current_registry.get(model_name)\\n        fallback = False\\n        if not model_config:\\n            warning_message = f\"Warning: A default provider for model \\'{model_name}\\' could not be found. Falling back to default OpenAI client from environment variables.\"\\n            if self.verbose:\\n                from colorama import Fore, Style\\n                _config_logger.warning(f\"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}\")\\n            else:\\n                _config_logger.debug(warning_message)\\n            client = self.default_client\\n            fallback = True\\n        else:\\n            client = model_config.default_client\\n        return client, fallback', filepath='src\\\\ell\\\\configurator.py', metadata=None, node_id='')]),\n",
       "  ClusteredTopic(name='Provider Abstraction and Implementation', chunks=[CodeChunk(id='ell\\\\provider.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='# XXX: Needs a better name.\\nclass Provider(ABC):\\n    \"\"\"\\n    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.\\n    For example, the OpenAI provider is an API interface to OpenAI\\'s API but also to Ollama and Azure OpenAI.\\n    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.\\n    \"\"\"\\n    dangerous_disable_validation = False\\n\\n    ################################\\n    ### API PARAMETERS #############\\n    ################################\\n    @abstractmethod\\n    def provider_call_function(\\n        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None\\n    ) -> Callable[..., Any]:\\n        \"\"\"\\n        Implement this method to return the function that makes the API call to the language model.\\n        For example, if you\\'re implementing the OpenAI provider, you would return the function that makes the API call to OpenAI\\'s API.\\n        \"\"\"\\n        return NotImplemented\\n\\n    def disallowed_api_params(self) -> FrozenSet[str]:\\n        \"\"\"\\n        Returns a list of disallowed call params that ell will override.\\n        \"\"\"\\n        return frozenset({\"messages\", \"tools\", \"model\", \"stream\", \"stream_options\"})\\n\\n    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):\\n        params = _call_params(self.provider_call_function(client, api_params))\\n        return frozenset(params.keys()) - self.disallowed_api_params()\\n\\n    ################################\\n    ### TRANSLATION ###############\\n    ################################\\n    @abstractmethod\\n    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:\\n        \"\"\"Converts an ell call to provider call params!\"\"\"\\n        return NotImplemented\\n\\n    @abstractmethod\\n    def translate_from_provider(\\n        self,\\n        provider_response: Any,\\n        ell_call: EllCallParams,\\n        provider_call_params: Dict[str, Any],\\n        origin_id: Optional[str] = None,\\n        logger: Optional[Callable[..., None]] = None,\\n    ) -> Tuple[List[Message], Metadata]:\\n        \"\"\"Converts provider responses to universal format. with metadata\"\"\"\\n        return NotImplemented\\n\\n    ################################\\n    ### CALL MODEL ################\\n    ################################\\n    # Be careful to override this method in your provider.\\r', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id='ell\\\\provider.py::3'), CodeChunk(id='providers\\\\openai.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from abc import ABC, abstractmethod\\nfrom collections import defaultdict\\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\\n\\nfrom pydantic import BaseModel\\nfrom ell.provider import  EllCallParams, Metadata, Provider\\nfrom ell.types import Message, ContentBlock, ToolCall\\nfrom ell.types._lstr import _lstr\\nimport json\\nfrom ell.configurator import _Model, config, register_provider\\nfrom ell.types.message import LMP\\nfrom ell.util.serialization import serialize_image\\n\\ntry:\\n    # XXX: Could genericize.\\n    import openai\\n    from openai._streaming import Stream\\n    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam\\n\\n    class OpenAIProvider(Provider):\\n        dangerous_disable_validation = True\\n\\n        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\\n            if api_call_params and (isinstance(fmt := api_call_params.get(\"response_format\"), type)) and issubclass(fmt, BaseModel):\\n                return client.beta.chat.completions.parse\\n            else:\\n                return client.chat.completions.create\\n\\n        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:\\n            final_call_params = ell_call.api_params.copy()\\n            final_call_params[\"model\"] = ell_call.model\\n            # Stream by default for verbose logging.\\n            final_call_params[\"stream\"] = True\\n            final_call_params[\"stream_options\"] = {\"include_usage\": True}\\n\\n            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.\\n            if ell_call.tools or final_call_params.get(\"response_format\") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:\\n                final_call_params.pop(\"stream\", None)\\n                final_call_params.pop(\"stream_options\", None)\\n            if ell_call.tools:\\n                final_call_params.update(\\n                    tool_choice=final_call_params.get(\"tool_choice\", \"auto\"),\\n                    tools=[  \\n                        dict(\\n                            type=\"function\",\\n                            function=dict(\\n                                name=tool.__name__,\\n                                description=tool.__doc__,\\n                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore\\n                            )\\n                        ) for tool in ell_call.tools\\n                    ]\\n                )\\n            # messages\\n            openai_messages : List[ChatCompletionMessageParam] = []\\n            for message in ell_call.messages:\\n                if (tool_calls := message.tool_calls):\\n                    assert message.role == \"assistant\", \"Tool calls must be from the assistant.\"\\n                    assert all(t.tool_call_id for t in tool_calls), \"Tool calls must have tool call ids.\"\\n                    openai_messages.append(dict(\\n                        tool_calls=[\\n                            dict(\\n                                id=cast(str, tool_call.tool_call_id),\\n                                type=\"function\",\\n                                function=dict(\\n                                    name=tool_call.tool.__name__,\\n                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)\\n                                )\\n                            ) for tool_call in tool_calls ],\\n                        role=\"assistant\",\\n                        content=None,\\n                    ))\\n                elif (tool_results := message.tool_results):\\n                    for tool_result in tool_results:\\n                        assert all(cb.type == \"text\" for cb in tool_result.result), \"Tool result does not match expected content blocks.\"\\n                        openai_messages.append(dict(\\n                            role=\"tool\",\\n                            tool_call_id=tool_result.tool_call_id,\\n                            content=tool_result.text_only, \\n                        ))\\n                else:\\n                    openai_messages.append(cast(ChatCompletionMessageParam, dict(\\n                        role=message.role,\\n                        content=[_content_block_to_openai_format(c) for c in message.content] \\n                             if message.role != \"system\" \\n                             else message.text_only\\n                    )))\\n\\n            final_call_params[\"messages\"] = openai_messages\\n\\n            return final_call_params\\n\\n        def translate_from_provider(\\n            self,\\n            provider_response: Union[\\n                ChatCompletion, \\n                ParsedChatCompletion,\\n                Stream[ChatCompletionChunk], Any],\\n            ell_call: EllCallParams,\\n            provider_call_params: Dict[str, Any],\\n            origin_id: Optional[str] = None,\\n            logger: Optional[Callable[..., None]] = None,\\n        ) -> Tuple[List[Message], Metadata]:\\n\\n            metadata : Metadata = {}\\n            messages : List[Message] = []\\n            did_stream = provider_call_params.get(\"stream\", False)\\n\\n\\n            if did_stream:\\n                stream = cast(Stream[ChatCompletionChunk], provider_response)\\n                message_streams = defaultdict(list)\\n                role : Optional[str] = None\\n                for chunk in stream:\\n                    metadata.update(chunk.model_dump(exclude={\"choices\"}))\\n\\n                    for chat_compl_chunk in chunk.choices:\\n                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)\\n                        delta = chat_compl_chunk.delta\\n                        role = role or delta.role\\n                        if  chat_compl_chunk.index == 0 and logger:\\n                            logger(delta.content, is_refusal=hasattr(delta, \"refusal\") and delta.refusal)\\n                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):\\n                    text = \"\".join((choice.delta.content or \"\") for choice in message_stream)\\n                    messages.append(\\n                        Message(role=role, \\n                                content=_lstr(content=text,origin_trace=origin_id)))\\n                    #XXX: Support streaming other types.\\n            else:\\n                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)\\n                metadata = chat_completion.model_dump(exclude={\"choices\"})\\n                for oai_choice in chat_completion.choices:\\n                    role = oai_choice.message.role\\n                    content_blocks = []\\n                    if (hasattr(message := oai_choice.message, \"refusal\") and (refusal := message.refusal)):\\n                        raise ValueError(refusal)\\n                    if hasattr(message, \"parsed\"):\\n                        if (parsed := message.parsed):\\n                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing\\n                            if logger: logger(parsed.model_dump_json())\\n                    else:\\n                        if (content := message.content):\\n                            content_blocks.append(\\n                                ContentBlock(\\n                                    text=_lstr(content=content,origin_trace=origin_id)))\\n                            if logger: logger(content)\\n                        if (tool_calls := message.tool_calls):\\n                            for tool_call in tool_calls:\\n                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)\\n                                assert matching_tool, \"Model called tool not found in provided toolset.\"\\n                                content_blocks.append(\\n                                    ContentBlock(\\n                                        tool_call=ToolCall(\\n                                            tool=matching_tool,\\n                                            tool_call_id=_lstr(\\n                                                tool_call.id, origin_trace= origin_id),\\n                                            params=json.loads(tool_call.function.arguments),\\n                                        )\\n                                    )\\n                                )\\n                                if logger: logger(repr(tool_call))\\n                    messages.append(Message(role=role, content=content_blocks))\\n            return messages, metadata\\n\\n\\n    # xx: singleton needed\\n    openai_provider = OpenAIProvider()\\n    register_provider(openai_provider, openai.Client)\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\openai.py', metadata=None, node_id='providers\\\\openai.py::1'), CodeChunk(id='providers\\\\anthropic.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast\\nfrom ell.provider import  EllCallParams, Metadata, Provider\\nfrom ell.types import Message, ContentBlock, ToolCall, ImageContent\\n\\nfrom ell.types._lstr import _lstr\\nfrom ell.types.message import LMP\\nfrom ell.configurator import register_provider\\nfrom ell.util.serialization import serialize_image\\nimport base64\\nfrom io import BytesIO\\nimport json\\nimport requests\\nfrom PIL import Image as PILImage\\n\\ntry:\\n    import anthropic\\n    from anthropic import Anthropic\\n    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent\\n    from anthropic.types.message_create_params import MessageCreateParamsStreaming\\n    from anthropic._streaming import Stream\\n\\n    class AnthropicProvider(Provider):\\n        dangerous_disable_validation = True\\n\\n        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\\n            return client.messages.create\\n\\n        def translate_to_provider(self, ell_call : EllCallParams):\\n            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())\\n            # XXX: Helper, but should be depreicated due to ssot\\n            assert final_call_params.get(\"max_tokens\") is not None, f\"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{\\'max_tokens\\': your_max_tokens}})).\"\\n\\n            dirty_msgs = [\\n                MessageParam(\\n                    role=cast(Literal[\"user\", \"assistant\"], message.role), \\n                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]\\n            role_correct_msgs   : List[MessageParam] = []\\n            for msg in dirty_msgs:\\n                if (not len(role_correct_msgs) or role_correct_msgs[-1][\\'role\\'] != msg[\\'role\\']):\\n                    role_correct_msgs.append(msg)\\n                else: cast(List, role_correct_msgs[-1][\\'content\\']).extend(msg[\\'content\\'])\\n\\n            system_message = None\\n            if role_correct_msgs and role_correct_msgs[0][\"role\"] == \"system\":\\n                system_message = role_correct_msgs.pop(0)\\n\\n            if system_message:\\n                final_call_params[\"system\"] = system_message[\"content\"][0][\"text\"]\\n\\n\\n            final_call_params[\\'stream\\'] = True\\n            final_call_params[\"model\"] = ell_call.model\\n            final_call_params[\"messages\"] = role_correct_msgs\\n\\n            if ell_call.tools:\\n                final_call_params[\"tools\"] = [\\n                    #XXX: Cleaner with LMP\\'s as a class.\\n                    dict(\\n                        name=tool.__name__,\\n                        description=tool.__doc__,\\n                        input_schema=tool.__ell_params_model__.model_json_schema(),\\n                    )\\n                    for tool in ell_call.tools\\n                ]\\n\\n            # print(final_call_params)\\n            return final_call_params\\n\\n        def translate_from_provider(\\n            self,\\n            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],\\n            ell_call: EllCallParams,\\n            provider_call_params: Dict[str, Any],\\n            origin_id: Optional[str] = None,\\n            logger: Optional[Callable[..., None]] = None,\\n        ) -> Tuple[List[Message], Metadata]:\\n\\n            usage = {}\\n            tracked_results = []\\n            metadata = {}\\n\\n            #XXX: Support n > 0\\n\\n            if provider_call_params.get(\"stream\", False):\\n                content = []\\n                current_blocks: Dict[int, Dict[str, Any]] = {}\\n                message_metadata = {}\\n\\n                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:\\n                    for chunk in stream:\\n                        if chunk.type == \"message_start\":\\n                            message_metadata = chunk.message.model_dump()\\n                            message_metadata.pop(\"content\", None)  # Remove content as we\\'ll build it separately\\n\\n                        elif chunk.type == \"content_block_start\":\\n                            block = chunk.content_block.model_dump()\\n                            current_blocks[chunk.index] = block\\n                            if block[\"type\"] == \"tool_use\":\\n                                if logger: logger(f\" <tool_use: {block[\\'name\\']}(\")\\n                                block[\"input\"] = \"\" # force it to be a string, XXX: can implement partially parsed json later.\\n                        elif chunk.type == \"content_block_delta\":\\n                            if chunk.index in current_blocks:\\n                                block = current_blocks[chunk.index]\\n                                if (delta := chunk.delta).type == \"text_delta\":\\n                                    block[\"text\"] += delta.text\\n                                    if logger: logger(delta.text)\\n                                if delta.type == \"input_json_delta\":\\n                                    block[\"input\"] += delta.partial_json\\n                                    if logger: logger(delta.partial_json)\\n\\n                        elif chunk.type == \"content_block_stop\":\\n                            if chunk.index in current_blocks:\\n                                block = current_blocks.pop(chunk.index)\\n                                if block[\"type\"] == \"text\":\\n                                    content.append(ContentBlock(text=_lstr(block[\"text\"],origin_trace=origin_id)))\\n                                elif block[\"type\"] == \"tool_use\":\\n                                    try:\\n                                        matching_tool = ell_call.get_tool_by_name(block[\"name\"])\\n                                        if matching_tool:\\n                                            content.append(\\n                                                ContentBlock(\\n                                                    tool_call=ToolCall(\\n                                                        tool=matching_tool,\\n                                                        tool_call_id=_lstr(\\n                                                            block[\\'id\\'],origin_trace=origin_id\\n                                                        ),\\n                                                        params=json.loads(block[\\'input\\']) if block[\\'input\\'] else {},\\n                                                    )\\n                                                )\\n                                            )\\n                                    except json.JSONDecodeError:\\n                                        if logger: logger(f\" - FAILED TO PARSE JSON\")\\n                                        pass\\n                                    if logger: logger(f\")>\")\\n\\n                        elif chunk.type == \"message_delta\":\\n                            message_metadata.update(chunk.delta.model_dump())\\n                            if chunk.usage:\\n                                usage.update(chunk.usage.model_dump())\\n\\n                        elif chunk.type == \"message_stop\":\\n                            tracked_results.append(Message(role=\"assistant\", content=content))\\n\\n                        # print(chunk)\\n                metadata = message_metadata\\n\\n            # process metadata for ell\\n            # XXX: Unify an ell metadata format for ell studio.\\n            usage[\"prompt_tokens\"] = usage.get(\"input_tokens\", 0)\\n            usage[\"completion_tokens\"] = usage.get(\"output_tokens\", 0)\\n            usage[\"total_tokens\"] = usage[\\'prompt_tokens\\'] + usage[\\'completion_tokens\\']\\n\\n            metadata[\"usage\"] = usage\\n            return tracked_results, metadata\\n\\n    # XXX: Make a singleton.\\n    anthropic_provider = AnthropicProvider()\\n    register_provider(anthropic_provider, anthropic.Anthropic)\\n    register_provider(anthropic_provider, anthropic.AnthropicBedrock)\\n    register_provider(anthropic_provider, anthropic.AnthropicVertex)\\n\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\anthropic.py', metadata=None, node_id='providers\\\\anthropic.py::1'), CodeChunk(id='providers\\\\bedrock.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from abc import ABC, abstractmethod\\nfrom collections import defaultdict\\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\\nfrom ell.provider import  EllCallParams, Metadata, Provider\\nfrom ell.types import Message, ContentBlock, ToolCall, ImageContent\\nfrom ell.types._lstr import _lstr\\nimport json\\nfrom ell.configurator import config, register_provider\\nfrom ell.types.message import LMP\\nfrom ell.util.serialization import serialize_image\\nfrom io import BytesIO\\nimport requests\\nfrom PIL import Image as PILImage', filepath='src\\\\ell\\\\providers\\\\bedrock.py', metadata=None, node_id='providers\\\\bedrock.py::1'), CodeChunk(id='ell\\\\provider.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='# XXX: Might leave this internal to providers so that the complex code is simpler &\\n# we can literally jsut call provider.call like any openai fn.\\nclass EllCallParams(BaseModel):\\n    model: str = Field(..., description=\"Model identifier\")\\n    messages: List[Message] = Field(..., description=\"Conversation context\")\\n    client: Any = Field(..., description=\"API client\")\\n    tools: List[LMP] = Field(default_factory=list, description=\"Available tools\")\\n    api_params: Dict[str, Any] = Field(\\n        default_factory=dict, description=\"API parameters\"\\n    )\\n\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n\\n    def get_tool_by_name(self, name: str) -> Optional[LMP]:\\n        \"\"\"Get a tool by name.\"\"\"\\n        return next(\\n            (tool for tool in (self.tools or [])  if tool.__name__ == name), None\\n        )\\n\\n\\nMetadata = Dict[str, Any]', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id='ell\\\\provider.py::2'), CodeChunk(id='ell\\\\provider.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='def _validate_messages_are_tracked(\\n    messages: List[Message], origin_id: Optional[str] = None\\n):\\n    if origin_id is None:\\n        return\\n\\n    for message in messages:\\n        assert isinstance(\\n            message.text, _lstr\\n        ), f\"Provider implementation error: Message text should be an instance of _lstr, got {type(message.text)}\"\\n        assert (\\n            origin_id in message.text.__origin_trace__\\n        ), f\"Provider implementation error: Message origin_id {message.text.__origin_trace__} does not match the provided origin_id {origin_id}\"\\n    return True', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id='ell\\\\provider.py::6'), CodeChunk(id='providers\\\\openai.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='def _content_block_to_openai_format(content_block: ContentBlock) -> Dict[str, Any]:\\n    if (image := content_block.image):\\n        image_url = dict(url=serialize_image(image.image) if image.image else image.url)\\n        # XXX: Solve per content params better\\n        if image.detail: image_url[\"detail\"] = image.detail\\n        return {\\n            \"type\": \"image_url\",\\n            \"image_url\": image_url\\n        }\\n    elif ((text := content_block.text) is not None): return dict(type=\"text\", text=text)\\n    elif (parsed := content_block.parsed): return dict(type=\"text\", text=parsed.model_dump_json())\\n    else:\\n        raise ValueError(f\"Unsupported content block type for openai: {content_block}\")', filepath='src\\\\ell\\\\providers\\\\openai.py', metadata=None, node_id='providers\\\\openai.py::2'), CodeChunk(id='providers\\\\anthropic.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='def serialize_image_for_anthropic(img : ImageContent):\\n    if img.url:\\n        # Download the image from the URL\\n        response = requests.get(img.url)\\n        response.raise_for_status()  # Raise an exception for bad responses\\n        pil_image = PILImage.open(BytesIO(response.content))\\n    elif img.image:\\n        pil_image = img.image\\n    else:\\n        raise ValueError(\"Image object has neither url nor image data.\")\\n    buffer = BytesIO()\\n    pil_image.save(buffer, format=\"PNG\")\\n    base64_image =  base64.b64encode(buffer.getvalue()).decode()\\n    return dict(\\n        type=\"image\",\\n        source=dict(\\n            type=\"base64\",\\n            media_type=\"image/png\",\\n            data=base64_image\\n        )\\n    )', filepath='src\\\\ell\\\\providers\\\\anthropic.py', metadata=None, node_id='providers\\\\anthropic.py::2'), CodeChunk(id='providers\\\\bedrock.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='try:\\n    from botocore.client import BaseClient\\n    from botocore.eventstream import (EventStream)\\n    class BedrockProvider(Provider):\\n        dangerous_disable_validation = True\\n\\n        def provider_call_function(self, client : Any, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\\n            if api_call_params and api_call_params.get(\"stream\", False):\\n                api_call_params.pop(\\'stream\\')\\n                return client.converse_stream\\n            else:\\n                return client.converse\\n\\n        def translate_to_provider(self, ell_call : EllCallParams):\\n            final_call_params = {}\\n\\n            if ell_call.api_params.get(\\'api_params\\',{}).get(\\'stream\\', False):\\n                final_call_params[\\'stream\\'] = ell_call.api_params.get(\\'api_params\\',{}).get(\\'stream\\', False)\\n\\n            bedrock_converse_messages = [message_to_bedrock_message_format(message) for message in ell_call.messages]\\n\\n            system_message = None\\n            if bedrock_converse_messages and bedrock_converse_messages[0][\"role\"] == \"system\":\\n                system_message = bedrock_converse_messages.pop(0)\\n\\n            if system_message:\\n                final_call_params[\"system\"] = [{\\'text\\':system_message[\"content\"][0][\"text\"]}]\\n\\n            final_call_params[\"modelId\"] = ell_call.model\\n            final_call_params[\"messages\"] = bedrock_converse_messages\\n\\n            if ell_call.tools:\\n                tools = [\\n                    #XXX: Cleaner with LMP\\'s as a class.\\n                    dict(\\n                        toolSpec = dict(\\n                            name=tool.__name__,\\n                            description=tool.__doc__,\\n                            inputSchema=dict(\\n                                json=tool.__ell_params_model__.model_json_schema(),\\n                            )\\n                        )\\n                    )\\n                    for tool in ell_call.tools\\n                ]\\n                final_call_params[\"toolConfig\"] = {\\'tools\\':tools}\\n\\n            return final_call_params\\n\\n        def translate_from_provider(\\n                self,\\n                provider_response: Union[EventStream, Any],\\n                ell_call: EllCallParams,\\n                provider_call_params: Dict[str, Any],\\n                origin_id: Optional[str] = None,\\n                logger: Optional[Callable[..., None]] = None,\\n            ) -> Tuple[List[Message], Metadata]:\\n\\n            usage = {}\\n            metadata : Metadata = {}\\n\\n            metadata : Metadata = {}\\n            tracked_results : List[Message] = []\\n            did_stream = ell_call.api_params.get(\"api_params\", {}).get(\\'stream\\')\\n\\n            if did_stream:\\n                content = []\\n                current_block: Optional[Dict[str, Any]] = {}\\n                message_metadata = {}\\n                for chunk in provider_response.get(\\'stream\\'):\\n\\n                    if \"messageStart\" in chunk:\\n                        current_block[\\'content\\'] = \\'\\'\\n                        pass\\n                    elif \"contentBlockStart\" in chunk:\\n                        pass\\n                    elif \"contentBlockDelta\" in chunk:\\n                        delta = chunk.get(\"contentBlockDelta\", {}).get(\"delta\", {})\\n                        if \"text\" in delta:\\n                            current_block[\\'type\\'] = \\'text\\'\\n                            current_block[\\'content\\'] += delta.get(\"text\")\\n                            if logger:\\n                                logger(delta.get(\"text\"))\\n                        else:\\n                            pass\\n                    elif \"contentBlockStop\" in chunk:\\n                        if current_block is not None:\\n                            if current_block[\"type\"] == \"text\":\\n                                content.append(ContentBlock(text=_lstr(content=content, origin_trace=origin_id)))\\n\\n                    elif \"messageStop\" in chunk:\\n                        tracked_results.append(Message(role=\"assistant\", content=content))\\n\\n                    elif \"metadata\" in chunk:\\n                        if \"usage\" in chunk[\"metadata\"]:\\n                            usage[\"prompt_tokens\"] = chunk[\"metadata\"].get(\\'usage\\').get(\"inputTokens\", 0)\\n                            usage[\"completion_tokens\"] = chunk[\"metadata\"].get(\\'usage\\').get(\"outputTokens\", 0)\\n                            usage[\"total_tokens\"] = usage[\\'prompt_tokens\\'] + usage[\\'completion_tokens\\']\\n                            message_metadata[\"usage\"] = usage\\n                    else:\\n                        pass\\n\\n\\n                metadata = message_metadata\\n            else:\\n                # Non-streaming response processing (unchanged)\\n                cbs = []\\n                for content_block in provider_response.get(\\'output\\', {}).get(\\'message\\', {}).get(\\'content\\', []):\\n                    if \\'text\\' in content_block:\\n                        cbs.append(ContentBlock(text=_lstr(content_block.get(\\'text\\'), origin_trace=origin_id)))\\n                    elif \\'toolUse\\' in content_block:\\n                        assert ell_call.tools is not None, \"Tools were not provided to the model when calling it and yet bedrock returned a tool use.\"\\n                        try:\\n                            toolUse = content_block[\\'toolUse\\']\\n                            matching_tool = ell_call.get_tool_by_name(toolUse[\"name\"])\\n                            if matching_tool:\\n                                cbs.append(\\n                                    ContentBlock(\\n                                        tool_call=ToolCall(\\n                                            tool=matching_tool,\\n                                            tool_call_id=_lstr(\\n                                                toolUse[\\'toolUseId\\'],origin_trace=origin_id\\n                                            ),\\n                                            params=toolUse[\\'input\\'],\\n                                        )\\n                                    )\\n                                )\\n                        except json.JSONDecodeError:\\n                            if logger: logger(f\" - FAILED TO PARSE JSON\")\\n                            pass\\n                tracked_results.append(Message(role=\"assistant\", content=cbs))\\n                if logger:\\n                    logger(tracked_results[0].text)\\n\\n\\n                # usage = call_result.response.usage.dict() if call_result.response.get(\\'usage\\') else {}\\n                # metadata = call_result.response.model_dump()\\n                # del metadata[\"content\"]\\n\\n            # process metadata for ell\\n            # XXX: Unify an ell metadata format for ell studio.\\n            usage[\"prompt_tokens\"] = usage.get(\"inputTokens\", 0)\\n            usage[\"completion_tokens\"] = usage.get(\"outputTokens\", 0)\\n            usage[\"total_tokens\"] = usage[\\'prompt_tokens\\'] + usage[\\'completion_tokens\\']\\n\\n            metadata[\"usage\"] = usage\\n            return tracked_results, metadata\\n\\n\\n    # XXX: Make a singleton.\\n    register_provider(BedrockProvider(), BaseClient)\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\bedrock.py', metadata=None, node_id='providers\\\\bedrock.py::2'), CodeChunk(id='providers\\\\groq.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='\"\"\"\\nGroq provider.\\n\"\"\"\\n\\nfrom ell.providers.openai import OpenAIProvider\\nfrom ell.configurator import register_provider\\n\\n\\ntry:\\n    import groq\\n    class GroqProvider(OpenAIProvider):\\n        dangerous_disable_validation = True\\n        def translate_to_provider(self, *args, **kwargs):\\n            params = super().translate_to_provider(*args, **kwargs)\\n            params.pop(\\'stream_options\\', None)\\n            return params\\n\\n        def translate_from_provider(self, *args, **kwargs):\\n            res, meta = super().translate_from_provider(*args, **kwargs)\\n            if not meta[\\'usage\\']:\\n                meta[\\'usage\\'] = meta[\\'x_groq\\'][\\'usage\\']\\n            return res, meta\\n    register_provider(GroqProvider(), groq.Client)\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\groq.py', metadata=None, node_id='providers\\\\groq.py::1')]),\n",
       "  0.7,\n",
       "  0.7),\n",
       " (ClusteredTopic(name='Message and Content Handling', chunks=[CodeChunk(id='types\\\\message.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='# todo: implement tracing for structured outs. this a v2 feature.\\nimport json\\nfrom ell.types._lstr import _lstr\\nfrom functools import cached_property\\nimport numpy as np\\nimport base64\\nfrom io import BytesIO\\nfrom PIL import Image as PILImage\\n\\nfrom pydantic import BaseModel, ConfigDict, model_validator, field_serializer\\nfrom sqlmodel import Field\\n\\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\\n\\nfrom typing import Any, Callable, Dict, List, Optional, Union\\n\\nfrom ell.util.serialization import serialize_image\\n_lstr_generic = Union[_lstr, str]\\nInvocableTool = Callable[..., Union[\"ToolResult\", _lstr_generic, List[\"ContentBlock\"], ]]\\n\\n# AnyContent represents any type that can be passed to Message.\\nAnyContent = Union[\"ContentBlock\", str, \"ToolCall\", \"ToolResult\", \"ImageContent\", np.ndarray, PILImage.Image, BaseModel]', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='class ToolResult(BaseModel):\\n    tool_call_id: _lstr_generic\\n    result: List[\"ContentBlock\"]\\n\\n    @property\\n    def text(self) -> str:\\n        return _content_to_text(self.result)\\n\\n    @property\\n    def text_only(self) -> str:\\n        return _content_to_text_only(self.result)\\n\\n    # # XXX: Possibly deprecate\\n    # def readable_repr(self) -> str:\\n    #     return f\"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})\"\\n\\n    def __repr__(self):\\n        return f\"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})\"', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='class ToolCall(BaseModel):\\n    tool : InvocableTool\\n    tool_call_id : Optional[_lstr_generic] = Field(default=None)\\n    params : BaseModel\\n\\n    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):\\n        if not isinstance(params, BaseModel):\\n            params = tool.__ell_params_model__(**params) #convenience.\\n        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)\\n\\n    def __call__(self, **kwargs):\\n        assert not kwargs, \"Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall.\"\\n\\n        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.\\n        return self.tool(**self.params.model_dump())\\n\\n    # XXX: Deprecate in 0.1.0\\n    def call_and_collect_as_message_block(self):\\n        raise DeprecationWarning(\"call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.\")\\n\\n    def call_and_collect_as_content_block(self):\\n        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)\\n        return ContentBlock(tool_result=res)\\n\\n    def call_and_collect_as_message(self):\\n        return Message(role=\"user\", content=[self.call_and_collect_as_message_block()])\\n\\n    def __repr__(self):\\n        return f\"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id=\\'{self.tool_call_id}\\')\"', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='class ImageContent(BaseModel):\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n\\n    image: Optional[PILImage.Image] = Field(default=None)\\n    url: Optional[str] = Field(default=None)\\n    detail: Optional[str] = Field(default=None)\\n\\n    @model_validator(mode=\\'after\\')\\n    def check_image_or_url(self):\\n        if self.image is not None and self.url is not None:\\n            raise ValueError(\"Both \\'image\\' and \\'url\\' cannot be set simultaneously.\")\\n        if self.image is None and self.url is None:\\n            raise ValueError(\"Either \\'image\\' or \\'url\\' must be set.\")\\n        return self', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='class ImageContent(BaseModel):\\n\\n    @classmethod\\n    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, \"ImageContent\"]):\\n        if isinstance(value, cls):\\n            return value\\n\\n        if isinstance(value, str):\\n            if value.startswith(\\'http://\\') or value.startswith(\\'https://\\'):\\n                return cls(url=value)\\n            try:\\n                img_data = base64.b64decode(value)\\n                img = PILImage.open(BytesIO(img_data))\\n                if img.mode not in (\\'L\\', \\'RGB\\', \\'RGBA\\'):\\n                    return cls(image=img.convert(\\'RGB\\'))\\n            except:\\n                raise ValueError(\"Invalid base64 string or URL for image\")\\n\\n        if isinstance(value, np.ndarray):\\n            if value.ndim == 3 and value.shape[2] in (3, 4):\\n                mode = \\'RGB\\' if value.shape[2] == 3 else \\'RGBA\\'\\n                return cls(image=PILImage.fromarray(value, mode=mode))\\n            else:\\n                raise ValueError(f\"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.\")\\n\\n        if isinstance(value, PILImage.Image):\\n            if value.mode not in (\\'L\\', \\'RGB\\', \\'RGBA\\'):\\n                value = value.convert(\\'RGB\\')\\n            return cls(image=value)\\n\\n        raise ValueError(f\"Invalid image type: {type(value)}\")\\n\\n    @field_serializer(\\'image\\')\\n    def serialize_image(self, image: Optional[PILImage.Image], _info):\\n        if image is None:\\n            return None\\n        return serialize_image(image)', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='class ContentBlock(BaseModel):\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n\\n    text: Optional[_lstr_generic] = Field(default=None)\\n    image: Optional[ImageContent] = Field(default=None)\\n    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)\\n    tool_call: Optional[ToolCall] = Field(default=None)\\n    parsed: Optional[BaseModel] = Field(default=None)\\n    tool_result: Optional[ToolResult] = Field(default=None)\\n    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.\\n    # This breaks us maintaing parity with the openai python client in some sen but so does image.\\n\\n    def __init__(self, *args, **kwargs):\\n        if \"image\" in kwargs and not isinstance(kwargs[\"image\"], ImageContent):\\n            im = kwargs[\"image\"] = ImageContent.coerce(kwargs[\"image\"])\\n            # XXX: Backwards compatibility, Deprecate.\\n            if (d := kwargs.get(\"image_detail\", None)): im.detail = d\\n\\n        super().__init__(*args, **kwargs)\\n\\n\\n    @model_validator(mode=\\'after\\')\\n    def check_single_non_null(self):\\n        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]\\n        if len(non_null_fields) > 1:\\n            raise ValueError(f\"Only one field can be non-null. Found: {\\', \\'.join(non_null_fields)}\")\\n        return self\\n\\n    def __str__(self):\\n        return repr(self)\\n\\n    def __repr__(self):\\n        non_null_fields = [f\"{field}={value}\" for field, value in self.__dict__.items() if value is not None]\\n        return f\"ContentBlock({\\', \\'.join(non_null_fields)})\"\\n\\n    @property\\n    def type(self):\\n        if self.text is not None:\\n            return \"text\"\\n        if self.image is not None:\\n            return \"image\"\\n        if self.audio is not None:\\n            return \"audio\"\\n        if self.tool_call is not None:\\n            return \"tool_call\"\\n        if self.parsed is not None:\\n            return \"parsed\"\\n        if self.tool_result is not None:\\n            return \"tool_result\"\\n        return None\\n\\n    @property\\n    def content(self):\\n        return getattr(self, self.type)', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::7', input_type=<ClusterInputType.FILE: 'file'>, content='class ContentBlock(BaseModel):\\n\\n    @classmethod\\n    def coerce(cls, content: AnyContent) -> \"ContentBlock\":\\n        \"\"\"\\n        Coerce various types of content into a ContentBlock.\\n\\n        This method provides a flexible way to create ContentBlock instances from different types of input.\\n\\n        Args:\\n        content: The content to be coerced into a ContentBlock. Can be one of the following types:\\n        - str: Will be converted to a text ContentBlock.\\n        - ToolCall: Will be converted to a tool_call ContentBlock.\\n        - ToolResult: Will be converted to a tool_result ContentBlock.\\n        - BaseModel: Will be converted to a parsed ContentBlock.\\n        - ContentBlock: Will be returned as-is.\\n        - Image: Will be converted to an image ContentBlock.\\n        - np.ndarray: Will be converted to an image ContentBlock.\\n        - PILImage.Image: Will be converted to an image ContentBlock.\\n\\n        Returns:\\n        ContentBlock: A new ContentBlock instance containing the coerced content.\\n\\n        Raises:\\n        ValueError: If the content cannot be coerced into a valid ContentBlock.\\n\\n        Examples:\\n        >>> ContentBlock.coerce(\"Hello, world!\")\\n        ContentBlock(text=\"Hello, world!\")\\n\\n        >>> tool_call = ToolCall(...)\\n        >>> ContentBlock.coerce(tool_call)\\n        ContentBlock(tool_call=tool_call)\\n\\n        >>> tool_result = ToolResult(...)\\n        >>> ContentBlock.coerce(tool_result)\\n        ContentBlock(tool_result=tool_result)\\n\\n        >>> class MyModel(BaseModel):\\n        ...     field: str\\n        >>> model_instance = MyModel(field=\"value\")\\n        >>> ContentBlock.coerce(model_instance)\\n        ContentBlock(parsed=model_instance)\\n\\n        >>> from PIL import Image as PILImage\\n        >>> img = PILImage.new(\\'RGB\\', (100, 100))\\n        >>> ContentBlock.coerce(img)\\n        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))\\n\\n        >>> import numpy as np\\n        >>> arr = np.random.rand(100, 100, 3)\\n        >>> ContentBlock.coerce(arr)\\n        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))\\n\\n        >>> image = Image(url=\"https://example.com/image.jpg\")\\n        >>> ContentBlock.coerce(image)\\n        ContentBlock(image=ImageContent(url=\"https://example.com/image.jpg\"))\\n\\n        Notes:\\n        - This method is particularly useful when working with heterogeneous content types\\n          and you want to ensure they are all properly encapsulated in ContentBlock instances.\\n        - The method performs type checking and appropriate conversions to ensure the resulting\\n          ContentBlock is valid according to the model\\'s constraints.\\n        - For image content, Image objects, PIL Image objects, and numpy arrays are supported,\\n          with automatic conversion to the appropriate format.\\n        - As a last resort, the method will attempt to create an image from the input before\\n          raising a ValueError.\\n        \"\"\"\\n        if isinstance(content, ContentBlock):\\n            return content\\n        if isinstance(content, str):\\n            return cls(text=content)\\n        if isinstance(content, ToolCall):\\n            return cls(tool_call=content)\\n        if isinstance(content, ToolResult):\\n            return cls(tool_result=content)\\n        if isinstance(content, (ImageContent, np.ndarray, PILImage.Image)):\\n            return cls(image=ImageContent.coerce(content))\\n        if isinstance(content, BaseModel):\\n            return cls(parsed=content)\\n\\n        raise ValueError(f\"Invalid content type: {type(content)}\")\\n\\n    @field_serializer(\\'parsed\\')\\n    def serialize_parsed(self, value: Optional[BaseModel], _info):\\n        if value is None:\\n            return None\\n        return value.model_dump(exclude_none=True, exclude_unset=True)', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::8', input_type=<ClusterInputType.FILE: 'file'>, content='def to_content_blocks(\\n    content: Optional[Union[AnyContent, List[AnyContent]]] = None,\\n    **content_block_kwargs\\n) -> List[ContentBlock]:\\n    \"\"\"\\n    Coerce a variety of input types into a list of ContentBlock objects.\\n\\n    Args:\\n    content: The content to be coerced. Can be a single item or a list of items.\\n             Supported types include str, ContentBlock, ToolCall, ToolResult, BaseModel, Image, np.ndarray, and PILImage.Image.\\n    **content_block_kwargs: Additional keyword arguments to pass to ContentBlock creation if content is None.\\n\\n    Returns:\\n    List[ContentBlock]: A list of ContentBlock objects created from the input content.\\n\\n    Examples:\\n    >>> coerce_content_list(\"Hello\")\\n    [ContentBlock(text=\"Hello\")]\\n\\n    >>> coerce_content_list([ContentBlock(text=\"Hello\"), \"World\"])\\n    [ContentBlock(text=\"Hello\"), ContentBlock(text=\"World\")]\\n\\n    >>> from PIL import Image as PILImage\\n    >>> pil_image = PILImage.new(\\'RGB\\', (100, 100))\\n    >>> coerce_content_list(pil_image)\\n    [ContentBlock(image=Image(image=<PIL.Image.Image object>))]\\n\\n    >>> coerce_content_list(Image(url=\"https://example.com/image.jpg\"))\\n    [ContentBlock(image=Image(url=\"https://example.com/image.jpg\"))]\\n\\n    >>> coerce_content_list(None, text=\"Default text\")\\n    [ContentBlock(text=\"Default text\")]\\n    \"\"\"\\n    if content is None:\\n        return [ContentBlock(**content_block_kwargs)]\\n\\n    if not isinstance(content, list):\\n        content = [content]\\n\\n    return [ContentBlock.model_validate(ContentBlock.coerce(c)) for c in content]', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::9', input_type=<ClusterInputType.FILE: 'file'>, content='class Message(BaseModel):\\n    role: str\\n    content: List[ContentBlock]\\n\\n\\n    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):\\n        content_blocks = to_content_blocks(content, **content_block_kwargs)\\n\\n        super().__init__(role=role, content=content_blocks)\\n\\n    # XXX: This choice of naming is unfortunate, but it is what it is.\\n    @property\\n    def text(self) -> str:\\n        \"\"\"Returns all text content, replacing non-text content with their representations.\\n\\n        Example:\\n            >>> message = Message(role=\"user\", content=[\"Hello\", PILImage.new(\\'RGB\\', (100, 100)), \"World\"])\\n            >>> message.text\\n            \\'Hello\\\\\\\\n<PilImage>\\\\\\\\nWorld\\'\\n        \"\"\"\\n        return _content_to_text(self.content)', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::10', input_type=<ClusterInputType.FILE: 'file'>, content='class Message(BaseModel):\\n\\n    @property\\n    def images(self) -> List[ImageContent]:\\n        \"\"\"Returns a list of all image content.\\n\\n        Example:\\n            >>> from PIL import Image as PILImage\\n            >>> image1 = Image(url=\"https://example.com/image.jpg\")\\n            >>> image2 = Image(image=PILImage.new(\\'RGB\\', (200, 200)))\\n            >>> message = Message(role=\"user\", content=[\"Text\", image1, \"More text\", image2])\\n            >>> len(message.images)\\n            2\\n            >>> isinstance(message.images[0], Image)\\n            True\\n            >>> message.images[0].url\\n            \\'https://example.com/image.jpg\\'\\n            >>> isinstance(message.images[1].image, PILImage.Image)\\n            True\\n        \"\"\"\\n        return [c.image for c in self.content if c.image]', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::11', input_type=<ClusterInputType.FILE: 'file'>, content='class Message(BaseModel):\\n\\n    @property\\n    def audios(self) -> List[Union[np.ndarray, List[float]]]:\\n        \"\"\"Returns a list of all audio content.\\n\\n        Example:\\n            >>> audio1 = np.array([0.1, 0.2, 0.3])\\n            >>> audio2 = np.array([0.4, 0.5, 0.6])\\n            >>> message = Message(role=\"user\", content=[\"Text\", audio1, \"More text\", audio2])\\n            >>> len(message.audios)\\n            2\\n        \"\"\"\\n        return [c.audio for c in self.content if c.audio]', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::12', input_type=<ClusterInputType.FILE: 'file'>, content='class Message(BaseModel):\\n\\n    @property\\n    def text_only(self) -> str:\\n        \"\"\"Returns only the text content, ignoring non-text content.\\n\\n        Example:\\n            >>> message = Message(role=\"user\", content=[\"Hello\", PILImage.new(\\'RGB\\', (100, 100)), \"World\"])\\n            >>> message.text_only\\n            \\'Hello\\\\\\\\nWorld\\'\\n        \"\"\"\\n        return _content_to_text_only(self.content)\\n\\n    @cached_property\\n    def tool_calls(self) -> List[ToolCall]:\\n        \"\"\"Returns a list of all tool calls.\\n\\n        Example:\\n            >>> tool_call = ToolCall(tool=lambda x: x, params=BaseModel())\\n            >>> message = Message(role=\"user\", content=[\"Text\", tool_call])\\n            >>> len(message.tool_calls)\\n            1\\n        \"\"\"\\n        return [c.tool_call for c in self.content if c.tool_call is not None]\\n\\n    @property\\n    def tool_results(self) -> List[ToolResult]:\\n        \"\"\"Returns a list of all tool results.\\n\\n        Example:\\n            >>> tool_result = ToolResult(tool_call_id=\"123\", result=[ContentBlock(text=\"Result\")])\\n            >>> message = Message(role=\"user\", content=[\"Text\", tool_result])\\n            >>> len(message.tool_results)\\n            1\\n        \"\"\"\\n        return [c.tool_result for c in self.content if c.tool_result is not None]', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::13', input_type=<ClusterInputType.FILE: 'file'>, content='class Message(BaseModel):\\n\\n    @property\\n    def parsed(self) -> Union[BaseModel, List[BaseModel]]:\\n        \"\"\"Returns a list of all parsed content.\\n\\n        Example:\\n            >>> class CustomModel(BaseModel):\\n            ...     value: int\\n            >>> parsed_content = CustomModel(value=42)\\n            >>> message = Message(role=\"user\", content=[\"Text\", ContentBlock(parsed=parsed_content)])\\n            >>> len(message.parsed)\\n            1\\n        \"\"\"\\n        parsed_content = [c.parsed for c in self.content if c.parsed is not None]\\n        return parsed_content[0] if len(parsed_content) == 1 else parsed_content', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::14', input_type=<ClusterInputType.FILE: 'file'>, content='class Message(BaseModel):\\n\\n    def call_tools_and_collect_as_message(self, parallel=False, max_workers=None):\\n        if parallel:\\n            with ThreadPoolExecutor(max_workers=max_workers) as executor:\\n                futures = [executor.submit(c.tool_call.call_and_collect_as_content_block) for c in self.content if c.tool_call]\\n                content = [future.result() for future in as_completed(futures)]\\n        else:\\n            content = [c.tool_call.call_and_collect_as_content_block() for c in self.content if c.tool_call]\\n        return Message(role=\"user\", content=content)', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id=''), CodeChunk(id='types\\\\message.py::15', input_type=<ClusterInputType.FILE: 'file'>, content='# HELPERS \\ndef system(content: Union[AnyContent, List[AnyContent]]) -> Message:\\n    \"\"\"\\n    Create a system message with the given content.\\n\\n    Args:\\n    content (str): The content of the system message.\\n\\n    Returns:\\n    Message: A Message object with role set to \\'system\\' and the provided content.\\n    \"\"\"\\n    return Message(role=\"system\", content=content)\\n\\n\\ndef user(content: Union[AnyContent, List[AnyContent]]) -> Message:\\n    \"\"\"\\n    Create a user message with the given content.\\n\\n    Args:\\n    content (str): The content of the user message.\\n\\n    Returns:\\n    Message: A Message object with role set to \\'user\\' and the provided content.\\n    \"\"\"\\n    return Message(role=\"user\", content=content)\\n\\n\\ndef assistant(content: Union[AnyContent, List[AnyContent]]) -> Message:\\n    \"\"\"\\n    Create an assistant message with the given content.\\n\\n    Args:\\n    content (str): The content of the assistant message.\\n\\n    Returns:\\n    Message: A Message object with role set to \\'assistant\\' and the provided content.\\n    \"\"\"\\n    return Message(role=\"assistant\", content=content)\\n\\n#XXX: Make a mixi for these properties.\\ndef _content_to_text_only(content: List[ContentBlock]) -> str:\\n    return _lstr(\"\\\\n\").join(\\n            available_text\\n            for c in content\\n            if (available_text := (c.tool_result.text_only if c.tool_result else c.text))\\n        )\\n\\n# Do we include the .text of a tool result? or its repr as in the current implementaiton?\\n# What is the user using .text for? I just want to see the result of the tools. text_only should get us the text of the tool results; the tool_call_id is irrelevant.\\ndef _content_to_text(content: List[ContentBlock]) -> str:\\n    return _lstr(\"\\\\n\").join(\\n            available_text\\n            for c in content\\n            if (available_text :=  c.text or repr(c.content))\\n        )\\n\\n\\n# want to enable a use case where the user can actually return a standrd oai chat format\\n# This is a placehodler will likely come back later for this\\nLMPParams = Dict[str, Any]\\n# Well this is disappointing, I wanted to effectively type hint by doign that data sync meta, but eh, at elast we can still reference role or content this way. Probably wil lcan the dict sync meta. TypedDict is the ticket ell oh ell.\\nMessageOrDict = Union[Message, Dict[str, str]]\\n# Can support iamge prompts later.\\nChat = List[\\n    Message\\n]  # [{\"role\": \"system\", \"content\": \"prompt\"}, {\"role\": \"user\", \"content\": \"message\"}]\\nMultiTurnLMP = Callable[..., Chat]\\nOneTurn = Callable[..., _lstr_generic]\\n# This is the specific LMP that must accept history as an argument and can take any additional arguments\\nChatLMP = Callable[[Chat, Any], Chat]\\nLMP = Union[OneTurn, MultiTurnLMP, ChatLMP]\\nInvocableLM = Callable[..., _lstr_generic]', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='')]),\n",
       "  ClusteredTopic(name='Message and Content Handling', chunks=[CodeChunk(id='types\\\\message.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='# todo: implement tracing for structured outs. this a v2 feature.\\nimport json\\nfrom ell.types._lstr import _lstr\\nfrom functools import cached_property\\nimport numpy as np\\nimport base64\\nfrom io import BytesIO\\nfrom PIL import Image as PILImage\\n\\nfrom pydantic import BaseModel, ConfigDict, model_validator, field_serializer\\nfrom sqlmodel import Field\\n\\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\\n\\nfrom typing import Any, Callable, Dict, List, Optional, Union\\n\\nfrom ell.util.serialization import serialize_image\\n_lstr_generic = Union[_lstr, str]\\nInvocableTool = Callable[..., Union[\"ToolResult\", _lstr_generic, List[\"ContentBlock\"], ]]\\n\\n# AnyContent represents any type that can be passed to Message.\\nAnyContent = Union[\"ContentBlock\", str, \"ToolCall\", \"ToolResult\", \"ImageContent\", np.ndarray, PILImage.Image, BaseModel]', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::1'), CodeChunk(id='types\\\\message.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='class ToolResult(BaseModel):\\n    tool_call_id: _lstr_generic\\n    result: List[\"ContentBlock\"]\\n\\n    @property\\n    def text(self) -> str:\\n        return _content_to_text(self.result)\\n\\n    @property\\n    def text_only(self) -> str:\\n        return _content_to_text_only(self.result)\\n\\n    # # XXX: Possibly deprecate\\n    # def readable_repr(self) -> str:\\n    #     return f\"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})\"\\n\\n    def __repr__(self):\\n        return f\"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})\"', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::2'), CodeChunk(id='types\\\\message.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='class ToolCall(BaseModel):\\n    tool : InvocableTool\\n    tool_call_id : Optional[_lstr_generic] = Field(default=None)\\n    params : BaseModel\\n\\n    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):\\n        if not isinstance(params, BaseModel):\\n            params = tool.__ell_params_model__(**params) #convenience.\\n        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)\\n\\n    def __call__(self, **kwargs):\\n        assert not kwargs, \"Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall.\"\\n\\n        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.\\n        return self.tool(**self.params.model_dump())\\n\\n    # XXX: Deprecate in 0.1.0\\n    def call_and_collect_as_message_block(self):\\n        raise DeprecationWarning(\"call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.\")\\n\\n    def call_and_collect_as_content_block(self):\\n        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)\\n        return ContentBlock(tool_result=res)\\n\\n    def call_and_collect_as_message(self):\\n        return Message(role=\"user\", content=[self.call_and_collect_as_message_block()])\\n\\n    def __repr__(self):\\n        return f\"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id=\\'{self.tool_call_id}\\')\"', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::3'), CodeChunk(id='types\\\\message.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='class ImageContent(BaseModel):\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n\\n    image: Optional[PILImage.Image] = Field(default=None)\\n    url: Optional[str] = Field(default=None)\\n    detail: Optional[str] = Field(default=None)\\n\\n    @model_validator(mode=\\'after\\')\\n    def check_image_or_url(self):\\n        if self.image is not None and self.url is not None:\\n            raise ValueError(\"Both \\'image\\' and \\'url\\' cannot be set simultaneously.\")\\n        if self.image is None and self.url is None:\\n            raise ValueError(\"Either \\'image\\' or \\'url\\' must be set.\")\\n        return self', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::4'), CodeChunk(id='types\\\\message.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='class ImageContent(BaseModel):\\n\\n    @classmethod\\n    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, \"ImageContent\"]):\\n        if isinstance(value, cls):\\n            return value\\n\\n        if isinstance(value, str):\\n            if value.startswith(\\'http://\\') or value.startswith(\\'https://\\'):\\n                return cls(url=value)\\n            try:\\n                img_data = base64.b64decode(value)\\n                img = PILImage.open(BytesIO(img_data))\\n                if img.mode not in (\\'L\\', \\'RGB\\', \\'RGBA\\'):\\n                    return cls(image=img.convert(\\'RGB\\'))\\n            except:\\n                raise ValueError(\"Invalid base64 string or URL for image\")\\n\\n        if isinstance(value, np.ndarray):\\n            if value.ndim == 3 and value.shape[2] in (3, 4):\\n                mode = \\'RGB\\' if value.shape[2] == 3 else \\'RGBA\\'\\n                return cls(image=PILImage.fromarray(value, mode=mode))\\n            else:\\n                raise ValueError(f\"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.\")\\n\\n        if isinstance(value, PILImage.Image):\\n            if value.mode not in (\\'L\\', \\'RGB\\', \\'RGBA\\'):\\n                value = value.convert(\\'RGB\\')\\n            return cls(image=value)\\n\\n        raise ValueError(f\"Invalid image type: {type(value)}\")\\n\\n    @field_serializer(\\'image\\')\\n    def serialize_image(self, image: Optional[PILImage.Image], _info):\\n        if image is None:\\n            return None\\n        return serialize_image(image)', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::5'), CodeChunk(id='types\\\\message.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='class ContentBlock(BaseModel):\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n\\n    text: Optional[_lstr_generic] = Field(default=None)\\n    image: Optional[ImageContent] = Field(default=None)\\n    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)\\n    tool_call: Optional[ToolCall] = Field(default=None)\\n    parsed: Optional[BaseModel] = Field(default=None)\\n    tool_result: Optional[ToolResult] = Field(default=None)\\n    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.\\n    # This breaks us maintaing parity with the openai python client in some sen but so does image.\\n\\n    def __init__(self, *args, **kwargs):\\n        if \"image\" in kwargs and not isinstance(kwargs[\"image\"], ImageContent):\\n            im = kwargs[\"image\"] = ImageContent.coerce(kwargs[\"image\"])\\n            # XXX: Backwards compatibility, Deprecate.\\n            if (d := kwargs.get(\"image_detail\", None)): im.detail = d\\n\\n        super().__init__(*args, **kwargs)\\n\\n\\n    @model_validator(mode=\\'after\\')\\n    def check_single_non_null(self):\\n        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]\\n        if len(non_null_fields) > 1:\\n            raise ValueError(f\"Only one field can be non-null. Found: {\\', \\'.join(non_null_fields)}\")\\n        return self\\n\\n    def __str__(self):\\n        return repr(self)\\n\\n    def __repr__(self):\\n        non_null_fields = [f\"{field}={value}\" for field, value in self.__dict__.items() if value is not None]\\n        return f\"ContentBlock({\\', \\'.join(non_null_fields)})\"\\n\\n    @property\\n    def type(self):\\n        if self.text is not None:\\n            return \"text\"\\n        if self.image is not None:\\n            return \"image\"\\n        if self.audio is not None:\\n            return \"audio\"\\n        if self.tool_call is not None:\\n            return \"tool_call\"\\n        if self.parsed is not None:\\n            return \"parsed\"\\n        if self.tool_result is not None:\\n            return \"tool_result\"\\n        return None\\n\\n    @property\\n    def content(self):\\n        return getattr(self, self.type)', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::6'), CodeChunk(id='types\\\\message.py::7', input_type=<ClusterInputType.FILE: 'file'>, content='class ContentBlock(BaseModel):\\n\\n    @classmethod\\n    def coerce(cls, content: AnyContent) -> \"ContentBlock\":\\n        \"\"\"\\n        Coerce various types of content into a ContentBlock.\\n\\n        This method provides a flexible way to create ContentBlock instances from different types of input.\\n\\n        Args:\\n        content: The content to be coerced into a ContentBlock. Can be one of the following types:\\n        - str: Will be converted to a text ContentBlock.\\n        - ToolCall: Will be converted to a tool_call ContentBlock.\\n        - ToolResult: Will be converted to a tool_result ContentBlock.\\n        - BaseModel: Will be converted to a parsed ContentBlock.\\n        - ContentBlock: Will be returned as-is.\\n        - Image: Will be converted to an image ContentBlock.\\n        - np.ndarray: Will be converted to an image ContentBlock.\\n        - PILImage.Image: Will be converted to an image ContentBlock.\\n\\n        Returns:\\n        ContentBlock: A new ContentBlock instance containing the coerced content.\\n\\n        Raises:\\n        ValueError: If the content cannot be coerced into a valid ContentBlock.\\n\\n        Examples:\\n        >>> ContentBlock.coerce(\"Hello, world!\")\\n        ContentBlock(text=\"Hello, world!\")\\n\\n        >>> tool_call = ToolCall(...)\\n        >>> ContentBlock.coerce(tool_call)\\n        ContentBlock(tool_call=tool_call)\\n\\n        >>> tool_result = ToolResult(...)\\n        >>> ContentBlock.coerce(tool_result)\\n        ContentBlock(tool_result=tool_result)\\n\\n        >>> class MyModel(BaseModel):\\n        ...     field: str\\n        >>> model_instance = MyModel(field=\"value\")\\n        >>> ContentBlock.coerce(model_instance)\\n        ContentBlock(parsed=model_instance)\\n\\n        >>> from PIL import Image as PILImage\\n        >>> img = PILImage.new(\\'RGB\\', (100, 100))\\n        >>> ContentBlock.coerce(img)\\n        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))\\n\\n        >>> import numpy as np\\n        >>> arr = np.random.rand(100, 100, 3)\\n        >>> ContentBlock.coerce(arr)\\n        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))\\n\\n        >>> image = Image(url=\"https://example.com/image.jpg\")\\n        >>> ContentBlock.coerce(image)\\n        ContentBlock(image=ImageContent(url=\"https://example.com/image.jpg\"))\\n\\n        Notes:\\n        - This method is particularly useful when working with heterogeneous content types\\n          and you want to ensure they are all properly encapsulated in ContentBlock instances.\\n        - The method performs type checking and appropriate conversions to ensure the resulting\\n          ContentBlock is valid according to the model\\'s constraints.\\n        - For image content, Image objects, PIL Image objects, and numpy arrays are supported,\\n          with automatic conversion to the appropriate format.\\n        - As a last resort, the method will attempt to create an image from the input before\\n          raising a ValueError.\\n        \"\"\"\\n        if isinstance(content, ContentBlock):\\n            return content\\n        if isinstance(content, str):\\n            return cls(text=content)\\n        if isinstance(content, ToolCall):\\n            return cls(tool_call=content)\\n        if isinstance(content, ToolResult):\\n            return cls(tool_result=content)\\n        if isinstance(content, (ImageContent, np.ndarray, PILImage.Image)):\\n            return cls(image=ImageContent.coerce(content))\\n        if isinstance(content, BaseModel):\\n            return cls(parsed=content)\\n\\n        raise ValueError(f\"Invalid content type: {type(content)}\")\\n\\n    @field_serializer(\\'parsed\\')\\n    def serialize_parsed(self, value: Optional[BaseModel], _info):\\n        if value is None:\\n            return None\\n        return value.model_dump(exclude_none=True, exclude_unset=True)', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::7'), CodeChunk(id='types\\\\message.py::8', input_type=<ClusterInputType.FILE: 'file'>, content='def to_content_blocks(\\n    content: Optional[Union[AnyContent, List[AnyContent]]] = None,\\n    **content_block_kwargs\\n) -> List[ContentBlock]:\\n    \"\"\"\\n    Coerce a variety of input types into a list of ContentBlock objects.\\n\\n    Args:\\n    content: The content to be coerced. Can be a single item or a list of items.\\n             Supported types include str, ContentBlock, ToolCall, ToolResult, BaseModel, Image, np.ndarray, and PILImage.Image.\\n    **content_block_kwargs: Additional keyword arguments to pass to ContentBlock creation if content is None.\\n\\n    Returns:\\n    List[ContentBlock]: A list of ContentBlock objects created from the input content.\\n\\n    Examples:\\n    >>> coerce_content_list(\"Hello\")\\n    [ContentBlock(text=\"Hello\")]\\n\\n    >>> coerce_content_list([ContentBlock(text=\"Hello\"), \"World\"])\\n    [ContentBlock(text=\"Hello\"), ContentBlock(text=\"World\")]\\n\\n    >>> from PIL import Image as PILImage\\n    >>> pil_image = PILImage.new(\\'RGB\\', (100, 100))\\n    >>> coerce_content_list(pil_image)\\n    [ContentBlock(image=Image(image=<PIL.Image.Image object>))]\\n\\n    >>> coerce_content_list(Image(url=\"https://example.com/image.jpg\"))\\n    [ContentBlock(image=Image(url=\"https://example.com/image.jpg\"))]\\n\\n    >>> coerce_content_list(None, text=\"Default text\")\\n    [ContentBlock(text=\"Default text\")]\\n    \"\"\"\\n    if content is None:\\n        return [ContentBlock(**content_block_kwargs)]\\n\\n    if not isinstance(content, list):\\n        content = [content]\\n\\n    return [ContentBlock.model_validate(ContentBlock.coerce(c)) for c in content]', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::8'), CodeChunk(id='types\\\\message.py::9', input_type=<ClusterInputType.FILE: 'file'>, content='class Message(BaseModel):\\n    role: str\\n    content: List[ContentBlock]\\n\\n\\n    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):\\n        content_blocks = to_content_blocks(content, **content_block_kwargs)\\n\\n        super().__init__(role=role, content=content_blocks)\\n\\n    # XXX: This choice of naming is unfortunate, but it is what it is.\\n    @property\\n    def text(self) -> str:\\n        \"\"\"Returns all text content, replacing non-text content with their representations.\\n\\n        Example:\\n            >>> message = Message(role=\"user\", content=[\"Hello\", PILImage.new(\\'RGB\\', (100, 100)), \"World\"])\\n            >>> message.text\\n            \\'Hello\\\\\\\\n<PilImage>\\\\\\\\nWorld\\'\\n        \"\"\"\\n        return _content_to_text(self.content)', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::9'), CodeChunk(id='types\\\\message.py::10', input_type=<ClusterInputType.FILE: 'file'>, content='class Message(BaseModel):\\n\\n    @property\\n    def images(self) -> List[ImageContent]:\\n        \"\"\"Returns a list of all image content.\\n\\n        Example:\\n            >>> from PIL import Image as PILImage\\n            >>> image1 = Image(url=\"https://example.com/image.jpg\")\\n            >>> image2 = Image(image=PILImage.new(\\'RGB\\', (200, 200)))\\n            >>> message = Message(role=\"user\", content=[\"Text\", image1, \"More text\", image2])\\n            >>> len(message.images)\\n            2\\n            >>> isinstance(message.images[0], Image)\\n            True\\n            >>> message.images[0].url\\n            \\'https://example.com/image.jpg\\'\\n            >>> isinstance(message.images[1].image, PILImage.Image)\\n            True\\n        \"\"\"\\n        return [c.image for c in self.content if c.image]', filepath='src\\\\ell\\\\types\\\\message.py', metadata=None, node_id='types\\\\message.py::10')]),\n",
       "  0.6666666666666666,\n",
       "  1.0),\n",
       " (ClusteredTopic(name='Closure and Dependency Handling', chunks=[CodeChunk(id='util\\\\closure.py::15', input_type=<ClusterInputType.FILE: 'file'>, content='def lexically_closured_source(func, forced_dependencies: Optional[Dict[str, Any]] = None):\\n    \"\"\"\\n    Generate a lexically closured source for a given function.\\n\\n    This function takes a callable object (function, method, or class) and generates\\n    a lexically closured source code. It captures all the dependencies, including\\n    global variables, free variables, and nested functions, to create a self-contained\\n    version of the function that can be executed independently.\\n\\n    Args:\\n        func (Callable): The function or callable object to process.\\n        forced_dependencies (Optional[Dict[str, Any]]): A dictionary of additional\\n            dependencies to include in the closure. Keys are variable names, and\\n            values are the corresponding objects.\\n\\n    Returns:\\n        Tuple[str, Set[Any]]: A tuple containing two elements:\\n            1. The lexically closured source code as a string.\\n            2. A set of function objects that this closure uses.\\n\\n    Raises:\\n        ValueError: If the input is not a callable object.\\n\\n    Example:\\n        def outer(x):\\n            y = 10\\n            def inner():\\n                return x + y\\n            return inner\\n\\n        closured_source, uses = lexically_closured_source(outer)\\n        print(closured_source)\\n        # Output will include the source for both outer and inner functions,\\n        # along with any necessary imports and variable definitions.\\n\\n    Note:\\n        This function relies on the `lexical_closure` function to perform the\\n        actual closure generation. It also uses the `__ell_closure__` attribute\\n        of the function, which is expected to be set by the `lexical_closure` function.\\n    \"\"\"\\n    if not callable(func):\\n        raise ValueError(\"Input must be a callable object (function, method, or class).\")\\n    _, fnclosure, uses = lexical_closure(func, initial_call=True, recursion_stack=[], forced_dependencies=forced_dependencies)\\n    return func.__ell_closure__, uses', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id=''), CodeChunk(id='util\\\\closure.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='\"\"\"\\nThis should do the following.\\n# prompt_consts.py\\nimport math\\ndef test():\\n    return math.sin(10)\\n\\n# lol3.py\\nimport prompt_consts\\n\\nX = 7\\ndef xD():\\n    print(X)\\n    return prompt_consts.test()\\n\\n###\\nOur goal is to use AST & dill to get a full lexical closured source of xD, with the exception of modules that are stored in site-packages. For example.\\n\\nlexical_extration(xD) returns\\n#closure.py\\nimport math\\ndef test():\\n    return math.sin(10)\\n\\nX = 7 \\ndef xD():\\n    print(X)\\n    return test()\\n\\n\"\"\"\\nimport collections\\nimport ast\\nimport hashlib\\nimport itertools\\nfrom typing import Any, Dict, Iterable, Optional, Set, Tuple, Callable\\nimport dill\\nimport inspect\\nimport types\\nfrom dill.source import getsource\\nimport re\\nfrom collections import deque\\nimport black\\n\\nfrom ell.util.serialization import is_immutable_variable\\nfrom ell.util.should_import import should_import\\n\\nDELIM = \"$$$$$$$$$$$$$$$$$$$$$$$$$\"\\nFORBIDDEN_NAMES = [\"ell\", \"lstr\"]', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id=''), CodeChunk(id='util\\\\closure.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='def _format_source(source: str) -> str:\\n    \"\"\"Format the source code using Black.\"\"\"\\n    try:\\n        return black.format_str(source, mode=black.Mode())\\n    except:\\n        # If Black formatting fails, return the original source\\n        return source\\n\\ndef _get_globals_and_frees(func: Callable) -> Dict[str, Dict]:\\n    \"\"\"Get global and free variables for a function.\"\"\"\\n    globals_dict = collections.OrderedDict(globalvars(func))\\n    frees_dict = collections.OrderedDict(dill.detect.freevars(func))\\n\\n    if isinstance(func, type):\\n        for name, method in collections.OrderedDict(func.__dict__).items():\\n            if isinstance(method, (types.FunctionType, types.MethodType)):\\n                globals_dict.update(collections.OrderedDict(dill.detect.globalvars(method)))\\n                frees_dict.update(collections.OrderedDict(dill.detect.freevars(method)))\\n\\n    return {\\'globals\\': globals_dict, \\'frees\\': frees_dict}', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id=''), CodeChunk(id='util\\\\closure.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='def _process_signature_dependency(val, dependencies, already_closed, recursion_stack, uses, name: Optional[str] = None):\\n    # Todo: Build general cattr like utility for unstructuring python objects with hooks that keep track of state variables.\\n    # Todo: break up closure into types and functions.\\n    # XXX: This is not exhaustive, we should determine should import on all dependencies\\n\\n    if name not in FORBIDDEN_NAMES:\\n        try:\\n            dep = None\\n            _uses = None\\n            if isinstance(val, (types.FunctionType, types.MethodType)):\\n                dep, _, _uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())\\n            elif isinstance(val, (list, tuple, set)):\\n                for item in val:\\n                    _process_signature_dependency(item, dependencies, already_closed, recursion_stack, uses)\\n            else:\\n                val_class = val if isinstance(val, type) else val.__class__\\n                try:\\n                    is_builtin = (val_class.__module__ == \"builtins\" or val_class.__module__ == \"__builtins__\")\\n                except:\\n                    is_builtin = False\\n\\n                if not is_builtin:\\n                    if should_import(val_class.__module__):\\n                        dependencies.append(dill.source.getimport(val_class, alias=val_class.__name__))\\n                    else:\\n                        dep, _, _uses = lexical_closure(val_class, already_closed=already_closed, recursion_stack=recursion_stack.copy())\\n\\n            if dep: dependencies.append(dep)\\n            if _uses: uses.update(_uses)\\n        except Exception as e:\\n            _raise_error(f\"Failed to capture the lexical closure of parameter or annotation {name}\", e, recursion_stack)', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id=''), CodeChunk(id='util\\\\closure.py::7', input_type=<ClusterInputType.FILE: 'file'>, content='def _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack , uses):\\n    \"\"\"Process a single variable.\"\"\"\\n    try:\\n        name = inspect.getmodule(var_value).__name__\\n        if should_import(name):\\n            imports.append(dill.source.getimport(var_value, alias=var_name))\\n            return\\n    except:\\n        pass\\n\\n    if isinstance(var_value, (types.FunctionType, type, types.MethodType)):\\n        _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses)\\n    elif isinstance(var_value, types.ModuleType):\\n        _process_module(var_name, var_value, modules, imports, uses)\\n    elif isinstance(var_value, types.BuiltinFunctionType):\\n        imports.append(dill.source.getimport(var_value, alias=var_name))\\n    else:\\n        _process_other_variable(var_name, var_value, dependencies, uses)', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id=''), CodeChunk(id='util\\\\closure.py::10', input_type=<ClusterInputType.FILE: 'file'>, content='def _build_initial_source(imports, dependencies, source):\\n    \"\"\"Build the initial source code.\"\"\"\\n    return f\"{DELIM}\\\\n\" + f\"\\\\n{DELIM}\\\\n\".join(imports + dependencies + [source]) + f\"\\\\n{DELIM}\\\\n\"\\n\\ndef _process_modules(modules, cur_src, already_closed, recursion_stack, uses):\\n    \"\"\"Process module dependencies.\"\"\"\\n    reverse_module_src = deque()\\n    while modules:\\n        mname, mval = modules.popleft()\\n        mdeps = []\\n        attrs_to_extract = get_referenced_names(cur_src.replace(DELIM, \"\"), mname)\\n        for attr in attrs_to_extract:\\n            _process_module_attribute(mname, mval, attr, mdeps, modules, already_closed, recursion_stack, uses)\\n\\n        mdeps.insert(0, f\"# Extracted from module: {mname}\")\\n        reverse_module_src.appendleft(\"\\\\n\".join(mdeps))\\n\\n        cur_src = _dereference_module_names(cur_src, mname, attrs_to_extract)\\n\\n    return list(reverse_module_src)', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id=''), CodeChunk(id='util\\\\closure.py::11', input_type=<ClusterInputType.FILE: 'file'>, content='def _process_module_attribute(mname, mval, attr, mdeps, modules, already_closed, recursion_stack, uses):\\n    \"\"\"Process a single attribute of a module.\"\"\"\\n    val = getattr(mval, attr)\\n    if isinstance(val, (types.FunctionType, type, types.MethodType)):\\n        try:\\n            dep, _, dep_uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())\\n            mdeps.append(dep)\\n            uses.update(dep_uses)\\n        except Exception as e:\\n            _raise_error(f\"Failed to capture the lexical closure of {mname}.{attr}\", e, recursion_stack)\\n    elif isinstance(val, types.ModuleType):\\n        modules.append((attr, val))\\n    else:\\n        mdeps.append(f\"{attr} = {repr(val)}\")', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id=''), CodeChunk(id='util\\\\closure.py::13', input_type=<ClusterInputType.FILE: 'file'>, content='def _update_ell_func(outer_ell_func, source, dsrc, globals_dict, frees_dict, fn_hash, uses):\\n    \"\"\"Update the ell function attributes.\"\"\"\\n    formatted_source = _format_source(source)\\n    formatted_dsrc = _format_source(dsrc)\\n\\n    if hasattr(outer_ell_func, \"__ell_func__\"):\\n\\n        outer_ell_func.__ell_closure__ = (formatted_source, formatted_dsrc, globals_dict, frees_dict)\\n        outer_ell_func.__ell_hash__ = fn_hash\\n        outer_ell_func.__ell_uses__ = uses\\n\\ndef _raise_error(message, exception, recursion_stack):\\n    \"\"\"Raise an error with detailed information.\"\"\"\\n    error_msg = f\"{message}. Error: {str(exception)}\\\\n\"\\n    error_msg += f\"Recursion stack: {\\' -> \\'.join(recursion_stack)}\"\\n    # print(error_msg)\\n    raise Exception(error_msg)', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id=''), CodeChunk(id='util\\\\closure_util.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='import ast\\nimport importlib\\nimport os\\nimport black\\nfrom dill.detect import nestedglobals\\n\\nimport inspect\\n\\nimport inspect\\n\\n#!/usr/bin/env python\\n#\\ndef globalvars(func, recurse=True, builtin=False):\\n    \"\"\"get objects defined in global scope that are referred to by func\\n\\n    return a dict of {name:object}\"\"\"\\n    while hasattr(func, \"__ell_func__\"):\\n        func = func.__ell_func__\\n    if inspect.ismethod(func): func = func.__func__\\n    while hasattr(func, \"__ell_func__\"):\\n        func = func.__ell_func__\\n    if inspect.isfunction(func):\\n        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}\\n        # get references from within closure\\n        orig_func, func = func, set()\\n        for obj in orig_func.__closure__ or {}:\\n            try:\\n                cell_contents = obj.cell_contents\\n            except ValueError: # cell is empty\\n                pass\\n            else:\\n                _vars = globalvars(cell_contents, recurse, builtin) or {}\\n                func.update(_vars) #XXX: (above) be wary of infinte recursion?\\n                globs.update(_vars)\\n        # get globals\\n        globs.update(orig_func.__globals__ or {})\\n        # get names of references\\n        if not recurse:\\n            func.update(orig_func.__code__.co_names)\\n        else:\\n            func.update(nestedglobals(orig_func.__code__))\\n            # find globals for all entries of func\\n            for key in func.copy(): #XXX: unnecessary...?\\n                nested_func = globs.get(key)\\n                if nested_func is orig_func:\\n                   #func.remove(key) if key in func else None\\n                    continue  #XXX: globalvars(func, False)?\\n                func.update(globalvars(nested_func, True, builtin))\\n    elif inspect.iscode(func):\\n        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}\\n       #globs.update(globals())\\n        if not recurse:\\n            func = func.co_names # get names\\n        else:\\n            orig_func = func.co_name # to stop infinite recursion\\n            func = set(nestedglobals(func))\\n            # find globals for all entries of func\\n            for key in func.copy(): #XXX: unnecessary...?\\n                if key is orig_func:\\n                   #func.remove(key) if key in func else None\\n                    continue  #XXX: globalvars(func, False)?\\n                nested_func = globs.get(key)\\n                func.update(globalvars(nested_func, True, builtin))\\n    else:\\n        return {}\\n    #NOTE: if name not in __globals__, then we skip it...\\n    return dict((name,globs[name]) for name in func if name in globs)', filepath='src\\\\ell\\\\util\\\\closure_util.py', metadata=None, node_id=''), CodeChunk(id='util\\\\closure_util.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='def get_referenced_names(code: str, module_name: str):\\n    \"\"\"\\n    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)\\n    and walks through the tree to find all instances where an attribute of the module is referenced in the code.\\n\\n    Parameters:\\n    code (str): The block of code to be parsed.\\n    module_name (str): The name of the module to look for in the code.\\n\\n    Returns:\\n    list: A list of all attributes of the module that are referenced in the code.\\n    \"\"\"\\n    tree = ast.parse(code)\\n    referenced_names = []\\n\\n    for node in ast.walk(tree):\\n        if isinstance(node, ast.Attribute):\\n            if isinstance(node.value, ast.Name) and node.value.id == module_name:\\n                referenced_names.append(node.attr)\\n\\n    return referenced_names', filepath='src\\\\ell\\\\util\\\\closure_util.py', metadata=None, node_id='')]),\n",
       "  ClusteredTopic(name='Utilities and Lexical Closure Management', chunks=[CodeChunk(id='util\\\\closure.py::11', input_type=<ClusterInputType.FILE: 'file'>, content='def _process_module_attribute(mname, mval, attr, mdeps, modules, already_closed, recursion_stack, uses):\\n    \"\"\"Process a single attribute of a module.\"\"\"\\n    val = getattr(mval, attr)\\n    if isinstance(val, (types.FunctionType, type, types.MethodType)):\\n        try:\\n            dep, _, dep_uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())\\n            mdeps.append(dep)\\n            uses.update(dep_uses)\\n        except Exception as e:\\n            _raise_error(f\"Failed to capture the lexical closure of {mname}.{attr}\", e, recursion_stack)\\n    elif isinstance(val, types.ModuleType):\\n        modules.append((attr, val))\\n    else:\\n        mdeps.append(f\"{attr} = {repr(val)}\")', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id='util\\\\closure.py::11'), CodeChunk(id='util\\\\closure.py::12', input_type=<ClusterInputType.FILE: 'file'>, content='def _dereference_module_names(cur_src, mname, attrs_to_extract):\\n    \"\"\"Dereference module names in the source code.\"\"\"\\n    for attr in attrs_to_extract:\\n        cur_src = cur_src.replace(f\"{mname}.{attr}\", attr)\\n    return cur_src\\n\\ndef _build_final_source(imports, module_src, dependencies, source):\\n    \"\"\"Build the final source code.\"\"\"\\n    seperated_dependencies = sorted(imports) + sorted(module_src) + sorted(dependencies) + ([source] if source else [])\\n    seperated_dependencies = list(dict.fromkeys(seperated_dependencies))\\n    return DELIM + \"\\\\n\" + f\"\\\\n{DELIM}\\\\n\".join(seperated_dependencies) + \"\\\\n\" + DELIM + \"\\\\n\"\\n\\ndef _generate_function_hash(source, dsrc, qualname):\\n    \"\"\"Generate a hash for the function.\"\"\"\\n    return \"lmp-\" + hashlib.md5(\"\\\\n\".join((source, dsrc, qualname)).encode()).hexdigest()', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id='util\\\\closure.py::12'), CodeChunk(id='util\\\\closure.py::13', input_type=<ClusterInputType.FILE: 'file'>, content='def _update_ell_func(outer_ell_func, source, dsrc, globals_dict, frees_dict, fn_hash, uses):\\n    \"\"\"Update the ell function attributes.\"\"\"\\n    formatted_source = _format_source(source)\\n    formatted_dsrc = _format_source(dsrc)\\n\\n    if hasattr(outer_ell_func, \"__ell_func__\"):\\n\\n        outer_ell_func.__ell_closure__ = (formatted_source, formatted_dsrc, globals_dict, frees_dict)\\n        outer_ell_func.__ell_hash__ = fn_hash\\n        outer_ell_func.__ell_uses__ = uses\\n\\ndef _raise_error(message, exception, recursion_stack):\\n    \"\"\"Raise an error with detailed information.\"\"\"\\n    error_msg = f\"{message}. Error: {str(exception)}\\\\n\"\\n    error_msg += f\"Recursion stack: {\\' -> \\'.join(recursion_stack)}\"\\n    # print(error_msg)\\n    raise Exception(error_msg)', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id='util\\\\closure.py::13'), CodeChunk(id='util\\\\closure.py::14', input_type=<ClusterInputType.FILE: 'file'>, content='def get_referenced_names(code: str, module_name: str):\\n    \"\"\"\\n    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)\\n    and walks through the tree to find all instances where an attribute of the module is referenced in the code.\\n\\n    Parameters:\\n    code (str): The block of code to be parsed.\\n    module_name (str): The name of the module to look for in the code.\\n\\n    Returns:\\n    list: A list of all attributes of the module that are referenced in the code.\\n    \"\"\"\\n    # Remove content between #<BV> and #</BV> tags\\n    code = re.sub(r\\'#<BV>\\\\n.*?\\\\n#</BV>\\', \\'\\', code, flags=re.DOTALL)\\n\\n    # Remove content between #<BmV> and #</BmV> tags\\n    code = re.sub(r\\'#<BmV>\\\\n.*?\\\\n#</BmV>\\', \\'\\', code, flags=re.DOTALL)\\n\\n    tree = ast.parse(code)\\n    referenced_names = []\\n\\n    for node in ast.walk(tree):\\n        if isinstance(node, ast.Attribute):\\n            if isinstance(node.value, ast.Name) and node.value.id == module_name:\\n                referenced_names.append(node.attr)\\n\\n    return referenced_names\\n\\nCLOSURE_SOURCE: Dict[str, str] = {}', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id='util\\\\closure.py::14'), CodeChunk(id='util\\\\closure.py::15', input_type=<ClusterInputType.FILE: 'file'>, content='def lexically_closured_source(func, forced_dependencies: Optional[Dict[str, Any]] = None):\\n    \"\"\"\\n    Generate a lexically closured source for a given function.\\n\\n    This function takes a callable object (function, method, or class) and generates\\n    a lexically closured source code. It captures all the dependencies, including\\n    global variables, free variables, and nested functions, to create a self-contained\\n    version of the function that can be executed independently.\\n\\n    Args:\\n        func (Callable): The function or callable object to process.\\n        forced_dependencies (Optional[Dict[str, Any]]): A dictionary of additional\\n            dependencies to include in the closure. Keys are variable names, and\\n            values are the corresponding objects.\\n\\n    Returns:\\n        Tuple[str, Set[Any]]: A tuple containing two elements:\\n            1. The lexically closured source code as a string.\\n            2. A set of function objects that this closure uses.\\n\\n    Raises:\\n        ValueError: If the input is not a callable object.\\n\\n    Example:\\n        def outer(x):\\n            y = 10\\n            def inner():\\n                return x + y\\n            return inner\\n\\n        closured_source, uses = lexically_closured_source(outer)\\n        print(closured_source)\\n        # Output will include the source for both outer and inner functions,\\n        # along with any necessary imports and variable definitions.\\n\\n    Note:\\n        This function relies on the `lexical_closure` function to perform the\\n        actual closure generation. It also uses the `__ell_closure__` attribute\\n        of the function, which is expected to be set by the `lexical_closure` function.\\n    \"\"\"\\n    if not callable(func):\\n        raise ValueError(\"Input must be a callable object (function, method, or class).\")\\n    _, fnclosure, uses = lexical_closure(func, initial_call=True, recursion_stack=[], forced_dependencies=forced_dependencies)\\n    return func.__ell_closure__, uses', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id='util\\\\closure.py::15'), CodeChunk(id='util\\\\closure.py::16', input_type=<ClusterInputType.FILE: 'file'>, content='import ast\\n\\ndef _clean_src(dirty_src):\\n    # Now remove all duplicates and preserve order\\n    split_by_setion = filter(lambda x: len(x.strip()) > 0, dirty_src.split(DELIM))\\n\\n    # Now we need to remove all the duplicates\\n    split_by_setion = list(dict.fromkeys(split_by_setion))\\n\\n    # Now we need to concat all together\\n    all_imports = []\\n    final_src = \"\\\\n\".join(split_by_setion)\\n    out_final_src = final_src[:]\\n    for line in final_src.split(\"\\\\n\"):\\n        if line.startswith(\"import\") or line.startswith(\"from\"):\\n            all_imports.append(line)\\n            out_final_src = out_final_src.replace(line, \"\")\\n\\n    all_imports = \"\\\\n\".join(sorted(all_imports))\\n    final_src = all_imports + \"\\\\n\" + out_final_src\\n\\n    # now replace all \"\\\\n\\\\n\\\\n\" or longer with \"\\\\n\\\\n\"\\n    final_src = re.sub(r\"\\\\n{3,}\", \"\\\\n\\\\n\", final_src)\\n\\n    return final_src', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id='util\\\\closure.py::16'), CodeChunk(id='util\\\\closure.py::17', input_type=<ClusterInputType.FILE: 'file'>, content='def is_function_called(func_name, source_code):\\n    \"\"\"\\n    Check if a function is called in the given source code.\\n\\n    Parameters:\\n    func_name (str): The name of the function to check.\\n    source_code (str): The source code to check.\\n\\n    Returns:\\n    bool: True if the function is called, False otherwise.\\n    \"\"\"\\n    # Parse the source code into an AST\\n    tree = ast.parse(source_code)\\n\\n    # Walk through all the nodes in the AST\\n    for node in ast.walk(tree):\\n        # If the node is a function call\\n        if isinstance(node, ast.Call):\\n            # If the function being called is the function we\\'re looking for\\n            if isinstance(node.func, ast.Name) and node.func.id == func_name:\\n                return True\\n\\n    # If we\\'ve gone through all the nodes and haven\\'t found a call to the function, it\\'s not called\\n    return False\\n\\n#!/usr/bin/env python\\n#\\nfrom dill.detect import nestedglobals\\nimport inspect', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id='util\\\\closure.py::17'), CodeChunk(id='util\\\\closure.py::18', input_type=<ClusterInputType.FILE: 'file'>, content='def globalvars(func, recurse=True, builtin=False):\\n    \"\"\"get objects defined in global scope that are referred to by func\\n\\n    return a dict of {name:object}\"\"\"\\n    while hasattr(func, \"__ell_func__\"):\\n        func = func.__ell_func__\\n    if inspect.ismethod(func): func = func.__func__\\n    while hasattr(func, \"__ell_func__\"):\\n        func = func.__ell_func__\\n    if inspect.isfunction(func):\\n        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}\\n        # get references from within closure\\n        orig_func, func = func, set()\\n        for obj in orig_func.__closure__ or {}:\\n            try:\\n                cell_contents = obj.cell_contents\\n            except ValueError: # cell is empty\\n                pass\\n            else:\\n                _vars = globalvars(cell_contents, recurse, builtin) or {}\\n                func.update(_vars) #XXX: (above) be wary of infinte recursion?\\n                globs.update(_vars)\\n        # get globals\\n        globs.update(orig_func.__globals__ or {})\\n        # get names of references\\n        if not recurse:\\n            func.update(orig_func.__code__.co_names)\\n        else:\\n            func.update(nestedglobals(orig_func.__code__))\\n            # find globals for all entries of func\\n            for key in func.copy(): #XXX: unnecessary...?\\n                nested_func = globs.get(key)\\n                if nested_func is orig_func:\\n                   #func.remove(key) if key in func else None\\n                    continue  #XXX: globalvars(func, False)?\\n                func.update(globalvars(nested_func, True, builtin))\\n    elif inspect.iscode(func):\\n        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}\\n       #globs.update(globals())\\n        if not recurse:\\n            func = func.co_names # get names\\n        else:\\n            orig_func = func.co_name # to stop infinite recursion\\n            func = set(nestedglobals(func))\\n            # find globals for all entries of func\\n            for key in func.copy(): #XXX: unnecessary...?\\n                if key is orig_func:\\n                   #func.remove(key) if key in func else None\\n                    continue  #XXX: globalvars(func, False)?\\n                nested_func = globs.get(key)\\n                func.update(globalvars(nested_func, True, builtin))\\n    # elif inspect.isclass(func):\\n    # XXX: We need to get lexical closures of all the methods and attributes of the class.\\\\\\n    # In the future we should exhaustively walk the AST here.\\n    else:\\n        return {}\\n    #NOTE: if name not in __globals__, then we skip it...\\n    return dict((name,globs[name]) for name in func if name in globs)', filepath='src\\\\ell\\\\util\\\\closure.py', metadata=None, node_id='util\\\\closure.py::18'), CodeChunk(id='util\\\\closure_util.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='import ast\\nimport importlib\\nimport os\\nimport black\\nfrom dill.detect import nestedglobals\\n\\nimport inspect\\n\\nimport inspect\\n\\n#!/usr/bin/env python\\n#\\ndef globalvars(func, recurse=True, builtin=False):\\n    \"\"\"get objects defined in global scope that are referred to by func\\n\\n    return a dict of {name:object}\"\"\"\\n    while hasattr(func, \"__ell_func__\"):\\n        func = func.__ell_func__\\n    if inspect.ismethod(func): func = func.__func__\\n    while hasattr(func, \"__ell_func__\"):\\n        func = func.__ell_func__\\n    if inspect.isfunction(func):\\n        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}\\n        # get references from within closure\\n        orig_func, func = func, set()\\n        for obj in orig_func.__closure__ or {}:\\n            try:\\n                cell_contents = obj.cell_contents\\n            except ValueError: # cell is empty\\n                pass\\n            else:\\n                _vars = globalvars(cell_contents, recurse, builtin) or {}\\n                func.update(_vars) #XXX: (above) be wary of infinte recursion?\\n                globs.update(_vars)\\n        # get globals\\n        globs.update(orig_func.__globals__ or {})\\n        # get names of references\\n        if not recurse:\\n            func.update(orig_func.__code__.co_names)\\n        else:\\n            func.update(nestedglobals(orig_func.__code__))\\n            # find globals for all entries of func\\n            for key in func.copy(): #XXX: unnecessary...?\\n                nested_func = globs.get(key)\\n                if nested_func is orig_func:\\n                   #func.remove(key) if key in func else None\\n                    continue  #XXX: globalvars(func, False)?\\n                func.update(globalvars(nested_func, True, builtin))\\n    elif inspect.iscode(func):\\n        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}\\n       #globs.update(globals())\\n        if not recurse:\\n            func = func.co_names # get names\\n        else:\\n            orig_func = func.co_name # to stop infinite recursion\\n            func = set(nestedglobals(func))\\n            # find globals for all entries of func\\n            for key in func.copy(): #XXX: unnecessary...?\\n                if key is orig_func:\\n                   #func.remove(key) if key in func else None\\n                    continue  #XXX: globalvars(func, False)?\\n                nested_func = globs.get(key)\\n                func.update(globalvars(nested_func, True, builtin))\\n    else:\\n        return {}\\n    #NOTE: if name not in __globals__, then we skip it...\\n    return dict((name,globs[name]) for name in func if name in globs)', filepath='src\\\\ell\\\\util\\\\closure_util.py', metadata=None, node_id='util\\\\closure_util.py::1'), CodeChunk(id='util\\\\closure_util.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='def get_referenced_names(code: str, module_name: str):\\n    \"\"\"\\n    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)\\n    and walks through the tree to find all instances where an attribute of the module is referenced in the code.\\n\\n    Parameters:\\n    code (str): The block of code to be parsed.\\n    module_name (str): The name of the module to look for in the code.\\n\\n    Returns:\\n    list: A list of all attributes of the module that are referenced in the code.\\n    \"\"\"\\n    tree = ast.parse(code)\\n    referenced_names = []\\n\\n    for node in ast.walk(tree):\\n        if isinstance(node, ast.Attribute):\\n            if isinstance(node.value, ast.Name) and node.value.id == module_name:\\n                referenced_names.append(node.attr)\\n\\n    return referenced_names', filepath='src\\\\ell\\\\util\\\\closure_util.py', metadata=None, node_id='util\\\\closure_util.py::3')]),\n",
       "  0.5,\n",
       "  0.5),\n",
       " (ClusteredTopic(name='API Call Parameter Management', chunks=[CodeChunk(id='ell\\\\provider.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='# XXX: Might leave this internal to providers so that the complex code is simpler &\\n# we can literally jsut call provider.call like any openai fn.\\nclass EllCallParams(BaseModel):\\n    model: str = Field(..., description=\"Model identifier\")\\n    messages: List[Message] = Field(..., description=\"Conversation context\")\\n    client: Any = Field(..., description=\"API client\")\\n    tools: List[LMP] = Field(default_factory=list, description=\"Available tools\")\\n    api_params: Dict[str, Any] = Field(\\n        default_factory=dict, description=\"API parameters\"\\n    )\\n\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n\\n    def get_tool_by_name(self, name: str) -> Optional[LMP]:\\n        \"\"\"Get a tool by name.\"\"\"\\n        return next(\\n            (tool for tool in (self.tools or [])  if tool.__name__ == name), None\\n        )\\n\\n\\nMetadata = Dict[str, Any]', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\provider.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='# XXX: Needs a better name.\\nclass Provider(ABC):\\n    \"\"\"\\n    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.\\n    For example, the OpenAI provider is an API interface to OpenAI\\'s API but also to Ollama and Azure OpenAI.\\n    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.\\n    \"\"\"\\n    dangerous_disable_validation = False\\n\\n    ################################\\n    ### API PARAMETERS #############\\n    ################################\\n    @abstractmethod\\n    def provider_call_function(\\n        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None\\n    ) -> Callable[..., Any]:\\n        \"\"\"\\n        Implement this method to return the function that makes the API call to the language model.\\n        For example, if you\\'re implementing the OpenAI provider, you would return the function that makes the API call to OpenAI\\'s API.\\n        \"\"\"\\n        return NotImplemented\\n\\n    def disallowed_api_params(self) -> FrozenSet[str]:\\n        \"\"\"\\n        Returns a list of disallowed call params that ell will override.\\n        \"\"\"\\n        return frozenset({\"messages\", \"tools\", \"model\", \"stream\", \"stream_options\"})\\n\\n    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):\\n        params = _call_params(self.provider_call_function(client, api_params))\\n        return frozenset(params.keys()) - self.disallowed_api_params()\\n\\n    ################################\\n    ### TRANSLATION ###############\\n    ################################\\n    @abstractmethod\\n    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:\\n        \"\"\"Converts an ell call to provider call params!\"\"\"\\n        return NotImplemented\\n\\n    @abstractmethod\\n    def translate_from_provider(\\n        self,\\n        provider_response: Any,\\n        ell_call: EllCallParams,\\n        provider_call_params: Dict[str, Any],\\n        origin_id: Optional[str] = None,\\n        logger: Optional[Callable[..., None]] = None,\\n    ) -> Tuple[List[Message], Metadata]:\\n        \"\"\"Converts provider responses to universal format. with metadata\"\"\"\\n        return NotImplemented\\n\\n    ################################\\n    ### CALL MODEL ################\\n    ################################\\n    # Be careful to override this method in your provider.\\r', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\provider.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='class Provider(ABC):\\n    def call(\\n        self,\\n        #XXX: In future refactors, we can fully enumerate the args and make ell_call\\'s internal to the _provider implementer interface.\\n        # This gives us a litellm style interface for free.\\n        ell_call: EllCallParams,\\n        origin_id: Optional[str] = None,\\n        logger: Optional[Any] = None,\\n    ) -> Tuple[List[Message], Dict[str, Any], Metadata]:\\n        # Automatic validation of params\\n        assert (\\n            not set(ell_call.api_params.keys()).intersection(self.disallowed_api_params()) \\n        ), f\"Disallowed api parameters: {ell_call.api_params}\"\\n\\n        final_api_call_params = self.translate_to_provider(ell_call)\\n\\n        call = self.provider_call_function(ell_call.client, final_api_call_params)\\n        assert self.dangerous_disable_validation or _validate_provider_call_params(final_api_call_params, call)\\n\\n\\n        provider_resp = call(**final_api_call_params)\\n\\n        messages, metadata = self.translate_from_provider(\\n            provider_resp, ell_call, final_api_call_params, origin_id, logger\\n        )\\n        assert \"choices\" not in metadata, \"choices should be in the metadata.\"\\n        assert self.dangerous_disable_validation or _validate_messages_are_tracked(messages, origin_id)\\n\\n        return messages, final_api_call_params, metadata', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\provider.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='# handhold the the implementer, in production mode we can turn these off for speed.\\n@lru_cache(maxsize=None)\\ndef _call_params(call: Callable[..., Any]) -> MappingProxyType[str, inspect.Parameter]:\\n    return inspect.signature(call).parameters\\n\\n\\ndef _validate_provider_call_params(\\n    api_call_params: Dict[str, Any], call: Callable[..., Any]\\n):\\n    provider_call_params = _call_params(call)\\n\\n    required_params = {\\n        name: param\\n        for name, param in provider_call_params.items()\\n        if param.default == param.empty and param.kind != param.VAR_KEYWORD\\n    }\\n\\n    for param_name in required_params:\\n        assert (\\n            param_name in api_call_params\\n        ), f\"Provider implementation error: Required parameter \\'{param_name}\\' is missing in the converted call parameters converted from ell call.\"\\n\\n    for param_name, param_value in api_call_params.items():\\n        assert (\\n            param_name in provider_call_params\\n        ), f\"Provider implementation error: Unexpected parameter \\'{param_name}\\' in the converted call parameters.\"\\n\\n    return True', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id=''), CodeChunk(id='ell\\\\provider.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='def _validate_messages_are_tracked(\\n    messages: List[Message], origin_id: Optional[str] = None\\n):\\n    if origin_id is None:\\n        return\\n\\n    for message in messages:\\n        assert isinstance(\\n            message.text, _lstr\\n        ), f\"Provider implementation error: Message text should be an instance of _lstr, got {type(message.text)}\"\\n        assert (\\n            origin_id in message.text.__origin_trace__\\n        ), f\"Provider implementation error: Message origin_id {message.text.__origin_trace__} does not match the provided origin_id {origin_id}\"\\n    return True', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\event_handler.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='import asyncio\\nfrom typing import Callable, Dict, List, Any\\n\\nclass RealtimeEventHandler:\\n    def __init__(self):\\n        self.event_handlers: Dict[str, List[Callable]] = {}\\n        self.next_event_handlers: Dict[str, List[Callable]] = {}\\n\\n    def clear_event_handlers(self):\\n        self.event_handlers.clear()\\n        self.next_event_handlers.clear()\\n        return True\\n\\n    def on(self, event_name: str, callback: Callable = None):\\n        def decorator(func):\\n            if event_name not in self.event_handlers:\\n                self.event_handlers[event_name] = []\\n            self.event_handlers[event_name].append(func)\\n            return func\\n\\n        if callback is None:\\n            return decorator\\n        else:\\n            return decorator(callback)\\n\\n    def on_next(self, event_name: str, callback: Callable):\\n        if event_name not in self.next_event_handlers:\\n            self.next_event_handlers[event_name] = []\\n        self.next_event_handlers[event_name].append(callback)\\n\\n    def off(self, event_name: str, callback: Callable = None):\\n        if event_name in self.event_handlers:\\n            if callback:\\n                self.event_handlers[event_name].remove(callback)\\n            else:\\n                del self.event_handlers[event_name]\\n        return True\\n\\n    def off_next(self, event_name: str, callback: Callable = None):\\n        if event_name in self.next_event_handlers:\\n            if callback:\\n                self.next_event_handlers[event_name].remove(callback)\\n            else:\\n                del self.next_event_handlers[event_name]\\n        return True\\n\\n    async def wait_for_next(self, event_name: str, timeout: float = None):\\n        next_event = None\\n        def set_next_event(event):\\n            nonlocal next_event\\n            next_event = event\\n\\n        self.on_next(event_name, set_next_event)\\n\\n        start_time = asyncio.get_event_loop().time()\\n        while not next_event:\\n            if timeout and asyncio.get_event_loop().time() - start_time > timeout:\\n                return None\\n            await asyncio.sleep(0.001)\\n\\n        return next_event\\n\\n    def dispatch(self, event_name: str, event: Any):\\n        handlers = self.event_handlers.get(event_name, []).copy()\\n        for handler in handlers:\\n            handler(event)\\n\\n        next_handlers = self.next_event_handlers.pop(event_name, [])\\n        for next_handler in next_handlers:\\n            next_handler(event)\\n\\n        return True', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\event_handler.py', metadata=None, node_id=''), CodeChunk(id='openai_realtime\\\\utils.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='import base64\\nimport numpy as np\\n\\nclass RealtimeUtils:\\n    @staticmethod\\n    def float_to_16bit_pcm(float32_array):\\n        int16_array = (np.clip(float32_array, -1, 1) * 32767).astype(np.int16)\\n        return int16_array.tobytes()\\n\\n    @staticmethod\\n    def base64_to_array_buffer(base64_string):\\n        return base64.b64decode(base64_string)\\n\\n    @staticmethod\\n    def array_buffer_to_base64(array_buffer):\\n        if isinstance(array_buffer, np.ndarray):\\n            if array_buffer.dtype == np.float32:\\n                array_buffer = RealtimeUtils.float_to_16bit_pcm(array_buffer)\\n            elif array_buffer.dtype == np.int16:\\n                array_buffer = array_buffer.tobytes()\\n        return base64.b64encode(array_buffer).decode(\\'utf-8\\')\\n\\n    @staticmethod\\n    def merge_int16_arrays(left, right):\\n        if isinstance(left, bytes):\\n            left = np.frombuffer(left, dtype=np.int16)\\n        if isinstance(right, bytes):\\n            right = np.frombuffer(right, dtype=np.int16)\\n        if not isinstance(left, np.ndarray) or not isinstance(right, np.ndarray):\\n            raise ValueError(\"Both items must be numpy arrays or bytes objects\")\\n        return np.concatenate((left, right))\\n\\n    @staticmethod\\n    def generate_id(prefix, length=21):\\n        import random\\n        chars = \\'123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz\\'\\n        return prefix + \\'\\'.join(random.choice(chars) for _ in range(length - len(prefix)))', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\utils.py', metadata=None, node_id=''), CodeChunk(id='models\\\\__init__.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='\"\"\"\\nAttempts to registeres model names with their respective API client bindings. This allows for the creation of a unified interface for interacting with different LLM providers.\\n\\nFor example, to register an OpenAI model:\\n@ell.simple(model=\\'gpt-4o-mini\\') -> @ell.simple(model=\\'gpt-4o-mini\\', client=openai.OpenAI())\\n\\n\"\"\"\\n\\nimport ell.models.openai\\nimport ell.models.anthropic\\nimport ell.models.ollama\\nimport ell.models.groq\\nimport ell.models.bedrock', filepath='src\\\\ell\\\\models\\\\__init__.py', metadata=None, node_id=''), CodeChunk(id='models\\\\anthropic.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from ell.configurator import config\\nimport logging\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\ntry:\\n    import anthropic\\n\\n    def register(client: anthropic.Anthropic):\\n        \"\"\"\\n        Register Anthropic models with the provided client.\\n\\n        This function takes an Anthropic client and registers various Anthropic models\\n        with the global configuration. It allows the system to use these models\\n        for different AI tasks.\\n\\n        Args:\\n            client (anthropic.Anthropic): An instance of the Anthropic client to be used\\n                                          for model registration.\\n\\n        Note:\\n            The function doesn\\'t return anything but updates the global\\n            configuration with the registered models.\\n        \"\"\"\\n        model_data = [\\n            (\\'claude-3-opus-20240229\\', \\'anthropic\\'),\\n            (\\'claude-3-sonnet-20240229\\', \\'anthropic\\'),\\n            (\\'claude-3-haiku-20240307\\', \\'anthropic\\'),\\n            (\\'claude-3-5-sonnet-20240620\\', \\'anthropic\\'),\\n        ]\\n        for model_id, owned_by in model_data:\\n            config.register_model(model_id, client)\\n\\n    try:\\n        default_client = anthropic.Anthropic()\\n        register(default_client)\\n    except Exception as e:\\n        # logger.warning(f\"Failed to create default Anthropic client: {e}\")\\n        pass\\n\\n\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\models\\\\anthropic.py', metadata=None, node_id=''), CodeChunk(id='models\\\\bedrock.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from typing import Any\\nfrom ell.configurator import config\\nimport logging\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\ndef register(client: Any):\\n    \"\"\"\\n    Register Bedrock models with the provided client.\\n\\n    This function takes an boto3 client and registers various Bedrock models\\n    with the global configuration. It allows the system to use these models\\n    for different AI tasks.\\n\\n    Args:\\n        client (boto3.client): An instance of the bedrock client to be used\\n                                        for model registration.\\n\\n    Note:\\n        The function doesn\\'t return anything but updates the global\\n        configuration with the registered models.\\n    \"\"\"\\n    model_data = [\\n        (\\'anthropic.claude-3-opus-20240229-v1:0\\', \\'bedrock\\'),\\n        (\\'anthropic.claude-3-sonnet-20240229-v1:0\\', \\'bedrock\\'),\\n        (\\'anthropic.claude-3-haiku-20240307-v1:0\\', \\'bedrock\\'),\\n        (\\'anthropic.claude-3-5-sonnet-20240620-v1:0\\', \\'bedrock\\'),\\n\\n        (\\'mistral.mistral-7b-instruct-v0:2\\', \\'bedrock\\'),\\n        (\\'mistral.mixtral-8x7b-instruct-v0:1\\', \\'bedrock\\'),\\n        (\\'mistral.mistral-large-2402-v1:0\\', \\'bedrock\\'),\\n        (\\'mistral.mistral-small-2402-v1:0\\', \\'bedrock\\'),\\n\\n\\n        (\\'ai21.jamba-instruct-v1:0\\',\\'bedrock\\'),\\n        (\\'ai21.j2-ultra-v1\\', \\'bedrock\\'),\\n        (\\'ai21.j2-mid-v1\\', \\'bedrock\\'),\\n\\n        (\\'amazon.titan-embed-text-v1\\', \\'bedrock\\'),\\n        (\\'amazon.titan-text-lite-v1\\', \\'bedrock\\'),\\n        (\\'amazon.titan-text-express-v1\\', \\'bedrock\\'),\\n        (\\'amazon.titan-image-generator-v2:0\\', \\'bedrock\\'),\\n        (\\'amazon.titan-image-generator-v1\\', \\'bedrock\\'),\\n\\n        (\\'cohere.command-r-plus-v1:0\\', \\'bedrock\\'),\\n        (\\'cohere.command-r-v1:0\\', \\'bedrock\\'),\\n        (\\'cohere.embed-english-v3\\', \\'bedrock\\'),\\n        (\\'cohere.embed-multilingual-v3\\', \\'bedrock\\'),\\n        (\\'cohere.command-text-v14\\', \\'bedrock\\'),\\n\\n        (\\'meta.llama3-8b-instruct-v1:0\\', \\'bedrock\\'),\\n        (\\'meta.llama3-70b-instruct-v1:0\\', \\'bedrock\\'),\\n        (\\'meta.llama2-13b-chat-v1\\', \\'bedrock\\'),\\n        (\\'meta.llama2-70b-chat-v1\\', \\'bedrock\\'),\\n        (\\'meta.llama2-13b-v1\\', \\'bedrock\\'),\\n\\n    ]\\n\\n    for model_id, owned_by in model_data:\\n        config.register_model(name=model_id, default_client=client, supports_streaming=True)\\n\\ndefault_client = None\\ntry:\\n\\n    import boto3\\n    default_client = boto3.client(\\'bedrock-runtime\\')\\nexcept Exception as e:\\n    pass\\n\\nregister(default_client)', filepath='src\\\\ell\\\\models\\\\bedrock.py', metadata=None, node_id=''), CodeChunk(id='models\\\\groq.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from typing import Optional\\nfrom ell.configurator import config\\n\\ntry:\\n    from groq import Groq\\n    def register(client: Optional[Groq] = None, **client_kwargs):\\n        if client is None:\\n            client = Groq(**client_kwargs)\\n        for model in client.models.list().data:\\n            config.register_model(model.id, default_client=client, supports_streaming=True)\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\models\\\\groq.py', metadata=None, node_id=''), CodeChunk(id='models\\\\ollama.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from ell.configurator import config\\nimport openai\\nimport requests\\nimport logging\\n\\n#XXX: May be deprecated soon because of the new provider framework.\\nlogger = logging.getLogger(__name__)\\nclient = None\\n\\ndef register(base_url):\\n    \"\"\"\\n    Registers Ollama models with the provided base URL.\\n\\n    This function sets up the Ollama client with the given base URL and\\n    fetches available models from the Ollama API. It then registers these\\n    models with the global configuration, allowing them to be used within\\n    the ell framework.\\n\\n    Args:\\n        base_url (str): The base URL of the Ollama API endpoint.\\n\\n    Note:\\n        This function updates the global client and configuration.\\n        It logs any errors encountered during the process.\\n    \"\"\"\\n    global client\\n    client = openai.Client(base_url=base_url)\\n\\n    try:\\n        response = requests.get(f\"{base_url}/../api/tags\")\\n        response.raise_for_status()\\n        models = response.json().get(\"models\", [])\\n\\n        for model in models:\\n            config.register_model(model[\"name\"], client)\\n    except requests.RequestException as e:\\n        logger.error(f\"Failed to fetch models from {base_url}: {e}\")\\n    except Exception as e:\\n        logger.error(f\"An error occurred: {e}\")', filepath='src\\\\ell\\\\models\\\\ollama.py', metadata=None, node_id='')]),\n",
       "  ClusteredTopic(name='Provider Integration', chunks=[CodeChunk(id='ell\\\\provider.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from abc import ABC, abstractmethod\\nfrom collections import defaultdict\\nfrom functools import lru_cache\\nimport inspect\\nfrom types import MappingProxyType\\nfrom typing import (\\n    Any,\\n    Callable,\\n    Dict,\\n    FrozenSet,\\n    List,\\n    Optional,\\n    Set,\\n    Tuple,\\n    Type,\\n    TypedDict,\\n    Union,\\n)\\n\\nfrom pydantic import BaseModel, ConfigDict, Field\\nfrom ell.types import Message, ContentBlock, ToolCall\\nfrom ell.types._lstr import _lstr\\nimport json\\nfrom dataclasses import dataclass\\nfrom ell.types.message import LMP', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id='ell\\\\provider.py::1'), CodeChunk(id='ell\\\\provider.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='# XXX: Might leave this internal to providers so that the complex code is simpler &\\n# we can literally jsut call provider.call like any openai fn.\\nclass EllCallParams(BaseModel):\\n    model: str = Field(..., description=\"Model identifier\")\\n    messages: List[Message] = Field(..., description=\"Conversation context\")\\n    client: Any = Field(..., description=\"API client\")\\n    tools: List[LMP] = Field(default_factory=list, description=\"Available tools\")\\n    api_params: Dict[str, Any] = Field(\\n        default_factory=dict, description=\"API parameters\"\\n    )\\n\\n    model_config = ConfigDict(arbitrary_types_allowed=True)\\n\\n    def get_tool_by_name(self, name: str) -> Optional[LMP]:\\n        \"\"\"Get a tool by name.\"\"\"\\n        return next(\\n            (tool for tool in (self.tools or [])  if tool.__name__ == name), None\\n        )\\n\\n\\nMetadata = Dict[str, Any]', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id='ell\\\\provider.py::2'), CodeChunk(id='ell\\\\provider.py::3', input_type=<ClusterInputType.FILE: 'file'>, content='# XXX: Needs a better name.\\nclass Provider(ABC):\\n    \"\"\"\\n    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.\\n    For example, the OpenAI provider is an API interface to OpenAI\\'s API but also to Ollama and Azure OpenAI.\\n    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.\\n    \"\"\"\\n    dangerous_disable_validation = False\\n\\n    ################################\\n    ### API PARAMETERS #############\\n    ################################\\n    @abstractmethod\\n    def provider_call_function(\\n        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None\\n    ) -> Callable[..., Any]:\\n        \"\"\"\\n        Implement this method to return the function that makes the API call to the language model.\\n        For example, if you\\'re implementing the OpenAI provider, you would return the function that makes the API call to OpenAI\\'s API.\\n        \"\"\"\\n        return NotImplemented\\n\\n    def disallowed_api_params(self) -> FrozenSet[str]:\\n        \"\"\"\\n        Returns a list of disallowed call params that ell will override.\\n        \"\"\"\\n        return frozenset({\"messages\", \"tools\", \"model\", \"stream\", \"stream_options\"})\\n\\n    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):\\n        params = _call_params(self.provider_call_function(client, api_params))\\n        return frozenset(params.keys()) - self.disallowed_api_params()\\n\\n    ################################\\n    ### TRANSLATION ###############\\n    ################################\\n    @abstractmethod\\n    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:\\n        \"\"\"Converts an ell call to provider call params!\"\"\"\\n        return NotImplemented\\n\\n    @abstractmethod\\n    def translate_from_provider(\\n        self,\\n        provider_response: Any,\\n        ell_call: EllCallParams,\\n        provider_call_params: Dict[str, Any],\\n        origin_id: Optional[str] = None,\\n        logger: Optional[Callable[..., None]] = None,\\n    ) -> Tuple[List[Message], Metadata]:\\n        \"\"\"Converts provider responses to universal format. with metadata\"\"\"\\n        return NotImplemented\\n\\n    ################################\\n    ### CALL MODEL ################\\n    ################################\\n    # Be careful to override this method in your provider.\\r', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id='ell\\\\provider.py::3'), CodeChunk(id='ell\\\\provider.py::4', input_type=<ClusterInputType.FILE: 'file'>, content='class Provider(ABC):\\n    def call(\\n        self,\\n        #XXX: In future refactors, we can fully enumerate the args and make ell_call\\'s internal to the _provider implementer interface.\\n        # This gives us a litellm style interface for free.\\n        ell_call: EllCallParams,\\n        origin_id: Optional[str] = None,\\n        logger: Optional[Any] = None,\\n    ) -> Tuple[List[Message], Dict[str, Any], Metadata]:\\n        # Automatic validation of params\\n        assert (\\n            not set(ell_call.api_params.keys()).intersection(self.disallowed_api_params()) \\n        ), f\"Disallowed api parameters: {ell_call.api_params}\"\\n\\n        final_api_call_params = self.translate_to_provider(ell_call)\\n\\n        call = self.provider_call_function(ell_call.client, final_api_call_params)\\n        assert self.dangerous_disable_validation or _validate_provider_call_params(final_api_call_params, call)\\n\\n\\n        provider_resp = call(**final_api_call_params)\\n\\n        messages, metadata = self.translate_from_provider(\\n            provider_resp, ell_call, final_api_call_params, origin_id, logger\\n        )\\n        assert \"choices\" not in metadata, \"choices should be in the metadata.\"\\n        assert self.dangerous_disable_validation or _validate_messages_are_tracked(messages, origin_id)\\n\\n        return messages, final_api_call_params, metadata', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id='ell\\\\provider.py::4'), CodeChunk(id='ell\\\\provider.py::5', input_type=<ClusterInputType.FILE: 'file'>, content='# handhold the the implementer, in production mode we can turn these off for speed.\\n@lru_cache(maxsize=None)\\ndef _call_params(call: Callable[..., Any]) -> MappingProxyType[str, inspect.Parameter]:\\n    return inspect.signature(call).parameters\\n\\n\\ndef _validate_provider_call_params(\\n    api_call_params: Dict[str, Any], call: Callable[..., Any]\\n):\\n    provider_call_params = _call_params(call)\\n\\n    required_params = {\\n        name: param\\n        for name, param in provider_call_params.items()\\n        if param.default == param.empty and param.kind != param.VAR_KEYWORD\\n    }\\n\\n    for param_name in required_params:\\n        assert (\\n            param_name in api_call_params\\n        ), f\"Provider implementation error: Required parameter \\'{param_name}\\' is missing in the converted call parameters converted from ell call.\"\\n\\n    for param_name, param_value in api_call_params.items():\\n        assert (\\n            param_name in provider_call_params\\n        ), f\"Provider implementation error: Unexpected parameter \\'{param_name}\\' in the converted call parameters.\"\\n\\n    return True', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id='ell\\\\provider.py::5'), CodeChunk(id='ell\\\\provider.py::6', input_type=<ClusterInputType.FILE: 'file'>, content='def _validate_messages_are_tracked(\\n    messages: List[Message], origin_id: Optional[str] = None\\n):\\n    if origin_id is None:\\n        return\\n\\n    for message in messages:\\n        assert isinstance(\\n            message.text, _lstr\\n        ), f\"Provider implementation error: Message text should be an instance of _lstr, got {type(message.text)}\"\\n        assert (\\n            origin_id in message.text.__origin_trace__\\n        ), f\"Provider implementation error: Message origin_id {message.text.__origin_trace__} does not match the provided origin_id {origin_id}\"\\n    return True', filepath='src\\\\ell\\\\provider.py', metadata=None, node_id='ell\\\\provider.py::6'), CodeChunk(id='providers\\\\openai.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from abc import ABC, abstractmethod\\nfrom collections import defaultdict\\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\\n\\nfrom pydantic import BaseModel\\nfrom ell.provider import  EllCallParams, Metadata, Provider\\nfrom ell.types import Message, ContentBlock, ToolCall\\nfrom ell.types._lstr import _lstr\\nimport json\\nfrom ell.configurator import _Model, config, register_provider\\nfrom ell.types.message import LMP\\nfrom ell.util.serialization import serialize_image\\n\\ntry:\\n    # XXX: Could genericize.\\n    import openai\\n    from openai._streaming import Stream\\n    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam\\n\\n    class OpenAIProvider(Provider):\\n        dangerous_disable_validation = True\\n\\n        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\\n            if api_call_params and (isinstance(fmt := api_call_params.get(\"response_format\"), type)) and issubclass(fmt, BaseModel):\\n                return client.beta.chat.completions.parse\\n            else:\\n                return client.chat.completions.create\\n\\n        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:\\n            final_call_params = ell_call.api_params.copy()\\n            final_call_params[\"model\"] = ell_call.model\\n            # Stream by default for verbose logging.\\n            final_call_params[\"stream\"] = True\\n            final_call_params[\"stream_options\"] = {\"include_usage\": True}\\n\\n            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.\\n            if ell_call.tools or final_call_params.get(\"response_format\") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:\\n                final_call_params.pop(\"stream\", None)\\n                final_call_params.pop(\"stream_options\", None)\\n            if ell_call.tools:\\n                final_call_params.update(\\n                    tool_choice=final_call_params.get(\"tool_choice\", \"auto\"),\\n                    tools=[  \\n                        dict(\\n                            type=\"function\",\\n                            function=dict(\\n                                name=tool.__name__,\\n                                description=tool.__doc__,\\n                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore\\n                            )\\n                        ) for tool in ell_call.tools\\n                    ]\\n                )\\n            # messages\\n            openai_messages : List[ChatCompletionMessageParam] = []\\n            for message in ell_call.messages:\\n                if (tool_calls := message.tool_calls):\\n                    assert message.role == \"assistant\", \"Tool calls must be from the assistant.\"\\n                    assert all(t.tool_call_id for t in tool_calls), \"Tool calls must have tool call ids.\"\\n                    openai_messages.append(dict(\\n                        tool_calls=[\\n                            dict(\\n                                id=cast(str, tool_call.tool_call_id),\\n                                type=\"function\",\\n                                function=dict(\\n                                    name=tool_call.tool.__name__,\\n                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)\\n                                )\\n                            ) for tool_call in tool_calls ],\\n                        role=\"assistant\",\\n                        content=None,\\n                    ))\\n                elif (tool_results := message.tool_results):\\n                    for tool_result in tool_results:\\n                        assert all(cb.type == \"text\" for cb in tool_result.result), \"Tool result does not match expected content blocks.\"\\n                        openai_messages.append(dict(\\n                            role=\"tool\",\\n                            tool_call_id=tool_result.tool_call_id,\\n                            content=tool_result.text_only, \\n                        ))\\n                else:\\n                    openai_messages.append(cast(ChatCompletionMessageParam, dict(\\n                        role=message.role,\\n                        content=[_content_block_to_openai_format(c) for c in message.content] \\n                             if message.role != \"system\" \\n                             else message.text_only\\n                    )))\\n\\n            final_call_params[\"messages\"] = openai_messages\\n\\n            return final_call_params\\n\\n        def translate_from_provider(\\n            self,\\n            provider_response: Union[\\n                ChatCompletion, \\n                ParsedChatCompletion,\\n                Stream[ChatCompletionChunk], Any],\\n            ell_call: EllCallParams,\\n            provider_call_params: Dict[str, Any],\\n            origin_id: Optional[str] = None,\\n            logger: Optional[Callable[..., None]] = None,\\n        ) -> Tuple[List[Message], Metadata]:\\n\\n            metadata : Metadata = {}\\n            messages : List[Message] = []\\n            did_stream = provider_call_params.get(\"stream\", False)\\n\\n\\n            if did_stream:\\n                stream = cast(Stream[ChatCompletionChunk], provider_response)\\n                message_streams = defaultdict(list)\\n                role : Optional[str] = None\\n                for chunk in stream:\\n                    metadata.update(chunk.model_dump(exclude={\"choices\"}))\\n\\n                    for chat_compl_chunk in chunk.choices:\\n                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)\\n                        delta = chat_compl_chunk.delta\\n                        role = role or delta.role\\n                        if  chat_compl_chunk.index == 0 and logger:\\n                            logger(delta.content, is_refusal=hasattr(delta, \"refusal\") and delta.refusal)\\n                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):\\n                    text = \"\".join((choice.delta.content or \"\") for choice in message_stream)\\n                    messages.append(\\n                        Message(role=role, \\n                                content=_lstr(content=text,origin_trace=origin_id)))\\n                    #XXX: Support streaming other types.\\n            else:\\n                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)\\n                metadata = chat_completion.model_dump(exclude={\"choices\"})\\n                for oai_choice in chat_completion.choices:\\n                    role = oai_choice.message.role\\n                    content_blocks = []\\n                    if (hasattr(message := oai_choice.message, \"refusal\") and (refusal := message.refusal)):\\n                        raise ValueError(refusal)\\n                    if hasattr(message, \"parsed\"):\\n                        if (parsed := message.parsed):\\n                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing\\n                            if logger: logger(parsed.model_dump_json())\\n                    else:\\n                        if (content := message.content):\\n                            content_blocks.append(\\n                                ContentBlock(\\n                                    text=_lstr(content=content,origin_trace=origin_id)))\\n                            if logger: logger(content)\\n                        if (tool_calls := message.tool_calls):\\n                            for tool_call in tool_calls:\\n                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)\\n                                assert matching_tool, \"Model called tool not found in provided toolset.\"\\n                                content_blocks.append(\\n                                    ContentBlock(\\n                                        tool_call=ToolCall(\\n                                            tool=matching_tool,\\n                                            tool_call_id=_lstr(\\n                                                tool_call.id, origin_trace= origin_id),\\n                                            params=json.loads(tool_call.function.arguments),\\n                                        )\\n                                    )\\n                                )\\n                                if logger: logger(repr(tool_call))\\n                    messages.append(Message(role=role, content=content_blocks))\\n            return messages, metadata\\n\\n\\n    # xx: singleton needed\\n    openai_provider = OpenAIProvider()\\n    register_provider(openai_provider, openai.Client)\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\openai.py', metadata=None, node_id='providers\\\\openai.py::1'), CodeChunk(id='providers\\\\openai.py::2', input_type=<ClusterInputType.FILE: 'file'>, content='def _content_block_to_openai_format(content_block: ContentBlock) -> Dict[str, Any]:\\n    if (image := content_block.image):\\n        image_url = dict(url=serialize_image(image.image) if image.image else image.url)\\n        # XXX: Solve per content params better\\n        if image.detail: image_url[\"detail\"] = image.detail\\n        return {\\n            \"type\": \"image_url\",\\n            \"image_url\": image_url\\n        }\\n    elif ((text := content_block.text) is not None): return dict(type=\"text\", text=text)\\n    elif (parsed := content_block.parsed): return dict(type=\"text\", text=parsed.model_dump_json())\\n    else:\\n        raise ValueError(f\"Unsupported content block type for openai: {content_block}\")', filepath='src\\\\ell\\\\providers\\\\openai.py', metadata=None, node_id='providers\\\\openai.py::2'), CodeChunk(id='providers\\\\anthropic.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast\\nfrom ell.provider import  EllCallParams, Metadata, Provider\\nfrom ell.types import Message, ContentBlock, ToolCall, ImageContent\\n\\nfrom ell.types._lstr import _lstr\\nfrom ell.types.message import LMP\\nfrom ell.configurator import register_provider\\nfrom ell.util.serialization import serialize_image\\nimport base64\\nfrom io import BytesIO\\nimport json\\nimport requests\\nfrom PIL import Image as PILImage\\n\\ntry:\\n    import anthropic\\n    from anthropic import Anthropic\\n    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent\\n    from anthropic.types.message_create_params import MessageCreateParamsStreaming\\n    from anthropic._streaming import Stream\\n\\n    class AnthropicProvider(Provider):\\n        dangerous_disable_validation = True\\n\\n        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\\n            return client.messages.create\\n\\n        def translate_to_provider(self, ell_call : EllCallParams):\\n            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())\\n            # XXX: Helper, but should be depreicated due to ssot\\n            assert final_call_params.get(\"max_tokens\") is not None, f\"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{\\'max_tokens\\': your_max_tokens}})).\"\\n\\n            dirty_msgs = [\\n                MessageParam(\\n                    role=cast(Literal[\"user\", \"assistant\"], message.role), \\n                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]\\n            role_correct_msgs   : List[MessageParam] = []\\n            for msg in dirty_msgs:\\n                if (not len(role_correct_msgs) or role_correct_msgs[-1][\\'role\\'] != msg[\\'role\\']):\\n                    role_correct_msgs.append(msg)\\n                else: cast(List, role_correct_msgs[-1][\\'content\\']).extend(msg[\\'content\\'])\\n\\n            system_message = None\\n            if role_correct_msgs and role_correct_msgs[0][\"role\"] == \"system\":\\n                system_message = role_correct_msgs.pop(0)\\n\\n            if system_message:\\n                final_call_params[\"system\"] = system_message[\"content\"][0][\"text\"]\\n\\n\\n            final_call_params[\\'stream\\'] = True\\n            final_call_params[\"model\"] = ell_call.model\\n            final_call_params[\"messages\"] = role_correct_msgs\\n\\n            if ell_call.tools:\\n                final_call_params[\"tools\"] = [\\n                    #XXX: Cleaner with LMP\\'s as a class.\\n                    dict(\\n                        name=tool.__name__,\\n                        description=tool.__doc__,\\n                        input_schema=tool.__ell_params_model__.model_json_schema(),\\n                    )\\n                    for tool in ell_call.tools\\n                ]\\n\\n            # print(final_call_params)\\n            return final_call_params\\n\\n        def translate_from_provider(\\n            self,\\n            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],\\n            ell_call: EllCallParams,\\n            provider_call_params: Dict[str, Any],\\n            origin_id: Optional[str] = None,\\n            logger: Optional[Callable[..., None]] = None,\\n        ) -> Tuple[List[Message], Metadata]:\\n\\n            usage = {}\\n            tracked_results = []\\n            metadata = {}\\n\\n            #XXX: Support n > 0\\n\\n            if provider_call_params.get(\"stream\", False):\\n                content = []\\n                current_blocks: Dict[int, Dict[str, Any]] = {}\\n                message_metadata = {}\\n\\n                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:\\n                    for chunk in stream:\\n                        if chunk.type == \"message_start\":\\n                            message_metadata = chunk.message.model_dump()\\n                            message_metadata.pop(\"content\", None)  # Remove content as we\\'ll build it separately\\n\\n                        elif chunk.type == \"content_block_start\":\\n                            block = chunk.content_block.model_dump()\\n                            current_blocks[chunk.index] = block\\n                            if block[\"type\"] == \"tool_use\":\\n                                if logger: logger(f\" <tool_use: {block[\\'name\\']}(\")\\n                                block[\"input\"] = \"\" # force it to be a string, XXX: can implement partially parsed json later.\\n                        elif chunk.type == \"content_block_delta\":\\n                            if chunk.index in current_blocks:\\n                                block = current_blocks[chunk.index]\\n                                if (delta := chunk.delta).type == \"text_delta\":\\n                                    block[\"text\"] += delta.text\\n                                    if logger: logger(delta.text)\\n                                if delta.type == \"input_json_delta\":\\n                                    block[\"input\"] += delta.partial_json\\n                                    if logger: logger(delta.partial_json)\\n\\n                        elif chunk.type == \"content_block_stop\":\\n                            if chunk.index in current_blocks:\\n                                block = current_blocks.pop(chunk.index)\\n                                if block[\"type\"] == \"text\":\\n                                    content.append(ContentBlock(text=_lstr(block[\"text\"],origin_trace=origin_id)))\\n                                elif block[\"type\"] == \"tool_use\":\\n                                    try:\\n                                        matching_tool = ell_call.get_tool_by_name(block[\"name\"])\\n                                        if matching_tool:\\n                                            content.append(\\n                                                ContentBlock(\\n                                                    tool_call=ToolCall(\\n                                                        tool=matching_tool,\\n                                                        tool_call_id=_lstr(\\n                                                            block[\\'id\\'],origin_trace=origin_id\\n                                                        ),\\n                                                        params=json.loads(block[\\'input\\']) if block[\\'input\\'] else {},\\n                                                    )\\n                                                )\\n                                            )\\n                                    except json.JSONDecodeError:\\n                                        if logger: logger(f\" - FAILED TO PARSE JSON\")\\n                                        pass\\n                                    if logger: logger(f\")>\")\\n\\n                        elif chunk.type == \"message_delta\":\\n                            message_metadata.update(chunk.delta.model_dump())\\n                            if chunk.usage:\\n                                usage.update(chunk.usage.model_dump())\\n\\n                        elif chunk.type == \"message_stop\":\\n                            tracked_results.append(Message(role=\"assistant\", content=content))\\n\\n                        # print(chunk)\\n                metadata = message_metadata\\n\\n            # process metadata for ell\\n            # XXX: Unify an ell metadata format for ell studio.\\n            usage[\"prompt_tokens\"] = usage.get(\"input_tokens\", 0)\\n            usage[\"completion_tokens\"] = usage.get(\"output_tokens\", 0)\\n            usage[\"total_tokens\"] = usage[\\'prompt_tokens\\'] + usage[\\'completion_tokens\\']\\n\\n            metadata[\"usage\"] = usage\\n            return tracked_results, metadata\\n\\n    # XXX: Make a singleton.\\n    anthropic_provider = AnthropicProvider()\\n    register_provider(anthropic_provider, anthropic.Anthropic)\\n    register_provider(anthropic_provider, anthropic.AnthropicBedrock)\\n    register_provider(anthropic_provider, anthropic.AnthropicVertex)\\n\\nexcept ImportError:\\n    pass', filepath='src\\\\ell\\\\providers\\\\anthropic.py', metadata=None, node_id='providers\\\\anthropic.py::1'), CodeChunk(id='providers\\\\bedrock.py::1', input_type=<ClusterInputType.FILE: 'file'>, content='from abc import ABC, abstractmethod\\nfrom collections import defaultdict\\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\\nfrom ell.provider import  EllCallParams, Metadata, Provider\\nfrom ell.types import Message, ContentBlock, ToolCall, ImageContent\\nfrom ell.types._lstr import _lstr\\nimport json\\nfrom ell.configurator import config, register_provider\\nfrom ell.types.message import LMP\\nfrom ell.util.serialization import serialize_image\\nfrom io import BytesIO\\nimport requests\\nfrom PIL import Image as PILImage', filepath='src\\\\ell\\\\providers\\\\bedrock.py', metadata=None, node_id='providers\\\\bedrock.py::1')]),\n",
       "  0.4166666666666667,\n",
       "  0.5)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 15 23\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.7\n",
      "Cluster 1: Real-time API Functionality:\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\conversation.py::1\n",
      "openai_realtime\\conversation.py::2\n",
      "\n",
      "\n",
      "B matched:  0.7\n",
      "Real-Time Client and Interaction Handlers:\n",
      "openai_realtime\\__init__.py::1\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\client.py::7\n",
      "\n",
      "\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 15.0)]\n",
      "Result:  Score: 15.0\n",
      "A coherence:  role='assistant' content=[ContentBlock(text=Score: 14.5)] 1.3664507869271207 1.1310022624588287\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 15.0)]\n",
      "Result:  Score: 15.0\n",
      "B coherence:  role='assistant' content=[ContentBlock(text=Score: 15.0)] 1.0496209930261546 1.122930625348124\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.6\n",
      "Cluster 2: Reinforcement Learning with CEM:\n",
      "0.1.0\\cem.py::6\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::6\n",
      "0.1.0\\cpbo.py::7\n",
      "\n",
      "\n",
      "B matched:  0.8571428571428571\n",
      "Cross-Entropy Method Implementation:\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cem.py::6\n",
      "lmp\\complex.py::5\n",
      "\n",
      "\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 14.5)]\n",
      "Result:  Score: 14.5\n",
      "A coherence:  role='assistant' content=[ContentBlock(text=Score: 13.5)] 1.736922555623363 1.8880776108773927\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 11.5)]\n",
      "Result:  Score: 11.5\n",
      "B coherence:  role='assistant' content=[ContentBlock(text=Score: 11.5)] 1.7508702380547272 4.8787433003814025\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.8\n",
      "Cluster 3: Model Configuration and Interaction:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "models\\openai.py::1\n",
      "\n",
      "\n",
      "B matched:  0.8\n",
      "Configuration and Initialization:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "models\\__init__.py::1\n",
      "models\\openai.py::1\n",
      "models\\openai.py::2\n",
      "\n",
      "\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 14.5)]\n",
      "Result:  Score: 14.5\n",
      "A coherence:  role='assistant' content=[ContentBlock(text=Score: 14.5)] 1.2973225790588834 1.6015609160344513\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 15.0)]\n",
      "Result:  Score: 15.0\n",
      "B coherence:  role='assistant' content=[ContentBlock(text=Score: 14.0)] 1.1796288942284858 1.8506060671290216\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.7\n",
      "Cluster 5: Providers and Service Integrations:\n",
      "providers\\anthropic.py::1\n",
      "providers\\anthropic.py::2\n",
      "providers\\anthropic.py::3\n",
      "providers\\openai.py::1\n",
      "providers\\openai.py::2\n",
      "models\\openai.py::2\n",
      "providers\\bedrock.py::1\n",
      "providers\\bedrock.py::2\n",
      "providers\\groq.py::1\n",
      "ell\\configurator.py::5\n",
      "\n",
      "\n",
      "B matched:  0.7\n",
      "Provider Abstraction and Implementation:\n",
      "ell\\provider.py::3\n",
      "providers\\openai.py::1\n",
      "providers\\anthropic.py::1\n",
      "providers\\bedrock.py::1\n",
      "ell\\provider.py::2\n",
      "ell\\provider.py::6\n",
      "providers\\openai.py::2\n",
      "providers\\anthropic.py::2\n",
      "providers\\bedrock.py::2\n",
      "providers\\groq.py::1\n",
      "\n",
      "\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 12.0)]\n",
      "Result:  Score: 12.0\n",
      "A coherence:  role='assistant' content=[ContentBlock(text=Score: 13.0)] 1.3681399629525728 2.314054398070822\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 15.0)]\n",
      "Result:  Score: 15.0\n",
      "B coherence:  role='assistant' content=[ContentBlock(text=Score: 14.5)] 1.3839623427412695 1.285366601694968\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.6666666666666666\n",
      "Message and Content Handling:\n",
      "types\\message.py::1\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::7\n",
      "types\\message.py::8\n",
      "types\\message.py::9\n",
      "types\\message.py::10\n",
      "types\\message.py::11\n",
      "types\\message.py::12\n",
      "types\\message.py::13\n",
      "types\\message.py::14\n",
      "types\\message.py::15\n",
      "\n",
      "\n",
      "B matched:  1.0\n",
      "Message and Content Handling:\n",
      "types\\message.py::1\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::7\n",
      "types\\message.py::8\n",
      "types\\message.py::9\n",
      "types\\message.py::10\n",
      "\n",
      "\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 14.0)]\n",
      "Result:  Score: 14.0\n",
      "A coherence:  role='assistant' content=[ContentBlock(text=Score: 15.0)] 1.164562198094399 1.489691555061506\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=Score: 14.0)]\n",
      "Result:  Score: 14.0\n",
      "B coherence:  role='assistant' content=[ContentBlock(text=Score: 14.0)] 1.2023931963042265 1.4605947998094597\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.5\n",
      "Closure and Dependency Handling:\n",
      "util\\closure.py::15\n",
      "util\\closure.py::1\n",
      "util\\closure.py::3\n",
      "util\\closure.py::6\n",
      "util\\closure.py::7\n",
      "util\\closure.py::10\n",
      "util\\closure.py::11\n",
      "util\\closure.py::13\n",
      "util\\closure_util.py::1\n",
      "util\\closure_util.py::3\n",
      "\n",
      "\n",
      "B matched:  0.5\n",
      "Utilities and Lexical Closure Management:\n",
      "util\\closure.py::11\n",
      "util\\closure.py::12\n",
      "util\\closure.py::13\n",
      "util\\closure.py::14\n",
      "util\\closure.py::15\n",
      "util\\closure.py::16\n",
      "util\\closure.py::17\n",
      "util\\closure.py::18\n",
      "util\\closure_util.py::1\n",
      "util\\closure_util.py::3\n",
      "\n",
      "\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "[ELL] use_cache for eval_coherence_single_lmp: False\n",
      "Return_metadata True\n",
      "Result:  role='assistant' content=[ContentBlock(text=The provided cluster of code revolves around the concept of generating a lexically closured source for a given function. It includes multiple supporting functions and utilities that are consistently focused on the task of analyzing and manipulating Python functions, particularly for the purpose of capturing and handling their dependencies. The snippets share a strong thematic and functional coherence aimed at lexical closure generation, utilizing AST parsing, dependency processing, and source formatting. \n",
      "\n",
      "The provided cluster appears to be a cohesive functional unit with functions that have clear dependencies and interactions with each other to achieve the goal of lexical closure generation. Minor outliers or inconsistencies are either absent or inconsequential to the overall functional aim.\n",
      "\n",
      "Considering these observations, at least 80% of the code clearly belongs together with strong dependencies and shared purpose, leading to the conclusion that this cluster should be rated in the 11-13 range, indicating a closely related and mostly cohesive unit.\n",
      "\n",
      "Score: 12.0)]\n",
      "Result:  The provided cluster of code revolves around the concept of generating a lexically closured source for a given function. It includes multiple supporting functions and utilities that are consistently focused on the task of analyzing and manipulating Python functions, particularly for the purpose of capturing and handling their dependencies. The snippets share a strong thematic and functional coherence aimed at lexical closure generation, utilizing AST parsing, dependency processing, and source formatting. \n",
      "\n",
      "The provided cluster appears to be a cohesive functional unit with functions that have clear dependencies and interactions with each other to achieve the goal of lexical closure generation. Minor outliers or inconsistencies are either absent or inconsequential to the overall functional aim.\n",
      "\n",
      "Considering these observations, at least 80% of the code clearly belongs together with strong dependencies and shared purpose, leading to the conclusion that this cluster should be rated in the 11-13 range, indicating a closely related and mostly cohesive unit.\n",
      "\n",
      "Score: 12.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output regex pattern 'Score\\s*:\\s*(\\d+\\.?\\d*)' did not match any tokens in the response: role='assistant' content=[ContentBlock(text=The provided cluster of code revolves around the concept of generating a lexically closured source for a given function. It includes multiple supporting functions and utilities that are consistently focused on the task of analyzing and manipulating Python functions, particularly for the purpose of capturing and handling their dependencies. The snippets share a strong thematic and functional coherence aimed at lexical closure generation, utilizing AST parsing, dependency processing, and source formatting. \n\nThe provided cluster appears to be a cohesive functional unit with functions that have clear dependencies and interactions with each other to achieve the goal of lexical closure generation. Minor outliers or inconsistencies are either absent or inconsequential to the overall functional aim.\n\nConsidering these observations, at least 80% of the code clearly belongs together with strong dependencies and shared purpose, leading to the conclusion that this cluster should be rated in the 11-13 range, indicating a closely related and mostly cohesive unit.\n\nScore: 12.0)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\llm\\lmp_base\\logprobs.py:46\u001b[0m, in \u001b[0;36mLogProbLMP.call\u001b[1;34m(self, output_rgx, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_rgx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB matched: \u001b[39m\u001b[38;5;124m\"\u001b[39m, b_perc)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n\u001b[1;32m---> 13\u001b[0m score1, score2 \u001b[38;5;241m=\u001b[39m \u001b[43meval_coherence_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m a_result, a_perp1 \u001b[38;5;241m=\u001b[39m score1\n\u001b[0;32m     15\u001b[0m _, a_perp2 \u001b[38;5;241m=\u001b[39m score2\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\llm\\evals\\lmps\\eval_coherence.py:45\u001b[0m, in \u001b[0;36meval_coherence_single\u001b[1;34m(cluster)\u001b[0m\n\u001b[0;32m     42\u001b[0m lmp_func2 \u001b[38;5;241m=\u001b[39m LogProbLMP(eval_coherence_single_lmp)\n\u001b[0;32m     44\u001b[0m result1 \u001b[38;5;241m=\u001b[39m lmp_func\u001b[38;5;241m.\u001b[39mcall(cluster, output_rgx\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m result2 \u001b[38;5;241m=\u001b[39m \u001b[43mlmp_func2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rgx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mScore\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms*:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms*(\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.?\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md*)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m perp1 \u001b[38;5;241m=\u001b[39m lmp_func\u001b[38;5;241m.\u001b[39mlogprobs()\u001b[38;5;241m.\u001b[39mperplexity()\n\u001b[0;32m     48\u001b[0m perp2 \u001b[38;5;241m=\u001b[39m lmp_func2\u001b[38;5;241m.\u001b[39mlogprobs()\u001b[38;5;241m.\u001b[39mperplexity()\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\llm\\lmp_base\\logprobs.py:48\u001b[0m, in \u001b[0;36mLogProbLMP.call\u001b[1;34m(self, output_rgx, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(output_rgx, result\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput regex pattern \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_rgx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m did not match any tokens in the response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mValueError\u001b[0m: Output regex pattern 'Score\\s*:\\s*(\\d+\\.?\\d*)' did not match any tokens in the response: role='assistant' content=[ContentBlock(text=The provided cluster of code revolves around the concept of generating a lexically closured source for a given function. It includes multiple supporting functions and utilities that are consistently focused on the task of analyzing and manipulating Python functions, particularly for the purpose of capturing and handling their dependencies. The snippets share a strong thematic and functional coherence aimed at lexical closure generation, utilizing AST parsing, dependency processing, and source formatting. \n\nThe provided cluster appears to be a cohesive functional unit with functions that have clear dependencies and interactions with each other to achieve the goal of lexical closure generation. Minor outliers or inconsistencies are either absent or inconsequential to the overall functional aim.\n\nConsidering these observations, at least 80% of the code clearly belongs together with strong dependencies and shared purpose, leading to the conclusion that this cluster should be rated in the 11-13 range, indicating a closely related and mostly cohesive unit.\n\nScore: 12.0)]"
     ]
    }
   ],
   "source": [
    "from src.llm.evals.lmps.eval_coherence import eval_coherence_single\n",
    "from src.llm.evals.utils import match_clusters\n",
    "\n",
    "matched = match_clusters(summary_clusters, vanilla_clusters, min_match=5)\n",
    "print(len(matched), len(summary_clusters), len(vanilla_clusters))\n",
    "\n",
    "for a, b, a_perc, b_perc in matched:\n",
    "    print(\"Matched clusters: __________________________\")\n",
    "    print(\"A matched: \", a_perc)\n",
    "    print(a)\n",
    "    print(\"B matched: \", b_perc)\n",
    "    print(b)\n",
    "    # score1, score2 = eval_coherence_single(a )\n",
    "    # a_result, a_perp1 = score1\n",
    "    # _, a_perp2 = score2\n",
    "    # print(\"A coherence: \", a_result, a_perp1, a_perp2)\n",
    "    # score1, score2 = eval_coherence_single(b )\n",
    "    # b_result, b_perp1 = score1\n",
    "    # _, b_perp2 = score2\n",
    "    # print(\"B coherence: \", b_result, b_perp1, b_perp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 17 17\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.5\n",
      "Language Model API Management:\n",
      "lmp\\complex.py::2\n",
      "providers\\openai.py::1\n",
      "ell\\provider.py::3\n",
      "providers\\openai.py::2\n",
      "ell\\provider.py::4\n",
      "providers\\anthropic.py::1\n",
      "providers\\bedrock.py::2\n",
      "ell\\provider.py::2\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "\n",
      "\n",
      "B matched:  0.625\n",
      "Provider and API Interaction:\n",
      "ell\\provider.py::1\n",
      "ell\\provider.py::2\n",
      "ell\\provider.py::3\n",
      "ell\\provider.py::4\n",
      "providers\\openai.py::1\n",
      "providers\\bedrock.py::1\n",
      "providers\\anthropic.py::1\n",
      "providers\\groq.py::1\n",
      "\n",
      "\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.7\n",
      "Real-Time API and Event Handling:\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\conversation.py::1\n",
      "openai_realtime\\conversation.py::2\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\event_handler.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::4\n",
      "\n",
      "\n",
      "B matched:  0.7\n",
      "Real-time API Connection and Management:\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\client.py::6\n",
      "\n",
      "\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.5\n",
      "Message Handling and Content Processing:\n",
      "types\\message.py::12\n",
      "types\\message.py::9\n",
      "types\\message.py::6\n",
      "types\\message.py::15\n",
      "types\\message.py::7\n",
      "types\\message.py::13\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::1\n",
      "types\\message.py::8\n",
      "\n",
      "\n",
      "B matched:  0.5\n",
      "Message and Content Handling:\n",
      "types\\message.py::1\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::7\n",
      "types\\studio.py::1\n",
      "types\\studio.py::2\n",
      "types\\studio.py::3\n",
      "\n",
      "\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.8\n",
      "ELL Provider and Configuration Management:\n",
      "ell\\provider.py::3\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::1\n",
      "ell\\__init__.py::1\n",
      "\n",
      "\n",
      "B matched:  0.8\n",
      "Model Configuration and Registration:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "models\\__init__.py::1\n",
      "\n",
      "\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.4166666666666667\n",
      "Content and Message Management:\n",
      "types\\message.py::6\n",
      "types\\message.py::9\n",
      "types\\message.py::3\n",
      "types\\message.py::2\n",
      "types\\message.py::5\n",
      "types\\message.py::7\n",
      "types\\message.py::8\n",
      "types\\message.py::15\n",
      "types\\message.py::12\n",
      "types\\message.py::10\n",
      "openai_realtime\\client.py::6\n",
      "openai_realtime\\conversation.py::4\n",
      "\n",
      "\n",
      "B matched:  0.5\n",
      "Message and Content Handling:\n",
      "types\\message.py::1\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::7\n",
      "types\\studio.py::1\n",
      "types\\studio.py::2\n",
      "types\\studio.py::3\n",
      "\n",
      "\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.5454545454545454\n",
      "Real-Time Communication and Event Handling:\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\event_handler.py::1\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\conversation.py::1\n",
      "openai_realtime\\conversation.py::2\n",
      "openai_realtime\\client.py::9\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\conversation.py::6\n",
      "\n",
      "\n",
      "B matched:  0.6\n",
      "Real-time API Connection and Management:\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\client.py::6\n",
      "\n",
      "\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.9\n",
      "Language Model Program Configuration:\n",
      "lmp\\complex.py::1\n",
      "lmp\\complex.py::3\n",
      "lmp\\complex.py::4\n",
      "lmp\\complex.py::5\n",
      "lmp\\simple.py::1\n",
      "lmp\\simple.py::2\n",
      "lmp\\tool.py::1\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::3\n",
      "lmp\\tool.py::4\n",
      "\n",
      "\n",
      "B matched:  0.9\n",
      "LMP and Complex Interactions:\n",
      "lmp\\complex.py::1\n",
      "lmp\\complex.py::2\n",
      "lmp\\complex.py::3\n",
      "lmp\\complex.py::4\n",
      "lmp\\simple.py::1\n",
      "lmp\\simple.py::2\n",
      "lmp\\tool.py::1\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::3\n",
      "lmp\\tool.py::4\n",
      "\n",
      "\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.9\n",
      "Lexical Closure and Code Introspection:\n",
      "util\\closure.py::1\n",
      "util\\closure.py::2\n",
      "util\\closure.py::3\n",
      "util\\closure.py::4\n",
      "util\\closure.py::5\n",
      "util\\closure.py::6\n",
      "util\\closure.py::7\n",
      "util\\closure.py::8\n",
      "util\\closure.py::9\n",
      "util\\closure.py::10\n",
      "\n",
      "\n",
      "B matched:  0.9\n",
      "Function and Execution Closure Handling:\n",
      "util\\closure.py::2\n",
      "util\\closure.py::3\n",
      "util\\closure.py::4\n",
      "util\\closure.py::5\n",
      "util\\closure.py::6\n",
      "util\\closure.py::7\n",
      "util\\closure.py::8\n",
      "util\\closure.py::9\n",
      "util\\closure.py::10\n",
      "util\\closure_util.py::1\n",
      "\n",
      "\n",
      "Matched clusters: __________________________\n",
      "A matched:  0.45454545454545453\n",
      "SQLAlchemy and Data Persistence:\n",
      "types\\studio.py::5\n",
      "stores\\sql.py::1\n",
      "stores\\sql.py::2\n",
      "stores\\sql.py::4\n",
      "stores\\sql.py::5\n",
      "stores\\sql.py::11\n",
      "studio\\server.py::1\n",
      "studio\\server.py::4\n",
      "studio\\server.py::5\n",
      "studio\\server.py::6\n",
      "studio\\connection_manager.py::1\n",
      "\n",
      "\n",
      "B matched:  0.5\n",
      "Server and API Functionality:\n",
      "studio\\server.py::1\n",
      "studio\\server.py::2\n",
      "studio\\server.py::3\n",
      "studio\\server.py::4\n",
      "studio\\server.py::5\n",
      "studio\\server.py::6\n",
      "studio\\server.py::7\n",
      "studio\\connection_manager.py::1\n",
      "studio\\config.py::1\n",
      "studio\\datamodels.py::1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.llm.evals.utils import match_clusters\n",
    "\n",
    "\n",
    "matched = match_clusters(summary_clusters, vanilla_clusters, min_match=5)\n",
    "print(len(matched), len(summary_clusters), len(vanilla_clusters))\n",
    "\n",
    "for a, b, a_perc, b_perc in matched:\n",
    "    print(\"Matched clusters: __________________________\")\n",
    "    print(\"A matched: \", a_perc)\n",
    "    print(a)\n",
    "    print(\"B matched: \", b_perc)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wwff\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Message(role='assistant', content=[ContentBlock(text=It looks like there might be a typo in your question. If you're asking about \"the index,\" could you please clarify which index you are referring to? For example, are you asking about a stock market index, such as the S&P 500 or Dow Jones Industrial Average, an economic index like the Consumer Price Index, or something else entirely? Let me know so I can assist you better!)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.llm.evals.lmps.eval_compare import test_ell\n",
    "\n",
    "test_ell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given the following set of source code chunks:\n",
      "from abc import ABC, abstractmethod\n",
      "from collections import defaultdict\n",
      "from functools import lru_cache\n",
      "import inspect\n",
      "from types import MappingProxyType\n",
      "from typing import (\n",
      "    Any,\n",
      "    Callable,\n",
      "    Dict,\n",
      "    FrozenSet,\n",
      "    List,\n",
      "    Optional,\n",
      "    Set,\n",
      "    Tuple,\n",
      "    Type,\n",
      "    TypedDict,\n",
      "    Union,\n",
      ")\n",
      "\n",
      "from pydantic import BaseModel, ConfigDict, Field\n",
      "from ell.types import Message, ContentBlock, ToolCall\n",
      "from ell.types._lstr import _lstr\n",
      "import json\n",
      "from dataclasses import dataclass\n",
      "from ell.types.message import LMP\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _content_block_to_openai_format(content_block: ContentBlock) -> Dict[str, Any]:\n",
      "    if (image := content_block.image):\n",
      "        image_url = dict(url=serialize_image(image.image) if image.image else image.url)\n",
      "        # XXX: Solve per content params better\n",
      "        if image.detail: image_url[\"detail\"] = image.detail\n",
      "        return {\n",
      "            \"type\": \"image_url\",\n",
      "            \"image_url\": image_url\n",
      "        }\n",
      "    elif ((text := content_block.text) is not None): return dict(type=\"text\", text=text)\n",
      "    elif (parsed := content_block.parsed): return dict(type=\"text\", text=parsed.model_dump_json())\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported content block type for openai: {content_block}\")\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from abc import ABC, abstractmethod\n",
      "from collections import defaultdict\n",
      "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\n",
      "\n",
      "from pydantic import BaseModel\n",
      "from ell.provider import  EllCallParams, Metadata, Provider\n",
      "from ell.types import Message, ContentBlock, ToolCall\n",
      "from ell.types._lstr import _lstr\n",
      "import json\n",
      "from ell.configurator import _Model, config, register_provider\n",
      "from ell.types.message import LMP\n",
      "from ell.util.serialization import serialize_image\n",
      "\n",
      "try:\n",
      "    # XXX: Could genericize.\n",
      "    import openai\n",
      "    from openai._streaming import Stream\n",
      "    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam\n",
      "\n",
      "    class OpenAIProvider(Provider):\n",
      "        dangerous_disable_validation = True\n",
      "\n",
      "        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\n",
      "            if api_call_params and (isinstance(fmt := api_call_params.get(\"response_format\"), type)) and issubclass(fmt, BaseModel):\n",
      "                return client.beta.chat.completions.parse\n",
      "            else:\n",
      "                return client.chat.completions.create\n",
      "\n",
      "        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:\n",
      "            final_call_params = ell_call.api_params.copy()\n",
      "            final_call_params[\"model\"] = ell_call.model\n",
      "            # Stream by default for verbose logging.\n",
      "            final_call_params[\"stream\"] = True\n",
      "            final_call_params[\"stream_options\"] = {\"include_usage\": True}\n",
      "\n",
      "            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.\n",
      "            if ell_call.tools or final_call_params.get(\"response_format\") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:\n",
      "                final_call_params.pop(\"stream\", None)\n",
      "                final_call_params.pop(\"stream_options\", None)\n",
      "            if ell_call.tools:\n",
      "                final_call_params.update(\n",
      "                    tool_choice=final_call_params.get(\"tool_choice\", \"auto\"),\n",
      "                    tools=[  \n",
      "                        dict(\n",
      "                            type=\"function\",\n",
      "                            function=dict(\n",
      "                                name=tool.__name__,\n",
      "                                description=tool.__doc__,\n",
      "                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore\n",
      "                            )\n",
      "                        ) for tool in ell_call.tools\n",
      "                    ]\n",
      "                )\n",
      "            # messages\n",
      "            openai_messages : List[ChatCompletionMessageParam] = []\n",
      "            for message in ell_call.messages:\n",
      "                if (tool_calls := message.tool_calls):\n",
      "                    assert message.role == \"assistant\", \"Tool calls must be from the assistant.\"\n",
      "                    assert all(t.tool_call_id for t in tool_calls), \"Tool calls must have tool call ids.\"\n",
      "                    openai_messages.append(dict(\n",
      "                        tool_calls=[\n",
      "                            dict(\n",
      "                                id=cast(str, tool_call.tool_call_id),\n",
      "                                type=\"function\",\n",
      "                                function=dict(\n",
      "                                    name=tool_call.tool.__name__,\n",
      "                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)\n",
      "                                )\n",
      "                            ) for tool_call in tool_calls ],\n",
      "                        role=\"assistant\",\n",
      "                        content=None,\n",
      "                    ))\n",
      "                elif (tool_results := message.tool_results):\n",
      "                    for tool_result in tool_results:\n",
      "                        assert all(cb.type == \"text\" for cb in tool_result.result), \"Tool result does not match expected content blocks.\"\n",
      "                        openai_messages.append(dict(\n",
      "                            role=\"tool\",\n",
      "                            tool_call_id=tool_result.tool_call_id,\n",
      "                            content=tool_result.text_only, \n",
      "                        ))\n",
      "                else:\n",
      "                    openai_messages.append(cast(ChatCompletionMessageParam, dict(\n",
      "                        role=message.role,\n",
      "                        content=[_content_block_to_openai_format(c) for c in message.content] \n",
      "                             if message.role != \"system\" \n",
      "                             else message.text_only\n",
      "                    )))\n",
      "\n",
      "            final_call_params[\"messages\"] = openai_messages\n",
      "\n",
      "            return final_call_params\n",
      "\n",
      "        def translate_from_provider(\n",
      "            self,\n",
      "            provider_response: Union[\n",
      "                ChatCompletion, \n",
      "                ParsedChatCompletion,\n",
      "                Stream[ChatCompletionChunk], Any],\n",
      "            ell_call: EllCallParams,\n",
      "            provider_call_params: Dict[str, Any],\n",
      "            origin_id: Optional[str] = None,\n",
      "            logger: Optional[Callable[..., None]] = None,\n",
      "        ) -> Tuple[List[Message], Metadata]:\n",
      "\n",
      "            metadata : Metadata = {}\n",
      "            messages : List[Message] = []\n",
      "            did_stream = provider_call_params.get(\"stream\", False)\n",
      "\n",
      "\n",
      "            if did_stream:\n",
      "                stream = cast(Stream[ChatCompletionChunk], provider_response)\n",
      "                message_streams = defaultdict(list)\n",
      "                role : Optional[str] = None\n",
      "                for chunk in stream:\n",
      "                    metadata.update(chunk.model_dump(exclude={\"choices\"}))\n",
      "\n",
      "                    for chat_compl_chunk in chunk.choices:\n",
      "                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)\n",
      "                        delta = chat_compl_chunk.delta\n",
      "                        role = role or delta.role\n",
      "                        if  chat_compl_chunk.index == 0 and logger:\n",
      "                            logger(delta.content, is_refusal=hasattr(delta, \"refusal\") and delta.refusal)\n",
      "                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):\n",
      "                    text = \"\".join((choice.delta.content or \"\") for choice in message_stream)\n",
      "                    messages.append(\n",
      "                        Message(role=role, \n",
      "                                content=_lstr(content=text,origin_trace=origin_id)))\n",
      "                    #XXX: Support streaming other types.\n",
      "            else:\n",
      "                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)\n",
      "                metadata = chat_completion.model_dump(exclude={\"choices\"})\n",
      "                for oai_choice in chat_completion.choices:\n",
      "                    role = oai_choice.message.role\n",
      "                    content_blocks = []\n",
      "                    if (hasattr(message := oai_choice.message, \"refusal\") and (refusal := message.refusal)):\n",
      "                        raise ValueError(refusal)\n",
      "                    if hasattr(message, \"parsed\"):\n",
      "                        if (parsed := message.parsed):\n",
      "                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing\n",
      "                            if logger: logger(parsed.model_dump_json())\n",
      "                    else:\n",
      "                        if (content := message.content):\n",
      "                            content_blocks.append(\n",
      "                                ContentBlock(\n",
      "                                    text=_lstr(content=content,origin_trace=origin_id)))\n",
      "                            if logger: logger(content)\n",
      "                        if (tool_calls := message.tool_calls):\n",
      "                            for tool_call in tool_calls:\n",
      "                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)\n",
      "                                assert matching_tool, \"Model called tool not found in provided toolset.\"\n",
      "                                content_blocks.append(\n",
      "                                    ContentBlock(\n",
      "                                        tool_call=ToolCall(\n",
      "                                            tool=matching_tool,\n",
      "                                            tool_call_id=_lstr(\n",
      "                                                tool_call.id, origin_trace= origin_id),\n",
      "                                            params=json.loads(tool_call.function.arguments),\n",
      "                                        )\n",
      "                                    )\n",
      "                                )\n",
      "                                if logger: logger(repr(tool_call))\n",
      "                    messages.append(Message(role=role, content=content_blocks))\n",
      "            return messages, metadata\n",
      "\n",
      "\n",
      "    # xx: singleton needed\n",
      "    openai_provider = OpenAIProvider()\n",
      "    register_provider(openai_provider, openai.Client)\n",
      "except ImportError:\n",
      "    pass\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "try:\n",
      "    from botocore.client import BaseClient\n",
      "    from botocore.eventstream import (EventStream)\n",
      "    class BedrockProvider(Provider):\n",
      "        dangerous_disable_validation = True\n",
      "\n",
      "        def provider_call_function(self, client : Any, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\n",
      "            if api_call_params and api_call_params.get(\"stream\", False):\n",
      "                api_call_params.pop('stream')\n",
      "                return client.converse_stream\n",
      "            else:\n",
      "                return client.converse\n",
      "\n",
      "        def translate_to_provider(self, ell_call : EllCallParams):\n",
      "            final_call_params = {}\n",
      "\n",
      "            if ell_call.api_params.get('api_params',{}).get('stream', False):\n",
      "                final_call_params['stream'] = ell_call.api_params.get('api_params',{}).get('stream', False)\n",
      "\n",
      "            bedrock_converse_messages = [message_to_bedrock_message_format(message) for message in ell_call.messages]\n",
      "\n",
      "            system_message = None\n",
      "            if bedrock_converse_messages and bedrock_converse_messages[0][\"role\"] == \"system\":\n",
      "                system_message = bedrock_converse_messages.pop(0)\n",
      "\n",
      "            if system_message:\n",
      "                final_call_params[\"system\"] = [{'text':system_message[\"content\"][0][\"text\"]}]\n",
      "\n",
      "            final_call_params[\"modelId\"] = ell_call.model\n",
      "            final_call_params[\"messages\"] = bedrock_converse_messages\n",
      "\n",
      "            if ell_call.tools:\n",
      "                tools = [\n",
      "                    #XXX: Cleaner with LMP's as a class.\n",
      "                    dict(\n",
      "                        toolSpec = dict(\n",
      "                            name=tool.__name__,\n",
      "                            description=tool.__doc__,\n",
      "                            inputSchema=dict(\n",
      "                                json=tool.__ell_params_model__.model_json_schema(),\n",
      "                            )\n",
      "                        )\n",
      "                    )\n",
      "                    for tool in ell_call.tools\n",
      "                ]\n",
      "                final_call_params[\"toolConfig\"] = {'tools':tools}\n",
      "\n",
      "            return final_call_params\n",
      "\n",
      "        def translate_from_provider(\n",
      "                self,\n",
      "                provider_response: Union[EventStream, Any],\n",
      "                ell_call: EllCallParams,\n",
      "                provider_call_params: Dict[str, Any],\n",
      "                origin_id: Optional[str] = None,\n",
      "                logger: Optional[Callable[..., None]] = None,\n",
      "            ) -> Tuple[List[Message], Metadata]:\n",
      "\n",
      "            usage = {}\n",
      "            metadata : Metadata = {}\n",
      "\n",
      "            metadata : Metadata = {}\n",
      "            tracked_results : List[Message] = []\n",
      "            did_stream = ell_call.api_params.get(\"api_params\", {}).get('stream')\n",
      "\n",
      "            if did_stream:\n",
      "                content = []\n",
      "                current_block: Optional[Dict[str, Any]] = {}\n",
      "                message_metadata = {}\n",
      "                for chunk in provider_response.get('stream'):\n",
      "\n",
      "                    if \"messageStart\" in chunk:\n",
      "                        current_block['content'] = ''\n",
      "                        pass\n",
      "                    elif \"contentBlockStart\" in chunk:\n",
      "                        pass\n",
      "                    elif \"contentBlockDelta\" in chunk:\n",
      "                        delta = chunk.get(\"contentBlockDelta\", {}).get(\"delta\", {})\n",
      "                        if \"text\" in delta:\n",
      "                            current_block['type'] = 'text'\n",
      "                            current_block['content'] += delta.get(\"text\")\n",
      "                            if logger:\n",
      "                                logger(delta.get(\"text\"))\n",
      "                        else:\n",
      "                            pass\n",
      "                    elif \"contentBlockStop\" in chunk:\n",
      "                        if current_block is not None:\n",
      "                            if current_block[\"type\"] == \"text\":\n",
      "                                content.append(ContentBlock(text=_lstr(content=content, origin_trace=origin_id)))\n",
      "\n",
      "                    elif \"messageStop\" in chunk:\n",
      "                        tracked_results.append(Message(role=\"assistant\", content=content))\n",
      "\n",
      "                    elif \"metadata\" in chunk:\n",
      "                        if \"usage\" in chunk[\"metadata\"]:\n",
      "                            usage[\"prompt_tokens\"] = chunk[\"metadata\"].get('usage').get(\"inputTokens\", 0)\n",
      "                            usage[\"completion_tokens\"] = chunk[\"metadata\"].get('usage').get(\"outputTokens\", 0)\n",
      "                            usage[\"total_tokens\"] = usage['prompt_tokens'] + usage['completion_tokens']\n",
      "                            message_metadata[\"usage\"] = usage\n",
      "                    else:\n",
      "                        pass\n",
      "\n",
      "\n",
      "                metadata = message_metadata\n",
      "            else:\n",
      "                # Non-streaming response processing (unchanged)\n",
      "                cbs = []\n",
      "                for content_block in provider_response.get('output', {}).get('message', {}).get('content', []):\n",
      "                    if 'text' in content_block:\n",
      "                        cbs.append(ContentBlock(text=_lstr(content_block.get('text'), origin_trace=origin_id)))\n",
      "                    elif 'toolUse' in content_block:\n",
      "                        assert ell_call.tools is not None, \"Tools were not provided to the model when calling it and yet bedrock returned a tool use.\"\n",
      "                        try:\n",
      "                            toolUse = content_block['toolUse']\n",
      "                            matching_tool = ell_call.get_tool_by_name(toolUse[\"name\"])\n",
      "                            if matching_tool:\n",
      "                                cbs.append(\n",
      "                                    ContentBlock(\n",
      "                                        tool_call=ToolCall(\n",
      "                                            tool=matching_tool,\n",
      "                                            tool_call_id=_lstr(\n",
      "                                                toolUse['toolUseId'],origin_trace=origin_id\n",
      "                                            ),\n",
      "                                            params=toolUse['input'],\n",
      "                                        )\n",
      "                                    )\n",
      "                                )\n",
      "                        except json.JSONDecodeError:\n",
      "                            if logger: logger(f\" - FAILED TO PARSE JSON\")\n",
      "                            pass\n",
      "                tracked_results.append(Message(role=\"assistant\", content=cbs))\n",
      "                if logger:\n",
      "                    logger(tracked_results[0].text)\n",
      "\n",
      "\n",
      "                # usage = call_result.response.usage.dict() if call_result.response.get('usage') else {}\n",
      "                # metadata = call_result.response.model_dump()\n",
      "                # del metadata[\"content\"]\n",
      "\n",
      "            # process metadata for ell\n",
      "            # XXX: Unify an ell metadata format for ell studio.\n",
      "            usage[\"prompt_tokens\"] = usage.get(\"inputTokens\", 0)\n",
      "            usage[\"completion_tokens\"] = usage.get(\"outputTokens\", 0)\n",
      "            usage[\"total_tokens\"] = usage['prompt_tokens'] + usage['completion_tokens']\n",
      "\n",
      "            metadata[\"usage\"] = usage\n",
      "            return tracked_results, metadata\n",
      "\n",
      "\n",
      "    # XXX: Make a singleton.\n",
      "    register_provider(BedrockProvider(), BaseClient)\n",
      "except ImportError:\n",
      "    pass\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast\n",
      "from ell.provider import  EllCallParams, Metadata, Provider\n",
      "from ell.types import Message, ContentBlock, ToolCall, ImageContent\n",
      "\n",
      "from ell.types._lstr import _lstr\n",
      "from ell.types.message import LMP\n",
      "from ell.configurator import register_provider\n",
      "from ell.util.serialization import serialize_image\n",
      "import base64\n",
      "from io import BytesIO\n",
      "import json\n",
      "import requests\n",
      "from PIL import Image as PILImage\n",
      "\n",
      "try:\n",
      "    import anthropic\n",
      "    from anthropic import Anthropic\n",
      "    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent\n",
      "    from anthropic.types.message_create_params import MessageCreateParamsStreaming\n",
      "    from anthropic._streaming import Stream\n",
      "\n",
      "    class AnthropicProvider(Provider):\n",
      "        dangerous_disable_validation = True\n",
      "\n",
      "        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\n",
      "            return client.messages.create\n",
      "\n",
      "        def translate_to_provider(self, ell_call : EllCallParams):\n",
      "            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())\n",
      "            # XXX: Helper, but should be depreicated due to ssot\n",
      "            assert final_call_params.get(\"max_tokens\") is not None, f\"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}})).\"\n",
      "\n",
      "            dirty_msgs = [\n",
      "                MessageParam(\n",
      "                    role=cast(Literal[\"user\", \"assistant\"], message.role), \n",
      "                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]\n",
      "            role_correct_msgs   : List[MessageParam] = []\n",
      "            for msg in dirty_msgs:\n",
      "                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):\n",
      "                    role_correct_msgs.append(msg)\n",
      "                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])\n",
      "\n",
      "            system_message = None\n",
      "            if role_correct_msgs and role_correct_msgs[0][\"role\"] == \"system\":\n",
      "                system_message = role_correct_msgs.pop(0)\n",
      "\n",
      "            if system_message:\n",
      "                final_call_params[\"system\"] = system_message[\"content\"][0][\"text\"]\n",
      "\n",
      "\n",
      "            final_call_params['stream'] = True\n",
      "            final_call_params[\"model\"] = ell_call.model\n",
      "            final_call_params[\"messages\"] = role_correct_msgs\n",
      "\n",
      "            if ell_call.tools:\n",
      "                final_call_params[\"tools\"] = [\n",
      "                    #XXX: Cleaner with LMP's as a class.\n",
      "                    dict(\n",
      "                        name=tool.__name__,\n",
      "                        description=tool.__doc__,\n",
      "                        input_schema=tool.__ell_params_model__.model_json_schema(),\n",
      "                    )\n",
      "                    for tool in ell_call.tools\n",
      "                ]\n",
      "\n",
      "            # print(final_call_params)\n",
      "            return final_call_params\n",
      "\n",
      "        def translate_from_provider(\n",
      "            self,\n",
      "            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],\n",
      "            ell_call: EllCallParams,\n",
      "            provider_call_params: Dict[str, Any],\n",
      "            origin_id: Optional[str] = None,\n",
      "            logger: Optional[Callable[..., None]] = None,\n",
      "        ) -> Tuple[List[Message], Metadata]:\n",
      "\n",
      "            usage = {}\n",
      "            tracked_results = []\n",
      "            metadata = {}\n",
      "\n",
      "            #XXX: Support n > 0\n",
      "\n",
      "            if provider_call_params.get(\"stream\", False):\n",
      "                content = []\n",
      "                current_blocks: Dict[int, Dict[str, Any]] = {}\n",
      "                message_metadata = {}\n",
      "\n",
      "                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:\n",
      "                    for chunk in stream:\n",
      "                        if chunk.type == \"message_start\":\n",
      "                            message_metadata = chunk.message.model_dump()\n",
      "                            message_metadata.pop(\"content\", None)  # Remove content as we'll build it separately\n",
      "\n",
      "                        elif chunk.type == \"content_block_start\":\n",
      "                            block = chunk.content_block.model_dump()\n",
      "                            current_blocks[chunk.index] = block\n",
      "                            if block[\"type\"] == \"tool_use\":\n",
      "                                if logger: logger(f\" <tool_use: {block['name']}(\")\n",
      "                                block[\"input\"] = \"\" # force it to be a string, XXX: can implement partially parsed json later.\n",
      "                        elif chunk.type == \"content_block_delta\":\n",
      "                            if chunk.index in current_blocks:\n",
      "                                block = current_blocks[chunk.index]\n",
      "                                if (delta := chunk.delta).type == \"text_delta\":\n",
      "                                    block[\"text\"] += delta.text\n",
      "                                    if logger: logger(delta.text)\n",
      "                                if delta.type == \"input_json_delta\":\n",
      "                                    block[\"input\"] += delta.partial_json\n",
      "                                    if logger: logger(delta.partial_json)\n",
      "\n",
      "                        elif chunk.type == \"content_block_stop\":\n",
      "                            if chunk.index in current_blocks:\n",
      "                                block = current_blocks.pop(chunk.index)\n",
      "                                if block[\"type\"] == \"text\":\n",
      "                                    content.append(ContentBlock(text=_lstr(block[\"text\"],origin_trace=origin_id)))\n",
      "                                elif block[\"type\"] == \"tool_use\":\n",
      "                                    try:\n",
      "                                        matching_tool = ell_call.get_tool_by_name(block[\"name\"])\n",
      "                                        if matching_tool:\n",
      "                                            content.append(\n",
      "                                                ContentBlock(\n",
      "                                                    tool_call=ToolCall(\n",
      "                                                        tool=matching_tool,\n",
      "                                                        tool_call_id=_lstr(\n",
      "                                                            block['id'],origin_trace=origin_id\n",
      "                                                        ),\n",
      "                                                        params=json.loads(block['input']) if block['input'] else {},\n",
      "                                                    )\n",
      "                                                )\n",
      "                                            )\n",
      "                                    except json.JSONDecodeError:\n",
      "                                        if logger: logger(f\" - FAILED TO PARSE JSON\")\n",
      "                                        pass\n",
      "                                    if logger: logger(f\")>\")\n",
      "\n",
      "                        elif chunk.type == \"message_delta\":\n",
      "                            message_metadata.update(chunk.delta.model_dump())\n",
      "                            if chunk.usage:\n",
      "                                usage.update(chunk.usage.model_dump())\n",
      "\n",
      "                        elif chunk.type == \"message_stop\":\n",
      "                            tracked_results.append(Message(role=\"assistant\", content=content))\n",
      "\n",
      "                        # print(chunk)\n",
      "                metadata = message_metadata\n",
      "\n",
      "            # process metadata for ell\n",
      "            # XXX: Unify an ell metadata format for ell studio.\n",
      "            usage[\"prompt_tokens\"] = usage.get(\"input_tokens\", 0)\n",
      "            usage[\"completion_tokens\"] = usage.get(\"output_tokens\", 0)\n",
      "            usage[\"total_tokens\"] = usage['prompt_tokens'] + usage['completion_tokens']\n",
      "\n",
      "            metadata[\"usage\"] = usage\n",
      "            return tracked_results, metadata\n",
      "\n",
      "    # XXX: Make a singleton.\n",
      "    anthropic_provider = AnthropicProvider()\n",
      "    register_provider(anthropic_provider, anthropic.Anthropic)\n",
      "    register_provider(anthropic_provider, anthropic.AnthropicBedrock)\n",
      "    register_provider(anthropic_provider, anthropic.AnthropicVertex)\n",
      "\n",
      "except ImportError:\n",
      "    pass\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from abc import ABC, abstractmethod\n",
      "from collections import defaultdict\n",
      "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\n",
      "from ell.provider import  EllCallParams, Metadata, Provider\n",
      "from ell.types import Message, ContentBlock, ToolCall, ImageContent\n",
      "from ell.types._lstr import _lstr\n",
      "import json\n",
      "from ell.configurator import config, register_provider\n",
      "from ell.types.message import LMP\n",
      "from ell.util.serialization import serialize_image\n",
      "from io import BytesIO\n",
      "import requests\n",
      "from PIL import Image as PILImage\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Provider(ABC):\n",
      "    def call(\n",
      "        self,\n",
      "        #XXX: In future refactors, we can fully enumerate the args and make ell_call's internal to the _provider implementer interface.\n",
      "        # This gives us a litellm style interface for free.\n",
      "        ell_call: EllCallParams,\n",
      "        origin_id: Optional[str] = None,\n",
      "        logger: Optional[Any] = None,\n",
      "    ) -> Tuple[List[Message], Dict[str, Any], Metadata]:\n",
      "        # Automatic validation of params\n",
      "        assert (\n",
      "            not set(ell_call.api_params.keys()).intersection(self.disallowed_api_params()) \n",
      "        ), f\"Disallowed api parameters: {ell_call.api_params}\"\n",
      "\n",
      "        final_api_call_params = self.translate_to_provider(ell_call)\n",
      "\n",
      "        call = self.provider_call_function(ell_call.client, final_api_call_params)\n",
      "        assert self.dangerous_disable_validation or _validate_provider_call_params(final_api_call_params, call)\n",
      "\n",
      "\n",
      "        provider_resp = call(**final_api_call_params)\n",
      "\n",
      "        messages, metadata = self.translate_from_provider(\n",
      "            provider_resp, ell_call, final_api_call_params, origin_id, logger\n",
      "        )\n",
      "        assert \"choices\" not in metadata, \"choices should be in the metadata.\"\n",
      "        assert self.dangerous_disable_validation or _validate_messages_are_tracked(messages, origin_id)\n",
      "\n",
      "        return messages, final_api_call_params, metadata\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):\n",
      "    default_client_from_decorator = client\n",
      "    default_model_from_decorator = model\n",
      "    default_api_params_from_decorator = api_params\n",
      "    def parameterized_lm_decorator(\n",
      "        prompt: LMP,\n",
      "    ) -> Callable[..., Union[List[Message], Message]]:\n",
      "        _warnings(model, prompt, default_client_from_decorator)\n",
      "\n",
      "        @wraps(prompt)\n",
      "        def model_call(\n",
      "            *prompt_args,\n",
      "            _invocation_origin : Optional[str] = None,\n",
      "            client: Optional[Any] = None,\n",
      "            api_params: Optional[Dict[str, Any]] = None,\n",
      "            lm_params: Optional[DeprecationWarning] = None,\n",
      "            **prompt_kwargs,\n",
      "        ) -> Tuple[Any, Any, Any]:\n",
      "            # XXX: Deprecation in 0.1.0\n",
      "            if lm_params:\n",
      "                raise DeprecationWarning(\"lm_params is deprecated. Use api_params instead.\")\n",
      "\n",
      "            # promt -> str\n",
      "            res = prompt(*prompt_args, **prompt_kwargs)\n",
      "            # Convert prompt into ell messages\n",
      "            messages = _get_messages(res, prompt)\n",
      "\n",
      "            # XXX: move should log to a logger.\n",
      "            should_log = not exempt_from_tracking and config.verbose\n",
      "            # Cute verbose logging.\n",
      "            if should_log: model_usage_logger_pre(prompt, prompt_args, prompt_kwargs, \"[]\", messages) #type: ignore\n",
      "\n",
      "            # Call the model.\n",
      "            # Merge API params\n",
      "            merged_api_params = {**config.default_api_params, **default_api_params_from_decorator, **(api_params or {})}\n",
      "            n = merged_api_params.get(\"n\", 1)\n",
      "            # Merge client overrides & client registry\n",
      "            merged_client = _client_for_model(model, client or default_client_from_decorator)\n",
      "            ell_call = EllCallParams(\n",
      "                # XXX: Could change behaviour of overriding ell params for dyanmic tool calls.\n",
      "                model=merged_api_params.pop(\"model\", default_model_from_decorator),\n",
      "                messages=messages,\n",
      "                client = merged_client,\n",
      "                api_params=merged_api_params,\n",
      "                tools=tools or [],\n",
      "            )\n",
      "            # Get the provider for the model\n",
      "            provider = config.get_provider_for(ell_call.client)\n",
      "            assert provider is not None, f\"No provider found for client {ell_call.client}.\"\n",
      "\n",
      "            if should_log: model_usage_logger_post_start(n)\n",
      "            with model_usage_logger_post_intermediate(n) as _logger:\n",
      "                (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)\n",
      "                if isinstance(result, list) and len(result) == 1:\n",
      "                    result = result[0]\n",
      "\n",
      "            result = post_callback(result) if post_callback else result\n",
      "            if should_log:\n",
      "                model_usage_logger_post_end()\n",
      "            #\n",
      "            #  These get sent to track. This is wack.           \n",
      "            return result, final_api_params, metadata\n",
      "        # ... other code\n",
      "    # ... other code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Config(BaseModel):\n",
      "\n",
      "    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:\n",
      "        \"\"\"\n",
      "        Register a provider class for a specific client type.\n",
      "\n",
      "        :param provider_class: The provider class to register.\n",
      "        :type provider_class: Type[Provider]\n",
      "        \"\"\"\n",
      "        assert isinstance(client_type, type), \"client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))\"\n",
      "        with self._lock:\n",
      "            self.providers[client_type] = provider\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Config(BaseModel):\n",
      "\n",
      "    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:\n",
      "        \"\"\"\n",
      "        Get the OpenAI client for a specific model name.\n",
      "\n",
      "        :param model_name: The name of the model to get the client for.\n",
      "        :type model_name: str\n",
      "        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.\n",
      "        :rtype: Tuple[Optional[openai.Client], bool]\n",
      "        \"\"\"\n",
      "        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry\n",
      "        model_config = current_registry.get(model_name)\n",
      "        fallback = False\n",
      "        if not model_config:\n",
      "            warning_message = f\"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables.\"\n",
      "            if self.verbose:\n",
      "                from colorama import Fore, Style\n",
      "                _config_logger.warning(f\"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}\")\n",
      "            else:\n",
      "                _config_logger.debug(warning_message)\n",
      "            client = self.default_client\n",
      "            fallback = True\n",
      "        else:\n",
      "            client = model_config.default_client\n",
      "        return client, fallback\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\"\"\"\n",
      "Groq provider.\n",
      "\"\"\"\n",
      "\n",
      "from ell.providers.openai import OpenAIProvider\n",
      "from ell.configurator import register_provider\n",
      "\n",
      "\n",
      "try:\n",
      "    import groq\n",
      "    class GroqProvider(OpenAIProvider):\n",
      "        dangerous_disable_validation = True\n",
      "        def translate_to_provider(self, *args, **kwargs):\n",
      "            params = super().translate_to_provider(*args, **kwargs)\n",
      "            params.pop('stream_options', None)\n",
      "            return params\n",
      "\n",
      "        def translate_from_provider(self, *args, **kwargs):\n",
      "            res, meta = super().translate_from_provider(*args, **kwargs)\n",
      "            if not meta['usage']:\n",
      "                meta['usage'] = meta['x_groq']['usage']\n",
      "            return res, meta\n",
      "    register_provider(GroqProvider(), groq.Client)\n",
      "except ImportError:\n",
      "    pass\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# XXX: Needs a better name.\n",
      "class Provider(ABC):\n",
      "    \"\"\"\n",
      "    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.\n",
      "    For example, the OpenAI provider is an API interface to OpenAI's API but also to Ollama and Azure OpenAI.\n",
      "    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.\n",
      "    \"\"\"\n",
      "    dangerous_disable_validation = False\n",
      "\n",
      "    ################################\n",
      "    ### API PARAMETERS #############\n",
      "    ################################\n",
      "    @abstractmethod\n",
      "    def provider_call_function(\n",
      "        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None\n",
      "    ) -> Callable[..., Any]:\n",
      "        \"\"\"\n",
      "        Implement this method to return the function that makes the API call to the language model.\n",
      "        For example, if you're implementing the OpenAI provider, you would return the function that makes the API call to OpenAI's API.\n",
      "        \"\"\"\n",
      "        return NotImplemented\n",
      "\n",
      "    def disallowed_api_params(self) -> FrozenSet[str]:\n",
      "        \"\"\"\n",
      "        Returns a list of disallowed call params that ell will override.\n",
      "        \"\"\"\n",
      "        return frozenset({\"messages\", \"tools\", \"model\", \"stream\", \"stream_options\"})\n",
      "\n",
      "    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):\n",
      "        params = _call_params(self.provider_call_function(client, api_params))\n",
      "        return frozenset(params.keys()) - self.disallowed_api_params()\n",
      "\n",
      "    ################################\n",
      "    ### TRANSLATION ###############\n",
      "    ################################\n",
      "    @abstractmethod\n",
      "    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:\n",
      "        \"\"\"Converts an ell call to provider call params!\"\"\"\n",
      "        return NotImplemented\n",
      "\n",
      "    @abstractmethod\n",
      "    def translate_from_provider(\n",
      "        self,\n",
      "        provider_response: Any,\n",
      "        ell_call: EllCallParams,\n",
      "        provider_call_params: Dict[str, Any],\n",
      "        origin_id: Optional[str] = None,\n",
      "        logger: Optional[Callable[..., None]] = None,\n",
      "    ) -> Tuple[List[Message], Metadata]:\n",
      "        \"\"\"Converts provider responses to universal format. with metadata\"\"\"\n",
      "        return NotImplemented\n",
      "\n",
      "    ################################\n",
      "    ### CALL MODEL ################\n",
      "    ################################\n",
      "    # Be careful to override this method in your provider.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# XXX: Might leave this internal to providers so that the complex code is simpler &\n",
      "# we can literally jsut call provider.call like any openai fn.\n",
      "class EllCallParams(BaseModel):\n",
      "    model: str = Field(..., description=\"Model identifier\")\n",
      "    messages: List[Message] = Field(..., description=\"Conversation context\")\n",
      "    client: Any = Field(..., description=\"API client\")\n",
      "    tools: List[LMP] = Field(default_factory=list, description=\"Available tools\")\n",
      "    api_params: Dict[str, Any] = Field(\n",
      "        default_factory=dict, description=\"API parameters\"\n",
      "    )\n",
      "\n",
      "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
      "\n",
      "    def get_tool_by_name(self, name: str) -> Optional[LMP]:\n",
      "        \"\"\"Get a tool by name.\"\"\"\n",
      "        return next(\n",
      "            (tool for tool in (self.tools or [])  if tool.__name__ == name), None\n",
      "        )\n",
      "\n",
      "\n",
      "Metadata = Dict[str, Any]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Your task is to compare the following clusters and choose which cluster forms a more cohesive grouping of source code chunks. You should\n",
      "penalize a cluster for including too much unrelated code or for missing important code that should have been included.\n",
      "Here are the clusters:\n",
      "Cluster 0:\n",
      "Language Model API Management:\n",
      "lmp\\complex.py::2\n",
      "providers\\openai.py::1\n",
      "ell\\provider.py::3\n",
      "providers\\openai.py::2\n",
      "ell\\provider.py::4\n",
      "providers\\anthropic.py::1\n",
      "providers\\bedrock.py::2\n",
      "ell\\provider.py::2\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cluster 1:\n",
      "Provider and API Interaction:\n",
      "ell\\provider.py::1\n",
      "ell\\provider.py::2\n",
      "ell\\provider.py::3\n",
      "ell\\provider.py::4\n",
      "providers\\openai.py::1\n",
      "providers\\bedrock.py::1\n",
      "providers\\anthropic.py::1\n",
      "providers\\groq.py::1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now output your decision by selecting the number of the cluster that you would score higher on overall cohesion\n",
      "\n",
      "\n",
      "You are given the following set of source code chunks:\n",
      "class RealtimeAPI(RealtimeEventHandler):\n",
      "\n",
      "    async def connect(self, model='gpt-4o-realtime-preview-2024-10-01'):\n",
      "        if self.is_connected():\n",
      "            raise Exception(\"Already connected\")\n",
      "\n",
      "        headers = {\n",
      "            'Authorization': f'Bearer {self.api_key}',\n",
      "            'OpenAI-Beta': 'realtime=v1'\n",
      "        }\n",
      "\n",
      "        self.ws = await websockets.connect(f\"{self.url}?model={model}\", extra_headers=headers)\n",
      "\n",
      "        self.log(f\"Connected to {self.url}\")\n",
      "\n",
      "        asyncio.create_task(self._message_handler())\n",
      "\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "import numpy as np\n",
      "import json\n",
      "from .utils import RealtimeUtils\n",
      "import copy\n",
      "\n",
      "class RealtimeConversation:\n",
      "    def __init__(self):\n",
      "        self.default_frequency = 24000  # 24,000 Hz\n",
      "        self.clear()\n",
      "\n",
      "    def clear(self):\n",
      "        self.item_lookup = {}\n",
      "        self.items = []\n",
      "        self.response_lookup = {}\n",
      "        self.responses = []\n",
      "        self.queued_speech_items = {}\n",
      "        self.queued_transcript_items = {}\n",
      "        self.queued_input_audio = None\n",
      "        return True\n",
      "\n",
      "    def queue_input_audio(self, input_audio):\n",
      "        self.queued_input_audio = input_audio\n",
      "        return input_audio\n",
      "\n",
      "    def process_event(self, event, *args):\n",
      "        if 'event_id' not in event:\n",
      "            raise ValueError(\"Missing 'event_id' on event\")\n",
      "        if 'type' not in event:\n",
      "            raise ValueError(\"Missing 'type' on event\")\n",
      "\n",
      "        event_processor = getattr(self, f\"_process_{event['type'].replace('.', '_')}\", None)\n",
      "        if not event_processor:\n",
      "            raise ValueError(f\"Missing conversation event processor for '{event['type']}'\")\n",
      "\n",
      "        return event_processor(event, *args)\n",
      "\n",
      "    def get_item(self, id):\n",
      "        return self.item_lookup.get(id)\n",
      "\n",
      "    def get_items(self):\n",
      "        return self.items.copy()\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    def _add_api_event_handlers(self):\n",
      "        self.realtime.on('client.*', lambda event: self.dispatch('realtime.event', {\n",
      "            'time': RealtimeUtils.generate_id('time_'),\n",
      "            'source': 'client',\n",
      "            'event': event\n",
      "        }))\n",
      "        self.realtime.on('server.*', lambda event: self.dispatch('realtime.event', {\n",
      "            'time': RealtimeUtils.generate_id('time_'),\n",
      "            'source': 'server',\n",
      "            'event': event\n",
      "        }))\n",
      "        self.realtime.on('server.session.created', lambda _: setattr(self, 'session_created', True))\n",
      "\n",
      "        def handle_conversation_event(event, *args):\n",
      "            result = self.conversation.process_event(event, *args)\n",
      "            if result['item']:\n",
      "                self.dispatch('conversation.updated', result)\n",
      "            return result\n",
      "\n",
      "        self.realtime.on('server.response.created', handle_conversation_event)\n",
      "        self.realtime.on('server.response.output_item.added', handle_conversation_event)\n",
      "        self.realtime.on('server.response.content_part.added', handle_conversation_event)\n",
      "        self.realtime.on('server.input_audio_buffer.speech_started', lambda event: (\n",
      "            handle_conversation_event(event),\n",
      "            self.dispatch('conversation.interrupted', event)\n",
      "        ))\n",
      "        self.realtime.on('server.input_audio_buffer.speech_stopped', lambda event: \n",
      "            handle_conversation_event(event, self.input_audio_buffer)\n",
      "        )\n",
      "        self.realtime.on('server.conversation.item.created', lambda event: (\n",
      "            handle_conversation_event(event),\n",
      "            self.dispatch('conversation.item.appended', {'item': event['item']})\n",
      "        ))\n",
      "        self.realtime.on('server.conversation.item.truncated', handle_conversation_event)\n",
      "        self.realtime.on('server.conversation.item.deleted', handle_conversation_event)\n",
      "        self.realtime.on('server.conversation.item.input_audio_transcription.completed', handle_conversation_event)\n",
      "        self.realtime.on('server.response.audio_transcript.delta', handle_conversation_event)\n",
      "        self.realtime.on('server.response.audio.delta', handle_conversation_event)\n",
      "        self.realtime.on('server.response.text.delta', handle_conversation_event)\n",
      "        self.realtime.on('server.response.function_call_arguments.delta', handle_conversation_event)\n",
      "        def handle_output_item_done( event):\n",
      "            handle_conversation_event(event)\n",
      "            item = event.get('item', {})\n",
      "\n",
      "            if item.get('status') == 'completed':\n",
      "                self.dispatch('conversation.item.completed', {'item': item})\n",
      "\n",
      "            formatted = item.get('formatted', {})\n",
      "            tool = formatted.get('tool') if isinstance(formatted, dict) else None\n",
      "\n",
      "            if tool:\n",
      "                asyncio.create_task(self._call_tool(tool))\n",
      "        self.realtime.on('server.response.output_item.done', handle_output_item_done)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import websockets\n",
      "from .event_handler import RealtimeEventHandler\n",
      "from .utils import RealtimeUtils\n",
      "\n",
      "class RealtimeAPI(RealtimeEventHandler):\n",
      "    def __init__(self, url=None, api_key=None, dangerously_allow_api_key_in_browser=False, debug=False):\n",
      "        super().__init__()\n",
      "        self.default_url = 'wss://api.openai.com/v1/realtime'\n",
      "        self.url = url or self.default_url\n",
      "        self.api_key = api_key\n",
      "        self.debug = debug\n",
      "        self.ws = None\n",
      "\n",
      "    def is_connected(self):\n",
      "        return self.ws is not None and self.ws.open\n",
      "\n",
      "    def log(self, *args):\n",
      "        if self.debug:\n",
      "            print(*args)\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "import asyncio\n",
      "from typing import Callable, Dict, List, Any\n",
      "\n",
      "class RealtimeEventHandler:\n",
      "    def __init__(self):\n",
      "        self.event_handlers: Dict[str, List[Callable]] = {}\n",
      "        self.next_event_handlers: Dict[str, List[Callable]] = {}\n",
      "\n",
      "    def clear_event_handlers(self):\n",
      "        self.event_handlers.clear()\n",
      "        self.next_event_handlers.clear()\n",
      "        return True\n",
      "\n",
      "    def on(self, event_name: str, callback: Callable = None):\n",
      "        def decorator(func):\n",
      "            if event_name not in self.event_handlers:\n",
      "                self.event_handlers[event_name] = []\n",
      "            self.event_handlers[event_name].append(func)\n",
      "            return func\n",
      "\n",
      "        if callback is None:\n",
      "            return decorator\n",
      "        else:\n",
      "            return decorator(callback)\n",
      "\n",
      "    def on_next(self, event_name: str, callback: Callable):\n",
      "        if event_name not in self.next_event_handlers:\n",
      "            self.next_event_handlers[event_name] = []\n",
      "        self.next_event_handlers[event_name].append(callback)\n",
      "\n",
      "    def off(self, event_name: str, callback: Callable = None):\n",
      "        if event_name in self.event_handlers:\n",
      "            if callback:\n",
      "                self.event_handlers[event_name].remove(callback)\n",
      "            else:\n",
      "                del self.event_handlers[event_name]\n",
      "        return True\n",
      "\n",
      "    def off_next(self, event_name: str, callback: Callable = None):\n",
      "        if event_name in self.next_event_handlers:\n",
      "            if callback:\n",
      "                self.next_event_handlers[event_name].remove(callback)\n",
      "            else:\n",
      "                del self.next_event_handlers[event_name]\n",
      "        return True\n",
      "\n",
      "    async def wait_for_next(self, event_name: str, timeout: float = None):\n",
      "        next_event = None\n",
      "        def set_next_event(event):\n",
      "            nonlocal next_event\n",
      "            next_event = event\n",
      "\n",
      "        self.on_next(event_name, set_next_event)\n",
      "\n",
      "        start_time = asyncio.get_event_loop().time()\n",
      "        while not next_event:\n",
      "            if timeout and asyncio.get_event_loop().time() - start_time > timeout:\n",
      "                return None\n",
      "            await asyncio.sleep(0.001)\n",
      "\n",
      "        return next_event\n",
      "\n",
      "    def dispatch(self, event_name: str, event: Any):\n",
      "        handlers = self.event_handlers.get(event_name, []).copy()\n",
      "        for handler in handlers:\n",
      "            handler(event)\n",
      "\n",
      "        next_handlers = self.next_event_handlers.pop(event_name, [])\n",
      "        for next_handler in next_handlers:\n",
      "            next_handler(event)\n",
      "\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "\n",
      "\n",
      "    def is_connected(self):\n",
      "        return self.realtime.is_connected() and self.session_created\n",
      "\n",
      "    def reset(self):\n",
      "        self.disconnect()\n",
      "        self.clear_event_handlers()\n",
      "        self.realtime.clear_event_handlers()\n",
      "        self._reset_config()\n",
      "        self._add_api_event_handlers()\n",
      "        return True\n",
      "\n",
      "    async def connect(self):\n",
      "        if self.is_connected():\n",
      "            raise Exception(\"Already connected, use .disconnect() first\")\n",
      "        await self.realtime.connect()\n",
      "        self.update_session()\n",
      "        return True\n",
      "\n",
      "    async def wait_for_session_created(self):\n",
      "        if not self.realtime.is_connected():\n",
      "            raise Exception(\"Not connected, use .connect() first\")\n",
      "        while not self.session_created:\n",
      "            await asyncio.sleep(0.001)\n",
      "        return True\n",
      "\n",
      "    def disconnect(self):\n",
      "        self.session_created = False\n",
      "        self.conversation.clear()\n",
      "        if self.realtime.is_connected():\n",
      "            self.realtime.disconnect()\n",
      "\n",
      "    def get_turn_detection_type(self):\n",
      "        turn_detection = self.session_config.get('turn_detection')\n",
      "        if isinstance(turn_detection, dict):\n",
      "            return turn_detection.get('type')\n",
      "        return None\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    def add_tool(self, definition, handler):\n",
      "        if not definition.get('name'):\n",
      "            raise ValueError(\"Missing tool name in definition\")\n",
      "        name = definition['name']\n",
      "        if name in self.tools:\n",
      "            raise ValueError(f\"Tool '{name}' already added. Please use .remove_tool('{name}') before trying to add again.\")\n",
      "        if not callable(handler):\n",
      "            raise ValueError(f\"Tool '{name}' handler must be a function\")\n",
      "        self.tools[name] = {'definition': definition, 'handler': handler}\n",
      "        self.update_session()\n",
      "        return self.tools[name]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "import asyncio\n",
      "import numpy as np\n",
      "from .event_handler import RealtimeEventHandler\n",
      "from .api import RealtimeAPI\n",
      "from .conversation import RealtimeConversation\n",
      "from .utils import RealtimeUtils\n",
      "import json\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "    def __init__(self, url=None, api_key=None, instructions='', dangerously_allow_api_key_in_browser=False, debug=False):\n",
      "        super().__init__()\n",
      "        self.default_session_config = {\n",
      "            'modalities': ['text', 'audio'],\n",
      "            'instructions': instructions,\n",
      "            'voice': 'alloy',\n",
      "            'input_audio_format': 'pcm16',\n",
      "            'output_audio_format': 'pcm16',\n",
      "            'input_audio_transcription': None,\n",
      "            'turn_detection': None,\n",
      "            'tools': [],\n",
      "            'tool_choice': 'auto',\n",
      "            'temperature': 0.8,\n",
      "            'max_response_output_tokens': 4096,\n",
      "        }\n",
      "        self.session_config = {}\n",
      "        self.transcription_models = [{'model': 'whisper-1'}]\n",
      "        self.default_server_vad_config = {\n",
      "            'type': 'server_vad',\n",
      "            'threshold': 0.5,\n",
      "            'prefix_padding_ms': 300,\n",
      "            'silence_duration_ms': 200,\n",
      "        }\n",
      "        self.realtime = RealtimeAPI(url, api_key, dangerously_allow_api_key_in_browser, debug)\n",
      "        self.conversation = RealtimeConversation()\n",
      "        self._reset_config()\n",
      "        self._add_api_event_handlers()\n",
      "\n",
      "    def _reset_config(self):\n",
      "        self.session_created = False\n",
      "        self.tools = {}\n",
      "        self.session_config = self.default_session_config.copy()\n",
      "        self.input_audio_buffer = np.array([], dtype=np.int16)\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    def remove_tool(self, name):\n",
      "        if name not in self.tools:\n",
      "            raise ValueError(f\"Tool '{name}' does not exist, cannot be removed.\")\n",
      "        del self.tools[name]\n",
      "        return True\n",
      "\n",
      "    def delete_item(self, id):\n",
      "        self.realtime.send('conversation.item.delete', {'item_id': id})\n",
      "        return True\n",
      "\n",
      "    def update_session(self, **kwargs):\n",
      "        self.session_config.update(kwargs)\n",
      "        use_tools = [\n",
      "            {**tool.get('definition', {}), 'type': 'function'}\n",
      "            for tool in self.tools.values()\n",
      "        ]\n",
      "        session = {**self.session_config, 'tools': use_tools}\n",
      "        if self.realtime.is_connected():\n",
      "            self.realtime.send('session.update', {'session': session})\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    def send_user_message_content(self, content=None):\n",
      "        content = content or []\n",
      "        for c in content:\n",
      "            if c['type'] == 'input_audio':\n",
      "                if isinstance(c['audio'], (np.ndarray, bytes)):\n",
      "                    c['audio'] = RealtimeUtils.array_buffer_to_base64(c['audio'])\n",
      "        if content:\n",
      "            self.realtime.send('conversation.item.create', {\n",
      "                'item': {\n",
      "                    'type': 'message',\n",
      "                    'role': 'user',\n",
      "                    'content': content\n",
      "                }\n",
      "            })\n",
      "        self.create_response()\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeAPI(RealtimeEventHandler):\n",
      "\n",
      "    async def _message_handler(self):\n",
      "        try:\n",
      "            async for message in self.ws:\n",
      "                data = json.loads(message)\n",
      "                self.receive(data['type'], data)\n",
      "        except websockets.exceptions.ConnectionClosed:\n",
      "            self.disconnect()\n",
      "            self.dispatch('close', {'error': True})\n",
      "\n",
      "    def disconnect(self):\n",
      "        if self.ws:\n",
      "            asyncio.create_task(self.ws.close())\n",
      "            self.ws = None\n",
      "        return True\n",
      "\n",
      "    def receive(self, event_name, event):\n",
      "        self.log(\"received:\", event_name, event)\n",
      "        self.dispatch(f\"server.{event_name}\", event)\n",
      "        self.dispatch(\"server.*\", event)\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeConversation:\n",
      "\n",
      "    def _process_conversation_item_created(self, event):\n",
      "        item = event['item']\n",
      "        new_item = copy.deepcopy(item)\n",
      "        if new_item['id'] not in self.item_lookup:\n",
      "            self.item_lookup[new_item['id']] = new_item\n",
      "            self.items.append(new_item)\n",
      "\n",
      "        new_item['formatted'] = {\n",
      "            'audio': np.array([], dtype=np.int16),\n",
      "            'text': '',\n",
      "            'transcript': ''\n",
      "        }\n",
      "\n",
      "        if new_item['type'] == 'message':\n",
      "            if new_item['role'] == 'user':\n",
      "                new_item['status'] = 'completed'\n",
      "                if self.queued_input_audio is not None:\n",
      "                    new_item['formatted']['audio'] = self.queued_input_audio\n",
      "                    self.queued_input_audio = None\n",
      "            else:\n",
      "                new_item['status'] = 'in_progress'\n",
      "        elif new_item['type'] == 'function_call':\n",
      "            new_item['formatted']['tool'] = {\n",
      "                'type': 'function',\n",
      "                'name': new_item['name'],\n",
      "                'call_id': new_item['call_id'],\n",
      "                'arguments': ''\n",
      "            }\n",
      "            new_item['status'] = 'in_progress'\n",
      "        elif new_item['type'] == 'function_call_output':\n",
      "            new_item['status'] = 'completed'\n",
      "            new_item['formatted']['output'] = new_item['output']\n",
      "\n",
      "        if new_item.get('content'):\n",
      "            text_content = [c for c in new_item['content'] if c['type'] in ['text', 'input_text']]\n",
      "            for content in text_content:\n",
      "                new_item['formatted']['text'] += content['text']\n",
      "\n",
      "        if new_item['id'] in self.queued_speech_items:\n",
      "            new_item['formatted']['audio'] = self.queued_speech_items[new_item['id']]['audio']\n",
      "            del self.queued_speech_items[new_item['id']]\n",
      "\n",
      "        if new_item['id'] in self.queued_transcript_items:\n",
      "            new_item['formatted']['transcript'] = self.queued_transcript_items[new_item['id']]['transcript']\n",
      "            del self.queued_transcript_items[new_item['id']]\n",
      "\n",
      "        return {'item': new_item, 'delta': None}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeAPI(RealtimeEventHandler):\n",
      "\n",
      "    def send(self, event_name, data=None):\n",
      "        if not self.is_connected():\n",
      "            raise Exception(\"RealtimeAPI is not connected\")\n",
      "\n",
      "        data = data or {}\n",
      "        if not isinstance(data, dict):\n",
      "            raise ValueError(\"data must be a dictionary\")\n",
      "\n",
      "        event = {\n",
      "            \"event_id\": RealtimeUtils.generate_id(\"evt_\"),\n",
      "            \"type\": event_name,\n",
      "            **data\n",
      "        }\n",
      "\n",
      "        self.dispatch(f\"client.{event_name}\", event)\n",
      "        self.dispatch(\"client.*\", event)\n",
      "        self.log(\"sent:\", event_name, event)\n",
      "\n",
      "        asyncio.create_task(self.ws.send(json.dumps(event, ensure_ascii=False)))\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Your task is to compare the following clusters and choose which cluster forms a more cohesive grouping of source code chunks. You should\n",
      "penalize a cluster for including too much unrelated code or for missing important code that should have been included.\n",
      "Here are the clusters:\n",
      "Cluster 0:\n",
      "Real-Time API and Event Handling:\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\conversation.py::1\n",
      "openai_realtime\\conversation.py::2\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\event_handler.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::4\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cluster 1:\n",
      "Real-time API Connection and Management:\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\client.py::6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now output your decision by selecting the number of the cluster that you would score higher on overall cohesion\n",
      "\n",
      "\n",
      "You are given the following set of source code chunks:\n",
      "class ContentBlock(BaseModel):\n",
      "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
      "\n",
      "    text: Optional[_lstr_generic] = Field(default=None)\n",
      "    image: Optional[ImageContent] = Field(default=None)\n",
      "    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)\n",
      "    tool_call: Optional[ToolCall] = Field(default=None)\n",
      "    parsed: Optional[BaseModel] = Field(default=None)\n",
      "    tool_result: Optional[ToolResult] = Field(default=None)\n",
      "    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.\n",
      "    # This breaks us maintaing parity with the openai python client in some sen but so does image.\n",
      "\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        if \"image\" in kwargs and not isinstance(kwargs[\"image\"], ImageContent):\n",
      "            im = kwargs[\"image\"] = ImageContent.coerce(kwargs[\"image\"])\n",
      "            # XXX: Backwards compatibility, Deprecate.\n",
      "            if (d := kwargs.get(\"image_detail\", None)): im.detail = d\n",
      "\n",
      "        super().__init__(*args, **kwargs)\n",
      "\n",
      "\n",
      "    @model_validator(mode='after')\n",
      "    def check_single_non_null(self):\n",
      "        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]\n",
      "        if len(non_null_fields) > 1:\n",
      "            raise ValueError(f\"Only one field can be non-null. Found: {', '.join(non_null_fields)}\")\n",
      "        return self\n",
      "\n",
      "    def __str__(self):\n",
      "        return repr(self)\n",
      "\n",
      "    def __repr__(self):\n",
      "        non_null_fields = [f\"{field}={value}\" for field, value in self.__dict__.items() if value is not None]\n",
      "        return f\"ContentBlock({', '.join(non_null_fields)})\"\n",
      "\n",
      "    @property\n",
      "    def type(self):\n",
      "        if self.text is not None:\n",
      "            return \"text\"\n",
      "        if self.image is not None:\n",
      "            return \"image\"\n",
      "        if self.audio is not None:\n",
      "            return \"audio\"\n",
      "        if self.tool_call is not None:\n",
      "            return \"tool_call\"\n",
      "        if self.parsed is not None:\n",
      "            return \"parsed\"\n",
      "        if self.tool_result is not None:\n",
      "            return \"tool_result\"\n",
      "        return None\n",
      "\n",
      "    @property\n",
      "    def content(self):\n",
      "        return getattr(self, self.type)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# todo: implement tracing for structured outs. this a v2 feature.\n",
      "import json\n",
      "from ell.types._lstr import _lstr\n",
      "from functools import cached_property\n",
      "import numpy as np\n",
      "import base64\n",
      "from io import BytesIO\n",
      "from PIL import Image as PILImage\n",
      "\n",
      "from pydantic import BaseModel, ConfigDict, model_validator, field_serializer\n",
      "from sqlmodel import Field\n",
      "\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "\n",
      "from typing import Any, Callable, Dict, List, Optional, Union\n",
      "\n",
      "from ell.util.serialization import serialize_image\n",
      "_lstr_generic = Union[_lstr, str]\n",
      "InvocableTool = Callable[..., Union[\"ToolResult\", _lstr_generic, List[\"ContentBlock\"], ]]\n",
      "\n",
      "# AnyContent represents any type that can be passed to Message.\n",
      "AnyContent = Union[\"ContentBlock\", str, \"ToolCall\", \"ToolResult\", \"ImageContent\", np.ndarray, PILImage.Image, BaseModel]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class SerializedLMPUses(SQLModel, table=True):\n",
      "    \"\"\"\n",
      "    Represents the many-to-many relationship between SerializedLMPs.\n",
      "\n",
      "    This class is used to track which LMPs use or are used by other LMPs.\n",
      "    \"\"\"\n",
      "\n",
      "    lmp_user_id: Optional[str] = Field(default=None, foreign_key=\"serializedlmp.lmp_id\", primary_key=True, index=True)  # ID of the LMP that is being used\n",
      "    lmp_using_id: Optional[str] = Field(default=None, foreign_key=\"serializedlmp.lmp_id\", primary_key=True, index=True)  # ID of the LMP that is using the other LMP\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Message(BaseModel):\n",
      "\n",
      "    @property\n",
      "    def text_only(self) -> str:\n",
      "        \"\"\"Returns only the text content, ignoring non-text content.\n",
      "\n",
      "        Example:\n",
      "            >>> message = Message(role=\"user\", content=[\"Hello\", PILImage.new('RGB', (100, 100)), \"World\"])\n",
      "            >>> message.text_only\n",
      "            'Hello\\\\nWorld'\n",
      "        \"\"\"\n",
      "        return _content_to_text_only(self.content)\n",
      "\n",
      "    @cached_property\n",
      "    def tool_calls(self) -> List[ToolCall]:\n",
      "        \"\"\"Returns a list of all tool calls.\n",
      "\n",
      "        Example:\n",
      "            >>> tool_call = ToolCall(tool=lambda x: x, params=BaseModel())\n",
      "            >>> message = Message(role=\"user\", content=[\"Text\", tool_call])\n",
      "            >>> len(message.tool_calls)\n",
      "            1\n",
      "        \"\"\"\n",
      "        return [c.tool_call for c in self.content if c.tool_call is not None]\n",
      "\n",
      "    @property\n",
      "    def tool_results(self) -> List[ToolResult]:\n",
      "        \"\"\"Returns a list of all tool results.\n",
      "\n",
      "        Example:\n",
      "            >>> tool_result = ToolResult(tool_call_id=\"123\", result=[ContentBlock(text=\"Result\")])\n",
      "            >>> message = Message(role=\"user\", content=[\"Text\", tool_result])\n",
      "            >>> len(message.tool_results)\n",
      "            1\n",
      "        \"\"\"\n",
      "        return [c.tool_result for c in self.content if c.tool_result is not None]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from datetime import datetime, timezone\n",
      "import enum\n",
      "from functools import cached_property\n",
      "\n",
      "import sqlalchemy.types as types\n",
      "\n",
      "from ell.types.message import Any, Any, Field, Message, Optional\n",
      "\n",
      "from sqlmodel import Column, Field, SQLModel\n",
      "from typing import Optional\n",
      "from dataclasses import dataclass\n",
      "from typing import Dict, List, Literal, Union, Any, Optional\n",
      "\n",
      "from pydantic import BaseModel, field_validator\n",
      "\n",
      "from datetime import datetime\n",
      "from typing import Any, List, Optional\n",
      "from sqlmodel import Field, SQLModel, Relationship, JSON, Column\n",
      "from sqlalchemy import Index, func\n",
      "\n",
      "from typing import TypeVar, Any\n",
      "\n",
      "def utc_now() -> datetime:\n",
      "    \"\"\"\n",
      "    Returns the current UTC timestamp.\n",
      "    Serializes to ISO-8601.\n",
      "    \"\"\"\n",
      "    return datetime.now(tz=timezone.utc)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Message(BaseModel):\n",
      "    role: str\n",
      "    content: List[ContentBlock]\n",
      "\n",
      "\n",
      "    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):\n",
      "        content_blocks = to_content_blocks(content, **content_block_kwargs)\n",
      "\n",
      "        super().__init__(role=role, content=content_blocks)\n",
      "\n",
      "    # XXX: This choice of naming is unfortunate, but it is what it is.\n",
      "    @property\n",
      "    def text(self) -> str:\n",
      "        \"\"\"Returns all text content, replacing non-text content with their representations.\n",
      "\n",
      "        Example:\n",
      "            >>> message = Message(role=\"user\", content=[\"Hello\", PILImage.new('RGB', (100, 100)), \"World\"])\n",
      "            >>> message.text\n",
      "            'Hello\\\\n<PilImage>\\\\nWorld'\n",
      "        \"\"\"\n",
      "        return _content_to_text(self.content)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ContentBlock(BaseModel):\n",
      "\n",
      "    @classmethod\n",
      "    def coerce(cls, content: AnyContent) -> \"ContentBlock\":\n",
      "        \"\"\"\n",
      "        Coerce various types of content into a ContentBlock.\n",
      "\n",
      "        This method provides a flexible way to create ContentBlock instances from different types of input.\n",
      "\n",
      "        Args:\n",
      "        content: The content to be coerced into a ContentBlock. Can be one of the following types:\n",
      "        - str: Will be converted to a text ContentBlock.\n",
      "        - ToolCall: Will be converted to a tool_call ContentBlock.\n",
      "        - ToolResult: Will be converted to a tool_result ContentBlock.\n",
      "        - BaseModel: Will be converted to a parsed ContentBlock.\n",
      "        - ContentBlock: Will be returned as-is.\n",
      "        - Image: Will be converted to an image ContentBlock.\n",
      "        - np.ndarray: Will be converted to an image ContentBlock.\n",
      "        - PILImage.Image: Will be converted to an image ContentBlock.\n",
      "\n",
      "        Returns:\n",
      "        ContentBlock: A new ContentBlock instance containing the coerced content.\n",
      "\n",
      "        Raises:\n",
      "        ValueError: If the content cannot be coerced into a valid ContentBlock.\n",
      "\n",
      "        Examples:\n",
      "        >>> ContentBlock.coerce(\"Hello, world!\")\n",
      "        ContentBlock(text=\"Hello, world!\")\n",
      "\n",
      "        >>> tool_call = ToolCall(...)\n",
      "        >>> ContentBlock.coerce(tool_call)\n",
      "        ContentBlock(tool_call=tool_call)\n",
      "\n",
      "        >>> tool_result = ToolResult(...)\n",
      "        >>> ContentBlock.coerce(tool_result)\n",
      "        ContentBlock(tool_result=tool_result)\n",
      "\n",
      "        >>> class MyModel(BaseModel):\n",
      "        ...     field: str\n",
      "        >>> model_instance = MyModel(field=\"value\")\n",
      "        >>> ContentBlock.coerce(model_instance)\n",
      "        ContentBlock(parsed=model_instance)\n",
      "\n",
      "        >>> from PIL import Image as PILImage\n",
      "        >>> img = PILImage.new('RGB', (100, 100))\n",
      "        >>> ContentBlock.coerce(img)\n",
      "        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> arr = np.random.rand(100, 100, 3)\n",
      "        >>> ContentBlock.coerce(arr)\n",
      "        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))\n",
      "\n",
      "        >>> image = Image(url=\"https://example.com/image.jpg\")\n",
      "        >>> ContentBlock.coerce(image)\n",
      "        ContentBlock(image=ImageContent(url=\"https://example.com/image.jpg\"))\n",
      "\n",
      "        Notes:\n",
      "        - This method is particularly useful when working with heterogeneous content types\n",
      "          and you want to ensure they are all properly encapsulated in ContentBlock instances.\n",
      "        - The method performs type checking and appropriate conversions to ensure the resulting\n",
      "          ContentBlock is valid according to the model's constraints.\n",
      "        - For image content, Image objects, PIL Image objects, and numpy arrays are supported,\n",
      "          with automatic conversion to the appropriate format.\n",
      "        - As a last resort, the method will attempt to create an image from the input before\n",
      "          raising a ValueError.\n",
      "        \"\"\"\n",
      "        if isinstance(content, ContentBlock):\n",
      "            return content\n",
      "        if isinstance(content, str):\n",
      "            return cls(text=content)\n",
      "        if isinstance(content, ToolCall):\n",
      "            return cls(tool_call=content)\n",
      "        if isinstance(content, ToolResult):\n",
      "            return cls(tool_result=content)\n",
      "        if isinstance(content, (ImageContent, np.ndarray, PILImage.Image)):\n",
      "            return cls(image=ImageContent.coerce(content))\n",
      "        if isinstance(content, BaseModel):\n",
      "            return cls(parsed=content)\n",
      "\n",
      "        raise ValueError(f\"Invalid content type: {type(content)}\")\n",
      "\n",
      "    @field_serializer('parsed')\n",
      "    def serialize_parsed(self, value: Optional[BaseModel], _info):\n",
      "        if value is None:\n",
      "            return None\n",
      "        return value.model_dump(exclude_none=True, exclude_unset=True)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ToolResult(BaseModel):\n",
      "    tool_call_id: _lstr_generic\n",
      "    result: List[\"ContentBlock\"]\n",
      "\n",
      "    @property\n",
      "    def text(self) -> str:\n",
      "        return _content_to_text(self.result)\n",
      "\n",
      "    @property\n",
      "    def text_only(self) -> str:\n",
      "        return _content_to_text_only(self.result)\n",
      "\n",
      "    # # XXX: Possibly deprecate\n",
      "    # def readable_repr(self) -> str:\n",
      "    #     return f\"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})\"\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ToolCall(BaseModel):\n",
      "    tool : InvocableTool\n",
      "    tool_call_id : Optional[_lstr_generic] = Field(default=None)\n",
      "    params : BaseModel\n",
      "\n",
      "    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):\n",
      "        if not isinstance(params, BaseModel):\n",
      "            params = tool.__ell_params_model__(**params) #convenience.\n",
      "        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)\n",
      "\n",
      "    def __call__(self, **kwargs):\n",
      "        assert not kwargs, \"Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall.\"\n",
      "\n",
      "        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.\n",
      "        return self.tool(**self.params.model_dump())\n",
      "\n",
      "    # XXX: Deprecate in 0.1.0\n",
      "    def call_and_collect_as_message_block(self):\n",
      "        raise DeprecationWarning(\"call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.\")\n",
      "\n",
      "    def call_and_collect_as_content_block(self):\n",
      "        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)\n",
      "        return ContentBlock(tool_result=res)\n",
      "\n",
      "    def call_and_collect_as_message(self):\n",
      "        return Message(role=\"user\", content=[self.call_and_collect_as_message_block()])\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id='{self.tool_call_id}')\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def to_content_blocks(\n",
      "    content: Optional[Union[AnyContent, List[AnyContent]]] = None,\n",
      "    **content_block_kwargs\n",
      ") -> List[ContentBlock]:\n",
      "    \"\"\"\n",
      "    Coerce a variety of input types into a list of ContentBlock objects.\n",
      "\n",
      "    Args:\n",
      "    content: The content to be coerced. Can be a single item or a list of items.\n",
      "             Supported types include str, ContentBlock, ToolCall, ToolResult, BaseModel, Image, np.ndarray, and PILImage.Image.\n",
      "    **content_block_kwargs: Additional keyword arguments to pass to ContentBlock creation if content is None.\n",
      "\n",
      "    Returns:\n",
      "    List[ContentBlock]: A list of ContentBlock objects created from the input content.\n",
      "\n",
      "    Examples:\n",
      "    >>> coerce_content_list(\"Hello\")\n",
      "    [ContentBlock(text=\"Hello\")]\n",
      "\n",
      "    >>> coerce_content_list([ContentBlock(text=\"Hello\"), \"World\"])\n",
      "    [ContentBlock(text=\"Hello\"), ContentBlock(text=\"World\")]\n",
      "\n",
      "    >>> from PIL import Image as PILImage\n",
      "    >>> pil_image = PILImage.new('RGB', (100, 100))\n",
      "    >>> coerce_content_list(pil_image)\n",
      "    [ContentBlock(image=Image(image=<PIL.Image.Image object>))]\n",
      "\n",
      "    >>> coerce_content_list(Image(url=\"https://example.com/image.jpg\"))\n",
      "    [ContentBlock(image=Image(url=\"https://example.com/image.jpg\"))]\n",
      "\n",
      "    >>> coerce_content_list(None, text=\"Default text\")\n",
      "    [ContentBlock(text=\"Default text\")]\n",
      "    \"\"\"\n",
      "    if content is None:\n",
      "        return [ContentBlock(**content_block_kwargs)]\n",
      "\n",
      "    if not isinstance(content, list):\n",
      "        content = [content]\n",
      "\n",
      "    return [ContentBlock.model_validate(ContentBlock.coerce(c)) for c in content]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ImageContent(BaseModel):\n",
      "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
      "\n",
      "    image: Optional[PILImage.Image] = Field(default=None)\n",
      "    url: Optional[str] = Field(default=None)\n",
      "    detail: Optional[str] = Field(default=None)\n",
      "\n",
      "    @model_validator(mode='after')\n",
      "    def check_image_or_url(self):\n",
      "        if self.image is not None and self.url is not None:\n",
      "            raise ValueError(\"Both 'image' and 'url' cannot be set simultaneously.\")\n",
      "        if self.image is None and self.url is None:\n",
      "            raise ValueError(\"Either 'image' or 'url' must be set.\")\n",
      "        return self\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ImageContent(BaseModel):\n",
      "\n",
      "    @classmethod\n",
      "    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, \"ImageContent\"]):\n",
      "        if isinstance(value, cls):\n",
      "            return value\n",
      "\n",
      "        if isinstance(value, str):\n",
      "            if value.startswith('http://') or value.startswith('https://'):\n",
      "                return cls(url=value)\n",
      "            try:\n",
      "                img_data = base64.b64decode(value)\n",
      "                img = PILImage.open(BytesIO(img_data))\n",
      "                if img.mode not in ('L', 'RGB', 'RGBA'):\n",
      "                    return cls(image=img.convert('RGB'))\n",
      "            except:\n",
      "                raise ValueError(\"Invalid base64 string or URL for image\")\n",
      "\n",
      "        if isinstance(value, np.ndarray):\n",
      "            if value.ndim == 3 and value.shape[2] in (3, 4):\n",
      "                mode = 'RGB' if value.shape[2] == 3 else 'RGBA'\n",
      "                return cls(image=PILImage.fromarray(value, mode=mode))\n",
      "            else:\n",
      "                raise ValueError(f\"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.\")\n",
      "\n",
      "        if isinstance(value, PILImage.Image):\n",
      "            if value.mode not in ('L', 'RGB', 'RGBA'):\n",
      "                value = value.convert('RGB')\n",
      "            return cls(image=value)\n",
      "\n",
      "        raise ValueError(f\"Invalid image type: {type(value)}\")\n",
      "\n",
      "    @field_serializer('image')\n",
      "    def serialize_image(self, image: Optional[PILImage.Image], _info):\n",
      "        if image is None:\n",
      "            return None\n",
      "        return serialize_image(image)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Message(BaseModel):\n",
      "\n",
      "    @property\n",
      "    def parsed(self) -> Union[BaseModel, List[BaseModel]]:\n",
      "        \"\"\"Returns a list of all parsed content.\n",
      "\n",
      "        Example:\n",
      "            >>> class CustomModel(BaseModel):\n",
      "            ...     value: int\n",
      "            >>> parsed_content = CustomModel(value=42)\n",
      "            >>> message = Message(role=\"user\", content=[\"Text\", ContentBlock(parsed=parsed_content)])\n",
      "            >>> len(message.parsed)\n",
      "            1\n",
      "        \"\"\"\n",
      "        parsed_content = [c.parsed for c in self.content if c.parsed is not None]\n",
      "        return parsed_content[0] if len(parsed_content) == 1 else parsed_content\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class UTCTimestamp(types.TypeDecorator[datetime]):\n",
      "    cache_ok = True\n",
      "    impl = types.TIMESTAMP\n",
      "    def process_result_value(self, value: datetime, dialect:Any):\n",
      "        return value.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\n",
      "def UTCTimestampField(index:bool=False, **kwargs:Any):\n",
      "    return Field(\n",
      "        sa_column=Column(UTCTimestamp(timezone=True), index=index, **kwargs))\n",
      "\n",
      "\n",
      "class LMPType(str, enum.Enum):\n",
      "    LM = \"LM\"\n",
      "    TOOL = \"TOOL\"\n",
      "    MULTIMODAL = \"MULTIMODAL\"\n",
      "    OTHER = \"OTHER\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# HELPERS \n",
      "def system(content: Union[AnyContent, List[AnyContent]]) -> Message:\n",
      "    \"\"\"\n",
      "    Create a system message with the given content.\n",
      "\n",
      "    Args:\n",
      "    content (str): The content of the system message.\n",
      "\n",
      "    Returns:\n",
      "    Message: A Message object with role set to 'system' and the provided content.\n",
      "    \"\"\"\n",
      "    return Message(role=\"system\", content=content)\n",
      "\n",
      "\n",
      "def user(content: Union[AnyContent, List[AnyContent]]) -> Message:\n",
      "    \"\"\"\n",
      "    Create a user message with the given content.\n",
      "\n",
      "    Args:\n",
      "    content (str): The content of the user message.\n",
      "\n",
      "    Returns:\n",
      "    Message: A Message object with role set to 'user' and the provided content.\n",
      "    \"\"\"\n",
      "    return Message(role=\"user\", content=content)\n",
      "\n",
      "\n",
      "def assistant(content: Union[AnyContent, List[AnyContent]]) -> Message:\n",
      "    \"\"\"\n",
      "    Create an assistant message with the given content.\n",
      "\n",
      "    Args:\n",
      "    content (str): The content of the assistant message.\n",
      "\n",
      "    Returns:\n",
      "    Message: A Message object with role set to 'assistant' and the provided content.\n",
      "    \"\"\"\n",
      "    return Message(role=\"assistant\", content=content)\n",
      "\n",
      "#XXX: Make a mixi for these properties.\n",
      "def _content_to_text_only(content: List[ContentBlock]) -> str:\n",
      "    return _lstr(\"\\n\").join(\n",
      "            available_text\n",
      "            for c in content\n",
      "            if (available_text := (c.tool_result.text_only if c.tool_result else c.text))\n",
      "        )\n",
      "\n",
      "# Do we include the .text of a tool result? or its repr as in the current implementaiton?\n",
      "# What is the user using .text for? I just want to see the result of the tools. text_only should get us the text of the tool results; the tool_call_id is irrelevant.\n",
      "def _content_to_text(content: List[ContentBlock]) -> str:\n",
      "    return _lstr(\"\\n\").join(\n",
      "            available_text\n",
      "            for c in content\n",
      "            if (available_text :=  c.text or repr(c.content))\n",
      "        )\n",
      "\n",
      "\n",
      "# want to enable a use case where the user can actually return a standrd oai chat format\n",
      "# This is a placehodler will likely come back later for this\n",
      "LMPParams = Dict[str, Any]\n",
      "# Well this is disappointing, I wanted to effectively type hint by doign that data sync meta, but eh, at elast we can still reference role or content this way. Probably wil lcan the dict sync meta. TypedDict is the ticket ell oh ell.\n",
      "MessageOrDict = Union[Message, Dict[str, str]]\n",
      "# Can support iamge prompts later.\n",
      "Chat = List[\n",
      "    Message\n",
      "]  # [{\"role\": \"system\", \"content\": \"prompt\"}, {\"role\": \"user\", \"content\": \"message\"}]\n",
      "MultiTurnLMP = Callable[..., Chat]\n",
      "OneTurn = Callable[..., _lstr_generic]\n",
      "# This is the specific LMP that must accept history as an argument and can take any additional arguments\n",
      "ChatLMP = Callable[[Chat, Any], Chat]\n",
      "LMP = Union[OneTurn, MultiTurnLMP, ChatLMP]\n",
      "InvocableLM = Callable[..., _lstr_generic]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Your task is to compare the following clusters and choose which cluster forms a more cohesive grouping of source code chunks. You should\n",
      "penalize a cluster for including too much unrelated code or for missing important code that should have been included.\n",
      "Here are the clusters:\n",
      "Cluster 0:\n",
      "Message Handling and Content Processing:\n",
      "types\\message.py::12\n",
      "types\\message.py::9\n",
      "types\\message.py::6\n",
      "types\\message.py::15\n",
      "types\\message.py::7\n",
      "types\\message.py::13\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::1\n",
      "types\\message.py::8\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cluster 1:\n",
      "Message and Content Handling:\n",
      "types\\message.py::1\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::7\n",
      "types\\studio.py::1\n",
      "types\\studio.py::2\n",
      "types\\studio.py::3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now output your decision by selecting the number of the cluster that you would score higher on overall cohesion\n",
      "\n",
      "\n",
      "You are given the following set of source code chunks:\n",
      "from functools import lru_cache, wraps\n",
      "from typing import Dict, Any, Optional, Tuple, Union, Type\n",
      "import openai\n",
      "import logging\n",
      "from contextlib import contextmanager\n",
      "import threading\n",
      "from pydantic import BaseModel, ConfigDict, Field\n",
      "from ell.store import Store\n",
      "from ell.provider import Provider\n",
      "from dataclasses import dataclass, field\n",
      "\n",
      "_config_logger = logging.getLogger(__name__)\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class _Model:\n",
      "    name: str\n",
      "    default_client: Optional[Union[openai.Client, Any]] = None\n",
      "    #XXX: Deprecation in 0.1.0\n",
      "    #XXX: We will depreciate this when streaming is implemented. \n",
      "    # Currently we stream by default for the verbose renderer,\n",
      "    # but in the future we will not support streaming by default \n",
      "    # and stream=True must be passed which will then make API providers the\n",
      "    # single source of truth for whether or not a model supports an api parameter.\n",
      "    # This makes our implementation extremely light, only requiring us to provide\n",
      "    # a list of model names in registration.\n",
      "    supports_streaming : Optional[bool] = field(default=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\"\"\"\n",
      "ell is a Python library for language model programming (LMP). It provides a simple\n",
      "and intuitive interface for working with large language models.\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "from ell.lmp.simple import simple\n",
      "from ell.lmp.tool import tool\n",
      "from ell.lmp.complex import complex\n",
      "from ell.types.message import system, user, assistant, Message, ContentBlock\n",
      "from ell.__version__ import __version__\n",
      "\n",
      "# Import all models\n",
      "import ell.providers\n",
      "import ell.models\n",
      "\n",
      "\n",
      "# Import everything from configurator\n",
      "from ell.configurator import *\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# Existing helper functions\n",
      "def get_store() -> Union[Store, None]:\n",
      "    return config.store\n",
      "\n",
      "# Will be deprecated at 0.1.0 \n",
      "\n",
      "# You can add more helper functions here if needed\n",
      "def register_provider(provider: Provider, client_type: Type[Any]) -> None:\n",
      "    return config.register_provider(provider, client_type)\n",
      "\n",
      "# Deprecated now (remove at 0.1.0)\n",
      "def set_store(*args, **kwargs) -> None:\n",
      "    raise DeprecationWarning(\"The set_store function is deprecated and will be removed in a future version. Use ell.init(store=...) instead.\")\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Config(BaseModel):\n",
      "\n",
      "\n",
      "    def register_model(\n",
      "        self, \n",
      "        name: str,\n",
      "        default_client: Optional[Union[openai.Client, Any]] = None,\n",
      "        supports_streaming: Optional[bool] = None\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Register a model with its configuration.\n",
      "        \"\"\"\n",
      "        with self._lock:\n",
      "            # XXX: Will be deprecated in 0.1.0\n",
      "            self.registry[name] = _Model(\n",
      "                name=name,\n",
      "                default_client=default_client,\n",
      "                supports_streaming=supports_streaming\n",
      "            )\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def init(\n",
      "    store: Optional[Union[Store, str]] = None,\n",
      "    verbose: bool = False,\n",
      "    autocommit: bool = True,\n",
      "    lazy_versioning: bool = True,\n",
      "    default_api_params: Optional[Dict[str, Any]] = None,\n",
      "    default_client: Optional[Any] = None,\n",
      "    autocommit_model: str = \"gpt-4o-mini\"\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Initialize the ELL configuration with various settings.\n",
      "\n",
      "    :param verbose: Set verbosity of ELL operations.\n",
      "    :type verbose: bool\n",
      "    :param store: Set the store for ELL. Can be a Store instance or a string path for SQLiteStore.\n",
      "    :type store: Union[Store, str], optional\n",
      "    :param autocommit: Set autocommit for the store operations.\n",
      "    :type autocommit: bool\n",
      "    :param lazy_versioning: Enable or disable lazy versioning.\n",
      "    :type lazy_versioning: bool\n",
      "    :param default_api_params: Set default parameters for language models.\n",
      "    :type default_api_params: Dict[str, Any], optional\n",
      "    :param default_openai_client: Set the default OpenAI client.\n",
      "    :type default_openai_client: openai.Client, optional\n",
      "    :param autocommit_model: Set the model used for autocommitting.\n",
      "    :type autocommit_model: str\n",
      "    \"\"\"\n",
      "    # XXX: prevent double init\n",
      "    config.verbose = verbose\n",
      "    config.lazy_versioning = lazy_versioning\n",
      "\n",
      "    if isinstance(store, str):\n",
      "        from ell.stores.sql import SQLiteStore\n",
      "        config.store = SQLiteStore(store)\n",
      "    else:\n",
      "        config.store = store\n",
      "    config.autocommit = autocommit or config.autocommit\n",
      "\n",
      "    if default_api_params is not None:\n",
      "        config.default_api_params.update(default_api_params)\n",
      "\n",
      "    if default_client is not None:\n",
      "        config.default_client = default_client\n",
      "\n",
      "    if autocommit_model is not None:\n",
      "        config.autocommit_model = autocommit_model\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Config(BaseModel):\n",
      "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
      "    registry: Dict[str, _Model] = Field(default_factory=dict, description=\"A dictionary mapping model names to their configurations.\")\n",
      "    verbose: bool = Field(default=False, description=\"If True, enables verbose logging.\")\n",
      "    wrapped_logging: bool = Field(default=True, description=\"If True, enables wrapped logging for better readability.\")\n",
      "    override_wrapped_logging_width: Optional[int] = Field(default=None, description=\"If set, overrides the default width for wrapped logging.\")\n",
      "    store: Optional[Store] = Field(default=None, description=\"An optional Store instance for persistence.\")\n",
      "    autocommit: bool = Field(default=False, description=\"If True, enables automatic committing of changes to the store.\")\n",
      "    lazy_versioning: bool = Field(default=True, description=\"If True, enables lazy versioning for improved performance.\")\n",
      "    default_api_params: Dict[str, Any] = Field(default_factory=dict, description=\"Default parameters for language models.\")\n",
      "    default_client: Optional[openai.Client] = Field(default=None, description=\"The default OpenAI client used when a specific model client is not found.\")\n",
      "    autocommit_model: str = Field(default=\"gpt-4o-mini\", description=\"When set, changes the default autocommit model from GPT 4o mini.\")\n",
      "    providers: Dict[Type, Provider] = Field(default_factory=dict, description=\"A dictionary mapping client types to provider classes.\")\n",
      "    def __init__(self, **data):\n",
      "        super().__init__(**data)\n",
      "        self._lock = threading.Lock()\n",
      "        self._local = threading.local()\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Config(BaseModel):\n",
      "\n",
      "    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:\n",
      "        \"\"\"\n",
      "        Register a provider class for a specific client type.\n",
      "\n",
      "        :param provider_class: The provider class to register.\n",
      "        :type provider_class: Type[Provider]\n",
      "        \"\"\"\n",
      "        assert isinstance(client_type, type), \"client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))\"\n",
      "        with self._lock:\n",
      "            self.providers[client_type] = provider\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Config(BaseModel):\n",
      "\n",
      "    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:\n",
      "        \"\"\"\n",
      "        Get the provider instance for a specific client instance.\n",
      "\n",
      "        :param client: The client instance to get the provider for.\n",
      "        :type client: Any\n",
      "        :return: The provider instance for the specified client, or None if not found.\n",
      "        :rtype: Optional[Provider]\n",
      "        \"\"\"\n",
      "\n",
      "        client_type = type(client) if not isinstance(client, type) else client\n",
      "        for provider_type, provider in self.providers.items():\n",
      "            if issubclass(client_type, provider_type) or client_type == provider_type:\n",
      "                return provider\n",
      "        return None\n",
      "\n",
      "# Single* instance\n",
      "# XXX: Make a singleton\n",
      "config = Config()\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Config(BaseModel):\n",
      "\n",
      "    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:\n",
      "        \"\"\"\n",
      "        Get the OpenAI client for a specific model name.\n",
      "\n",
      "        :param model_name: The name of the model to get the client for.\n",
      "        :type model_name: str\n",
      "        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.\n",
      "        :rtype: Tuple[Optional[openai.Client], bool]\n",
      "        \"\"\"\n",
      "        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry\n",
      "        model_config = current_registry.get(model_name)\n",
      "        fallback = False\n",
      "        if not model_config:\n",
      "            warning_message = f\"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables.\"\n",
      "            if self.verbose:\n",
      "                from colorama import Fore, Style\n",
      "                _config_logger.warning(f\"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}\")\n",
      "            else:\n",
      "                _config_logger.debug(warning_message)\n",
      "            client = self.default_client\n",
      "            fallback = True\n",
      "        else:\n",
      "            client = model_config.default_client\n",
      "        return client, fallback\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\"\"\"\n",
      "Attempts to registeres model names with their respective API client bindings. This allows for the creation of a unified interface for interacting with different LLM providers.\n",
      "\n",
      "For example, to register an OpenAI model:\n",
      "@ell.simple(model='gpt-4o-mini') -> @ell.simple(model='gpt-4o-mini', client=openai.OpenAI())\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import ell.models.openai\n",
      "import ell.models.anthropic\n",
      "import ell.models.ollama\n",
      "import ell.models.groq\n",
      "import ell.models.bedrock\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# XXX: Needs a better name.\n",
      "class Provider(ABC):\n",
      "    \"\"\"\n",
      "    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.\n",
      "    For example, the OpenAI provider is an API interface to OpenAI's API but also to Ollama and Azure OpenAI.\n",
      "    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.\n",
      "    \"\"\"\n",
      "    dangerous_disable_validation = False\n",
      "\n",
      "    ################################\n",
      "    ### API PARAMETERS #############\n",
      "    ################################\n",
      "    @abstractmethod\n",
      "    def provider_call_function(\n",
      "        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None\n",
      "    ) -> Callable[..., Any]:\n",
      "        \"\"\"\n",
      "        Implement this method to return the function that makes the API call to the language model.\n",
      "        For example, if you're implementing the OpenAI provider, you would return the function that makes the API call to OpenAI's API.\n",
      "        \"\"\"\n",
      "        return NotImplemented\n",
      "\n",
      "    def disallowed_api_params(self) -> FrozenSet[str]:\n",
      "        \"\"\"\n",
      "        Returns a list of disallowed call params that ell will override.\n",
      "        \"\"\"\n",
      "        return frozenset({\"messages\", \"tools\", \"model\", \"stream\", \"stream_options\"})\n",
      "\n",
      "    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):\n",
      "        params = _call_params(self.provider_call_function(client, api_params))\n",
      "        return frozenset(params.keys()) - self.disallowed_api_params()\n",
      "\n",
      "    ################################\n",
      "    ### TRANSLATION ###############\n",
      "    ################################\n",
      "    @abstractmethod\n",
      "    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:\n",
      "        \"\"\"Converts an ell call to provider call params!\"\"\"\n",
      "        return NotImplemented\n",
      "\n",
      "    @abstractmethod\n",
      "    def translate_from_provider(\n",
      "        self,\n",
      "        provider_response: Any,\n",
      "        ell_call: EllCallParams,\n",
      "        provider_call_params: Dict[str, Any],\n",
      "        origin_id: Optional[str] = None,\n",
      "        logger: Optional[Callable[..., None]] = None,\n",
      "    ) -> Tuple[List[Message], Metadata]:\n",
      "        \"\"\"Converts provider responses to universal format. with metadata\"\"\"\n",
      "        return NotImplemented\n",
      "\n",
      "    ################################\n",
      "    ### CALL MODEL ################\n",
      "    ################################\n",
      "    # Be careful to override this method in your provider.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Config(BaseModel):\n",
      "\n",
      "\n",
      "\n",
      "    @contextmanager\n",
      "    def model_registry_override(self, overrides: Dict[str, _Model]):\n",
      "        \"\"\"\n",
      "        Temporarily override the model registry with new model configurations.\n",
      "\n",
      "        :param overrides: A dictionary of model names to ModelConfig instances to override.\n",
      "        :type overrides: Dict[str, ModelConfig]\n",
      "        \"\"\"\n",
      "        if not hasattr(self._local, 'stack'):\n",
      "            self._local.stack = []\n",
      "\n",
      "        with self._lock:\n",
      "            current_registry = self._local.stack[-1] if self._local.stack else self.registry\n",
      "            new_registry = current_registry.copy()\n",
      "            new_registry.update(overrides)\n",
      "\n",
      "        self._local.stack.append(new_registry)\n",
      "        try:\n",
      "            yield\n",
      "        finally:\n",
      "            self._local.stack.pop()\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Your task is to compare the following clusters and choose which cluster forms a more cohesive grouping of source code chunks. You should\n",
      "penalize a cluster for including too much unrelated code or for missing important code that should have been included.\n",
      "Here are the clusters:\n",
      "Cluster 0:\n",
      "ELL Provider and Configuration Management:\n",
      "ell\\provider.py::3\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::1\n",
      "ell\\__init__.py::1\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cluster 1:\n",
      "Model Configuration and Registration:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "models\\__init__.py::1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now output your decision by selecting the number of the cluster that you would score higher on overall cohesion\n",
      "\n",
      "\n",
      "You are given the following set of source code chunks:\n",
      "class ContentBlock(BaseModel):\n",
      "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
      "\n",
      "    text: Optional[_lstr_generic] = Field(default=None)\n",
      "    image: Optional[ImageContent] = Field(default=None)\n",
      "    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)\n",
      "    tool_call: Optional[ToolCall] = Field(default=None)\n",
      "    parsed: Optional[BaseModel] = Field(default=None)\n",
      "    tool_result: Optional[ToolResult] = Field(default=None)\n",
      "    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.\n",
      "    # This breaks us maintaing parity with the openai python client in some sen but so does image.\n",
      "\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        if \"image\" in kwargs and not isinstance(kwargs[\"image\"], ImageContent):\n",
      "            im = kwargs[\"image\"] = ImageContent.coerce(kwargs[\"image\"])\n",
      "            # XXX: Backwards compatibility, Deprecate.\n",
      "            if (d := kwargs.get(\"image_detail\", None)): im.detail = d\n",
      "\n",
      "        super().__init__(*args, **kwargs)\n",
      "\n",
      "\n",
      "    @model_validator(mode='after')\n",
      "    def check_single_non_null(self):\n",
      "        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]\n",
      "        if len(non_null_fields) > 1:\n",
      "            raise ValueError(f\"Only one field can be non-null. Found: {', '.join(non_null_fields)}\")\n",
      "        return self\n",
      "\n",
      "    def __str__(self):\n",
      "        return repr(self)\n",
      "\n",
      "    def __repr__(self):\n",
      "        non_null_fields = [f\"{field}={value}\" for field, value in self.__dict__.items() if value is not None]\n",
      "        return f\"ContentBlock({', '.join(non_null_fields)})\"\n",
      "\n",
      "    @property\n",
      "    def type(self):\n",
      "        if self.text is not None:\n",
      "            return \"text\"\n",
      "        if self.image is not None:\n",
      "            return \"image\"\n",
      "        if self.audio is not None:\n",
      "            return \"audio\"\n",
      "        if self.tool_call is not None:\n",
      "            return \"tool_call\"\n",
      "        if self.parsed is not None:\n",
      "            return \"parsed\"\n",
      "        if self.tool_result is not None:\n",
      "            return \"tool_result\"\n",
      "        return None\n",
      "\n",
      "    @property\n",
      "    def content(self):\n",
      "        return getattr(self, self.type)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# todo: implement tracing for structured outs. this a v2 feature.\n",
      "import json\n",
      "from ell.types._lstr import _lstr\n",
      "from functools import cached_property\n",
      "import numpy as np\n",
      "import base64\n",
      "from io import BytesIO\n",
      "from PIL import Image as PILImage\n",
      "\n",
      "from pydantic import BaseModel, ConfigDict, model_validator, field_serializer\n",
      "from sqlmodel import Field\n",
      "\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "\n",
      "from typing import Any, Callable, Dict, List, Optional, Union\n",
      "\n",
      "from ell.util.serialization import serialize_image\n",
      "_lstr_generic = Union[_lstr, str]\n",
      "InvocableTool = Callable[..., Union[\"ToolResult\", _lstr_generic, List[\"ContentBlock\"], ]]\n",
      "\n",
      "# AnyContent represents any type that can be passed to Message.\n",
      "AnyContent = Union[\"ContentBlock\", str, \"ToolCall\", \"ToolResult\", \"ImageContent\", np.ndarray, PILImage.Image, BaseModel]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeConversation:\n",
      "\n",
      "    def _process_conversation_item_input_audio_transcription_completed(self, event):\n",
      "        item_id, content_index, transcript = event['item_id'], event['content_index'], event['transcript']\n",
      "        item = self.item_lookup.get(item_id)\n",
      "        formatted_transcript = transcript or ' '\n",
      "\n",
      "        if not item:\n",
      "            self.queued_transcript_items[item_id] = {'transcript': formatted_transcript}\n",
      "            return {'item': None, 'delta': None}\n",
      "\n",
      "        item['content'][content_index]['transcript'] = transcript\n",
      "        item['formatted']['transcript'] = formatted_transcript\n",
      "        return {'item': item, 'delta': {'transcript': transcript}}\n",
      "\n",
      "    def _process_input_audio_buffer_speech_started(self, event):\n",
      "        item_id, audio_start_ms = event['item_id'], event['audio_start_ms']\n",
      "        self.queued_speech_items[item_id] = {'audio_start_ms': audio_start_ms}\n",
      "        return {'item': None, 'delta': None}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Message(BaseModel):\n",
      "\n",
      "    @property\n",
      "    def text_only(self) -> str:\n",
      "        \"\"\"Returns only the text content, ignoring non-text content.\n",
      "\n",
      "        Example:\n",
      "            >>> message = Message(role=\"user\", content=[\"Hello\", PILImage.new('RGB', (100, 100)), \"World\"])\n",
      "            >>> message.text_only\n",
      "            'Hello\\\\nWorld'\n",
      "        \"\"\"\n",
      "        return _content_to_text_only(self.content)\n",
      "\n",
      "    @cached_property\n",
      "    def tool_calls(self) -> List[ToolCall]:\n",
      "        \"\"\"Returns a list of all tool calls.\n",
      "\n",
      "        Example:\n",
      "            >>> tool_call = ToolCall(tool=lambda x: x, params=BaseModel())\n",
      "            >>> message = Message(role=\"user\", content=[\"Text\", tool_call])\n",
      "            >>> len(message.tool_calls)\n",
      "            1\n",
      "        \"\"\"\n",
      "        return [c.tool_call for c in self.content if c.tool_call is not None]\n",
      "\n",
      "    @property\n",
      "    def tool_results(self) -> List[ToolResult]:\n",
      "        \"\"\"Returns a list of all tool results.\n",
      "\n",
      "        Example:\n",
      "            >>> tool_result = ToolResult(tool_call_id=\"123\", result=[ContentBlock(text=\"Result\")])\n",
      "            >>> message = Message(role=\"user\", content=[\"Text\", tool_result])\n",
      "            >>> len(message.tool_results)\n",
      "            1\n",
      "        \"\"\"\n",
      "        return [c.tool_result for c in self.content if c.tool_result is not None]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from datetime import datetime, timezone\n",
      "import enum\n",
      "from functools import cached_property\n",
      "\n",
      "import sqlalchemy.types as types\n",
      "\n",
      "from ell.types.message import Any, Any, Field, Message, Optional\n",
      "\n",
      "from sqlmodel import Column, Field, SQLModel\n",
      "from typing import Optional\n",
      "from dataclasses import dataclass\n",
      "from typing import Dict, List, Literal, Union, Any, Optional\n",
      "\n",
      "from pydantic import BaseModel, field_validator\n",
      "\n",
      "from datetime import datetime\n",
      "from typing import Any, List, Optional\n",
      "from sqlmodel import Field, SQLModel, Relationship, JSON, Column\n",
      "from sqlalchemy import Index, func\n",
      "\n",
      "from typing import TypeVar, Any\n",
      "\n",
      "def utc_now() -> datetime:\n",
      "    \"\"\"\n",
      "    Returns the current UTC timestamp.\n",
      "    Serializes to ISO-8601.\n",
      "    \"\"\"\n",
      "    return datetime.now(tz=timezone.utc)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class SerializedLMPUses(SQLModel, table=True):\n",
      "    \"\"\"\n",
      "    Represents the many-to-many relationship between SerializedLMPs.\n",
      "\n",
      "    This class is used to track which LMPs use or are used by other LMPs.\n",
      "    \"\"\"\n",
      "\n",
      "    lmp_user_id: Optional[str] = Field(default=None, foreign_key=\"serializedlmp.lmp_id\", primary_key=True, index=True)  # ID of the LMP that is being used\n",
      "    lmp_using_id: Optional[str] = Field(default=None, foreign_key=\"serializedlmp.lmp_id\", primary_key=True, index=True)  # ID of the LMP that is using the other LMP\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ToolResult(BaseModel):\n",
      "    tool_call_id: _lstr_generic\n",
      "    result: List[\"ContentBlock\"]\n",
      "\n",
      "    @property\n",
      "    def text(self) -> str:\n",
      "        return _content_to_text(self.result)\n",
      "\n",
      "    @property\n",
      "    def text_only(self) -> str:\n",
      "        return _content_to_text_only(self.result)\n",
      "\n",
      "    # # XXX: Possibly deprecate\n",
      "    # def readable_repr(self) -> str:\n",
      "    #     return f\"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})\"\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Message(BaseModel):\n",
      "    role: str\n",
      "    content: List[ContentBlock]\n",
      "\n",
      "\n",
      "    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):\n",
      "        content_blocks = to_content_blocks(content, **content_block_kwargs)\n",
      "\n",
      "        super().__init__(role=role, content=content_blocks)\n",
      "\n",
      "    # XXX: This choice of naming is unfortunate, but it is what it is.\n",
      "    @property\n",
      "    def text(self) -> str:\n",
      "        \"\"\"Returns all text content, replacing non-text content with their representations.\n",
      "\n",
      "        Example:\n",
      "            >>> message = Message(role=\"user\", content=[\"Hello\", PILImage.new('RGB', (100, 100)), \"World\"])\n",
      "            >>> message.text\n",
      "            'Hello\\\\n<PilImage>\\\\nWorld'\n",
      "        \"\"\"\n",
      "        return _content_to_text(self.content)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ToolCall(BaseModel):\n",
      "    tool : InvocableTool\n",
      "    tool_call_id : Optional[_lstr_generic] = Field(default=None)\n",
      "    params : BaseModel\n",
      "\n",
      "    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):\n",
      "        if not isinstance(params, BaseModel):\n",
      "            params = tool.__ell_params_model__(**params) #convenience.\n",
      "        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)\n",
      "\n",
      "    def __call__(self, **kwargs):\n",
      "        assert not kwargs, \"Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall.\"\n",
      "\n",
      "        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.\n",
      "        return self.tool(**self.params.model_dump())\n",
      "\n",
      "    # XXX: Deprecate in 0.1.0\n",
      "    def call_and_collect_as_message_block(self):\n",
      "        raise DeprecationWarning(\"call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.\")\n",
      "\n",
      "    def call_and_collect_as_content_block(self):\n",
      "        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)\n",
      "        return ContentBlock(tool_result=res)\n",
      "\n",
      "    def call_and_collect_as_message(self):\n",
      "        return Message(role=\"user\", content=[self.call_and_collect_as_message_block()])\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id='{self.tool_call_id}')\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ContentBlock(BaseModel):\n",
      "\n",
      "    @classmethod\n",
      "    def coerce(cls, content: AnyContent) -> \"ContentBlock\":\n",
      "        \"\"\"\n",
      "        Coerce various types of content into a ContentBlock.\n",
      "\n",
      "        This method provides a flexible way to create ContentBlock instances from different types of input.\n",
      "\n",
      "        Args:\n",
      "        content: The content to be coerced into a ContentBlock. Can be one of the following types:\n",
      "        - str: Will be converted to a text ContentBlock.\n",
      "        - ToolCall: Will be converted to a tool_call ContentBlock.\n",
      "        - ToolResult: Will be converted to a tool_result ContentBlock.\n",
      "        - BaseModel: Will be converted to a parsed ContentBlock.\n",
      "        - ContentBlock: Will be returned as-is.\n",
      "        - Image: Will be converted to an image ContentBlock.\n",
      "        - np.ndarray: Will be converted to an image ContentBlock.\n",
      "        - PILImage.Image: Will be converted to an image ContentBlock.\n",
      "\n",
      "        Returns:\n",
      "        ContentBlock: A new ContentBlock instance containing the coerced content.\n",
      "\n",
      "        Raises:\n",
      "        ValueError: If the content cannot be coerced into a valid ContentBlock.\n",
      "\n",
      "        Examples:\n",
      "        >>> ContentBlock.coerce(\"Hello, world!\")\n",
      "        ContentBlock(text=\"Hello, world!\")\n",
      "\n",
      "        >>> tool_call = ToolCall(...)\n",
      "        >>> ContentBlock.coerce(tool_call)\n",
      "        ContentBlock(tool_call=tool_call)\n",
      "\n",
      "        >>> tool_result = ToolResult(...)\n",
      "        >>> ContentBlock.coerce(tool_result)\n",
      "        ContentBlock(tool_result=tool_result)\n",
      "\n",
      "        >>> class MyModel(BaseModel):\n",
      "        ...     field: str\n",
      "        >>> model_instance = MyModel(field=\"value\")\n",
      "        >>> ContentBlock.coerce(model_instance)\n",
      "        ContentBlock(parsed=model_instance)\n",
      "\n",
      "        >>> from PIL import Image as PILImage\n",
      "        >>> img = PILImage.new('RGB', (100, 100))\n",
      "        >>> ContentBlock.coerce(img)\n",
      "        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))\n",
      "\n",
      "        >>> import numpy as np\n",
      "        >>> arr = np.random.rand(100, 100, 3)\n",
      "        >>> ContentBlock.coerce(arr)\n",
      "        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))\n",
      "\n",
      "        >>> image = Image(url=\"https://example.com/image.jpg\")\n",
      "        >>> ContentBlock.coerce(image)\n",
      "        ContentBlock(image=ImageContent(url=\"https://example.com/image.jpg\"))\n",
      "\n",
      "        Notes:\n",
      "        - This method is particularly useful when working with heterogeneous content types\n",
      "          and you want to ensure they are all properly encapsulated in ContentBlock instances.\n",
      "        - The method performs type checking and appropriate conversions to ensure the resulting\n",
      "          ContentBlock is valid according to the model's constraints.\n",
      "        - For image content, Image objects, PIL Image objects, and numpy arrays are supported,\n",
      "          with automatic conversion to the appropriate format.\n",
      "        - As a last resort, the method will attempt to create an image from the input before\n",
      "          raising a ValueError.\n",
      "        \"\"\"\n",
      "        if isinstance(content, ContentBlock):\n",
      "            return content\n",
      "        if isinstance(content, str):\n",
      "            return cls(text=content)\n",
      "        if isinstance(content, ToolCall):\n",
      "            return cls(tool_call=content)\n",
      "        if isinstance(content, ToolResult):\n",
      "            return cls(tool_result=content)\n",
      "        if isinstance(content, (ImageContent, np.ndarray, PILImage.Image)):\n",
      "            return cls(image=ImageContent.coerce(content))\n",
      "        if isinstance(content, BaseModel):\n",
      "            return cls(parsed=content)\n",
      "\n",
      "        raise ValueError(f\"Invalid content type: {type(content)}\")\n",
      "\n",
      "    @field_serializer('parsed')\n",
      "    def serialize_parsed(self, value: Optional[BaseModel], _info):\n",
      "        if value is None:\n",
      "            return None\n",
      "        return value.model_dump(exclude_none=True, exclude_unset=True)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def to_content_blocks(\n",
      "    content: Optional[Union[AnyContent, List[AnyContent]]] = None,\n",
      "    **content_block_kwargs\n",
      ") -> List[ContentBlock]:\n",
      "    \"\"\"\n",
      "    Coerce a variety of input types into a list of ContentBlock objects.\n",
      "\n",
      "    Args:\n",
      "    content: The content to be coerced. Can be a single item or a list of items.\n",
      "             Supported types include str, ContentBlock, ToolCall, ToolResult, BaseModel, Image, np.ndarray, and PILImage.Image.\n",
      "    **content_block_kwargs: Additional keyword arguments to pass to ContentBlock creation if content is None.\n",
      "\n",
      "    Returns:\n",
      "    List[ContentBlock]: A list of ContentBlock objects created from the input content.\n",
      "\n",
      "    Examples:\n",
      "    >>> coerce_content_list(\"Hello\")\n",
      "    [ContentBlock(text=\"Hello\")]\n",
      "\n",
      "    >>> coerce_content_list([ContentBlock(text=\"Hello\"), \"World\"])\n",
      "    [ContentBlock(text=\"Hello\"), ContentBlock(text=\"World\")]\n",
      "\n",
      "    >>> from PIL import Image as PILImage\n",
      "    >>> pil_image = PILImage.new('RGB', (100, 100))\n",
      "    >>> coerce_content_list(pil_image)\n",
      "    [ContentBlock(image=Image(image=<PIL.Image.Image object>))]\n",
      "\n",
      "    >>> coerce_content_list(Image(url=\"https://example.com/image.jpg\"))\n",
      "    [ContentBlock(image=Image(url=\"https://example.com/image.jpg\"))]\n",
      "\n",
      "    >>> coerce_content_list(None, text=\"Default text\")\n",
      "    [ContentBlock(text=\"Default text\")]\n",
      "    \"\"\"\n",
      "    if content is None:\n",
      "        return [ContentBlock(**content_block_kwargs)]\n",
      "\n",
      "    if not isinstance(content, list):\n",
      "        content = [content]\n",
      "\n",
      "    return [ContentBlock.model_validate(ContentBlock.coerce(c)) for c in content]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class Message(BaseModel):\n",
      "\n",
      "    @property\n",
      "    def images(self) -> List[ImageContent]:\n",
      "        \"\"\"Returns a list of all image content.\n",
      "\n",
      "        Example:\n",
      "            >>> from PIL import Image as PILImage\n",
      "            >>> image1 = Image(url=\"https://example.com/image.jpg\")\n",
      "            >>> image2 = Image(image=PILImage.new('RGB', (200, 200)))\n",
      "            >>> message = Message(role=\"user\", content=[\"Text\", image1, \"More text\", image2])\n",
      "            >>> len(message.images)\n",
      "            2\n",
      "            >>> isinstance(message.images[0], Image)\n",
      "            True\n",
      "            >>> message.images[0].url\n",
      "            'https://example.com/image.jpg'\n",
      "            >>> isinstance(message.images[1].image, PILImage.Image)\n",
      "            True\n",
      "        \"\"\"\n",
      "        return [c.image for c in self.content if c.image]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ImageContent(BaseModel):\n",
      "\n",
      "    @classmethod\n",
      "    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, \"ImageContent\"]):\n",
      "        if isinstance(value, cls):\n",
      "            return value\n",
      "\n",
      "        if isinstance(value, str):\n",
      "            if value.startswith('http://') or value.startswith('https://'):\n",
      "                return cls(url=value)\n",
      "            try:\n",
      "                img_data = base64.b64decode(value)\n",
      "                img = PILImage.open(BytesIO(img_data))\n",
      "                if img.mode not in ('L', 'RGB', 'RGBA'):\n",
      "                    return cls(image=img.convert('RGB'))\n",
      "            except:\n",
      "                raise ValueError(\"Invalid base64 string or URL for image\")\n",
      "\n",
      "        if isinstance(value, np.ndarray):\n",
      "            if value.ndim == 3 and value.shape[2] in (3, 4):\n",
      "                mode = 'RGB' if value.shape[2] == 3 else 'RGBA'\n",
      "                return cls(image=PILImage.fromarray(value, mode=mode))\n",
      "            else:\n",
      "                raise ValueError(f\"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.\")\n",
      "\n",
      "        if isinstance(value, PILImage.Image):\n",
      "            if value.mode not in ('L', 'RGB', 'RGBA'):\n",
      "                value = value.convert('RGB')\n",
      "            return cls(image=value)\n",
      "\n",
      "        raise ValueError(f\"Invalid image type: {type(value)}\")\n",
      "\n",
      "    @field_serializer('image')\n",
      "    def serialize_image(self, image: Optional[PILImage.Image], _info):\n",
      "        if image is None:\n",
      "            return None\n",
      "        return serialize_image(image)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class ImageContent(BaseModel):\n",
      "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
      "\n",
      "    image: Optional[PILImage.Image] = Field(default=None)\n",
      "    url: Optional[str] = Field(default=None)\n",
      "    detail: Optional[str] = Field(default=None)\n",
      "\n",
      "    @model_validator(mode='after')\n",
      "    def check_image_or_url(self):\n",
      "        if self.image is not None and self.url is not None:\n",
      "            raise ValueError(\"Both 'image' and 'url' cannot be set simultaneously.\")\n",
      "        if self.image is None and self.url is None:\n",
      "            raise ValueError(\"Either 'image' or 'url' must be set.\")\n",
      "        return self\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    def send_user_message_content(self, content=None):\n",
      "        content = content or []\n",
      "        for c in content:\n",
      "            if c['type'] == 'input_audio':\n",
      "                if isinstance(c['audio'], (np.ndarray, bytes)):\n",
      "                    c['audio'] = RealtimeUtils.array_buffer_to_base64(c['audio'])\n",
      "        if content:\n",
      "            self.realtime.send('conversation.item.create', {\n",
      "                'item': {\n",
      "                    'type': 'message',\n",
      "                    'role': 'user',\n",
      "                    'content': content\n",
      "                }\n",
      "            })\n",
      "        self.create_response()\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class UTCTimestamp(types.TypeDecorator[datetime]):\n",
      "    cache_ok = True\n",
      "    impl = types.TIMESTAMP\n",
      "    def process_result_value(self, value: datetime, dialect:Any):\n",
      "        return value.replace(tzinfo=timezone.utc)\n",
      "\n",
      "\n",
      "def UTCTimestampField(index:bool=False, **kwargs:Any):\n",
      "    return Field(\n",
      "        sa_column=Column(UTCTimestamp(timezone=True), index=index, **kwargs))\n",
      "\n",
      "\n",
      "class LMPType(str, enum.Enum):\n",
      "    LM = \"LM\"\n",
      "    TOOL = \"TOOL\"\n",
      "    MULTIMODAL = \"MULTIMODAL\"\n",
      "    OTHER = \"OTHER\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "# HELPERS \n",
      "def system(content: Union[AnyContent, List[AnyContent]]) -> Message:\n",
      "    \"\"\"\n",
      "    Create a system message with the given content.\n",
      "\n",
      "    Args:\n",
      "    content (str): The content of the system message.\n",
      "\n",
      "    Returns:\n",
      "    Message: A Message object with role set to 'system' and the provided content.\n",
      "    \"\"\"\n",
      "    return Message(role=\"system\", content=content)\n",
      "\n",
      "\n",
      "def user(content: Union[AnyContent, List[AnyContent]]) -> Message:\n",
      "    \"\"\"\n",
      "    Create a user message with the given content.\n",
      "\n",
      "    Args:\n",
      "    content (str): The content of the user message.\n",
      "\n",
      "    Returns:\n",
      "    Message: A Message object with role set to 'user' and the provided content.\n",
      "    \"\"\"\n",
      "    return Message(role=\"user\", content=content)\n",
      "\n",
      "\n",
      "def assistant(content: Union[AnyContent, List[AnyContent]]) -> Message:\n",
      "    \"\"\"\n",
      "    Create an assistant message with the given content.\n",
      "\n",
      "    Args:\n",
      "    content (str): The content of the assistant message.\n",
      "\n",
      "    Returns:\n",
      "    Message: A Message object with role set to 'assistant' and the provided content.\n",
      "    \"\"\"\n",
      "    return Message(role=\"assistant\", content=content)\n",
      "\n",
      "#XXX: Make a mixi for these properties.\n",
      "def _content_to_text_only(content: List[ContentBlock]) -> str:\n",
      "    return _lstr(\"\\n\").join(\n",
      "            available_text\n",
      "            for c in content\n",
      "            if (available_text := (c.tool_result.text_only if c.tool_result else c.text))\n",
      "        )\n",
      "\n",
      "# Do we include the .text of a tool result? or its repr as in the current implementaiton?\n",
      "# What is the user using .text for? I just want to see the result of the tools. text_only should get us the text of the tool results; the tool_call_id is irrelevant.\n",
      "def _content_to_text(content: List[ContentBlock]) -> str:\n",
      "    return _lstr(\"\\n\").join(\n",
      "            available_text\n",
      "            for c in content\n",
      "            if (available_text :=  c.text or repr(c.content))\n",
      "        )\n",
      "\n",
      "\n",
      "# want to enable a use case where the user can actually return a standrd oai chat format\n",
      "# This is a placehodler will likely come back later for this\n",
      "LMPParams = Dict[str, Any]\n",
      "# Well this is disappointing, I wanted to effectively type hint by doign that data sync meta, but eh, at elast we can still reference role or content this way. Probably wil lcan the dict sync meta. TypedDict is the ticket ell oh ell.\n",
      "MessageOrDict = Union[Message, Dict[str, str]]\n",
      "# Can support iamge prompts later.\n",
      "Chat = List[\n",
      "    Message\n",
      "]  # [{\"role\": \"system\", \"content\": \"prompt\"}, {\"role\": \"user\", \"content\": \"message\"}]\n",
      "MultiTurnLMP = Callable[..., Chat]\n",
      "OneTurn = Callable[..., _lstr_generic]\n",
      "# This is the specific LMP that must accept history as an argument and can take any additional arguments\n",
      "ChatLMP = Callable[[Chat, Any], Chat]\n",
      "LMP = Union[OneTurn, MultiTurnLMP, ChatLMP]\n",
      "InvocableLM = Callable[..., _lstr_generic]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Your task is to compare the following clusters and choose which cluster forms a more cohesive grouping of source code chunks. You should\n",
      "penalize a cluster for including too much unrelated code or for missing important code that should have been included.\n",
      "Here are the clusters:\n",
      "Cluster 0:\n",
      "Content and Message Management:\n",
      "types\\message.py::6\n",
      "types\\message.py::9\n",
      "types\\message.py::3\n",
      "types\\message.py::2\n",
      "types\\message.py::5\n",
      "types\\message.py::7\n",
      "types\\message.py::8\n",
      "types\\message.py::15\n",
      "types\\message.py::12\n",
      "types\\message.py::10\n",
      "openai_realtime\\client.py::6\n",
      "openai_realtime\\conversation.py::4\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cluster 1:\n",
      "Message and Content Handling:\n",
      "types\\message.py::1\n",
      "types\\message.py::2\n",
      "types\\message.py::3\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::7\n",
      "types\\studio.py::1\n",
      "types\\studio.py::2\n",
      "types\\studio.py::3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now output your decision by selecting the number of the cluster that you would score higher on overall cohesion\n",
      "\n",
      "\n",
      "You are given the following set of source code chunks:\n",
      "class RealtimeAPI(RealtimeEventHandler):\n",
      "\n",
      "    async def connect(self, model='gpt-4o-realtime-preview-2024-10-01'):\n",
      "        if self.is_connected():\n",
      "            raise Exception(\"Already connected\")\n",
      "\n",
      "        headers = {\n",
      "            'Authorization': f'Bearer {self.api_key}',\n",
      "            'OpenAI-Beta': 'realtime=v1'\n",
      "        }\n",
      "\n",
      "        self.ws = await websockets.connect(f\"{self.url}?model={model}\", extra_headers=headers)\n",
      "\n",
      "        self.log(f\"Connected to {self.url}\")\n",
      "\n",
      "        asyncio.create_task(self._message_handler())\n",
      "\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "import numpy as np\n",
      "import json\n",
      "from .utils import RealtimeUtils\n",
      "import copy\n",
      "\n",
      "class RealtimeConversation:\n",
      "    def __init__(self):\n",
      "        self.default_frequency = 24000  # 24,000 Hz\n",
      "        self.clear()\n",
      "\n",
      "    def clear(self):\n",
      "        self.item_lookup = {}\n",
      "        self.items = []\n",
      "        self.response_lookup = {}\n",
      "        self.responses = []\n",
      "        self.queued_speech_items = {}\n",
      "        self.queued_transcript_items = {}\n",
      "        self.queued_input_audio = None\n",
      "        return True\n",
      "\n",
      "    def queue_input_audio(self, input_audio):\n",
      "        self.queued_input_audio = input_audio\n",
      "        return input_audio\n",
      "\n",
      "    def process_event(self, event, *args):\n",
      "        if 'event_id' not in event:\n",
      "            raise ValueError(\"Missing 'event_id' on event\")\n",
      "        if 'type' not in event:\n",
      "            raise ValueError(\"Missing 'type' on event\")\n",
      "\n",
      "        event_processor = getattr(self, f\"_process_{event['type'].replace('.', '_')}\", None)\n",
      "        if not event_processor:\n",
      "            raise ValueError(f\"Missing conversation event processor for '{event['type']}'\")\n",
      "\n",
      "        return event_processor(event, *args)\n",
      "\n",
      "    def get_item(self, id):\n",
      "        return self.item_lookup.get(id)\n",
      "\n",
      "    def get_items(self):\n",
      "        return self.items.copy()\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeConversation:\n",
      "\n",
      "    def _process_response_created(self, event):\n",
      "        response = event['response']\n",
      "        if response['id'] not in self.response_lookup:\n",
      "            self.response_lookup[response['id']] = response\n",
      "            self.responses.append(response)\n",
      "        return {'item': None, 'delta': None}\n",
      "\n",
      "    def _process_response_output_item_added(self, event):\n",
      "        response_id, item = event['response_id'], event['item']\n",
      "        response = self.response_lookup.get(response_id)\n",
      "        if not response:\n",
      "            raise ValueError(f\"response.output_item.added: Response '{response_id}' not found\")\n",
      "        response['output'].append(item['id'])\n",
      "        return {'item': None, 'delta': None}\n",
      "\n",
      "    def _process_response_output_item_done(self, event):\n",
      "        item = event['item']\n",
      "        if not item:\n",
      "            raise ValueError(\"response.output_item.done: Missing 'item'\")\n",
      "        found_item = self.item_lookup.get(item['id'])\n",
      "        if not found_item:\n",
      "            raise ValueError(f\"response.output_item.done: Item '{item['id']}' not found\")\n",
      "        found_item['status'] = item['status']\n",
      "        return {'item': found_item, 'delta': None}\n",
      "\n",
      "    def _process_response_content_part_added(self, event):\n",
      "        item_id, part = event['item_id'], event['part']\n",
      "        item = self.item_lookup.get(item_id)\n",
      "        if not item:\n",
      "            raise ValueError(f\"response.content_part.added: Item '{item_id}' not found\")\n",
      "        item['content'].append(part)\n",
      "        return {'item': item, 'delta': None}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    def _add_api_event_handlers(self):\n",
      "        self.realtime.on('client.*', lambda event: self.dispatch('realtime.event', {\n",
      "            'time': RealtimeUtils.generate_id('time_'),\n",
      "            'source': 'client',\n",
      "            'event': event\n",
      "        }))\n",
      "        self.realtime.on('server.*', lambda event: self.dispatch('realtime.event', {\n",
      "            'time': RealtimeUtils.generate_id('time_'),\n",
      "            'source': 'server',\n",
      "            'event': event\n",
      "        }))\n",
      "        self.realtime.on('server.session.created', lambda _: setattr(self, 'session_created', True))\n",
      "\n",
      "        def handle_conversation_event(event, *args):\n",
      "            result = self.conversation.process_event(event, *args)\n",
      "            if result['item']:\n",
      "                self.dispatch('conversation.updated', result)\n",
      "            return result\n",
      "\n",
      "        self.realtime.on('server.response.created', handle_conversation_event)\n",
      "        self.realtime.on('server.response.output_item.added', handle_conversation_event)\n",
      "        self.realtime.on('server.response.content_part.added', handle_conversation_event)\n",
      "        self.realtime.on('server.input_audio_buffer.speech_started', lambda event: (\n",
      "            handle_conversation_event(event),\n",
      "            self.dispatch('conversation.interrupted', event)\n",
      "        ))\n",
      "        self.realtime.on('server.input_audio_buffer.speech_stopped', lambda event: \n",
      "            handle_conversation_event(event, self.input_audio_buffer)\n",
      "        )\n",
      "        self.realtime.on('server.conversation.item.created', lambda event: (\n",
      "            handle_conversation_event(event),\n",
      "            self.dispatch('conversation.item.appended', {'item': event['item']})\n",
      "        ))\n",
      "        self.realtime.on('server.conversation.item.truncated', handle_conversation_event)\n",
      "        self.realtime.on('server.conversation.item.deleted', handle_conversation_event)\n",
      "        self.realtime.on('server.conversation.item.input_audio_transcription.completed', handle_conversation_event)\n",
      "        self.realtime.on('server.response.audio_transcript.delta', handle_conversation_event)\n",
      "        self.realtime.on('server.response.audio.delta', handle_conversation_event)\n",
      "        self.realtime.on('server.response.text.delta', handle_conversation_event)\n",
      "        self.realtime.on('server.response.function_call_arguments.delta', handle_conversation_event)\n",
      "        def handle_output_item_done( event):\n",
      "            handle_conversation_event(event)\n",
      "            item = event.get('item', {})\n",
      "\n",
      "            if item.get('status') == 'completed':\n",
      "                self.dispatch('conversation.item.completed', {'item': item})\n",
      "\n",
      "            formatted = item.get('formatted', {})\n",
      "            tool = formatted.get('tool') if isinstance(formatted, dict) else None\n",
      "\n",
      "            if tool:\n",
      "                asyncio.create_task(self._call_tool(tool))\n",
      "        self.realtime.on('server.response.output_item.done', handle_output_item_done)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "import asyncio\n",
      "import json\n",
      "import websockets\n",
      "from .event_handler import RealtimeEventHandler\n",
      "from .utils import RealtimeUtils\n",
      "\n",
      "class RealtimeAPI(RealtimeEventHandler):\n",
      "    def __init__(self, url=None, api_key=None, dangerously_allow_api_key_in_browser=False, debug=False):\n",
      "        super().__init__()\n",
      "        self.default_url = 'wss://api.openai.com/v1/realtime'\n",
      "        self.url = url or self.default_url\n",
      "        self.api_key = api_key\n",
      "        self.debug = debug\n",
      "        self.ws = None\n",
      "\n",
      "    def is_connected(self):\n",
      "        return self.ws is not None and self.ws.open\n",
      "\n",
      "    def log(self, *args):\n",
      "        if self.debug:\n",
      "            print(*args)\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "import asyncio\n",
      "from typing import Callable, Dict, List, Any\n",
      "\n",
      "class RealtimeEventHandler:\n",
      "    def __init__(self):\n",
      "        self.event_handlers: Dict[str, List[Callable]] = {}\n",
      "        self.next_event_handlers: Dict[str, List[Callable]] = {}\n",
      "\n",
      "    def clear_event_handlers(self):\n",
      "        self.event_handlers.clear()\n",
      "        self.next_event_handlers.clear()\n",
      "        return True\n",
      "\n",
      "    def on(self, event_name: str, callback: Callable = None):\n",
      "        def decorator(func):\n",
      "            if event_name not in self.event_handlers:\n",
      "                self.event_handlers[event_name] = []\n",
      "            self.event_handlers[event_name].append(func)\n",
      "            return func\n",
      "\n",
      "        if callback is None:\n",
      "            return decorator\n",
      "        else:\n",
      "            return decorator(callback)\n",
      "\n",
      "    def on_next(self, event_name: str, callback: Callable):\n",
      "        if event_name not in self.next_event_handlers:\n",
      "            self.next_event_handlers[event_name] = []\n",
      "        self.next_event_handlers[event_name].append(callback)\n",
      "\n",
      "    def off(self, event_name: str, callback: Callable = None):\n",
      "        if event_name in self.event_handlers:\n",
      "            if callback:\n",
      "                self.event_handlers[event_name].remove(callback)\n",
      "            else:\n",
      "                del self.event_handlers[event_name]\n",
      "        return True\n",
      "\n",
      "    def off_next(self, event_name: str, callback: Callable = None):\n",
      "        if event_name in self.next_event_handlers:\n",
      "            if callback:\n",
      "                self.next_event_handlers[event_name].remove(callback)\n",
      "            else:\n",
      "                del self.next_event_handlers[event_name]\n",
      "        return True\n",
      "\n",
      "    async def wait_for_next(self, event_name: str, timeout: float = None):\n",
      "        next_event = None\n",
      "        def set_next_event(event):\n",
      "            nonlocal next_event\n",
      "            next_event = event\n",
      "\n",
      "        self.on_next(event_name, set_next_event)\n",
      "\n",
      "        start_time = asyncio.get_event_loop().time()\n",
      "        while not next_event:\n",
      "            if timeout and asyncio.get_event_loop().time() - start_time > timeout:\n",
      "                return None\n",
      "            await asyncio.sleep(0.001)\n",
      "\n",
      "        return next_event\n",
      "\n",
      "    def dispatch(self, event_name: str, event: Any):\n",
      "        handlers = self.event_handlers.get(event_name, []).copy()\n",
      "        for handler in handlers:\n",
      "            handler(event)\n",
      "\n",
      "        next_handlers = self.next_event_handlers.pop(event_name, [])\n",
      "        for next_handler in next_handlers:\n",
      "            next_handler(event)\n",
      "\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    async def wait_for_next_item(self):\n",
      "        event = await self.wait_for_next('conversation.item.appended')\n",
      "        return {'item': event['item']}\n",
      "\n",
      "    async def wait_for_next_completed_item(self):\n",
      "        event = await self.wait_for_next('conversation.item.completed')\n",
      "        return {'item': event['item']}\n",
      "\n",
      "    async def _call_tool(self, tool):\n",
      "        try:\n",
      "            json_arguments = json.loads(tool['arguments'])\n",
      "            tool_config = self.tools.get(tool['name'])\n",
      "            if not tool_config:\n",
      "                raise ValueError(f\"Tool '{tool['name']}' has not been added\")\n",
      "            result = await tool_config['handler'](json_arguments)\n",
      "            self.realtime.send('conversation.item.create', {\n",
      "                'item': {\n",
      "                    'type': 'function_call_output',\n",
      "                    'call_id': tool['call_id'],\n",
      "                    'output': json.dumps(result, ensure_ascii=False)\n",
      "                }\n",
      "            })\n",
      "        except Exception as e:\n",
      "            self.realtime.send('conversation.item.create', {\n",
      "                'item': {\n",
      "                    'type': 'function_call_output',\n",
      "                    'call_id': tool['call_id'],\n",
      "                    'output': json.dumps({'error': str(e)}, ensure_ascii=False)\n",
      "                }\n",
      "            })\n",
      "        self.create_response()\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "import asyncio\n",
      "import numpy as np\n",
      "from .event_handler import RealtimeEventHandler\n",
      "from .api import RealtimeAPI\n",
      "from .conversation import RealtimeConversation\n",
      "from .utils import RealtimeUtils\n",
      "import json\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "    def __init__(self, url=None, api_key=None, instructions='', dangerously_allow_api_key_in_browser=False, debug=False):\n",
      "        super().__init__()\n",
      "        self.default_session_config = {\n",
      "            'modalities': ['text', 'audio'],\n",
      "            'instructions': instructions,\n",
      "            'voice': 'alloy',\n",
      "            'input_audio_format': 'pcm16',\n",
      "            'output_audio_format': 'pcm16',\n",
      "            'input_audio_transcription': None,\n",
      "            'turn_detection': None,\n",
      "            'tools': [],\n",
      "            'tool_choice': 'auto',\n",
      "            'temperature': 0.8,\n",
      "            'max_response_output_tokens': 4096,\n",
      "        }\n",
      "        self.session_config = {}\n",
      "        self.transcription_models = [{'model': 'whisper-1'}]\n",
      "        self.default_server_vad_config = {\n",
      "            'type': 'server_vad',\n",
      "            'threshold': 0.5,\n",
      "            'prefix_padding_ms': 300,\n",
      "            'silence_duration_ms': 200,\n",
      "        }\n",
      "        self.realtime = RealtimeAPI(url, api_key, dangerously_allow_api_key_in_browser, debug)\n",
      "        self.conversation = RealtimeConversation()\n",
      "        self._reset_config()\n",
      "        self._add_api_event_handlers()\n",
      "\n",
      "    def _reset_config(self):\n",
      "        self.session_created = False\n",
      "        self.tools = {}\n",
      "        self.session_config = self.default_session_config.copy()\n",
      "        self.input_audio_buffer = np.array([], dtype=np.int16)\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeAPI(RealtimeEventHandler):\n",
      "\n",
      "    async def _message_handler(self):\n",
      "        try:\n",
      "            async for message in self.ws:\n",
      "                data = json.loads(message)\n",
      "                self.receive(data['type'], data)\n",
      "        except websockets.exceptions.ConnectionClosed:\n",
      "            self.disconnect()\n",
      "            self.dispatch('close', {'error': True})\n",
      "\n",
      "    def disconnect(self):\n",
      "        if self.ws:\n",
      "            asyncio.create_task(self.ws.close())\n",
      "            self.ws = None\n",
      "        return True\n",
      "\n",
      "    def receive(self, event_name, event):\n",
      "        self.log(\"received:\", event_name, event)\n",
      "        self.dispatch(f\"server.{event_name}\", event)\n",
      "        self.dispatch(\"server.*\", event)\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    def add_tool(self, definition, handler):\n",
      "        if not definition.get('name'):\n",
      "            raise ValueError(\"Missing tool name in definition\")\n",
      "        name = definition['name']\n",
      "        if name in self.tools:\n",
      "            raise ValueError(f\"Tool '{name}' already added. Please use .remove_tool('{name}') before trying to add again.\")\n",
      "        if not callable(handler):\n",
      "            raise ValueError(f\"Tool '{name}' handler must be a function\")\n",
      "        self.tools[name] = {'definition': definition, 'handler': handler}\n",
      "        self.update_session()\n",
      "        return self.tools[name]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    def remove_tool(self, name):\n",
      "        if name not in self.tools:\n",
      "            raise ValueError(f\"Tool '{name}' does not exist, cannot be removed.\")\n",
      "        del self.tools[name]\n",
      "        return True\n",
      "\n",
      "    def delete_item(self, id):\n",
      "        self.realtime.send('conversation.item.delete', {'item_id': id})\n",
      "        return True\n",
      "\n",
      "    def update_session(self, **kwargs):\n",
      "        self.session_config.update(kwargs)\n",
      "        use_tools = [\n",
      "            {**tool.get('definition', {}), 'type': 'function'}\n",
      "            for tool in self.tools.values()\n",
      "        ]\n",
      "        session = {**self.session_config, 'tools': use_tools}\n",
      "        if self.realtime.is_connected():\n",
      "            self.realtime.send('session.update', {'session': session})\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "    def send_user_message_content(self, content=None):\n",
      "        content = content or []\n",
      "        for c in content:\n",
      "            if c['type'] == 'input_audio':\n",
      "                if isinstance(c['audio'], (np.ndarray, bytes)):\n",
      "                    c['audio'] = RealtimeUtils.array_buffer_to_base64(c['audio'])\n",
      "        if content:\n",
      "            self.realtime.send('conversation.item.create', {\n",
      "                'item': {\n",
      "                    'type': 'message',\n",
      "                    'role': 'user',\n",
      "                    'content': content\n",
      "                }\n",
      "            })\n",
      "        self.create_response()\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeClient(RealtimeEventHandler):\n",
      "\n",
      "\n",
      "\n",
      "    def is_connected(self):\n",
      "        return self.realtime.is_connected() and self.session_created\n",
      "\n",
      "    def reset(self):\n",
      "        self.disconnect()\n",
      "        self.clear_event_handlers()\n",
      "        self.realtime.clear_event_handlers()\n",
      "        self._reset_config()\n",
      "        self._add_api_event_handlers()\n",
      "        return True\n",
      "\n",
      "    async def connect(self):\n",
      "        if self.is_connected():\n",
      "            raise Exception(\"Already connected, use .disconnect() first\")\n",
      "        await self.realtime.connect()\n",
      "        self.update_session()\n",
      "        return True\n",
      "\n",
      "    async def wait_for_session_created(self):\n",
      "        if not self.realtime.is_connected():\n",
      "            raise Exception(\"Not connected, use .connect() first\")\n",
      "        while not self.session_created:\n",
      "            await asyncio.sleep(0.001)\n",
      "        return True\n",
      "\n",
      "    def disconnect(self):\n",
      "        self.session_created = False\n",
      "        self.conversation.clear()\n",
      "        if self.realtime.is_connected():\n",
      "            self.realtime.disconnect()\n",
      "\n",
      "    def get_turn_detection_type(self):\n",
      "        turn_detection = self.session_config.get('turn_detection')\n",
      "        if isinstance(turn_detection, dict):\n",
      "            return turn_detection.get('type')\n",
      "        return None\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeConversation:\n",
      "\n",
      "    def _process_conversation_item_created(self, event):\n",
      "        item = event['item']\n",
      "        new_item = copy.deepcopy(item)\n",
      "        if new_item['id'] not in self.item_lookup:\n",
      "            self.item_lookup[new_item['id']] = new_item\n",
      "            self.items.append(new_item)\n",
      "\n",
      "        new_item['formatted'] = {\n",
      "            'audio': np.array([], dtype=np.int16),\n",
      "            'text': '',\n",
      "            'transcript': ''\n",
      "        }\n",
      "\n",
      "        if new_item['type'] == 'message':\n",
      "            if new_item['role'] == 'user':\n",
      "                new_item['status'] = 'completed'\n",
      "                if self.queued_input_audio is not None:\n",
      "                    new_item['formatted']['audio'] = self.queued_input_audio\n",
      "                    self.queued_input_audio = None\n",
      "            else:\n",
      "                new_item['status'] = 'in_progress'\n",
      "        elif new_item['type'] == 'function_call':\n",
      "            new_item['formatted']['tool'] = {\n",
      "                'type': 'function',\n",
      "                'name': new_item['name'],\n",
      "                'call_id': new_item['call_id'],\n",
      "                'arguments': ''\n",
      "            }\n",
      "            new_item['status'] = 'in_progress'\n",
      "        elif new_item['type'] == 'function_call_output':\n",
      "            new_item['status'] = 'completed'\n",
      "            new_item['formatted']['output'] = new_item['output']\n",
      "\n",
      "        if new_item.get('content'):\n",
      "            text_content = [c for c in new_item['content'] if c['type'] in ['text', 'input_text']]\n",
      "            for content in text_content:\n",
      "                new_item['formatted']['text'] += content['text']\n",
      "\n",
      "        if new_item['id'] in self.queued_speech_items:\n",
      "            new_item['formatted']['audio'] = self.queued_speech_items[new_item['id']]['audio']\n",
      "            del self.queued_speech_items[new_item['id']]\n",
      "\n",
      "        if new_item['id'] in self.queued_transcript_items:\n",
      "            new_item['formatted']['transcript'] = self.queued_transcript_items[new_item['id']]['transcript']\n",
      "            del self.queued_transcript_items[new_item['id']]\n",
      "\n",
      "        return {'item': new_item, 'delta': None}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class RealtimeAPI(RealtimeEventHandler):\n",
      "\n",
      "    def send(self, event_name, data=None):\n",
      "        if not self.is_connected():\n",
      "            raise Exception(\"RealtimeAPI is not connected\")\n",
      "\n",
      "        data = data or {}\n",
      "        if not isinstance(data, dict):\n",
      "            raise ValueError(\"data must be a dictionary\")\n",
      "\n",
      "        event = {\n",
      "            \"event_id\": RealtimeUtils.generate_id(\"evt_\"),\n",
      "            \"type\": event_name,\n",
      "            **data\n",
      "        }\n",
      "\n",
      "        self.dispatch(f\"client.{event_name}\", event)\n",
      "        self.dispatch(\"client.*\", event)\n",
      "        self.log(\"sent:\", event_name, event)\n",
      "\n",
      "        asyncio.create_task(self.ws.send(json.dumps(event, ensure_ascii=False)))\n",
      "        return True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Your task is to compare the following clusters and choose which cluster forms a more cohesive grouping of source code chunks. You should\n",
      "penalize a cluster for including too much unrelated code or for missing important code that should have been included.\n",
      "Here are the clusters:\n",
      "Cluster 0:\n",
      "Real-Time Communication and Event Handling:\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\event_handler.py::1\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\conversation.py::1\n",
      "openai_realtime\\conversation.py::2\n",
      "openai_realtime\\client.py::9\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\conversation.py::6\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cluster 1:\n",
      "Real-time API Connection and Management:\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::1\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\api.py::4\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\client.py::5\n",
      "openai_realtime\\client.py::6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now output your decision by selecting the number of the cluster that you would score higher on overall cohesion\n",
      "\n",
      "\n",
      "You are given the following set of source code chunks:\n",
      "from functools import wraps\n",
      "from typing import Any, Optional\n",
      "\n",
      "from ell.lmp.complex import complex\n",
      "\n",
      "\n",
      "def simple(model: str, client: Optional[Any] = None,  exempt_from_tracking=False, **api_params):\n",
      "    assert 'tools' not in api_params, \"tools are not supported in lm decorator, use multimodal decorator instead\"\n",
      "    assert 'tool_choice' not in api_params, \"tool_choice is not supported in lm decorator, use multimodal decorator instead\"\n",
      "    assert 'response_format' not in api_params or isinstance(api_params.get('response_format', None), dict), \"response_format is not supported in lm decorator, use multimodal decorator instead\"\n",
      "\n",
      "    def convert_multimodal_response_to_lstr(response):\n",
      "        return [x.content[0].text for x in response] if isinstance(response, list) else response.content[0].text\n",
      "    return complex(model, client,  exempt_from_tracking=exempt_from_tracking, **api_params, post_callback=convert_multimodal_response_to_lstr)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "simple.__doc__ = \"\"\"The fundamental unit of language model programming in ell.\n",
      "\n",
      "  This decorator simplifies the process of creating Language Model Programs (LMPs) \n",
      "  that return text-only outputs from language models, while supporting multimodal inputs.\n",
      "  It wraps the more complex 'complex' decorator, providing a streamlined interface for common use cases.\n",
      "\n",
      "  :param model: The name or identifier of the language model to use.\n",
      "  :type model: str\n",
      "  :param client: An optional OpenAI client instance. If not provided, a default client will be used.\n",
      "  :type client: Optional[openai.Client]\n",
      "  :param exempt_from_tracking: If True, the LMP usage won't be tracked. Default is False.\n",
      "  :type exempt_from_tracking: bool\n",
      "  :param api_params: Additional keyword arguments to pass to the underlying API call.\n",
      "  :type api_params: Any\n",
      "\n",
      "  Usage:\n",
      "  The decorated function can return either a single prompt or a list of ell.Message objects:\n",
      "\n",
      "  .. code-block:: python\n",
      "\n",
      "      @ell.simple(model=\"gpt-4\", temperature=0.7)\n",
      "      def summarize_text(text: str) -> str:\n",
      "          '''You are an expert at summarizing text.''' # System prompt\n",
      "          return f\"Please summarize the following text:\\\\n\\\\n{text}\" # User prompt\n",
      "\n",
      "\n",
      "      @ell.simple(model=\"gpt-4\", temperature=0.7)\n",
      "      def describe_image(image : PIL.Image.Image) -> List[ell.Message]:\n",
      "          '''Describe the contents of an image.''' # unused because we're returning a list of Messages\n",
      "          return [\n",
      "              # helper function for ell.Message(text=\"...\", role=\"system\")\n",
      "              ell.system(\"You are an AI trained to describe images.\"),\n",
      "              # helper function for ell.Message(content=\"...\", role=\"user\")\n",
      "              ell.user([\"Describe this image in detail.\", image]),\n",
      "          ]\n",
      "\n",
      "\n",
      "      image_description = describe_image(PIL.Image.open(\"https://example.com/image.jpg\"))\n",
      "      print(image_description) \n",
      "      # Output will be a string text-only description of the image\n",
      "\n",
      "      summary = summarize_text(\"Long text to summarize...\")\n",
      "      print(summary)\n",
      "      # Output will be a text-only summary\n",
      "\n",
      "  Notes:\n",
      "\n",
      "  - This decorator is designed for text-only model outputs, but supports multimodal inputs.\n",
      "  - It simplifies complex responses from language models to text-only format, regardless of \n",
      "    the model's capability for structured outputs, function calling, or multimodal outputs.\n",
      "  - For preserving complex model outputs (e.g., structured data, function calls, or multimodal \n",
      "    outputs), use the @ell.complex decorator instead. @ell.complex returns a Message object (role='assistant')\n",
      "  - The decorated function can return a string or a list of ell.Message objects for more \n",
      "    complex prompts, including multimodal inputs.\n",
      "  - If called with n > 1 in api_params, the wrapped LMP will return a list of strings for the n parallel outputs\n",
      "    of the model instead of just one string. Otherwise, it will return a single string.\n",
      "  - You can pass LM API parameters either in the decorator or when calling the decorated function.\n",
      "    Parameters passed during the function call will override those set in the decorator.\n",
      "\n",
      "  Example of passing LM API params:\n",
      "\n",
      "  .. code-block:: python\n",
      "\n",
      "      @ell.simple(model=\"gpt-4\", temperature=0.7)\n",
      "      def generate_story(prompt: str) -> str:\n",
      "          return f\"Write a short story based on this prompt: {prompt}\"\n",
      "\n",
      "      # Using default parameters\n",
      "      story1 = generate_story(\"A day in the life of a time traveler\")\n",
      "\n",
      "      # Overriding parameters during function call\n",
      "      story2 = generate_story(\"An AI's first day of consciousness\", api_params={\"temperature\": 0.9, \"max_tokens\": 500})\n",
      "\n",
      "  See Also:\n",
      "\n",
      "  - :func:`ell.complex`: For LMPs that preserve full structure of model responses, including multimodal outputs.\n",
      "  - :func:`ell.tool`: For defining tools that can be used within complex LMPs.\n",
      "  - :mod:`ell.studio`: For visualizing and analyzing LMP executions.\n",
      "    \"\"\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from ell.configurator import config\n",
      "from ell.lmp._track import _track\n",
      "from ell.provider import EllCallParams\n",
      "from ell.types._lstr import _lstr\n",
      "from ell.types import Message, ContentBlock\n",
      "from ell.types.message import LMP, InvocableLM, LMPParams, MessageOrDict, _lstr_generic\n",
      "from ell.types.studio import LMPType\n",
      "from ell.util._warnings import _no_api_key_warning, _warnings\n",
      "from ell.util.verbosity import compute_color, model_usage_logger_pre\n",
      "\n",
      "from ell.util.verbosity import model_usage_logger_post_end, model_usage_logger_post_intermediate, model_usage_logger_post_start\n",
      "\n",
      "from functools import wraps\n",
      "from typing import Any, Dict, Optional, List, Callable, Tuple, Union\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):\n",
      "    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:\n",
      "        # ... other code\n",
      "\n",
      "\n",
      "        wrapper.__ell_tool_kwargs__ = tool_kwargs\n",
      "        wrapper.__ell_func__ = _under_fn\n",
      "        wrapper.__ell_type__ = LMPType.TOOL\n",
      "        wrapper.__ell_exempt_from_tracking = exempt_from_tracking\n",
      "\n",
      "        # Construct the pydantic mdoel for the _under_fn's function signature parameters.\n",
      "        # 1. Get the function signature.\n",
      "\n",
      "        sig = inspect.signature(fn)\n",
      "\n",
      "        # 2. Create a dictionary of field definitions for the Pydantic model\n",
      "        fields = {}\n",
      "        for param_name, param in sig.parameters.items():\n",
      "            # Skip *args and **kwargs\n",
      "            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n",
      "                continue\n",
      "\n",
      "            # Determine the type annotation\n",
      "            if param.annotation == inspect.Parameter.empty:\n",
      "                raise ValueError(f\"Parameter {param_name} has no type annotation, and cannot be converted into a tool schema for OpenAI and other provisders. Should OpenAI produce a string or an integer, etc, for this parameter?\")\n",
      "            annotation = param.annotation\n",
      "\n",
      "            # Determine the default value\n",
      "            default = param.default\n",
      "\n",
      "            # Check if the parameter has a Field with description\n",
      "            if isinstance(param.default, FieldInfo):\n",
      "                field = param.default\n",
      "                fields[param_name] = (annotation, field)\n",
      "            elif param.default != inspect.Parameter.empty:\n",
      "                fields[param_name] = (annotation, param.default)\n",
      "            else:\n",
      "                # If no default value, use Field without default\n",
      "                fields[param_name] = (annotation, Field(...))\n",
      "\n",
      "        # 3. Create the Pydantic model\n",
      "        model_name = f\"{fn.__name__}\"\n",
      "        ParamsModel = create_model(model_name, **fields)\n",
      "\n",
      "        # Attach the Pydantic model to the wrapper function\n",
      "        wrapper.__ell_params_model__ = ParamsModel\n",
      "\n",
      "        # handle tracking last.\n",
      "        if exempt_from_tracking:\n",
      "            ret = wrapper\n",
      "        else:\n",
      "            ret=  _track(wrapper)\n",
      "\n",
      "        # Helper function to get the Pydantic model for the tool\n",
      "        def get_params_model():\n",
      "            return wrapper.__ell_params_model__\n",
      "\n",
      "        # Attach the helper function to the wrapper\n",
      "        wrapper.get_params_model = get_params_model\n",
      "        ret.get_params_model = get_params_model\n",
      "        return ret\n",
      "\n",
      "    return tool_decorator\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from functools import wraps\n",
      "import json\n",
      "from typing import Any, Callable, Optional\n",
      "\n",
      "from pydantic import Field, create_model\n",
      "from pydantic.fields import FieldInfo\n",
      "from ell.lmp._track import _track\n",
      "# from ell.types import ToolFunction, InvocableTool, ToolParams\n",
      "# from ell.util.verbosity import compute_color, tool_usage_logger_pre\n",
      "from ell.configurator import config\n",
      "from ell.types._lstr import _lstr\n",
      "from ell.types.studio import LMPType\n",
      "import inspect\n",
      "\n",
      "from ell.types.message import ContentBlock, InvocableTool, ToolResult, to_content_blocks\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "tool.__doc__ = \"\"\"Defines a tool for use in language model programs (LMPs) that support tool use.\n",
      "\n",
      "This decorator wraps a function, adding metadata and handling for tool invocations.\n",
      "It automatically extracts the tool's description and parameters from the function's\n",
      "docstring and type annotations, creating a structured representation for LMs to use.\n",
      "\n",
      ":param exempt_from_tracking: If True, the tool usage won't be tracked. Default is False.\n",
      ":type exempt_from_tracking: bool\n",
      ":param tool_kwargs: Additional keyword arguments for tool configuration.\n",
      ":return: A wrapped version of the original function, usable as a tool by LMs.\n",
      ":rtype: Callable\n",
      "\n",
      "Requirements:\n",
      "\n",
      "- Function must have fully typed arguments (Pydantic-serializable).\n",
      "- Return value must be one of: str, JSON-serializable object, Pydantic model, or List[ContentBlock].\n",
      "- All parameters must have type annotations.\n",
      "- Complex types should be Pydantic models.\n",
      "- Function should have a descriptive docstring.\n",
      "- Can only be used in LMPs with @ell.complex decorators\n",
      "\n",
      "Functionality:\n",
      "\n",
      "1. Metadata Extraction:\n",
      "    - Uses function docstring as tool description.\n",
      "    - Extracts parameter info from type annotations and docstring.\n",
      "    - Creates a Pydantic model for parameter validation and schema generation.\n",
      "\n",
      "2. Integration with LMs:\n",
      "    - Can be passed to @ell.complex decorators.\n",
      "    - Provides structured tool information to LMs.\n",
      "\n",
      "3. Invocation Handling:\n",
      "    - Manages tracking, logging, and result processing.\n",
      "    - Wraps results in appropriate types (e.g., _lstr) for tracking.\n",
      "\n",
      "Usage Modes:\n",
      "\n",
      "1. Normal Function Call:\n",
      "    - Behaves like a regular Python function.\n",
      "    - Example: result = my_tool(arg1=\"value\", arg2=123)\n",
      "\n",
      "2. LMP Tool Call:\n",
      "    - Used within LMPs or with explicit _tool_call_id.\n",
      "    - Returns a ToolResult object.\n",
      "    - Example: result = my_tool(arg1=\"value\", arg2=123, _tool_call_id=\"unique_id\")\n",
      "\n",
      "Result Coercion:\n",
      "\n",
      "- String  ContentBlock(text=result)\n",
      "- Pydantic BaseModel  ContentBlock(parsed=result)\n",
      "- List[ContentBlock]  Used as-is\n",
      "- Other types  ContentBlock(text=json.dumps(result))\n",
      "\n",
      "Example::\n",
      "\n",
      "    @ell.tool()\n",
      "    def create_claim_draft(\n",
      "        claim_details: str,\n",
      "        claim_type: str,\n",
      "        claim_amount: float,\n",
      "        claim_date: str = Field(description=\"Date format: YYYY-MM-DD\")\n",
      "    ) -> str:\n",
      "        '''Create a claim draft. Returns the created claim ID.'''\n",
      "        return \"12345\"\n",
      "\n",
      "    # For use in a complex LMP:\n",
      "    @ell.complex(model=\"gpt-4\", tools=[create_claim_draft], temperature=0.1)\n",
      "    def insurance_chatbot(message_history: List[Message]) -> List[Message]:\n",
      "        # Chatbot implementation...\n",
      "\n",
      "    x = insurance_chatbot([\n",
      "        ell.user(\"I crashed my car into a tree.\"),\n",
      "        ell.assistant(\"I'm sorry to hear that. Can you provide more details?\"),\n",
      "        ell.user(\"The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000\")\n",
      "    ]) \n",
      "    print(x)\n",
      "    '''ell.Message(content=[\n",
      "        ContentBlock(tool_call(\n",
      "            tool_call_id=\"asdas4e\",\n",
      "            tool_fn=create_claim_draft,\n",
      "            input=create_claim_draftParams({\n",
      "                claim_details=\"The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000\",\n",
      "                claim_type=\"car\",\n",
      "                claim_amount=5000,\n",
      "                claim_date=\"2024-08-01\"\n",
      "            })\n",
      "        ))\n",
      "    ], role='assistant')'''\n",
      "    \n",
      "    if x.tool_calls:\n",
      "        next_user_message = response_message.call_tools_and_collect_as_message()\n",
      "        # This actually calls create_claim_draft\n",
      "        print(next_user_message)\n",
      "        '''\n",
      "        ell.Message(content=[\n",
      "            ContentBlock(tool_result=ToolResult(\n",
      "                tool_call_id=\"asdas4e\",\n",
      "                result=[ContentBlock(text=\"12345\")]\n",
      "            ))\n",
      "        ], role='user')\n",
      "        '''\n",
      "        y = insurance_chatbot(message_history + [x, next_user_message])\n",
      "        print(y)\n",
      "        '''\n",
      "        ell.Message(\"I've filed that for you!\", role='assistant')\n",
      "        '''\n",
      "\n",
      "Note:\n",
      "- Tools are integrated into LMP calls via the 'tools' parameter in @ell.complex.\n",
      "- LMs receive structured tool information, enabling understanding and usage within the conversation context.\n",
      "    \"\"\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _get_messages(prompt_ret: Union[str, list[MessageOrDict]], prompt: LMP) -> list[Message]:\n",
      "    \"\"\"\n",
      "    Helper function to convert the output of an LMP into a list of Messages.\n",
      "    \"\"\"\n",
      "    if isinstance(prompt_ret, str):\n",
      "        has_system_prompt = prompt.__doc__ is not None and prompt.__doc__.strip() != \"\"\n",
      "        messages =     [Message(role=\"system\", content=[ContentBlock(text=_lstr(prompt.__doc__ ) )])] if has_system_prompt else []\n",
      "        return messages + [\n",
      "            Message(role=\"user\", content=[ContentBlock(text=prompt_ret)])\n",
      "        ]\n",
      "    else:\n",
      "        assert isinstance(\n",
      "            prompt_ret, list\n",
      "        ), \"Need to pass a list of Messages to the language model\"\n",
      "        return prompt_ret\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):\n",
      "    def parameterized_lm_decorator(\n",
      "        prompt: LMP,\n",
      "    ) -> Callable[..., Union[List[Message], Message]]:\n",
      "        # ... other code\n",
      "\n",
      "\n",
      "\n",
      "        model_call.__ell_api_params__ = default_api_params_from_decorator #type: ignore\n",
      "        model_call.__ell_func__ = prompt #type: ignore\n",
      "        model_call.__ell_type__ = LMPType.LM #type: ignore\n",
      "        model_call.__ell_exempt_from_tracking = exempt_from_tracking #type: ignore\n",
      "\n",
      "\n",
      "        if exempt_from_tracking:\n",
      "            return model_call\n",
      "        else:\n",
      "            # XXX: Analyze decorators with AST instead.\n",
      "            return _track(model_call, forced_dependencies=dict(tools=tools, response_format=api_params.get(\"response_format\", {})))\n",
      "    return parameterized_lm_decorator\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):\n",
      "    default_client_from_decorator = client\n",
      "    default_model_from_decorator = model\n",
      "    default_api_params_from_decorator = api_params\n",
      "    def parameterized_lm_decorator(\n",
      "        prompt: LMP,\n",
      "    ) -> Callable[..., Union[List[Message], Message]]:\n",
      "        _warnings(model, prompt, default_client_from_decorator)\n",
      "\n",
      "        @wraps(prompt)\n",
      "        def model_call(\n",
      "            *prompt_args,\n",
      "            _invocation_origin : Optional[str] = None,\n",
      "            client: Optional[Any] = None,\n",
      "            api_params: Optional[Dict[str, Any]] = None,\n",
      "            lm_params: Optional[DeprecationWarning] = None,\n",
      "            **prompt_kwargs,\n",
      "        ) -> Tuple[Any, Any, Any]:\n",
      "            # XXX: Deprecation in 0.1.0\n",
      "            if lm_params:\n",
      "                raise DeprecationWarning(\"lm_params is deprecated. Use api_params instead.\")\n",
      "\n",
      "            # promt -> str\n",
      "            res = prompt(*prompt_args, **prompt_kwargs)\n",
      "            # Convert prompt into ell messages\n",
      "            messages = _get_messages(res, prompt)\n",
      "\n",
      "            # XXX: move should log to a logger.\n",
      "            should_log = not exempt_from_tracking and config.verbose\n",
      "            # Cute verbose logging.\n",
      "            if should_log: model_usage_logger_pre(prompt, prompt_args, prompt_kwargs, \"[]\", messages) #type: ignore\n",
      "\n",
      "            # Call the model.\n",
      "            # Merge API params\n",
      "            merged_api_params = {**config.default_api_params, **default_api_params_from_decorator, **(api_params or {})}\n",
      "            n = merged_api_params.get(\"n\", 1)\n",
      "            # Merge client overrides & client registry\n",
      "            merged_client = _client_for_model(model, client or default_client_from_decorator)\n",
      "            ell_call = EllCallParams(\n",
      "                # XXX: Could change behaviour of overriding ell params for dyanmic tool calls.\n",
      "                model=merged_api_params.pop(\"model\", default_model_from_decorator),\n",
      "                messages=messages,\n",
      "                client = merged_client,\n",
      "                api_params=merged_api_params,\n",
      "                tools=tools or [],\n",
      "            )\n",
      "            # Get the provider for the model\n",
      "            provider = config.get_provider_for(ell_call.client)\n",
      "            assert provider is not None, f\"No provider found for client {ell_call.client}.\"\n",
      "\n",
      "            if should_log: model_usage_logger_post_start(n)\n",
      "            with model_usage_logger_post_intermediate(n) as _logger:\n",
      "                (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)\n",
      "                if isinstance(result, list) and len(result) == 1:\n",
      "                    result = result[0]\n",
      "\n",
      "            result = post_callback(result) if post_callback else result\n",
      "            if should_log:\n",
      "                model_usage_logger_post_end()\n",
      "            #\n",
      "            #  These get sent to track. This is wack.           \n",
      "            return result, final_api_params, metadata\n",
      "        # ... other code\n",
      "    # ... other code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _client_for_model(\n",
      "    model: str,\n",
      "    client: Optional[Any] = None,\n",
      "    _name: Optional[str] = None,\n",
      ") -> Any:\n",
      "    # XXX: Move to config to centralize api keys etc.\n",
      "    if not client:\n",
      "        client, was_fallback = config.get_client_for(model)\n",
      "\n",
      "        # XXX: Wrong.\n",
      "        if not client and not was_fallback:\n",
      "            raise RuntimeError(_no_api_key_warning(model, _name, '', long=True, error=True))\n",
      "\n",
      "    if client is None:\n",
      "        raise ValueError(f\"No client found for model '{model}'. Ensure the model is registered using 'register_model' in 'config.py' or specify a client directly using the 'client' argument in the decorator or function call.\")\n",
      "    return client\n",
      "\n",
      "\n",
      "complex.__doc__ =\n",
      " # ... other code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):\n",
      "    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:\n",
      "        _under_fn = fn\n",
      "\n",
      "        @wraps(fn)\n",
      "        def wrapper(\n",
      "            *fn_args,\n",
      "            _invocation_origin: str = None,\n",
      "            _tool_call_id: str = None,\n",
      "            **fn_kwargs\n",
      "        ):\n",
      "            #XXX: Post release, we need to wrap all tool arguments in type primitives for tracking I guess or change that tool makes the tool function inoperable.\n",
      "            #XXX: Most people are not going to manually try and call the tool without a type primitive and if they do it will most likely be wrapped with l strs.\n",
      "\n",
      "            if config.verbose and not exempt_from_tracking:\n",
      "                pass\n",
      "                # tool_usage_logger_pre(fn, fn_args, fn_kwargs, name, color)\n",
      "\n",
      "            result = fn(*fn_args, **fn_kwargs)\n",
      "\n",
      "            _invocation_api_params = dict(tool_kwargs=tool_kwargs)\n",
      "\n",
      "            # Here you might want to add logic for tracking the tool usage\n",
      "            # Similar to how it's done in the lm decorator # Use _invocation_origin\n",
      "\n",
      "            if isinstance(result, str) and _invocation_origin:\n",
      "                result = _lstr(result,origin_trace=_invocation_origin)\n",
      "\n",
      "            #XXX: This _tool_call_id thing is a hack. Tracking should happen via params in the api\n",
      "            # So if you call wiuth a _tool_callId\n",
      "            if _tool_call_id:\n",
      "                # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.\n",
      "                try:\n",
      "                    if isinstance(result, ContentBlock):\n",
      "                        content_results = [result]\n",
      "                    elif isinstance(result, list) and all(isinstance(c, ContentBlock) for c in result):\n",
      "                        content_results = result\n",
      "                    else:\n",
      "                        content_results = [ContentBlock(text=_lstr(json.dumps(result, ensure_ascii=False),origin_trace=_invocation_origin))]\n",
      "                except TypeError as e:\n",
      "                    raise TypeError(f\"Failed to convert tool use result to ContentBlock: {e}. Tools must return json serializable objects. or a list of ContentBlocks.\")\n",
      "                # XXX: Need to support images and other content types somehow. We should look for images inside of the the result and then go from there.\n",
      "                # try:\n",
      "                #     content_results = coerce_content_list(result)\n",
      "                # except ValueError as e:\n",
      "\n",
      "                # TODO: poolymorphic validation here is important (cant have tool_call or formatted_response in the result)\n",
      "                # XXX: Should we put this coercion here or in the tool call/result area.\n",
      "                for c in content_results:\n",
      "                    assert not c.tool_call, \"Tool call in tool result\"\n",
      "                    # assert not c.formatted_response, \"Formatted response in tool result\"\n",
      "                    if c.parsed:\n",
      "                        # Warning: Formatted response in tool result will be converted to text\n",
      "                        # TODO: Logging needs to produce not print.\n",
      "                        print(f\"Warning: Formatted response in tool result will be converted to text. Original: {c.parsed}\")\n",
      "                        c.text = _lstr(c.parsed.model_dump_json(),origin_trace=_invocation_origin)\n",
      "                        c.parsed = None\n",
      "                    assert not c.audio, \"Audio in tool result\"\n",
      "                return ToolResult(tool_call_id=_tool_call_id, result=content_results), _invocation_api_params, {}\n",
      "            else:\n",
      "                return result, _invocation_api_params, {}\n",
      "        # ... other code\n",
      "    # ... other code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Your task is to compare the following clusters and choose which cluster forms a more cohesive grouping of source code chunks. You should\n",
      "penalize a cluster for including too much unrelated code or for missing important code that should have been included.\n",
      "Here are the clusters:\n",
      "Cluster 0:\n",
      "Language Model Program Configuration:\n",
      "lmp\\complex.py::1\n",
      "lmp\\complex.py::3\n",
      "lmp\\complex.py::4\n",
      "lmp\\complex.py::5\n",
      "lmp\\simple.py::1\n",
      "lmp\\simple.py::2\n",
      "lmp\\tool.py::1\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::3\n",
      "lmp\\tool.py::4\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cluster 1:\n",
      "LMP and Complex Interactions:\n",
      "lmp\\complex.py::1\n",
      "lmp\\complex.py::2\n",
      "lmp\\complex.py::3\n",
      "lmp\\complex.py::4\n",
      "lmp\\simple.py::1\n",
      "lmp\\simple.py::2\n",
      "lmp\\tool.py::1\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::3\n",
      "lmp\\tool.py::4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now output your decision by selecting the number of the cluster that you would score higher on overall cohesion\n",
      "\n",
      "\n",
      "You are given the following set of source code chunks:\n",
      "def _process_dependencies(func, globals_and_frees, already_closed, recursion_stack, uses):\n",
      "    \"\"\"Process function dependencies.\"\"\"\n",
      "    dependencies = []\n",
      "    modules = deque()\n",
      "    imports = []\n",
      "\n",
      "    if isinstance(func, (types.FunctionType, types.MethodType)):\n",
      "        _process_default_kwargs(func, dependencies, already_closed, recursion_stack, uses)\n",
      "\n",
      "    for var_name, var_value in itertools.chain(globals_and_frees['globals'].items(), globals_and_frees['frees'].items()):\n",
      "        _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack, uses)\n",
      "\n",
      "    return dependencies, imports, modules\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _build_initial_source(imports, dependencies, source):\n",
      "    \"\"\"Build the initial source code.\"\"\"\n",
      "    return f\"{DELIM}\\n\" + f\"\\n{DELIM}\\n\".join(imports + dependencies + [source]) + f\"\\n{DELIM}\\n\"\n",
      "\n",
      "def _process_modules(modules, cur_src, already_closed, recursion_stack, uses):\n",
      "    \"\"\"Process module dependencies.\"\"\"\n",
      "    reverse_module_src = deque()\n",
      "    while modules:\n",
      "        mname, mval = modules.popleft()\n",
      "        mdeps = []\n",
      "        attrs_to_extract = get_referenced_names(cur_src.replace(DELIM, \"\"), mname)\n",
      "        for attr in attrs_to_extract:\n",
      "            _process_module_attribute(mname, mval, attr, mdeps, modules, already_closed, recursion_stack, uses)\n",
      "\n",
      "        mdeps.insert(0, f\"# Extracted from module: {mname}\")\n",
      "        reverse_module_src.appendleft(\"\\n\".join(mdeps))\n",
      "\n",
      "        cur_src = _dereference_module_names(cur_src, mname, attrs_to_extract)\n",
      "\n",
      "    return list(reverse_module_src)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _process_default_kwargs(func, dependencies, already_closed, recursion_stack, uses):\n",
      "    \"\"\"Process default keyword arguments and annotations of a function.\"\"\"\n",
      "    ps = inspect.signature(func).parameters\n",
      "    for name, param in ps.items():\n",
      "        if param.default is not inspect.Parameter.empty:\n",
      "            _process_signature_dependency(param.default, dependencies, already_closed, recursion_stack, uses, name)\n",
      "        if param.annotation is not inspect.Parameter.empty:\n",
      "            _process_signature_dependency(param.annotation, dependencies, already_closed, recursion_stack, uses, f\"{name}_annotation\")\n",
      "    if func.__annotations__.get('return') is not None:\n",
      "        _process_signature_dependency(func.__annotations__['return'], dependencies, already_closed, recursion_stack, uses, \"return_annotation\")\n",
      "    # XXX: In order to properly analyze this we should walk the AST rather than inspexting the signature; e.g. Field is FieldInfo not Field.\n",
      "    # I don't care about the actual default at time of execution just the symbols required to statically reproduce the prompt.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\"\"\"\n",
      "This should do the following.\n",
      "# prompt_consts.py\n",
      "import math\n",
      "def test():\n",
      "    return math.sin(10)\n",
      "\n",
      "# lol3.py\n",
      "import prompt_consts\n",
      "\n",
      "X = 7\n",
      "def xD():\n",
      "    print(X)\n",
      "    return prompt_consts.test()\n",
      "\n",
      "###\n",
      "Our goal is to use AST & dill to get a full lexical closured source of xD, with the exception of modules that are stored in site-packages. For example.\n",
      "\n",
      "lexical_extration(xD) returns\n",
      "#closure.py\n",
      "import math\n",
      "def test():\n",
      "    return math.sin(10)\n",
      "\n",
      "X = 7 \n",
      "def xD():\n",
      "    print(X)\n",
      "    return test()\n",
      "\n",
      "\"\"\"\n",
      "import collections\n",
      "import ast\n",
      "import hashlib\n",
      "import itertools\n",
      "from typing import Any, Dict, Iterable, Optional, Set, Tuple, Callable\n",
      "import dill\n",
      "import inspect\n",
      "import types\n",
      "from dill.source import getsource\n",
      "import re\n",
      "from collections import deque\n",
      "import black\n",
      "\n",
      "from ell.util.serialization import is_immutable_variable\n",
      "from ell.util.should_import import should_import\n",
      "\n",
      "DELIM = \"$$$$$$$$$$$$$$$$$$$$$$$$$\"\n",
      "FORBIDDEN_NAMES = [\"ell\", \"lstr\"]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def lexical_closure(\n",
      "    func: Any,\n",
      "    already_closed: Set[int] = None,\n",
      "    initial_call: bool = False,\n",
      "    recursion_stack: list = None,\n",
      "    forced_dependencies: Optional[Dict[str, Any]] = None\n",
      ") -> Tuple[str, Tuple[str, str], Set[str]]:\n",
      "    \"\"\"\n",
      "    Generate a lexical closure for a given function or callable.\n",
      "\n",
      "    Args:\n",
      "        func: The function or callable to process.\n",
      "        already_closed: Set of already processed function hashes.\n",
      "        initial_call: Whether this is the initial call to the function.\n",
      "        recursion_stack: Stack to keep track of the recursion path.\n",
      "\n",
      "    Returns:\n",
      "        A tuple containing:\n",
      "        - The full source code of the closure\n",
      "        - A tuple of (function source, dependencies source)\n",
      "        - A set of function hashes that this closure uses\n",
      "    \"\"\"\n",
      "    already_closed = already_closed or set()\n",
      "    uses = set()\n",
      "    forced_dependencies = forced_dependencies or {}\n",
      "    recursion_stack = recursion_stack or []\n",
      "\n",
      "    if hash(func) in already_closed:\n",
      "        return \"\", (\"\", \"\"), set()\n",
      "\n",
      "    recursion_stack.append(getattr(func, '__qualname__', str(func)))\n",
      "\n",
      "    outer_ell_func = func\n",
      "    while hasattr(func, \"__ell_func__\"):\n",
      "        func = func.__ell_func__\n",
      "\n",
      "    source = getsource(func, lstrip=True)\n",
      "    already_closed.add(hash(func))\n",
      "\n",
      "    globals_and_frees = _get_globals_and_frees(func)\n",
      "    dependencies, imports, modules = _process_dependencies(func, globals_and_frees, already_closed, recursion_stack, uses)\n",
      "    for k,v in forced_dependencies.items():\n",
      "        # Todo: dictionary not necessary\n",
      "        _process_signature_dependency(v, dependencies, already_closed, recursion_stack, uses, k)\n",
      "\n",
      "    cur_src = _build_initial_source(imports, dependencies, source)\n",
      "\n",
      "    module_src = _process_modules(modules, cur_src, already_closed, recursion_stack, uses)\n",
      "\n",
      "    dirty_src = _build_final_source(imports, module_src, dependencies, source)\n",
      "    dirty_src_without_func = _build_final_source(imports, module_src, dependencies, \"\")\n",
      "\n",
      "    CLOSURE_SOURCE[hash(func)] = dirty_src\n",
      "\n",
      "    dsrc = _clean_src(dirty_src_without_func)\n",
      "\n",
      "    # Format the sorce and dsrc soruce using Black\n",
      "    source = _format_source(source)\n",
      "    dsrc = _format_source(dsrc)\n",
      "\n",
      "    fn_hash = _generate_function_hash(source, dsrc, func.__qualname__)\n",
      "\n",
      "    _update_ell_func(outer_ell_func, source, dsrc, globals_and_frees['globals'], globals_and_frees['frees'], fn_hash, uses)\n",
      "\n",
      "    return (dirty_src, (source, dsrc), ({outer_ell_func} if not initial_call and hasattr(outer_ell_func, \"__ell_func__\") else uses))\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses):\n",
      "    \"\"\"Process a callable (function, method, or class).\"\"\"\n",
      "    try:\n",
      "        module_is_ell = 'ell' in inspect.getmodule(var_value).__name__\n",
      "    except:\n",
      "        module_is_ell = False\n",
      "\n",
      "    if var_name not in FORBIDDEN_NAMES and not module_is_ell:\n",
      "        try:\n",
      "            dep, _, _uses = lexical_closure(var_value, already_closed=already_closed, recursion_stack=recursion_stack.copy())\n",
      "            dependencies.append(dep)\n",
      "            uses.update(_uses)\n",
      "        except Exception as e:\n",
      "            _raise_error(f\"Failed to capture the lexical closure of global or free variable {var_name}\", e, recursion_stack)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack , uses):\n",
      "    \"\"\"Process a single variable.\"\"\"\n",
      "    try:\n",
      "        name = inspect.getmodule(var_value).__name__\n",
      "        if should_import(name):\n",
      "            imports.append(dill.source.getimport(var_value, alias=var_name))\n",
      "            return\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    if isinstance(var_value, (types.FunctionType, type, types.MethodType)):\n",
      "        _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses)\n",
      "    elif isinstance(var_value, types.ModuleType):\n",
      "        _process_module(var_name, var_value, modules, imports, uses)\n",
      "    elif isinstance(var_value, types.BuiltinFunctionType):\n",
      "        imports.append(dill.source.getimport(var_value, alias=var_name))\n",
      "    else:\n",
      "        _process_other_variable(var_name, var_value, dependencies, uses)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _format_source(source: str) -> str:\n",
      "    \"\"\"Format the source code using Black.\"\"\"\n",
      "    try:\n",
      "        return black.format_str(source, mode=black.Mode())\n",
      "    except:\n",
      "        # If Black formatting fails, return the original source\n",
      "        return source\n",
      "\n",
      "def _get_globals_and_frees(func: Callable) -> Dict[str, Dict]:\n",
      "    \"\"\"Get global and free variables for a function.\"\"\"\n",
      "    globals_dict = collections.OrderedDict(globalvars(func))\n",
      "    frees_dict = collections.OrderedDict(dill.detect.freevars(func))\n",
      "\n",
      "    if isinstance(func, type):\n",
      "        for name, method in collections.OrderedDict(func.__dict__).items():\n",
      "            if isinstance(method, (types.FunctionType, types.MethodType)):\n",
      "                globals_dict.update(collections.OrderedDict(dill.detect.globalvars(method)))\n",
      "                frees_dict.update(collections.OrderedDict(dill.detect.freevars(method)))\n",
      "\n",
      "    return {'globals': globals_dict, 'frees': frees_dict}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "import ast\n",
      "import importlib\n",
      "import os\n",
      "import black\n",
      "from dill.detect import nestedglobals\n",
      "\n",
      "import inspect\n",
      "\n",
      "import inspect\n",
      "\n",
      "#!/usr/bin/env python\n",
      "#\n",
      "def globalvars(func, recurse=True, builtin=False):\n",
      "    \"\"\"get objects defined in global scope that are referred to by func\n",
      "\n",
      "    return a dict of {name:object}\"\"\"\n",
      "    while hasattr(func, \"__ell_func__\"):\n",
      "        func = func.__ell_func__\n",
      "    if inspect.ismethod(func): func = func.__func__\n",
      "    while hasattr(func, \"__ell_func__\"):\n",
      "        func = func.__ell_func__\n",
      "    if inspect.isfunction(func):\n",
      "        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}\n",
      "        # get references from within closure\n",
      "        orig_func, func = func, set()\n",
      "        for obj in orig_func.__closure__ or {}:\n",
      "            try:\n",
      "                cell_contents = obj.cell_contents\n",
      "            except ValueError: # cell is empty\n",
      "                pass\n",
      "            else:\n",
      "                _vars = globalvars(cell_contents, recurse, builtin) or {}\n",
      "                func.update(_vars) #XXX: (above) be wary of infinte recursion?\n",
      "                globs.update(_vars)\n",
      "        # get globals\n",
      "        globs.update(orig_func.__globals__ or {})\n",
      "        # get names of references\n",
      "        if not recurse:\n",
      "            func.update(orig_func.__code__.co_names)\n",
      "        else:\n",
      "            func.update(nestedglobals(orig_func.__code__))\n",
      "            # find globals for all entries of func\n",
      "            for key in func.copy(): #XXX: unnecessary...?\n",
      "                nested_func = globs.get(key)\n",
      "                if nested_func is orig_func:\n",
      "                   #func.remove(key) if key in func else None\n",
      "                    continue  #XXX: globalvars(func, False)?\n",
      "                func.update(globalvars(nested_func, True, builtin))\n",
      "    elif inspect.iscode(func):\n",
      "        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}\n",
      "       #globs.update(globals())\n",
      "        if not recurse:\n",
      "            func = func.co_names # get names\n",
      "        else:\n",
      "            orig_func = func.co_name # to stop infinite recursion\n",
      "            func = set(nestedglobals(func))\n",
      "            # find globals for all entries of func\n",
      "            for key in func.copy(): #XXX: unnecessary...?\n",
      "                if key is orig_func:\n",
      "                   #func.remove(key) if key in func else None\n",
      "                    continue  #XXX: globalvars(func, False)?\n",
      "                nested_func = globs.get(key)\n",
      "                func.update(globalvars(nested_func, True, builtin))\n",
      "    else:\n",
      "        return {}\n",
      "    #NOTE: if name not in __globals__, then we skip it...\n",
      "    return dict((name,globs[name]) for name in func if name in globs)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _process_module(var_name, var_value, modules, imports, uses):\n",
      "    \"\"\"Process a module.\"\"\"\n",
      "    if should_import(var_value.__name__):\n",
      "        imports.append(dill.source.getimport(var_value, alias=var_name))\n",
      "    else:\n",
      "        modules.append((var_name, var_value))\n",
      "\n",
      "def _process_other_variable(var_name, var_value, dependencies, uses):\n",
      "    \"\"\"Process variables that are not callables or modules.\"\"\"\n",
      "    if isinstance(var_value, str) and '\\n' in var_value:\n",
      "        dependencies.append(f\"{var_name} = '''{var_value}'''\")\n",
      "    elif is_immutable_variable(var_value):\n",
      "        dependencies.append(f\"#<BV>\\n{var_name} = {repr(var_value)}\\n#</BV>\")\n",
      "    else:\n",
      "        dependencies.append(f\"#<BmV>\\n{var_name} = <{type(var_value).__name__} object>\\n#</BmV>\")\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def _process_signature_dependency(val, dependencies, already_closed, recursion_stack, uses, name: Optional[str] = None):\n",
      "    # Todo: Build general cattr like utility for unstructuring python objects with hooks that keep track of state variables.\n",
      "    # Todo: break up closure into types and functions.\n",
      "    # XXX: This is not exhaustive, we should determine should import on all dependencies\n",
      "\n",
      "    if name not in FORBIDDEN_NAMES:\n",
      "        try:\n",
      "            dep = None\n",
      "            _uses = None\n",
      "            if isinstance(val, (types.FunctionType, types.MethodType)):\n",
      "                dep, _, _uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())\n",
      "            elif isinstance(val, (list, tuple, set)):\n",
      "                for item in val:\n",
      "                    _process_signature_dependency(item, dependencies, already_closed, recursion_stack, uses)\n",
      "            else:\n",
      "                val_class = val if isinstance(val, type) else val.__class__\n",
      "                try:\n",
      "                    is_builtin = (val_class.__module__ == \"builtins\" or val_class.__module__ == \"__builtins__\")\n",
      "                except:\n",
      "                    is_builtin = False\n",
      "\n",
      "                if not is_builtin:\n",
      "                    if should_import(val_class.__module__):\n",
      "                        dependencies.append(dill.source.getimport(val_class, alias=val_class.__name__))\n",
      "                    else:\n",
      "                        dep, _, _uses = lexical_closure(val_class, already_closed=already_closed, recursion_stack=recursion_stack.copy())\n",
      "\n",
      "            if dep: dependencies.append(dep)\n",
      "            if _uses: uses.update(_uses)\n",
      "        except Exception as e:\n",
      "            _raise_error(f\"Failed to capture the lexical closure of parameter or annotation {name}\", e, recursion_stack)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Your task is to compare the following clusters and choose which cluster forms a more cohesive grouping of source code chunks. You should\n",
      "penalize a cluster for including too much unrelated code or for missing important code that should have been included.\n",
      "Here are the clusters:\n",
      "Cluster 0:\n",
      "Lexical Closure and Code Introspection:\n",
      "util\\closure.py::1\n",
      "util\\closure.py::2\n",
      "util\\closure.py::3\n",
      "util\\closure.py::4\n",
      "util\\closure.py::5\n",
      "util\\closure.py::6\n",
      "util\\closure.py::7\n",
      "util\\closure.py::8\n",
      "util\\closure.py::9\n",
      "util\\closure.py::10\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cluster 1:\n",
      "Function and Execution Closure Handling:\n",
      "util\\closure.py::2\n",
      "util\\closure.py::3\n",
      "util\\closure.py::4\n",
      "util\\closure.py::5\n",
      "util\\closure.py::6\n",
      "util\\closure.py::7\n",
      "util\\closure.py::8\n",
      "util\\closure.py::9\n",
      "util\\closure.py::10\n",
      "util\\closure_util.py::1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now output your decision by selecting the number of the cluster that you would score higher on overall cohesion\n",
      "\n",
      "\n",
      "You are given the following set of source code chunks:\n",
      "from datetime import datetime\n",
      "from typing import List, Optional, Dict, Any\n",
      "from sqlmodel import SQLModel\n",
      "from ell.types import SerializedLMPBase, InvocationBase, InvocationContentsBase\n",
      "\n",
      "\n",
      "class SerializedLMPWithUses(SerializedLMPBase):\n",
      "    lmp_id : str\n",
      "    uses: List[SerializedLMPBase]\n",
      "\n",
      "\n",
      "class InvocationPublic(InvocationBase):\n",
      "    lmp: SerializedLMPBase\n",
      "    uses: List[\"InvocationPublicWithConsumes\"]\n",
      "    contents: InvocationContentsBase\n",
      "\n",
      "class InvocationPublicWithConsumes(InvocationPublic):\n",
      "    consumes: List[InvocationPublic]\n",
      "    consumed_by: List[InvocationPublic]\n",
      "\n",
      "\n",
      "\n",
      "from pydantic import BaseModel\n",
      "\n",
      "class GraphDataPoint(BaseModel):\n",
      "    date: datetime\n",
      "    count: int\n",
      "    avg_latency: float\n",
      "    tokens: int\n",
      "    # cost: float\n",
      "\n",
      "class InvocationsAggregate(BaseModel):\n",
      "    total_invocations: int\n",
      "    total_tokens: int\n",
      "    avg_latency: float\n",
      "    # total_cost: float\n",
      "    unique_lmps: int\n",
      "    # successful_invocations: int\n",
      "    # success_rate: float\n",
      "    graph_data: List[GraphDataPoint]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def create_app(config:Config):\n",
      "    # ... other code\n",
      "\n",
      "\n",
      "\n",
      "    @app.get(\"/api/lmps\", response_model=list[SerializedLMPWithUses])\n",
      "    def get_lmp(\n",
      "        lmp_id: Optional[str] = Query(None),\n",
      "        name: Optional[str] = Query(None),\n",
      "        skip: int = Query(0, ge=0),\n",
      "        limit: int = Query(100, ge=1, le=100),\n",
      "        session: Session = Depends(get_session)\n",
      "    ):\n",
      "\n",
      "        filters : Dict[str, Any] = {}\n",
      "        if name:\n",
      "            filters['name'] = name\n",
      "        if lmp_id:\n",
      "            filters['lmp_id'] = lmp_id\n",
      "\n",
      "        lmps = serializer.get_lmps(session, skip=skip, limit=limit, **filters)\n",
      "\n",
      "        if not lmps:\n",
      "            raise HTTPException(status_code=404, detail=\"LMP not found\")\n",
      "\n",
      "        print(lmps[0])\n",
      "        return lmps\n",
      "\n",
      "\n",
      "\n",
      "    @app.get(\"/api/invocation/{invocation_id}\", response_model=InvocationPublicWithConsumes)\n",
      "    def get_invocation(\n",
      "        invocation_id: str,\n",
      "        session: Session = Depends(get_session)\n",
      "    ):\n",
      "        invocation = serializer.get_invocations(session, lmp_filters=dict(), filters={\"id\": invocation_id})[0]\n",
      "        return invocation\n",
      "    # ... other code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from typing import Optional, Dict, Any\n",
      "\n",
      "from sqlmodel import Session\n",
      "from ell.stores.sql import PostgresStore, SQLiteStore\n",
      "from ell import __version__\n",
      "from fastapi import FastAPI, Query, HTTPException, Depends, Response, WebSocket, WebSocketDisconnect\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "import logging\n",
      "import json\n",
      "from ell.studio.config import Config\n",
      "from ell.studio.connection_manager import ConnectionManager\n",
      "from ell.studio.datamodels import InvocationPublicWithConsumes, SerializedLMPWithUses\n",
      "\n",
      "from ell.types import SerializedLMP\n",
      "from datetime import datetime, timedelta\n",
      "from sqlmodel import select\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "from ell.studio.datamodels import InvocationsAggregate\n",
      "\n",
      "\n",
      "def get_serializer(config: Config):\n",
      "    if config.pg_connection_string:\n",
      "        return PostgresStore(config.pg_connection_string)\n",
      "    elif config.storage_dir:\n",
      "        return SQLiteStore(config.storage_dir)\n",
      "    else:\n",
      "        raise ValueError(\"No storage configuration found\")\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from functools import lru_cache\n",
      "import os\n",
      "from typing import Optional\n",
      "from pydantic import BaseModel\n",
      "\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "# todo. maybe we default storage dir and other things in the future to a well-known location\n",
      "# like ~/.ell or something\n",
      "@lru_cache\n",
      "def ell_home() -> str:\n",
      "    return os.path.join(os.path.expanduser(\"~\"), \".ell\")\n",
      "\n",
      "\n",
      "class Config(BaseModel):\n",
      "    pg_connection_string: Optional[str] = None\n",
      "    storage_dir: Optional[str] = None\n",
      "\n",
      "    @classmethod\n",
      "    def create(\n",
      "        cls,\n",
      "        storage_dir: Optional[str] = None,\n",
      "        pg_connection_string: Optional[str] = None,\n",
      "    ) -> 'Config':\n",
      "        pg_connection_string = pg_connection_string or os.getenv(\"ELL_PG_CONNECTION_STRING\")\n",
      "        storage_dir = storage_dir or os.getenv(\"ELL_STORAGE_DIR\")\n",
      "\n",
      "        # Enforce that we use either sqlite or postgres, but not both\n",
      "        if pg_connection_string is not None and storage_dir is not None:\n",
      "            raise ValueError(\"Cannot use both sqlite and postgres\")\n",
      "\n",
      "        # For now, fall back to sqlite if no PostgreSQL connection string is provided\n",
      "        if pg_connection_string is None and storage_dir is None:\n",
      "            # This intends to honor the default we had set in the CLI\n",
      "            storage_dir = os.getcwd()\n",
      "\n",
      "        return cls(pg_connection_string=pg_connection_string, storage_dir=storage_dir)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def create_app(config:Config):\n",
      "    # ... other code\n",
      "\n",
      "\n",
      "    @app.get(\"/api/traces\")\n",
      "    def get_consumption_graph(\n",
      "        session: Session = Depends(get_session)\n",
      "    ):\n",
      "        traces = serializer.get_traces(session)\n",
      "        return traces\n",
      "\n",
      "\n",
      "\n",
      "    @app.get(\"/api/blob/{blob_id}\", response_class=Response)\n",
      "    def get_blob(\n",
      "        blob_id: str,\n",
      "        session: Session = Depends(get_session)\n",
      "    ):\n",
      "        if serializer.blob_store is None:\n",
      "            raise HTTPException(status_code=400, detail=\"Blob storage is not configured\")\n",
      "        try:\n",
      "            blob_data = serializer.blob_store.retrieve_blob(blob_id)\n",
      "            return Response(content=blob_data.decode('utf-8'), media_type=\"application/json\")\n",
      "        except FileNotFoundError:\n",
      "            raise HTTPException(status_code=404, detail=\"Blob not found\")\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error retrieving blob: {str(e)}\")\n",
      "            raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
      "    # ... other code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "import json\n",
      "import os\n",
      "from typing import Any, Optional, Dict, List, Set, Union\n",
      "from pydantic import BaseModel\n",
      "from sqlmodel import Session, SQLModel, create_engine, select\n",
      "import ell.store\n",
      "import cattrs\n",
      "import numpy as np\n",
      "from sqlalchemy.sql import text\n",
      "from ell.types import InvocationTrace, SerializedLMP, Invocation, InvocationContents\n",
      "from ell.types._lstr import _lstr\n",
      "from sqlalchemy import or_, func, and_, extract, FromClause\n",
      "from sqlalchemy.types import TypeDecorator, VARCHAR\n",
      "from ell.types.studio import SerializedLMPUses, utc_now\n",
      "from ell.util.serialization import pydantic_ltype_aware_cattr\n",
      "import gzip\n",
      "import json\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def create_app(config:Config):\n",
      "    # ... other code\n",
      "\n",
      "    @app.get(\"/api/invocations\", response_model=list[InvocationPublicWithConsumes])\n",
      "    def get_invocations(\n",
      "        id: Optional[str] = Query(None),\n",
      "        hierarchical: Optional[bool] = Query(False),\n",
      "        skip: int = Query(0, ge=0),\n",
      "        limit: int = Query(100, ge=1, le=100),\n",
      "        lmp_name: Optional[str] = Query(None),\n",
      "        lmp_id: Optional[str] = Query(None),\n",
      "        session: Session = Depends(get_session)\n",
      "    ):\n",
      "        lmp_filters = {}\n",
      "        if lmp_name:\n",
      "            lmp_filters[\"name\"] = lmp_name\n",
      "        if lmp_id:\n",
      "            lmp_filters[\"lmp_id\"] = lmp_id\n",
      "\n",
      "        invocation_filters = {}\n",
      "        if id:\n",
      "            invocation_filters[\"id\"] = id\n",
      "\n",
      "        invocations = serializer.get_invocations(\n",
      "            session,\n",
      "            lmp_filters=lmp_filters,\n",
      "            filters=invocation_filters,\n",
      "            skip=skip,\n",
      "            limit=limit,\n",
      "            hierarchical=hierarchical\n",
      "        )\n",
      "        return invocations\n",
      "    # ... other code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class SQLStore(ell.store.Store):\n",
      "    def __init__(self, db_uri: str, blob_store: Optional[ell.store.BlobStore] = None):\n",
      "        self.engine = create_engine(db_uri,\n",
      "                                    json_serializer=lambda obj: json.dumps(pydantic_ltype_aware_cattr.unstructure(obj), \n",
      "                                     sort_keys=True, default=repr, ensure_ascii=False))\n",
      "\n",
      "        SQLModel.metadata.create_all(self.engine)\n",
      "        self.open_files: Dict[str, Dict[str, Any]] = {}\n",
      "        super().__init__(blob_store)\n",
      "\n",
      "    def write_lmp(self, serialized_lmp: SerializedLMP, uses: Dict[str, Any]) -> Optional[Any]:\n",
      "        with Session(self.engine) as session:\n",
      "            # Bind the serialized_lmp to the session\n",
      "            lmp = session.exec(select(SerializedLMP).filter(SerializedLMP.lmp_id == serialized_lmp.lmp_id)).first()\n",
      "\n",
      "            if lmp:\n",
      "                # Already added to the DB.\n",
      "                return lmp\n",
      "            else:\n",
      "                session.add(serialized_lmp)\n",
      "\n",
      "            for use_id in uses:\n",
      "                used_lmp = session.exec(select(SerializedLMP).where(SerializedLMP.lmp_id == use_id)).first()\n",
      "                if used_lmp:\n",
      "                    serialized_lmp.uses.append(used_lmp)\n",
      "\n",
      "            session.commit()\n",
      "        return None\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class SerializedLMP(SerializedLMPBase, table=True):\n",
      "    invocations: List[\"Invocation\"] = Relationship(back_populates=\"lmp\")\n",
      "    used_by: Optional[List[\"SerializedLMP\"]] = Relationship(\n",
      "        back_populates=\"uses\",\n",
      "        link_model=SerializedLMPUses,\n",
      "        sa_relationship_kwargs=dict(\n",
      "            primaryjoin=\"SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id\",\n",
      "            secondaryjoin=\"SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id\",\n",
      "        ),\n",
      "    )\n",
      "    uses: List[\"SerializedLMP\"] = Relationship(\n",
      "        back_populates=\"used_by\",\n",
      "        link_model=SerializedLMPUses,\n",
      "        sa_relationship_kwargs=dict(\n",
      "            primaryjoin=\"SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id\",\n",
      "            secondaryjoin=\"SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id\",\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    class Config:\n",
      "        table_name = \"serializedlmp\"\n",
      "        unique_together = [(\"version_number\", \"name\")]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class SQLStore(ell.store.Store):\n",
      "\n",
      "    def get_cached_invocations(self, lmp_id :str, state_cache_key :str) -> List[Invocation]:\n",
      "        with Session(self.engine) as session:\n",
      "            return self.get_invocations(session, lmp_filters={\"lmp_id\": lmp_id}, filters={\"state_cache_key\": state_cache_key})\n",
      "\n",
      "    def get_versions_by_fqn(self, fqn :str) -> List[SerializedLMP]:\n",
      "        with Session(self.engine) as session:\n",
      "            return self.get_lmps(session, name=fqn)\n",
      "\n",
      "    ## HELPER METHODS FOR ELL STUDIO! :) \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def create_app(config:Config):\n",
      "    serializer = get_serializer(config)\n",
      "\n",
      "    def get_session():\n",
      "        with Session(serializer.engine) as session:\n",
      "            yield session\n",
      "\n",
      "    app = FastAPI(title=\"ell Studio\", version=__version__)\n",
      "\n",
      "    # Enable CORS for all origins\n",
      "    app.add_middleware(\n",
      "        CORSMiddleware,\n",
      "        allow_origins=[\"*\"],\n",
      "        allow_credentials=True,\n",
      "        allow_methods=[\"*\"],\n",
      "        allow_headers=[\"*\"],\n",
      "    )\n",
      "\n",
      "    manager = ConnectionManager()\n",
      "\n",
      "    @app.websocket(\"/ws\")\n",
      "    async def websocket_endpoint(websocket: WebSocket):\n",
      "        await manager.connect(websocket)\n",
      "        try:\n",
      "            while True:\n",
      "                data = await websocket.receive_text()\n",
      "                # Handle incoming WebSocket messages if needed\n",
      "        except WebSocketDisconnect:\n",
      "            manager.disconnect(websocket)\n",
      "\n",
      "\n",
      "    @app.get(\"/api/latest/lmps\", response_model=list[SerializedLMPWithUses])\n",
      "    def get_latest_lmps(\n",
      "        skip: int = Query(0, ge=0),\n",
      "        limit: int = Query(100, ge=1, le=100),\n",
      "        session: Session = Depends(get_session)\n",
      "    ):\n",
      "        lmps = serializer.get_latest_lmps(\n",
      "            session,\n",
      "            skip=skip, limit=limit,\n",
      "            )\n",
      "        return lmps\n",
      "\n",
      "    # TOOD: Create a get endpoint to efficient get on the index with /api/lmp/<lmp_id>\n",
      "    @app.get(\"/api/lmp/{lmp_id}\")\n",
      "    def get_lmp_by_id(lmp_id: str, session: Session = Depends(get_session)):\n",
      "        lmp = serializer.get_lmps(session, lmp_id=lmp_id)[0]\n",
      "        return lmp\n",
      "    # ... other code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class SQLStore(ell.store.Store):\n",
      "    def get_latest_lmps(self, session: Session, skip: int = 0, limit: int = 10) -> List[Dict[str, Any]]:\n",
      "        \"\"\"\n",
      "        Gets all the lmps grouped by unique name with the highest created at\n",
      "        \"\"\"\n",
      "        subquery = (\n",
      "            select(SerializedLMP.name, func.max(SerializedLMP.created_at).label(\"max_created_at\"))\n",
      "            .group_by(SerializedLMP.name)\n",
      "            .subquery()\n",
      "        )\n",
      "\n",
      "        filters = {\n",
      "            \"name\": subquery.c.name,\n",
      "            \"created_at\": subquery.c.max_created_at\n",
      "        }\n",
      "\n",
      "        return self.get_lmps(session, skip=skip, limit=limit, subquery=subquery, **filters)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "class SQLBlobStore(ell.store.BlobStore):\n",
      "\n",
      "    def _get_blob_path(self, id: str, depth: int = 2) -> str:\n",
      "        assert \"-\" in id, \"Blob id must have a single - in it to split on.\"\n",
      "        _type, _id = id.split(\"-\")\n",
      "        increment = 2\n",
      "        dirs = [_type] + [_id[i:i+increment] for i in range(0, depth*increment, increment)]\n",
      "        file_name = _id[depth*increment:]\n",
      "        return os.path.join(self.db_dir, *dirs, file_name)\n",
      "\n",
      "class PostgresStore(SQLStore):\n",
      "    def __init__(self, db_uri: str):\n",
      "        super().__init__(db_uri)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def create_app(config:Config):\n",
      "    # ... other code\n",
      "\n",
      "    async def notify_clients(entity: str, id: Optional[str] = None):\n",
      "        message = json.dumps({\"entity\": entity, \"id\": id})\n",
      "        await manager.broadcast(message)\n",
      "\n",
      "    # Add this method to the app object\n",
      "    app.notify_clients = notify_clients\n",
      "\n",
      "\n",
      "    @app.get(\"/api/invocations/aggregate\", response_model=InvocationsAggregate)\n",
      "    def get_invocations_aggregate(\n",
      "        lmp_name: Optional[str] = Query(None),\n",
      "        lmp_id: Optional[str] = Query(None),\n",
      "        days: int = Query(30, ge=1, le=365),\n",
      "        session: Session = Depends(get_session)\n",
      "    ):\n",
      "        lmp_filters = {}\n",
      "        if lmp_name:\n",
      "            lmp_filters[\"name\"] = lmp_name\n",
      "        if lmp_id:\n",
      "            lmp_filters[\"lmp_id\"] = lmp_id\n",
      "\n",
      "        aggregate_data = serializer.get_invocations_aggregate(session, lmp_filters=lmp_filters, days=days)\n",
      "        return InvocationsAggregate(**aggregate_data)\n",
      "\n",
      "\n",
      "\n",
      "    return app\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "def create_app(config:Config):\n",
      "    # ... other code\n",
      "\n",
      "    @app.get(\"/api/lmp-history\")\n",
      "    def get_lmp_history(\n",
      "        days: int = Query(365, ge=1, le=3650),  # Default to 1 year, max 10 years\n",
      "        session: Session = Depends(get_session)\n",
      "    ):\n",
      "        # Calculate the start date\n",
      "        start_date = datetime.utcnow() - timedelta(days=days)\n",
      "\n",
      "        # Query to get all LMP creation times within the date range\n",
      "        query = (\n",
      "            select(SerializedLMP.created_at)\n",
      "            .where(SerializedLMP.created_at >= start_date)\n",
      "            .order_by(SerializedLMP.created_at)\n",
      "        )\n",
      "\n",
      "        results = session.exec(query).all()\n",
      "\n",
      "        # Convert results to a list of dictionaries\n",
      "        history = [{\"date\": str(row), \"count\": 1} for row in results]\n",
      "\n",
      "        return history\n",
      "    # ... other code\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "from fastapi import WebSocket\n",
      "\n",
      "\n",
      "class ConnectionManager:\n",
      "    def __init__(self):\n",
      "        self.active_connections = []\n",
      "\n",
      "    async def connect(self, websocket: WebSocket):\n",
      "        await websocket.accept()\n",
      "        self.active_connections.append(websocket)\n",
      "\n",
      "    def disconnect(self, websocket: WebSocket):\n",
      "        self.active_connections.remove(websocket)\n",
      "\n",
      "    async def broadcast(self, message: str):\n",
      "        for connection in self.active_connections:\n",
      "            print(f\"Broadcasting message to {connection} {message}\")\n",
      "            await connection.send_text(message)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Your task is to compare the following clusters and choose which cluster forms a more cohesive grouping of source code chunks. You should\n",
      "penalize a cluster for including too much unrelated code or for missing important code that should have been included.\n",
      "Here are the clusters:\n",
      "Cluster 0:\n",
      "SQLAlchemy and Data Persistence:\n",
      "types\\studio.py::5\n",
      "stores\\sql.py::1\n",
      "stores\\sql.py::2\n",
      "stores\\sql.py::4\n",
      "stores\\sql.py::5\n",
      "stores\\sql.py::11\n",
      "studio\\server.py::1\n",
      "studio\\server.py::4\n",
      "studio\\server.py::5\n",
      "studio\\server.py::6\n",
      "studio\\connection_manager.py::1\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Cluster 1:\n",
      "Server and API Functionality:\n",
      "studio\\server.py::1\n",
      "studio\\server.py::2\n",
      "studio\\server.py::3\n",
      "studio\\server.py::4\n",
      "studio\\server.py::5\n",
      "studio\\server.py::6\n",
      "studio\\server.py::7\n",
      "studio\\connection_manager.py::1\n",
      "studio\\config.py::1\n",
      "studio\\datamodels.py::1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now output your decision by selecting the number of the cluster that you would score higher on overall cohesion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.llm.evals.lmps.eval_compare import eval_compare_clusters\n",
    "\n",
    "indices = []\n",
    "for a, b, a_perc, b_perc in matched:\n",
    "    indices.append(eval_compare_clusters(a, b).parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print([i.index for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion([{\"role\": \"user\", \"content\": \"Who is the king of france udring 1776?\"}], logprobs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TokenProb(token='During', logprob=85.5), TokenProb(token=' ', logprob=99.88), TokenProb(token='177', logprob=100.0), TokenProb(token='6', logprob=100.0), TokenProb(token=',', logprob=100.0), TokenProb(token=' the', logprob=99.92), TokenProb(token=' king', logprob=81.76), TokenProb(token=' of', logprob=100.0), TokenProb(token=' France', logprob=100.0), TokenProb(token=' was', logprob=100.0), TokenProb(token=' Louis', logprob=90.46), TokenProb(token=' XVI', logprob=99.89), TokenProb(token='.', logprob=100.0), TokenProb(token=' He', logprob=100.0), TokenProb(token=' re', logprob=63.21), TokenProb(token='igned', logprob=100.0), TokenProb(token=' from', logprob=100.0), TokenProb(token=' ', logprob=97.95), TokenProb(token='177', logprob=100.0), TokenProb(token='4', logprob=100.0), TokenProb(token=' until', logprob=99.97), TokenProb(token=' his', logprob=88.85), TokenProb(token=' execution', logprob=70.74), TokenProb(token=' in', logprob=99.66), TokenProb(token=' ', logprob=100.0), TokenProb(token='179', logprob=100.0), TokenProb(token='3', logprob=100.0), TokenProb(token='.', logprob=44.22), TokenProb(token=' His', logprob=51.65), TokenProb(token=' reign', logprob=95.93), TokenProb(token=' was', logprob=49.81), TokenProb(token=' marked', logprob=99.79), TokenProb(token=' by', logprob=100.0), TokenProb(token=' financial', logprob=63.92), TokenProb(token=' difficulties', logprob=78.16), TokenProb(token=' and', logprob=77.31), TokenProb(token=' social', logprob=30.35), TokenProb(token=' unrest', logprob=99.22), TokenProb(token=',', logprob=98.09), TokenProb(token=' which', logprob=95.57), TokenProb(token=' ultimately', logprob=59.25), TokenProb(token=' contributed', logprob=91.28), TokenProb(token=' to', logprob=100.0), TokenProb(token=' the', logprob=100.0), TokenProb(token=' French', logprob=73.29), TokenProb(token=' Revolution', logprob=100.0), TokenProb(token='.', logprob=99.99)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TokenProb:\n",
    "    token: str\n",
    "    logprob: float        \n",
    "\n",
    "def get_logprobs(response):\n",
    "    # print(response.cjoice)\n",
    "    logprobs = []\n",
    "    for lp in response.choices[0].logprobs.content:\n",
    "        logprobs.append(\n",
    "            TokenProb(\n",
    "                token=lp.token, \n",
    "                logprob=np.round(np.exp(lp.logprob)*100, 2)\n",
    "            )\n",
    "        )\n",
    "    return logprobs\n",
    "\n",
    "\n",
    "logprobs = get_logprobs(response)\n",
    "print(logprobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of src.llm.evals.lmps.eval_compare failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"c:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"c:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: eval_compare_clusters() requires a code object with 4 free vars, not 3\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from src.llm.evals.lmps import eval_compare_clusters\n",
    "\n",
    "# for a, b, a_perc, b_perc in matched:\n",
    "#     eval_compare_clusters(a,b)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
