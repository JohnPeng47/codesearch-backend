{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\.venv\\lib\\site-packages\\starlette\\config.py:66: UserWarning: Config file '.env' not found.\n",
      "  warnings.warn(f\"Config file '{env_file}' not found.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n",
      "Summary tokens: 8400,         Code tokens: 60918,         Ratio: 0.13789027873534915\n",
      "Saving chunks to file:  C:\\Users\\jpeng\\AppData\\Local\\Temp\\index\\ell\n",
      "[Chunker]: 212 chunks used\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from src.chunk.chunk import ChunkStrat\n",
    "from pathlib import Path\n",
    "\n",
    "from src.cluster.cluster import ClusterStrategy\n",
    "from src.cluster.lmp.cluster_v4 import generate_clusters\n",
    "from src.chunk.chunk import chunk_repo, ChunkStrat\n",
    "\n",
    "import ell\n",
    "ell.init(store=\"logdir\")\n",
    "\n",
    "repo_name = \"ell\"\n",
    "repo_path = Path(\"src/cluster/repos\") / repo_name\n",
    "\n",
    "chunks = chunk_repo(repo_path, ChunkStrat.VANILLA)\n",
    "chunks_summarized = chunk_repo(repo_path, ChunkStrat.SUMMARY)\n",
    "chunks_random = chunk_repo(repo_path, ChunkStrat.RANDOM)\n",
    "\n",
    "chunk_strat = ClusterStrategy(chunks, \n",
    "                              cluster_op=generate_clusters)\n",
    "summarized_strat = ClusterStrategy(chunks_summarized,\n",
    "                                   cluster_op=generate_clusters)\n",
    "random_strat = ClusterStrategy(chunks_random,\n",
    "                                 cluster_op=generate_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25%]: size:  74087\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Chunk Name: cem.py::6 hallucinated, skipping...\n",
      "Chunk Name: cpbo.py::6 hallucinated, skipping...\n",
      "Chunk Name: configurator.py::2 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_complex.py::2 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_complex.py::3 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_complex.py::4 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_complex.py::5 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_tool.py::3 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_tool.py::1 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_tool.py::2 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_tool.py::3 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_complex.py::2 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_complex.py::3 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_complex.py::4 hallucinated, skipping...\n",
      "Chunk Name: lmp\\_complex.py::5 hallucinated, skipping...\n",
      "Chunk Name: build.py::1 hallucinated, skipping...\n",
      "Chunks clustered this round: 111/53\n",
      "New inputs: 5\n",
      "Bytes score:  0.33732530096166463\n",
      "Chunks score:  0.6477812591448955\n",
      "[50%]: size:  153109\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Chunk Name: types\\message.py::1 hallucinated, skipping...\n",
      "Chunk Name: types\\studio.py::1 hallucinated, skipping...\n",
      "Chunk Name: types\\studio.py::2 hallucinated, skipping...\n",
      "Chunk Name: test.py::1 hallucinated, skipping...\n",
      "Chunk Name: myppytest.py::1 hallucinated, skipping...\n",
      "Chunk Name: autostreamprevention.py::1 hallucinated, skipping...\n",
      "Chunk Name: cem.py::1 hallucinated, skipping...\n",
      "Chunk Name: cem.py::2 hallucinated, skipping...\n",
      "Chunk Name: cem.py::3 hallucinated, skipping...\n",
      "Chunk Name: cem.py::4 hallucinated, skipping...\n",
      "Chunk Name: cem.py::5 hallucinated, skipping...\n",
      "Chunk Name: cem.py::6 hallucinated, skipping...\n",
      "Chunk Name: cpbo.py::1 hallucinated, skipping...\n",
      "Chunk Name: cpbo.py::2 hallucinated, skipping...\n",
      "Chunk Name: cpbo.py::3 hallucinated, skipping...\n",
      "Chunk Name: cpbo.py::4 hallucinated, skipping...\n",
      "Chunk Name: cpbo.py::5 hallucinated, skipping...\n",
      "Chunk Name: cpbo.py::6 hallucinated, skipping...\n",
      "Chunk Name: cpbo.py::7 hallucinated, skipping...\n",
      "Chunks clustered this round: 187/106\n",
      "New inputs: 8\n",
      "Bytes score:  0.39199265314066073\n",
      "Chunks score:  0.7767435758422476\n",
      "[75%]: size:  209054\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Chunk Name: 0.1.0\\cem.py::7 hallucinated, skipping...\n",
      "Chunk Name: providers\\openai.py::3 hallucinated, skipping...\n",
      "Chunks clustered this round: 145/159\n",
      "New inputs: 60\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Chunk Name: util\\closure.py::2 hallucinated, skipping...\n",
      "Chunks clustered this round: 104/159\n",
      "New inputs: 3\n",
      "Bytes score:  0.34833041354780486\n",
      "Chunks score:  0.6453097627010671\n"
     ]
    }
   ],
   "source": [
    "from src.llm.evals.closeness import measure_closeness_bytes, measure_closeness_chunks\n",
    "\n",
    "# Observe the performance of clustering on reduced input sizes\n",
    "chunk_75 = chunks[:int(len(chunks) * 0.75)]\n",
    "chunk_50 = chunks[:int(len(chunks) * 0.50)]\n",
    "chunk_25 = chunks[:int(len(chunks) * 0.25)]\n",
    "\n",
    "cluster_outputs = []\n",
    "chunk_inputs = [(\"25%\", chunk_25), (\"50%\", chunk_50), (\"75%\", chunk_75)]\n",
    "for c_name, chunks_i in chunk_inputs:\n",
    "    print(f\"[{c_name}]: size: \", sum([len(chunk.get_content()) for chunk in chunks_i]))\n",
    "\n",
    "    chunk_strat = ClusterStrategy(chunks_i, \n",
    "                              cluster_op=generate_clusters)\n",
    "    clusters = chunk_strat.run(iters=2)\n",
    "\n",
    "    cluster_outputs.append(clusters)\n",
    "\n",
    "    bytes_score = sum([measure_closeness_bytes(cluster, chunks_i) for cluster in clusters]) / len(clusters)\n",
    "    chunks_score = sum([measure_closeness_chunks(cluster, chunks_i) for cluster in clusters]) / len(clusters)\n",
    "    print(\"Bytes score: \", bytes_score)\n",
    "    print(\"Chunks score: \", chunks_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curr chunk:  ell\\configurator.py::8\n",
      "Prev chunk:  build.py::2\n",
      "Curr chunk:  ell\\configurator.py::9\n",
      "Prev chunk:  ell\\configurator.py::8\n",
      "Score!\n",
      "Curr chunk:  lmp\\_track.py::1\n",
      "Prev chunk:  ell\\configurator.py::9\n",
      "Score!\n",
      "Curr chunk:  lmp\\_track.py::2\n",
      "Prev chunk:  lmp\\_track.py::1\n",
      "Score!\n",
      "Curr chunk:  lmp\\_track.py::4\n",
      "Prev chunk:  lmp\\_track.py::2\n",
      "Score!\n",
      "Curr chunk:  models\\__init__.py::1\n",
      "Prev chunk:  lmp\\_track.py::4\n",
      "Curr chunk:  models\\anthropic.py::1\n",
      "Prev chunk:  models\\__init__.py::1\n",
      "Score!\n",
      "Curr chunk:  models\\bedrock.py::1\n",
      "Prev chunk:  models\\anthropic.py::1\n",
      "Score!\n",
      "Curr chunk:  models\\groq.py::1\n",
      "Prev chunk:  models\\bedrock.py::1\n",
      "Score!\n",
      "Curr chunk:  models\\ollama.py::1\n",
      "Prev chunk:  models\\groq.py::1\n",
      "Score!\n",
      "0.8\n",
      "Build and Testing Automation:\n",
      "build.py::2\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "lmp\\_track.py::1\n",
      "lmp\\_track.py::2\n",
      "lmp\\_track.py::4\n",
      "models\\__init__.py::1\n",
      "models\\anthropic.py::1\n",
      "models\\bedrock.py::1\n",
      "models\\groq.py::1\n",
      "models\\ollama.py::1\n",
      "\n",
      "\n",
      "Curr chunk:  0.1.0\\cem.py::2\n",
      "Prev chunk:  0.1.0\\cem.py::1\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::3\n",
      "Prev chunk:  0.1.0\\cem.py::2\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::4\n",
      "Prev chunk:  0.1.0\\cem.py::3\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::5\n",
      "Prev chunk:  0.1.0\\cem.py::4\n",
      "Score!\n",
      "1.0\n",
      "Cross-Entropy Method (CEM) for Reinforcement Learning:\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "\n",
      "\n",
      "Curr chunk:  0.1.0\\cpbo.py::2\n",
      "Prev chunk:  0.1.0\\cpbo.py::1\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::3\n",
      "Prev chunk:  0.1.0\\cpbo.py::2\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::4\n",
      "Prev chunk:  0.1.0\\cpbo.py::3\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::5\n",
      "Prev chunk:  0.1.0\\cpbo.py::4\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::7\n",
      "Prev chunk:  0.1.0\\cpbo.py::5\n",
      "Score!\n",
      "1.0\n",
      "Cross-Policy Behavioral Optimization (CBPO):\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::4\n",
      "0.1.0\\cpbo.py::5\n",
      "0.1.0\\cpbo.py::7\n",
      "\n",
      "\n",
      "Curr chunk:  ell\\configurator.py::2\n",
      "Prev chunk:  ell\\configurator.py::1\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::3\n",
      "Prev chunk:  ell\\configurator.py::2\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::4\n",
      "Prev chunk:  ell\\configurator.py::3\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::5\n",
      "Prev chunk:  ell\\configurator.py::4\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::6\n",
      "Prev chunk:  ell\\configurator.py::5\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::7\n",
      "Prev chunk:  ell\\configurator.py::6\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::8\n",
      "Prev chunk:  ell\\configurator.py::7\n",
      "Score!\n",
      "1.0\n",
      "Configuration Management and Initialization:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "\n",
      "\n",
      "Curr chunk:  lmp\\simple.py::2\n",
      "Prev chunk:  lmp\\simple.py::1\n",
      "Score!\n",
      "Curr chunk:  lmp\\tool.py::1\n",
      "Prev chunk:  lmp\\simple.py::2\n",
      "Score!\n",
      "Curr chunk:  lmp\\tool.py::2\n",
      "Prev chunk:  lmp\\tool.py::1\n",
      "Score!\n",
      "Curr chunk:  lmp\\tool.py::3\n",
      "Prev chunk:  lmp\\tool.py::2\n",
      "Score!\n",
      "Curr chunk:  lmp\\tool.py::4\n",
      "Prev chunk:  lmp\\tool.py::3\n",
      "Score!\n",
      "1.0\n",
      "Complex Language Model Programs (LMPs):\n",
      "lmp\\simple.py::1\n",
      "lmp\\simple.py::2\n",
      "lmp\\tool.py::1\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::3\n",
      "lmp\\tool.py::4\n",
      "\n",
      "\n",
      "0.0\n",
      "Tool Decoration and Integration:\n",
      "lmp\\tool.py::4\n",
      "\n",
      "\n",
      "Curr chunk:  ell\\configurator.py::1\n",
      "Prev chunk:  build.py::2\n",
      "Curr chunk:  ell\\configurator.py::2\n",
      "Prev chunk:  ell\\configurator.py::1\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::3\n",
      "Prev chunk:  ell\\configurator.py::2\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::4\n",
      "Prev chunk:  ell\\configurator.py::3\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::5\n",
      "Prev chunk:  ell\\configurator.py::4\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::6\n",
      "Prev chunk:  ell\\configurator.py::5\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::7\n",
      "Prev chunk:  ell\\configurator.py::6\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::8\n",
      "Prev chunk:  ell\\configurator.py::7\n",
      "Score!\n",
      "Curr chunk:  ell\\configurator.py::9\n",
      "Prev chunk:  ell\\configurator.py::8\n",
      "Score!\n",
      "Curr chunk:  models\\__init__.py::1\n",
      "Prev chunk:  ell\\configurator.py::9\n",
      "0.6666666666666666\n",
      "Build and Deployment Process:\n",
      "build.py::2\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "models\\__init__.py::1\n",
      "src\\conf.py::1\n",
      "src\\conf.py::2\n",
      "\n",
      "\n",
      "Curr chunk:  0.1.0\\cem.py::2\n",
      "Prev chunk:  0.1.0\\cem.py::1\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::3\n",
      "Prev chunk:  0.1.0\\cem.py::2\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::4\n",
      "Prev chunk:  0.1.0\\cem.py::3\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::5\n",
      "Prev chunk:  0.1.0\\cem.py::4\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::6\n",
      "Prev chunk:  0.1.0\\cem.py::5\n",
      "Score!\n",
      "Curr chunk:  models\\groq.py::1\n",
      "Prev chunk:  0.1.0\\cem.py::6\n",
      "0.38461538461538464\n",
      "Cross-Entropy Method (CEM) for Reinforcement Learning:\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cem.py::6\n",
      "models\\groq.py::1\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::4\n",
      "0.1.0\\cpbo.py::5\n",
      "0.1.0\\cpbo.py::6\n",
      "0.1.0\\cpbo.py::7\n",
      "\n",
      "\n",
      "Curr chunk:  ell\\configurator.py::1\n",
      "Prev chunk:  ell\\__init__.py::1\n",
      "Score!\n",
      "Curr chunk:  lmp\\__init__.py::1\n",
      "Prev chunk:  ell\\configurator.py::1\n",
      "Curr chunk:  lmp\\_track.py::1\n",
      "Prev chunk:  lmp\\__init__.py::1\n",
      "Score!\n",
      "Curr chunk:  lmp\\_track.py::2\n",
      "Prev chunk:  lmp\\_track.py::1\n",
      "Score!\n",
      "Curr chunk:  lmp\\_track.py::3\n",
      "Prev chunk:  lmp\\_track.py::2\n",
      "Score!\n",
      "Curr chunk:  lmp\\_track.py::4\n",
      "Prev chunk:  lmp\\_track.py::3\n",
      "Score!\n",
      "Curr chunk:  lmp\\complex.py::1\n",
      "Prev chunk:  lmp\\_track.py::4\n",
      "Score!\n",
      "Curr chunk:  lmp\\complex.py::2\n",
      "Prev chunk:  lmp\\complex.py::1\n",
      "Score!\n",
      "Curr chunk:  lmp\\complex.py::3\n",
      "Prev chunk:  lmp\\complex.py::2\n",
      "Score!\n",
      "Curr chunk:  lmp\\complex.py::4\n",
      "Prev chunk:  lmp\\complex.py::3\n",
      "Score!\n",
      "Curr chunk:  lmp\\complex.py::5\n",
      "Prev chunk:  lmp\\complex.py::4\n",
      "Score!\n",
      "Curr chunk:  lmp\\simple.py::1\n",
      "Prev chunk:  lmp\\complex.py::5\n",
      "Score!\n",
      "Curr chunk:  lmp\\simple.py::2\n",
      "Prev chunk:  lmp\\simple.py::1\n",
      "Score!\n",
      "Curr chunk:  lmp\\tool.py::1\n",
      "Prev chunk:  lmp\\simple.py::2\n",
      "Score!\n",
      "Curr chunk:  lmp\\tool.py::2\n",
      "Prev chunk:  lmp\\tool.py::1\n",
      "Score!\n",
      "Curr chunk:  lmp\\tool.py::3\n",
      "Prev chunk:  lmp\\tool.py::2\n",
      "Score!\n",
      "Curr chunk:  lmp\\tool.py::4\n",
      "Prev chunk:  lmp\\tool.py::3\n",
      "Score!\n",
      "0.9411764705882353\n",
      "Language Model Programming (LMP) and Tools Management:\n",
      "ell\\__init__.py::1\n",
      "ell\\configurator.py::1\n",
      "lmp\\__init__.py::1\n",
      "lmp\\_track.py::1\n",
      "lmp\\_track.py::2\n",
      "lmp\\_track.py::3\n",
      "lmp\\_track.py::4\n",
      "lmp\\complex.py::1\n",
      "lmp\\complex.py::2\n",
      "lmp\\complex.py::3\n",
      "lmp\\complex.py::4\n",
      "lmp\\complex.py::5\n",
      "lmp\\simple.py::1\n",
      "lmp\\simple.py::2\n",
      "lmp\\tool.py::1\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::3\n",
      "lmp\\tool.py::4\n",
      "\n",
      "\n",
      "Curr chunk:  models\\anthropic.py::1\n",
      "Prev chunk:  models\\__init__.py::1\n",
      "Score!\n",
      "Curr chunk:  models\\bedrock.py::1\n",
      "Prev chunk:  models\\anthropic.py::1\n",
      "Score!\n",
      "Curr chunk:  models\\groq.py::1\n",
      "Prev chunk:  models\\bedrock.py::1\n",
      "Score!\n",
      "Curr chunk:  models\\ollama.py::1\n",
      "Prev chunk:  models\\groq.py::1\n",
      "Score!\n",
      "0.2857142857142857\n",
      "Model Registration and Provider Integration:\n",
      "models\\__init__.py::1\n",
      "models\\anthropic.py::1\n",
      "models\\bedrock.py::1\n",
      "models\\groq.py::1\n",
      "models\\ollama.py::1\n",
      "ell\\__version__.py::1\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "\n",
      "\n",
      "Curr chunk:  0.1.0\\cem.py::1\n",
      "Prev chunk:  build.py::2\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::2\n",
      "Prev chunk:  0.1.0\\cem.py::1\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::3\n",
      "Prev chunk:  0.1.0\\cem.py::2\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::4\n",
      "Prev chunk:  0.1.0\\cem.py::3\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::5\n",
      "Prev chunk:  0.1.0\\cem.py::4\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cem.py::6\n",
      "Prev chunk:  0.1.0\\cem.py::5\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::1\n",
      "Prev chunk:  0.1.0\\cem.py::6\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::2\n",
      "Prev chunk:  0.1.0\\cpbo.py::1\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::3\n",
      "Prev chunk:  0.1.0\\cpbo.py::2\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::4\n",
      "Prev chunk:  0.1.0\\cpbo.py::3\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::5\n",
      "Prev chunk:  0.1.0\\cpbo.py::4\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::6\n",
      "Prev chunk:  0.1.0\\cpbo.py::5\n",
      "Score!\n",
      "Curr chunk:  0.1.0\\cpbo.py::7\n",
      "Prev chunk:  0.1.0\\cpbo.py::6\n",
      "Score!\n",
      "1.0\n",
      "Testing and Example Execution:\n",
      "build.py::2\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cem.py::6\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::4\n",
      "0.1.0\\cpbo.py::5\n",
      "0.1.0\\cpbo.py::6\n",
      "0.1.0\\cpbo.py::7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_25 = chunks[:int(len(chunks) * 0.25)]\n",
    "cluster_25 = cluster_outputs[0]\n",
    "for c in cluster_25:\n",
    "    print(measure_closeness_chunks(c, chunk_25))\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build.py::2\n",
      "0.1.0\\autostreamprevention.py::1\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cem.py::3\n",
      "0.1.0\\cem.py::4\n",
      "0.1.0\\cem.py::5\n",
      "0.1.0\\cem.py::6\n",
      "0.1.0\\context_versioning.py::1\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::4\n",
      "0.1.0\\cpbo.py::5\n",
      "0.1.0\\cpbo.py::6\n",
      "0.1.0\\cpbo.py::7\n",
      "0.1.0\\metapromptingtorch.py::1\n",
      "0.1.0\\mypytest.py::1\n",
      "0.1.0\\test.py::1\n",
      "src\\conf.py::1\n",
      "src\\conf.py::2\n",
      "ell\\__init__.py::1\n",
      "ell\\__version__.py::1\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::7\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "lmp\\__init__.py::1\n",
      "lmp\\_track.py::1\n",
      "lmp\\_track.py::2\n",
      "lmp\\_track.py::3\n",
      "lmp\\_track.py::4\n",
      "lmp\\complex.py::1\n",
      "lmp\\complex.py::2\n",
      "lmp\\complex.py::3\n",
      "lmp\\complex.py::4\n",
      "lmp\\complex.py::5\n",
      "lmp\\simple.py::1\n",
      "lmp\\simple.py::2\n",
      "lmp\\tool.py::1\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::3\n",
      "lmp\\tool.py::4\n",
      "models\\__init__.py::1\n",
      "models\\anthropic.py::1\n",
      "models\\bedrock.py::1\n",
      "models\\groq.py::1\n",
      "models\\ollama.py::1\n"
     ]
    }
   ],
   "source": [
    "for c in chunk_25:\n",
    "    print(c.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curr chunk:  models\\anthropic.py::1\n",
      "Prev chunk:  models\\__init__.py::1\n",
      "Score!\n",
      "Curr chunk:  models\\bedrock.py::1\n",
      "Prev chunk:  models\\anthropic.py::1\n",
      "Score!\n",
      "Curr chunk:  models\\groq.py::1\n",
      "Prev chunk:  models\\bedrock.py::1\n",
      "Score!\n",
      "Curr chunk:  models\\ollama.py::1\n",
      "Prev chunk:  models\\groq.py::1\n",
      "Score!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2857142857142857"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cluster = cluster_25[-2]\n",
    "measure_closeness_chunks(test_cluster, chunk_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Chunk Name: openai_realtime\\api.py::1 hallucinated, skipping...\n",
      "Chunk Name: providers\\anthropic.py::2 hallucinated, skipping...\n",
      "Chunk Name: types\\_lstr.py::1 hallucinated, skipping...\n",
      "Chunk Name: models\\openai.py::1 hallucinated, skipping...\n",
      "Chunk Name: models\\bedrock.py::1 hallucinated, skipping...\n",
      "Chunk Name: util\\closure_py::9 hallucinated, skipping...\n",
      "Chunk Name: store\\__init__.py::1 hallucinated, skipping...\n",
      "Chunk Name: stores\\sql.py::1 hallucinated, skipping...\n",
      "Chunk Name: stores\\sql.py::3 hallucinated, skipping...\n",
      "Chunk Name: util\\closure.py::11 hallucinated, skipping...\n",
      "Chunks clustered this round: 109/106\n",
      "New inputs: 34\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "[ELL] use_cache for generate_clusters_raw: False\n",
      "[ELL] use_cache for identify_key_chunks: False\n",
      "[ELL] use_cache for format_clusters: False\n",
      "Chunk Name: lmp\\simple.py hallucinated, skipping...\n",
      "Chunk Name: lmp\\complex.py hallucinated, skipping...\n",
      "Chunk Name: ell.types.message.py hallucinated, skipping...\n",
      "Chunk Name: ell.__version__.py hallucinated, skipping...\n",
      "Chunk Name: ell.models.py hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py hallucinated, skipping...\n",
      "Chunk Name: ell.providers.openai.py hallucinated, skipping...\n",
      "Chunk Name: ell.providers.anthropic.py hallucinated, skipping...\n",
      "Chunk Name: lmp\\_track.py hallucinated, skipping...\n",
      "Chunk Name: types\\studio.py hallucinated, skipping...\n",
      "Chunk Name: utils\\serialization.py hallucinated, skipping...\n",
      "Chunk Name: lmp\\_track.py hallucinated, skipping...\n",
      "Chunk Name: types\\message.py hallucinated, skipping...\n",
      "Chunk Name: 0.1.0\\autostreamprevention.py hallucinated, skipping...\n",
      "Chunk Name: openai_realtime\\__init__.py hallucinated, skipping...\n",
      "Chunk Name: studio\\server.py hallucinated, skipping...\n",
      "Chunk Name: 0.1.0\\cem.py hallucinated, skipping...\n",
      "Chunk Name: docs\\index.rst hallucinated, skipping...\n",
      "Chunk Name: docs\\reference.rst hallucinated, skipping...\n",
      "Chunk Name: docs\\conf.py hallucinated, skipping...\n",
      "Chunk Name: docs\\_static hallucinated, skipping...\n",
      "Chunk Name: templates_path hallucinated, skipping...\n",
      "Chunk Name: 0.1.0\\cem.py hallucinated, skipping...\n",
      "Chunk Name: 0.1.0\\cem.py::7 hallucinated, skipping...\n",
      "Chunk Name: types\\message.py::1 hallucinated, skipping...\n",
      "Chunk Name: providers\\openai.py::1 hallucinated, skipping...\n",
      "Chunk Name: ell.models.py::2 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::1 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::2 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::3 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::4 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::5 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::6 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::7 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::8 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::9 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::10 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::11 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::12 hallucinated, skipping...\n",
      "Chunk Name: ell.configurator.py::13 hallucinated, skipping...\n",
      "Chunk Name: types\\studio.py::5 hallucinated, skipping...\n",
      "Chunk Name: types\\studio.py::6 hallucinated, skipping...\n",
      "Chunk Name: types\\studio.py::7 hallucinated, skipping...\n",
      "Chunk Name: openai_realtime\\client.py::1 hallucinated, skipping...\n",
      "Chunk Name: openai_realtime\\api.py::1 hallucinated, skipping...\n",
      "Chunk Name: openai_realtime\\utils.py::1 hallucinated, skipping...\n",
      "Chunk Name: openai_realtime\\event_handler.py::3 hallucinated, skipping...\n",
      "Chunk Name: providers\\client_cls.py::1 hallucinated, skipping...\n",
      "Chunk Name: providers\\client_cls.py::2 hallucinated, skipping...\n",
      "Chunk Name: providers\\client_cls.py::3 hallucinated, skipping...\n",
      "Chunk Name: 0.1.0\\cem.py::3 hallucinated, skipping...\n",
      "Chunk Name: 0.1.0\\cem.py::4 hallucinated, skipping...\n",
      "Chunk Name: 0.1.0\\cpbo.py::4 hallucinated, skipping...\n",
      "Chunk Name: 0.1.0\\cpbo.py::5 hallucinated, skipping...\n",
      "Chunk Name: 0.1.0\\cpbo.py::6 hallucinated, skipping...\n",
      "Chunks clustered this round: 70/106\n",
      "New inputs: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "CodeChunk(id='openai_realtime\\\\client.py::2', input_type=<ClusterInputType.FILE: 'file'>, content=\"class RealtimeClient(RealtimeEventHandler):\\n\\n    def _add_api_event_handlers(self):\\n        self.realtime.on('client.*', lambda event: self.dispatch('realtime.event', {\\n            'time': RealtimeUtils.generate_id('time_'),\\n            'source': 'client',\\n            'event': event\\n        }))\\n        self.realtime.on('server.*', lambda event: self.dispatch('realtime.event', {\\n            'time': RealtimeUtils.generate_id('time_'),\\n            'source': 'server',\\n            'event': event\\n        }))\\n        self.realtime.on('server.session.created', lambda _: setattr(self, 'session_created', True))\\n\\n        def handle_conversation_event(event, *args):\\n            result = self.conversation.process_event(event, *args)\\n            if result['item']:\\n                self.dispatch('conversation.updated', result)\\n            return result\\n\\n        self.realtime.on('server.response.created', handle_conversation_event)\\n        self.realtime.on('server.response.output_item.added', handle_conversation_event)\\n        self.realtime.on('server.response.content_part.added', handle_conversation_event)\\n        self.realtime.on('server.input_audio_buffer.speech_started', lambda event: (\\n            handle_conversation_event(event),\\n            self.dispatch('conversation.interrupted', event)\\n        ))\\n        self.realtime.on('server.input_audio_buffer.speech_stopped', lambda event: \\n            handle_conversation_event(event, self.input_audio_buffer)\\n        )\\n        self.realtime.on('server.conversation.item.created', lambda event: (\\n            handle_conversation_event(event),\\n            self.dispatch('conversation.item.appended', {'item': event['item']})\\n        ))\\n        self.realtime.on('server.conversation.item.truncated', handle_conversation_event)\\n        self.realtime.on('server.conversation.item.deleted', handle_conversation_event)\\n        self.realtime.on('server.conversation.item.input_audio_transcription.completed', handle_conversation_event)\\n        self.realtime.on('server.response.audio_transcript.delta', handle_conversation_event)\\n        self.realtime.on('server.response.audio.delta', handle_conversation_event)\\n        self.realtime.on('server.response.text.delta', handle_conversation_event)\\n        self.realtime.on('server.response.function_call_arguments.delta', handle_conversation_event)\\n        def handle_output_item_done( event):\\n            handle_conversation_event(event)\\n            item = event.get('item', {})\\n\\n            if item.get('status') == 'completed':\\n                self.dispatch('conversation.item.completed', {'item': item})\\n\\n            formatted = item.get('formatted', {})\\n            tool = formatted.get('tool') if isinstance(formatted, dict) else None\\n\\n            if tool:\\n                asyncio.create_task(self._call_tool(tool))\\n        self.realtime.on('server.response.output_item.done', handle_output_item_done)\", summary='', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id='openai_realtime\\\\client.py::2') is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m chunk_strat \u001b[38;5;241m=\u001b[39m ClusterStrategy(chunks_rand_50, \n\u001b[0;32m      4\u001b[0m                             cluster_op\u001b[38;5;241m=\u001b[39mgenerate_clusters)\n\u001b[0;32m      5\u001b[0m r_clusters \u001b[38;5;241m=\u001b[39m chunk_strat\u001b[38;5;241m.\u001b[39mrun(iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m bytes_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([measure_closeness_bytes(cluster, chunks_i) \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m r_clusters]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(r_clusters)\n\u001b[0;32m      8\u001b[0m chunks_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([measure_closeness_chunks(cluster, chunks_i) \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m r_clusters]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(r_clusters)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBytes score: \u001b[39m\u001b[38;5;124m\"\u001b[39m, bytes_score)\n",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m chunk_strat \u001b[38;5;241m=\u001b[39m ClusterStrategy(chunks_rand_50, \n\u001b[0;32m      4\u001b[0m                             cluster_op\u001b[38;5;241m=\u001b[39mgenerate_clusters)\n\u001b[0;32m      5\u001b[0m r_clusters \u001b[38;5;241m=\u001b[39m chunk_strat\u001b[38;5;241m.\u001b[39mrun(iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m bytes_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[43mmeasure_closeness_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_i\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m r_clusters]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(r_clusters)\n\u001b[0;32m      8\u001b[0m chunks_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([measure_closeness_chunks(cluster, chunks_i) \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m r_clusters]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(r_clusters)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBytes score: \u001b[39m\u001b[38;5;124m\"\u001b[39m, bytes_score)\n",
      "File \u001b[1;32mc:\\Users\\jpeng\\Documents\\projects\\codesearch-backend\\src\\llm\\evals\\../../..\\src\\llm\\evals\\closeness.py:11\u001b[0m, in \u001b[0;36mmeasure_closeness_bytes\u001b[1;34m(cluster, input_chunks, window)\u001b[0m\n\u001b[0;32m      8\u001b[0m prev_chunk \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mchunks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cluster\u001b[38;5;241m.\u001b[39mchunks[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m---> 11\u001b[0m     curr_index \u001b[38;5;241m=\u001b[39m \u001b[43minput_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     prev_index \u001b[38;5;241m=\u001b[39m input_chunks\u001b[38;5;241m.\u001b[39mindex(prev_chunk)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# actually good case, out of order\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: CodeChunk(id='openai_realtime\\\\client.py::2', input_type=<ClusterInputType.FILE: 'file'>, content=\"class RealtimeClient(RealtimeEventHandler):\\n\\n    def _add_api_event_handlers(self):\\n        self.realtime.on('client.*', lambda event: self.dispatch('realtime.event', {\\n            'time': RealtimeUtils.generate_id('time_'),\\n            'source': 'client',\\n            'event': event\\n        }))\\n        self.realtime.on('server.*', lambda event: self.dispatch('realtime.event', {\\n            'time': RealtimeUtils.generate_id('time_'),\\n            'source': 'server',\\n            'event': event\\n        }))\\n        self.realtime.on('server.session.created', lambda _: setattr(self, 'session_created', True))\\n\\n        def handle_conversation_event(event, *args):\\n            result = self.conversation.process_event(event, *args)\\n            if result['item']:\\n                self.dispatch('conversation.updated', result)\\n            return result\\n\\n        self.realtime.on('server.response.created', handle_conversation_event)\\n        self.realtime.on('server.response.output_item.added', handle_conversation_event)\\n        self.realtime.on('server.response.content_part.added', handle_conversation_event)\\n        self.realtime.on('server.input_audio_buffer.speech_started', lambda event: (\\n            handle_conversation_event(event),\\n            self.dispatch('conversation.interrupted', event)\\n        ))\\n        self.realtime.on('server.input_audio_buffer.speech_stopped', lambda event: \\n            handle_conversation_event(event, self.input_audio_buffer)\\n        )\\n        self.realtime.on('server.conversation.item.created', lambda event: (\\n            handle_conversation_event(event),\\n            self.dispatch('conversation.item.appended', {'item': event['item']})\\n        ))\\n        self.realtime.on('server.conversation.item.truncated', handle_conversation_event)\\n        self.realtime.on('server.conversation.item.deleted', handle_conversation_event)\\n        self.realtime.on('server.conversation.item.input_audio_transcription.completed', handle_conversation_event)\\n        self.realtime.on('server.response.audio_transcript.delta', handle_conversation_event)\\n        self.realtime.on('server.response.audio.delta', handle_conversation_event)\\n        self.realtime.on('server.response.text.delta', handle_conversation_event)\\n        self.realtime.on('server.response.function_call_arguments.delta', handle_conversation_event)\\n        def handle_output_item_done( event):\\n            handle_conversation_event(event)\\n            item = event.get('item', {})\\n\\n            if item.get('status') == 'completed':\\n                self.dispatch('conversation.item.completed', {'item': item})\\n\\n            formatted = item.get('formatted', {})\\n            tool = formatted.get('tool') if isinstance(formatted, dict) else None\\n\\n            if tool:\\n                asyncio.create_task(self._call_tool(tool))\\n        self.realtime.on('server.response.output_item.done', handle_output_item_done)\", summary='', filepath='x\\\\openai_realtime\\\\src\\\\openai_realtime\\\\client.py', metadata=None, node_id='openai_realtime\\\\client.py::2') is not in list"
     ]
    }
   ],
   "source": [
    "# testing random chunks \n",
    "chunks_rand_50 = chunks_random[:int(len(chunks) * 0.50)]\n",
    "chunk_strat = ClusterStrategy(chunks_rand_50, \n",
    "                            cluster_op=generate_clusters)\n",
    "r_clusters = chunk_strat.run(iters=2)\n",
    "\n",
    "bytes_score = sum([measure_closeness_bytes(cluster, chunks_i) for cluster in r_clusters]) / len(r_clusters)\n",
    "chunks_score = sum([measure_closeness_chunks(cluster, chunks_i) for cluster in r_clusters]) / len(r_clusters)\n",
    "print(\"Bytes score: \", bytes_score)\n",
    "print(\"Chunks score: \", chunks_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bytes score:  0.0783816425120773\n",
      "Chunks score:  0.4035369220151829\n"
     ]
    }
   ],
   "source": [
    "bytes_score = sum([measure_closeness_bytes(cluster, chunks) for cluster in r_clusters]) / len(r_clusters)\n",
    "chunks_score = sum([measure_closeness_chunks(cluster, chunks) for cluster in r_clusters]) / len(r_clusters)\n",
    "print(\"Bytes score: \", bytes_score)\n",
    "print(\"Chunks score: \", chunks_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n",
      "Model Configuration and Registration:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "models\\groq.py::1\n",
      "models\\ollama.py::1\n",
      "models\\anthropic.py::1\n",
      "\n",
      "\n",
      "0.75\n",
      "Real-Time Client and Event Handling:\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\client.py::6\n",
      "openai_realtime\\client.py::7\n",
      "openai_realtime\\api.py::2\n",
      "openai_realtime\\api.py::3\n",
      "openai_realtime\\conversation.py::1\n",
      "\n",
      "\n",
      "1.0\n",
      "Message Handling and Processing:\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::9\n",
      "types\\message.py::10\n",
      "types\\message.py::11\n",
      "types\\message.py::12\n",
      "types\\message.py::13\n",
      "types\\message.py::14\n",
      "types\\message.py::15\n",
      "\n",
      "\n",
      "0.4444444444444444\n",
      "Storage and State Management:\n",
      "stores\\sql.py::4\n",
      "stores\\sql.py::5\n",
      "stores\\sql.py::7\n",
      "stores\\sql.py::9\n",
      "stores\\sql.py::10\n",
      "ell\\store.py::1\n",
      "ell\\store.py::3\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::8\n",
      "studio\\config.py::1\n",
      "\n",
      "\n",
      "0.14285714285714285\n",
      "Provider Integration and API Call Handling:\n",
      "providers\\openai.py::1\n",
      "providers\\openai.py::2\n",
      "providers\\anthropic.py::1\n",
      "providers\\bedrock.py::1\n",
      "providers\\bedrock.py::2\n",
      "ell\\provider.py::1\n",
      "ell\\provider.py::2\n",
      "ell\\provider.py::3\n",
      "\n",
      "\n",
      "0.7777777777777778\n",
      "Configuration Management and Initialization:\n",
      "ell\\configurator.py::1\n",
      "ell\\configurator.py::2\n",
      "ell\\configurator.py::3\n",
      "ell\\configurator.py::4\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::8\n",
      "ell\\configurator.py::9\n",
      "providers\\__init__.py::1\n",
      "util\\_warnings.py::2\n",
      "\n",
      "\n",
      "0.5555555555555556\n",
      "Complex LMP Handling and Tracking:\n",
      "lmp\\complex.py::1\n",
      "lmp\\complex.py::2\n",
      "util\\verbosity.py::2\n",
      "util\\verbosity.py::4\n",
      "util\\verbosity.py::5\n",
      "util\\verbosity.py::6\n",
      "util\\verbosity.py::8\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::3\n",
      "util\\closure.py::3\n",
      "\n",
      "\n",
      "0.6666666666666666\n",
      "Message Content Handling:\n",
      "types\\message.py::4\n",
      "types\\message.py::5\n",
      "types\\message.py::6\n",
      "types\\message.py::9\n",
      "types\\message.py::10\n",
      "types\\message.py::12\n",
      "types\\message.py::13\n",
      "types\\_lstr.py::3\n",
      "types\\_lstr.py::7\n",
      "types\\_lstr.py::18\n",
      "\n",
      "\n",
      "0.625\n",
      "OpenAI Integration:\n",
      "providers\\openai.py::1\n",
      "providers\\openai.py::2\n",
      "openai_realtime\\client.py::1\n",
      "openai_realtime\\client.py::2\n",
      "openai_realtime\\client.py::3\n",
      "openai_realtime\\client.py::4\n",
      "openai_realtime\\conversation.py::1\n",
      "openai_realtime\\conversation.py::6\n",
      "openai_realtime\\conversation.py::9\n",
      "\n",
      "\n",
      "0.14285714285714285\n",
      "Bedrock Integration:\n",
      "providers\\bedrock.py::1\n",
      "providers\\bedrock.py::2\n",
      "ell\\configurator.py::5\n",
      "ell\\configurator.py::6\n",
      "ell\\configurator.py::3\n",
      "util\\verbosity.py::5\n",
      "util\\closure.py::4\n",
      "util\\closure.py::8\n",
      "\n",
      "\n",
      "0.8333333333333334\n",
      "Serialization and Storage:\n",
      "stores\\sql.py::5\n",
      "stores\\sql.py::7\n",
      "stores\\sql.py::9\n",
      "util\\serialization.py::1\n",
      "util\\serialization.py::2\n",
      "util\\serialization.py::4\n",
      "util\\serialization.py::5\n",
      "\n",
      "\n",
      "0.5\n",
      "Utility Functions and Methods:\n",
      "util\\closure_util.py::2\n",
      "util\\closure_util.py::3\n",
      "util\\closure.py::9\n",
      "util\\closure.py::14\n",
      "util\\closure.py::18\n",
      "util\\verbosity.py::2\n",
      "util\\verbosity.py::4\n",
      "util\\verbosity.py::6\n",
      "util\\verbosity.py::8\n",
      "\n",
      "\n",
      "0.0\n",
      "Cluster 1: Initialization and Core Components:\n",
      "ell\\__init__.py::1\n",
      "src\\conf.py::1\n",
      "\n",
      "\n",
      "0.14285714285714285\n",
      "Cluster 2: Tool and Invocation Management:\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::4\n",
      "lmp\\_track.py::1\n",
      "lmp\\_track.py::3\n",
      "types\\studio.py::8\n",
      "providers\\anthropic.py::3\n",
      "models\\__init__.py::1\n",
      "types\\studio.py::2\n",
      "\n",
      "\n",
      "0.0\n",
      "Cluster 3: Function Closure and Serialization:\n",
      "util\\closure.py::1\n",
      "util\\closure.py::13\n",
      "0.1.0\\context_versioning.py::1\n",
      "lmp\\_track.py::3\n",
      "providers\\anthropic.py::3\n",
      "types\\studio.py::3\n",
      "lmp\\_track.py::1\n",
      "\n",
      "\n",
      "0.2\n",
      "Cluster 4: Real-time Interaction and Event Handling:\n",
      "studio\\__main__.py::3\n",
      "studio\\__main__.py::1\n",
      "studio\\connection_manager.py::1\n",
      "openai_realtime\\conversation.py::4\n",
      "studio\\server.py::1\n",
      "0.1.0\\test.py::1\n",
      "\n",
      "\n",
      "0.5\n",
      "Cluster 5: Configuration and Documentation:\n",
      "src\\conf.py::1\n",
      "src\\conf.py::2\n",
      "lmp\\tool.py::4\n",
      "models\\__init__.py::1\n",
      "util\\__init__.py::1\n",
      "\n",
      "\n",
      "0.0\n",
      "Cluster 6: Specialized Components and Extensions:\n",
      "providers\\anthropic.py::3\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\context_versioning.py::1\n",
      "0.1.0\\metapromptingtorch.py::1\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::7\n",
      "\n",
      "\n",
      "0.0\n",
      "Core Components and Initialization:\n",
      "ell\\__init__.py::1\n",
      "providers\\anthropic.py::3\n",
      "openai_realtime\\__init__.py::1\n",
      "models\\__init__.py::1\n",
      "util\\closure.py::1\n",
      "util\\closure.py::13\n",
      "util\\__init__.py::1\n",
      "\n",
      "\n",
      "1.0\n",
      "Configuration and Documentation:\n",
      "src\\conf.py::1\n",
      "src\\conf.py::2\n",
      "\n",
      "\n",
      "0.2222222222222222\n",
      "Language Model Programming (LMP) Tracking and Invocation Management:\n",
      "lmp\\_track.py::1\n",
      "lmp\\_track.py::3\n",
      "lmp\\tool.py::2\n",
      "lmp\\tool.py::4\n",
      "types\\studio.py::8\n",
      "types\\studio.py::3\n",
      "types\\studio.py::2\n",
      "types\\_lstr.py::17\n",
      "types\\_lstr.py::16\n",
      "types\\_lstr.py::19\n",
      "\n",
      "\n",
      "0.0\n",
      "Interfaces and Client Interaction:\n",
      "openai_realtime\\conversation.py::4\n",
      "studio\\server.py::1\n",
      "studio\\__main__.py::1\n",
      "studio\\__main__.py::3\n",
      "studio\\connection_manager.py::1\n",
      "\n",
      "\n",
      "0.0\n",
      "Sample Implementations and Examples:\n",
      "0.1.0\\test.py::1\n",
      "0.1.0\\cem.py::1\n",
      "0.1.0\\cem.py::2\n",
      "0.1.0\\cpbo.py::1\n",
      "0.1.0\\cpbo.py::2\n",
      "0.1.0\\cpbo.py::3\n",
      "0.1.0\\cpbo.py::7\n",
      "0.1.0\\metapromptingtorch.py::1\n",
      "0.1.0\\context_versioning.py::1\n",
      "0.1.0\\autostreamprevention.py::1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in r_clusters:\n",
    "    print(measure_closeness_chunks(c, chunks))\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test using anon chunk names\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
