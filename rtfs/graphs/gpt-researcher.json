{"cluster_depth": null, "cluster_roots": [], "link_data": {"directed": true, "multigraph": true, "graph": {}, "nodes": [{"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\__init__.py__from_multi_agents_import_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 5, "span_ids": ["imports"], "start_line": 1, "end_line": 1, "community": null}, "content": "from multi_agents import agents", "kind": "Chunk", "id": "backend/__init__.py#1.0"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\memory\\draft.py__DraftState.revision_notes", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\memory\\draft.py", "file_name": "draft.py", "file_type": "text/x-python", "category": "test", "tokens": 46, "span_ids": ["DraftState", "imports"], "start_line": 1, "end_line": 10, "community": null}, "content": "from typing import TypedDict, List, Annotated\nimport operator\n\n\nclass DraftState(TypedDict):\n    task: dict\n    topic: str\n    draft: dict\n    review: str\n    revision_notes: str", "kind": "Chunk", "id": "memory/draft.py#2.9"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\memory\\research.py__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\memory\\research.py", "file_name": "research.py", "file_type": "text/x-python", "category": "test", "tokens": 94, "span_ids": ["ResearchState", "imports"], "start_line": 1, "end_line": 21, "community": null}, "content": "from typing import TypedDict, List, Annotated\nimport operator\n\n\nclass ResearchState(TypedDict):\n    task: dict\n    initial_research: str\n    sections: List[str]\n    research_data: List[dict]\n    # Report layout\r\n    title: str\n    headers: dict\n    date: str\n    table_of_contents: str\n    introduction: str\n    conclusion: str\n    sources: List[str]\n    report: str", "kind": "Chunk", "id": "memory/research.py#3.20"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\__init__.py____all__._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 38, "span_ids": ["imports"], "start_line": 1, "end_line": 7, "community": null}, "content": "from .basic_report.basic_report import BasicReport\nfrom .detailed_report.detailed_report import DetailedReport\n\n__all__ = [\r\n    \"BasicReport\",\r\n    \"DetailedReport\"\r\n]", "kind": "Chunk", "id": "report_type/__init__.py#4.6"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\basic_report\\basic_report.py_from_fastapi_import_WebSo_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\basic_report\\basic_report.py", "file_name": "basic_report.py", "file_type": "text/x-python", "category": "test", "tokens": 245, "span_ids": ["imports", "BasicReport.run", "BasicReport.__init__", "BasicReport"], "start_line": 1, "end_line": 47, "community": null}, "content": "from fastapi import WebSocket\n\nfrom gpt_researcher.master.agent import GPTResearcher\nfrom gpt_researcher.utils.enum import Tone\n\nclass BasicReport:\n    def __init__(\r\n        self,\r\n        query: str,\r\n        report_type: str,\r\n        report_source: str,\r\n        source_urls,\r\n        tone: Tone,\r\n        config_path: str,\r\n        websocket: WebSocket,\r\n        headers=None\r\n    ):\n        self.query = query\n        self.report_type = report_type\n        self.report_source = report_source\n        self.source_urls = source_urls\n        self.tone = tone\n        self.config_path = config_path\n        self.websocket = websocket\n        self.headers = headers or {}\n\n    async def run(self):\n        # Initialize researcher\r\n        researcher = GPTResearcher(\r\n            query=self.query,\r\n            report_type=self.report_type,\r\n            report_source=self.report_source,\r\n            source_urls=self.source_urls,\r\n            tone=self.tone,\r\n            config_path=self.config_path,\r\n            websocket=self.websocket,\r\n            headers=self.headers\r\n        )\n\n        # Run research\r\n        await researcher.conduct_research()\n\n        # and generate report\r\n        report = await researcher.write_report()\n\n        return report", "kind": "Chunk", "id": "basic_report/basic_report.py#5.46"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py_asyncio_DetailedReport.__init__.if_self_source_urls_.self.global_urls.set_self_source_urls_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py", "file_name": "detailed_report.py", "file_type": "text/x-python", "category": "test", "tokens": 378, "span_ids": ["imports", "DetailedReport", "DetailedReport.__init__"], "start_line": 1, "end_line": 58, "community": null}, "content": "import asyncio\n\nfrom fastapi import WebSocket\n\nfrom gpt_researcher.master.actions import (\r\n    add_source_urls,\r\n    extract_headers,\r\n    extract_sections,\r\n    table_of_contents,\r\n)\nfrom gpt_researcher.master.agent import GPTResearcher\nfrom gpt_researcher.utils.enum import Tone\n\n\nclass DetailedReport:\n    def __init__(\r\n        self,\r\n        query: str,\r\n        report_type: str,\r\n        report_source: str,\r\n        source_urls,\r\n        config_path: str,\r\n        tone: Tone,\r\n        websocket: WebSocket,\r\n        subtopics=[],\r\n        headers=None\r\n    ):\n        self.query = query\n        self.report_type = report_type\n        self.report_source = report_source\n        self.source_urls = source_urls\n        self.config_path = config_path\n        self.tone = tone\n        self.websocket = websocket\n        self.subtopics = subtopics\n        self.headers = headers or {}\n\n        # A parent task assistant. Adding research_report as default\r\n        self.main_task_assistant = GPTResearcher(\r\n            query=self.query,\r\n            report_type=\"research_report\",\r\n            report_source=self.report_source,\r\n            source_urls=self.source_urls,\r\n            config_path=self.config_path,\r\n            tone=self.tone,\r\n            websocket=self.websocket,\r\n            headers=self.headers\r\n        )\n        self.existing_headers = []\n        # This is a global variable to store the entire context accumulated at any point through searching and scraping\r\n        self.global_context = []\n\n        # This is a global variable to store all written sections. It will be used to retrieve relevant written content before any subtopic report to prevent redundant content writing.\r\n        self.global_written_sections = []\n\n        # This is a global variable to store the entire url list accumulated at any point through searching and scraping\r\n        if self.source_urls:\n            self.global_urls = set(self.source_urls)", "kind": "Chunk", "id": "detailed_report/detailed_report.py#6.57"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py_DetailedReport.run_DetailedReport.run.return.report", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py", "file_name": "detailed_report.py", "file_type": "text/x-python", "category": "test", "tokens": 160, "span_ids": ["DetailedReport.run"], "start_line": 60, "end_line": 80, "community": null}, "content": "class DetailedReport:\n\n    async def run(self):\n\n        # Conduct initial research using the main assistant\r\n        await self._initial_research()\n\n        # Get list of all subtopics\r\n        subtopics = await self._get_all_subtopics()\n\n        # Generate report introduction\r\n        report_introduction = await self.main_task_assistant.write_introduction()\n\n        # Generate the subtopic reports based on the subtopics gathered\r\n        _, report_body = await self._generate_subtopic_reports(subtopics)\n\n        # Construct the final list of visited urls\r\n        self.main_task_assistant.visited_urls.update(self.global_urls)\n\n        # Construct the final detailed report (Optionally add more details to the report)\r\n        report = await self._construct_detailed_report(report_introduction, report_body)\n\n        return report", "kind": "Chunk", "id": "detailed_report/detailed_report.py#7.20"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py_DetailedReport._initial_research_DetailedReport._get_all_subtopics.return.subtopics_dict_get_sub", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py", "file_name": "detailed_report.py", "file_type": "text/x-python", "category": "test", "tokens": 128, "span_ids": ["DetailedReport._initial_research", "DetailedReport._get_all_subtopics"], "start_line": 82, "end_line": 92, "community": null}, "content": "class DetailedReport:\n\n    async def _initial_research(self):\n        # Conduct research using the main task assistant to gather content for generating subtopics\r\n        await self.main_task_assistant.conduct_research()\n        # Update context of the global context variable\r\n        self.global_context = self.main_task_assistant.context\n        # Update url list of the global list variable\r\n        self.global_urls = self.main_task_assistant.visited_urls\n\n    async def _get_all_subtopics(self) -> list:\n        subtopics = await self.main_task_assistant.get_subtopics()\n        return subtopics.dict().get(\"subtopics\", [])", "kind": "Chunk", "id": "detailed_report/detailed_report.py#8.10"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py_DetailedReport._generate_subtopic_reports_DetailedReport._generate_subtopic_reports.return.subtopic_reports_subtopi", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py", "file_name": "detailed_report.py", "file_type": "text/x-python", "category": "test", "tokens": 199, "span_ids": ["DetailedReport._generate_subtopic_reports"], "start_line": 94, "end_line": 117, "community": null}, "content": "class DetailedReport:\n\n    async def _generate_subtopic_reports(self, subtopics: list) -> tuple:\n        subtopic_reports = []\n        subtopics_report_body = \"\"\n\n        async def fetch_report(subtopic):\n\n            subtopic_report = await self._get_subtopic_report(subtopic)\n\n            return {\"topic\": subtopic, \"report\": subtopic_report}\n\n        # This is the asyncio version of the same code below\r\n        # Although this will definitely run faster, the problem\r\n        # lies in avoiding duplicate information.\r\n        # To solve this the headers from previous subtopic reports are extracted\r\n        # and passed to the next subtopic report generation.\r\n        # This is only possible to do sequentially\r\n\n        for subtopic in subtopics:\n            result = await fetch_report(subtopic)\n            if result[\"report\"]:\n                subtopic_reports.append(result)\n                subtopics_report_body += \"\\n\\n\\n\" + result[\"report\"]\n\n        return subtopic_reports, subtopics_report_body", "kind": "Chunk", "id": "detailed_report/detailed_report.py#9.23"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py_DetailedReport._get_subtopic_report_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\report_type\\detailed_report\\detailed_report.py", "file_name": "detailed_report.py", "file_type": "text/x-python", "category": "test", "tokens": 570, "span_ids": ["DetailedReport._construct_detailed_report", "DetailedReport._get_subtopic_report"], "start_line": 119, "end_line": 180, "community": null}, "content": "class DetailedReport:\n\n    async def _get_subtopic_report(self, subtopic: dict) -> str:\n        current_subtopic_task = subtopic.get(\"task\")\n        subtopic_assistant = GPTResearcher(\r\n            query=current_subtopic_task,\r\n            report_type=\"subtopic_report\",\r\n            report_source=self.report_source,\r\n            websocket=self.websocket,\r\n            headers=self.headers,\r\n            parent_query=self.query,\r\n            subtopics=self.subtopics,\r\n            visited_urls=self.global_urls,\r\n            agent=self.main_task_assistant.agent,\r\n            role=self.main_task_assistant.role,\r\n            tone=self.tone,\r\n        )\n\n        # The subtopics should start research from the context gathered till now\r\n        subtopic_assistant.context = list(set(self.global_context))\n\n        # Conduct research on the subtopic\r\n        await subtopic_assistant.conduct_research()\n\n        # Use research results to generate draft section titles\r\n        draft_section_titles = await subtopic_assistant.get_draft_section_titles()\n        parse_draft_section_titles = extract_headers(draft_section_titles)\n        parse_draft_section_titles_text = [header.get(\"text\", \"\") for header in parse_draft_section_titles]\n\n        # Use the draft section titles to get previous relevant written contents\r\n        relevant_contents = await subtopic_assistant.get_similar_written_contents_by_draft_section_titles(current_subtopic_task, parse_draft_section_titles_text, self.global_written_sections)\n\n        # Here the headers gathered from previous subtopic reports are passed to the write report function\r\n        # The LLM is later instructed to avoid generating any information relating to these headers as they have already been generated\r\n        subtopic_report = await subtopic_assistant.write_report(self.existing_headers, relevant_contents)\n\n        # Update the global written sections list\r\n        self.global_written_sections.extend(extract_sections(subtopic_report))\n        # Update context of the global context variable\r\n        self.global_context = list(set(subtopic_assistant.context))\n        # Update url list of the global list variable\r\n        self.global_urls.update(subtopic_assistant.visited_urls)\n\n        # After a subtopic report has been generated then append the headers of the report to existing headers\r\n        self.existing_headers.append(\r\n            {\r\n                \"subtopic task\": current_subtopic_task,\r\n                \"headers\": extract_headers(subtopic_report),\r\n            }\r\n        )\n\n        return subtopic_report\n\n    async def _construct_detailed_report(self, introduction: str, report_body: str):\n        # Generating a table of contents from report headers\r\n        toc = table_of_contents(report_body)\n\n        # Concatenating all source urls at the end of the report\r\n        report_with_references = add_source_urls(\r\n            report_body, self.main_task_assistant.visited_urls\r\n        )\n\n        return f\"{introduction}\\n\\n{toc}\\n\\n{report_with_references}\"", "kind": "Chunk", "id": "detailed_report/detailed_report.py#10.61"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py_json_ResearchRequest.agent", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py", "file_name": "server.py", "file_type": "text/x-python", "category": "test", "tokens": 165, "span_ids": ["imports", "ResearchRequest"], "start_line": 1, "end_line": 25, "community": null}, "content": "import json\nimport os\nimport re\nimport time\n\nfrom fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect, File, UploadFile, Header\nfrom fastapi.responses import JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom pydantic import BaseModel\n\nfrom backend.utils import write_md_to_pdf, write_md_to_word, write_text_to_md\nfrom backend.websocket_manager import WebSocketManager\n\nimport shutil\nfrom multi_agents.main import run_research_task\nfrom gpt_researcher.document.document import DocumentLoader\nfrom gpt_researcher.master.actions import stream_output\n\n\nclass ResearchRequest(BaseModel):\n    task: str\n    report_type: str\n    agent: str", "kind": "Chunk", "id": "backend/server.py#11.24"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py_ConfigRequest_ConfigRequest.SEARX_URL._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py", "file_name": "server.py", "file_type": "text/x-python", "category": "test", "tokens": 119, "span_ids": ["ConfigRequest"], "start_line": 27, "end_line": 40, "community": null}, "content": "class ConfigRequest(BaseModel):\n    ANTHROPIC_API_KEY: str\n    TAVILY_API_KEY: str\n    LANGCHAIN_TRACING_V2: str\n    LANGCHAIN_API_KEY: str\n    OPENAI_API_KEY: str\n    DOC_PATH: str\n    RETRIEVER: str\n    GOOGLE_API_KEY: str = ''\n    GOOGLE_CX_KEY: str = ''\n    BING_API_KEY: str = ''\n    SERPAPI_API_KEY: str = ''\n    SERPER_API_KEY: str = ''\n    SEARX_URL: str = ''", "kind": "Chunk", "id": "backend/server.py#12.13"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py_app_sanitize_filename.return.re_sub_r_w_s_f", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py", "file_name": "server.py", "file_type": "text/x-python", "category": "test", "tokens": 167, "span_ids": ["sanitize_filename", "startup_event", "read_root", "impl"], "start_line": 42, "end_line": 68, "community": null}, "content": "app = FastAPI()\n\napp.mount(\"/site\", StaticFiles(directory=\"./frontend\"), name=\"site\")\napp.mount(\"/static\", StaticFiles(directory=\"./frontend/static\"), name=\"static\")\n\ntemplates = Jinja2Templates(directory=\"./frontend\")\n\nmanager = WebSocketManager()\n\n# Dynamic directory for outputs once first research is run\r\n@app.on_event(\"startup\")\r\ndef startup_event():\n    if not os.path.isdir(\"outputs\"):\n        os.makedirs(\"outputs\")\n    app.mount(\"/outputs\", StaticFiles(directory=\"outputs\"), name=\"outputs\")\n\n\n@app.get(\"/\")\r\nasync def read_root(request: Request):\n    return templates.TemplateResponse(\r\n        \"index.html\", {\"request\": request, \"report\": None}\r\n    )\n\n\n# Add the sanitize_filename function here\r\ndef sanitize_filename(filename):\n    return re.sub(r\"[^\\w\\s-]\", \"\", filename).strip()", "kind": "Chunk", "id": "backend/server.py#13.26"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py_websocket_endpoint_run_multi_agents.if_websocket_.else_.return.JSONResponse_status_code_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py", "file_name": "server.py", "file_type": "text/x-python", "category": "test", "tokens": 525, "span_ids": ["run_multi_agents", "websocket_endpoint"], "start_line": 70, "end_line": 131, "community": null}, "content": "@app.websocket(\"/ws\")\r\nasync def websocket_endpoint(websocket: WebSocket):\n    await manager.connect(websocket)\n    try:\n        while True:\n            data = await websocket.receive_text()\n            if data.startswith(\"start\"):\n                json_data = json.loads(data[6:])\n                task = json_data.get(\"task\")\n                report_type = json_data.get(\"report_type\")\n                source_urls = json_data.get(\"source_urls\")\n                tone = json_data.get(\"tone\")\n                headers = json_data.get(\"headers\", {})\n                filename = f\"task_{int(time.time())}_{task}\"\n                sanitized_filename = sanitize_filename(\r\n                    filename\r\n                )  # Sanitize the filename\r\n                report_source = json_data.get(\"report_source\")\n                if task and report_type:\n                    report = await manager.start_streaming(\r\n                        task, report_type, report_source, source_urls, tone, websocket, headers\r\n                    )\n                    # Ensure report is a string\r\n                    if not isinstance(report, str):\n                        report = str(report)\n\n                    # Saving report as pdf\r\n                    pdf_path = await write_md_to_pdf(report, sanitized_filename)\n                    # Saving report as docx\r\n                    docx_path = await write_md_to_word(report, sanitized_filename)\n                    # Returning the path of saved report files\r\n                    md_path = await write_text_to_md(report, sanitized_filename)\n                    await websocket.send_json(\r\n                        {\r\n                            \"type\": \"path\",\r\n                            \"output\": {\r\n                                \"pdf\": pdf_path,\r\n                                \"docx\": docx_path,\r\n                                \"md\": md_path,\r\n                            },\r\n                        }\r\n                    )\n                elif data.startswith(\"human_feedback\"):\r\n                    # Handle human feedback\r\n                    feedback_data = json.loads(data[14:])  # Remove \"human_feedback\" prefix\r\n                    # Process the feedback data as needed\r\n                    # You might want to send this feedback to the appropriate agent or update the research state\r\n                    print(f\"Received human feedback: {feedback_data}\")\n                    # You can add logic here to forward the feedback to the appropriate agent or update the research state\r\n                else:\n                    print(\"Error: not enough parameters provided.\")\n    except WebSocketDisconnect:\n        await manager.disconnect(websocket)\n\n@app.post(\"/api/multi_agents\")\r\nasync def run_multi_agents():\n    websocket = manager.active_connections[0] if manager.active_connections else None\n    if websocket:\n        report = await run_research_task(\"Is AI in a hype cycle?\", websocket, stream_output)\n        return {\"report\": report}\n    else:\n        return JSONResponse(status_code=400, content={\"message\": \"No active WebSocket connection\"})", "kind": "Chunk", "id": "backend/server.py#14.61"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py_get_config_get_config.return.config", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py", "file_name": "server.py", "file_type": "text/x-python", "category": "test", "tokens": 442, "span_ids": ["get_config"], "start_line": 133, "end_line": 160, "community": null}, "content": "@app.get(\"/getConfig\")\r\nasync def get_config(\r\n    langchain_api_key: str = Header(None),\r\n    openai_api_key: str = Header(None),\r\n    tavily_api_key: str = Header(None),\r\n    google_api_key: str = Header(None),\r\n    google_cx_key: str = Header(None),\r\n    bing_api_key: str = Header(None),\r\n    serpapi_api_key: str = Header(None),\r\n    serper_api_key: str = Header(None),\r\n    searx_url: str = Header(None)\r\n):\n    config = {\r\n        \"LANGCHAIN_API_KEY\": langchain_api_key if langchain_api_key else os.getenv(\"LANGCHAIN_API_KEY\", \"\"),\r\n        \"OPENAI_API_KEY\": openai_api_key if openai_api_key else os.getenv(\"OPENAI_API_KEY\", \"\"),\r\n        \"TAVILY_API_KEY\": tavily_api_key if tavily_api_key else os.getenv(\"TAVILY_API_KEY\", \"\"),\r\n        \"GOOGLE_API_KEY\": google_api_key if google_api_key else os.getenv(\"GOOGLE_API_KEY\", \"\"),\r\n        \"GOOGLE_CX_KEY\": google_cx_key if google_cx_key else os.getenv(\"GOOGLE_CX_KEY\", \"\"),\r\n        \"BING_API_KEY\": bing_api_key if bing_api_key else os.getenv(\"BING_API_KEY\", \"\"),\r\n        \"SERPAPI_API_KEY\": serpapi_api_key if serpapi_api_key else os.getenv(\"SERPAPI_API_KEY\", \"\"),\r\n        \"SERPER_API_KEY\": serper_api_key if serper_api_key else os.getenv(\"SERPER_API_KEY\", \"\"),\r\n        \"SEARX_URL\": searx_url if searx_url else os.getenv(\"SEARX_URL\", \"\"),\r\n        \"LANGCHAIN_TRACING_V2\": os.getenv(\"LANGCHAIN_TRACING_V2\", \"true\"),\r\n        \"DOC_PATH\": os.getenv(\"DOC_PATH\", \"\"),\r\n        \"RETRIEVER\": os.getenv(\"RETRIEVER\", \"\"),\r\n        \"EMBEDDING_MODEL\": os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"\")\r\n    }\n    return config", "kind": "Chunk", "id": "backend/server.py#15.27"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py_set_config_set_config.return._message_Config_updat", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py", "file_name": "server.py", "file_type": "text/x-python", "category": "test", "tokens": 250, "span_ids": ["set_config"], "start_line": 162, "end_line": 177, "community": null}, "content": "@app.post(\"/setConfig\")\r\nasync def set_config(config: ConfigRequest):\n    os.environ[\"ANTHROPIC_API_KEY\"] = config.ANTHROPIC_API_KEY\n    os.environ[\"TAVILY_API_KEY\"] = config.TAVILY_API_KEY\n    os.environ[\"LANGCHAIN_TRACING_V2\"] = config.LANGCHAIN_TRACING_V2\n    os.environ[\"LANGCHAIN_API_KEY\"] = config.LANGCHAIN_API_KEY\n    os.environ[\"OPENAI_API_KEY\"] = config.OPENAI_API_KEY\n    os.environ[\"DOC_PATH\"] = config.DOC_PATH\n    os.environ[\"RETRIEVER\"] = config.RETRIEVER\n    os.environ[\"GOOGLE_API_KEY\"] = config.GOOGLE_API_KEY\n    os.environ[\"GOOGLE_CX_KEY\"] = config.GOOGLE_CX_KEY\n    os.environ[\"BING_API_KEY\"] = config.BING_API_KEY\n    os.environ[\"SERPAPI_API_KEY\"] = config.SERPAPI_API_KEY\n    os.environ[\"SERPER_API_KEY\"] = config.SERPER_API_KEY\n    os.environ[\"SEARX_URL\"] = config.SEARX_URL\n    return {\"message\": \"Config updated successfully\"}", "kind": "Chunk", "id": "backend/server.py#16.15"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py__Enable_CORS_for_your_fr_delete_file.if_os_path_exists_file_pa.else_.return.JSONResponse_status_code_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\server.py", "file_name": "server.py", "file_type": "text/x-python", "category": "test", "tokens": 324, "span_ids": ["impl:9", "set_config", "upload_file", "delete_file", "list_files"], "start_line": 179, "end_line": 224, "community": null}, "content": "# Enable CORS for your frontend domain (adjust accordingly)\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=[\"http://localhost:3000\"],\r\n    allow_credentials=True,\r\n    allow_methods=[\"*\"],\r\n    allow_headers=[\"*\"],\r\n)\n\n\n# Define DOC_PATH\r\nDOC_PATH = os.getenv(\"DOC_PATH\", \"./my-docs\")\nif not os.path.exists(DOC_PATH):\n    os.makedirs(DOC_PATH)\n\n\n@app.post(\"/upload/\")\r\nasync def upload_file(file: UploadFile = File(...)):\n    file_path = os.path.join(DOC_PATH, file.filename)\n    with open(file_path, \"wb\") as buffer:\n        shutil.copyfileobj(file.file, buffer)\n    print(f\"File uploaded to {file_path}\")\n\n    # Load documents after upload\r\n    document_loader = DocumentLoader(DOC_PATH)\n    await document_loader.load()\n\n    return {\"filename\": file.filename, \"path\": file_path}\n\n\n@app.get(\"/files/\")\r\nasync def list_files():\n    files = os.listdir(DOC_PATH)\n    print(f\"Files in {DOC_PATH}: {files}\")\n    return {\"files\": files}\n\n@app.delete(\"/files/{filename}\")\r\nasync def delete_file(filename: str):\n    file_path = os.path.join(DOC_PATH, filename)\n    if os.path.exists(file_path):\n        os.remove(file_path)\n        print(f\"File deleted: {file_path}\")\n        return {\"message\": \"File deleted successfully\"}\n    else:\n        print(f\"File not found: {file_path}\")\n        return JSONResponse(status_code=404, content={\"message\": \"File not found\"})", "kind": "Chunk", "id": "backend/server.py#17.45"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\utils.py_aiofiles_write_text_to_md.return.urllib_parse_quote_file_p", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\utils.py", "file_name": "utils.py", "file_type": "text/x-python", "category": "test", "tokens": 252, "span_ids": ["imports", "write_to_file", "write_text_to_md"], "start_line": 1, "end_line": 33, "community": null}, "content": "import aiofiles\nimport urllib\nimport mistune\n\nasync def write_to_file(filename: str, text: str) -> None:\n    \"\"\"Asynchronously write text to a file in UTF-8 encoding.\r\n\r\n    Args:\r\n        filename (str): The filename to write to.\r\n        text (str): The text to write.\r\n    \"\"\"\n    # Ensure text is a string\r\n    if not isinstance(text, str):\n        text = str(text)\n\n    # Convert text to UTF-8, replacing any problematic characters\r\n    text_utf8 = text.encode('utf-8', errors='replace').decode('utf-8')\n\n    async with aiofiles.open(filename, \"w\", encoding='utf-8') as file:\n        await file.write(text_utf8)\n\nasync def write_text_to_md(text: str, filename: str = \"\") -> str:\n    \"\"\"Writes text to a Markdown file and returns the file path.\r\n\r\n    Args:\r\n        text (str): Text to write to the Markdown file.\r\n\r\n    Returns:\r\n        str: The file path of the generated Markdown file.\r\n    \"\"\"\n    file_path = f\"outputs/{filename[:60]}.md\"\n    await write_to_file(file_path, text)\n    return urllib.parse.quote(file_path)", "kind": "Chunk", "id": "backend/utils.py#18.32"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\utils.py_write_md_to_pdf_write_md_to_pdf.return.encoded_file_path", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\utils.py", "file_name": "utils.py", "file_type": "text/x-python", "category": "test", "tokens": 182, "span_ids": ["write_md_to_pdf"], "start_line": 35, "end_line": 59, "community": null}, "content": "async def write_md_to_pdf(text: str, filename: str = \"\") -> str:\n    \"\"\"Converts Markdown text to a PDF file and returns the file path.\r\n\r\n    Args:\r\n        text (str): Markdown text to convert.\r\n\r\n    Returns:\r\n        str: The encoded file path of the generated PDF.\r\n    \"\"\"\n    file_path = f\"outputs/{filename[:60]}.pdf\"\n\n    try:\n        from md2pdf.core import md2pdf\n        md2pdf(file_path,\r\n               md_content=text,\r\n               # md_file_path=f\"{file_path}.md\",\r\n               css_file_path=\"./frontend/pdf_styles.css\",\r\n               base_url=None)\n        print(f\"Report written to {file_path}\")\n    except Exception as e:\n        print(f\"Error in converting Markdown to PDF: {e}\")\n        return \"\"\n\n    encoded_file_path = urllib.parse.quote(file_path)\n    return encoded_file_path", "kind": "Chunk", "id": "backend/utils.py#19.24"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\utils.py_write_md_to_word_write_md_to_word.try_.except_Exception_as_e_.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\utils.py", "file_name": "utils.py", "file_type": "text/x-python", "category": "test", "tokens": 229, "span_ids": ["write_md_to_word"], "start_line": 61, "end_line": 92, "community": null}, "content": "async def write_md_to_word(text: str, filename: str = \"\") -> str:\n    \"\"\"Converts Markdown text to a DOCX file and returns the file path.\r\n\r\n    Args:\r\n        text (str): Markdown text to convert.\r\n\r\n    Returns:\r\n        str: The encoded file path of the generated DOCX.\r\n    \"\"\"\n    file_path = f\"outputs/{filename[:60]}.docx\"\n\n    try:\n        from docx import Document\n        from htmldocx import HtmlToDocx\n        # Convert report markdown to HTML\r\n        html = mistune.html(text)\n        # Create a document object\r\n        doc = Document()\n        # Convert the html generated from the report to document format\r\n        HtmlToDocx().add_html_to_document(html, doc)\n\n        # Saving the docx document to file_path\r\n        doc.save(file_path)\n\n        print(f\"Report written to {file_path}\")\n\n        encoded_file_path = urllib.parse.quote(file_path)\n        return encoded_file_path\n\n    except Exception as e:\n        print(f\"Error in converting Markdown to DOCX: {e}\")\n        return \"\"", "kind": "Chunk", "id": "backend/utils.py#20.31"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\websocket_manager.py_asyncio_WebSocketManager.start_streaming.return.report", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\websocket_manager.py", "file_name": "websocket_manager.py", "file_type": "text/x-python", "category": "test", "tokens": 421, "span_ids": ["WebSocketManager", "imports", "WebSocketManager.start_sender", "WebSocketManager.connect", "WebSocketManager.start_streaming", "WebSocketManager.__init__", "WebSocketManager.disconnect"], "start_line": 1, "end_line": 61, "community": null}, "content": "import asyncio\nimport datetime\nfrom typing import Dict, List\n\nfrom fastapi import WebSocket\n\nfrom backend.report_type import BasicReport, DetailedReport\nfrom gpt_researcher.utils.enum import ReportType, Tone\nfrom multi_agents.main import run_research_task\nfrom gpt_researcher.master.actions import stream_output  # Import stream_output\r\n\nclass WebSocketManager:\n    \"\"\"Manage websockets\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the WebSocketManager class.\"\"\"\n        self.active_connections: List[WebSocket] = []\n        self.sender_tasks: Dict[WebSocket, asyncio.Task] = {}\n        self.message_queues: Dict[WebSocket, asyncio.Queue] = {}\n\n    async def start_sender(self, websocket: WebSocket):\n        \"\"\"Start the sender task.\"\"\"\n        queue = self.message_queues.get(websocket)\n        if not queue:\n            return\n\n        while True:\n            message = await queue.get()\n            if websocket in self.active_connections:\n                try:\n                    if message == \"ping\":\n                        await websocket.send_text(\"pong\")\n                    else:\n                        await websocket.send_text(message)\n                except:\n                    break\n            else:\n                break\n\n    async def connect(self, websocket: WebSocket):\n        \"\"\"Connect a websocket.\"\"\"\n        await websocket.accept()\n        self.active_connections.append(websocket)\n        self.message_queues[websocket] = asyncio.Queue()\n        self.sender_tasks[websocket] = asyncio.create_task(self.start_sender(websocket))\n\n    async def disconnect(self, websocket: WebSocket):\n        \"\"\"Disconnect a websocket.\"\"\"\n        if websocket in self.active_connections:\n            self.active_connections.remove(websocket)\n            self.sender_tasks[websocket].cancel()\n            await self.message_queues[websocket].put(None)\n            del self.sender_tasks[websocket]\n            del self.message_queues[websocket]\n\n\n    async def start_streaming(self, task, report_type, report_source, source_urls, tone, websocket, headers=None):\n        \"\"\"Start streaming the output.\"\"\"\n        tone = Tone[tone]\n        report = await run_agent(task, report_type, report_source, source_urls, tone, websocket, headers)\n        return report", "kind": "Chunk", "id": "backend/websocket_manager.py#21.60"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\websocket_manager.py_run_agent_run_agent.return.report", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\backend\\websocket_manager.py", "file_name": "websocket_manager.py", "file_type": "text/x-python", "category": "test", "tokens": 310, "span_ids": ["run_agent"], "start_line": 64, "end_line": 105, "community": null}, "content": "async def run_agent(task, report_type, report_source, source_urls, tone: Tone, websocket, headers=None):\n    \"\"\"Run the agent.\"\"\"\n    # measure time\r\n    start_time = datetime.datetime.now()\n    # add customized JSON config file path here\r\n    config_path = \"\"\n    # Instead of running the agent directly run it through the different report type classes\r\n    if report_type == \"multi_agents\":\n        report = await run_research_task(query=task, websocket=websocket, stream_output=stream_output, tone=tone, headers=headers)\n        report = report.get(\"report\", \"\")\n    elif report_type == ReportType.DetailedReport.value:\n        researcher = DetailedReport(\r\n            query=task,\r\n            report_type=report_type,\r\n            report_source=report_source,\r\n            source_urls=source_urls,\r\n            tone=tone,\r\n            config_path=config_path,\r\n            websocket=websocket,\r\n            headers=headers\r\n        )\n        report = await researcher.run()\n    else:\n        researcher = BasicReport(\r\n            query=task,\r\n            report_type=report_type,\r\n            report_source=report_source,\r\n            source_urls=source_urls,\r\n            tone=tone,\r\n            config_path=config_path,\r\n            websocket=websocket,\r\n            headers=headers\r\n        )\n        report = await researcher.run()\n\n    # measure time\r\n    end_time = datetime.datetime.now()\n    await websocket.send_json(\r\n        {\"type\": \"logs\", \"output\": f\"\\nTotal run time: {end_time - start_time}\\n\"}\r\n    )\n\n    return report", "kind": "Chunk", "id": "backend/websocket_manager.py#22.41"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\cli.py___None_1", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\cli.py", "file_name": "cli.py", "file_type": "text/x-python", "category": "test", "tokens": 354, "span_ids": ["docstring"], "start_line": 1, "end_line": 63, "community": null}, "content": "\"\"\"\r\nProvides a command line interface for the GPTResearcher class.\r\n\r\nUsage:\r\n\r\n```shell\r\npython cli.py \"<query>\" --report_type <report_type>\r\n```\r\n\r\n\"\"\"\nimport asyncio\nimport argparse\nfrom argparse import RawTextHelpFormatter\nfrom uuid import uuid4\n\nfrom dotenv import load_dotenv\n\nfrom gpt_researcher import GPTResearcher\nfrom gpt_researcher.utils.enum import ReportType\n\n# =============================================================================\r\n# CLI\r\n# =============================================================================\r\n\ncli = argparse.ArgumentParser(\r\n    description=\"Generate a research report.\",\r\n    # Enables the use of newlines in the help message\r\n    formatter_class=RawTextHelpFormatter)\n\n# =====================================\r\n# Arg: Query\r\n# =====================================\r\n\ncli.add_argument(\r\n    # Position 0 argument\r\n    \"query\",\r\n    type=str,\r\n    help=\"The query to conduct research on.\")\n\n# =====================================\r\n# Arg: Report Type\r\n# =====================================\r\n\nchoices = [report_type.value for report_type in ReportType]\n\nreport_type_descriptions = {\r\n    ReportType.ResearchReport.value: \"Summary - Short and fast (~2 min)\",\r\n    ReportType.DetailedReport.value: \"Detailed - In depth and longer (~5 min)\",\r\n    ReportType.ResourceReport.value: \"\",\r\n    ReportType.OutlineReport.value: \"\",\r\n    ReportType.CustomReport.value: \"\",\r\n    ReportType.SubtopicReport.value: \"\"\r\n}\n\ncli.add_argument(\r\n    \"--report_type\",\r\n    type=str,\r\n    help=\"The type of report to generate. Options:\\n\" + \"\\n\".join(\r\n        f\"  {choice}: {report_type_descriptions[choice]}\" for choice in choices\r\n    ),\r\n    # Deserialize ReportType as a List of strings:\r\n    choices=choices,\r\n    required=True)", "kind": "Chunk", "id": "gpt-researcher/cli.py#23.62"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\cli.py_None_10_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\cli.py", "file_name": "cli.py", "file_type": "text/x-python", "category": "test", "tokens": 163, "span_ids": ["impl:9", "docstring", "main"], "start_line": 65, "end_line": 94, "community": null}, "content": "# =============================================================================\r\n# Main\r\n# =============================================================================\r\n\n\nasync def main(args):\n    \"\"\" \r\n    Conduct research on the given query, generate the report, and write\r\n    it as a markdown file to the output directory.\r\n    \"\"\"\n    researcher = GPTResearcher(\r\n        query=args.query,\r\n        report_type=args.report_type)\n\n    await researcher.conduct_research()\n\n    report = await researcher.write_report()\n\n    # Write the report to a file\r\n    artifact_filepath = f\"outputs/{uuid4()}.md\"\n    with open(artifact_filepath, \"w\") as f:\n        f.write(report)\n\n    print(f\"Report written to '{artifact_filepath}'\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n    args = cli.parse_args()\n    asyncio.run(main(args))", "kind": "Chunk", "id": "gpt-researcher/cli.py#24.29"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\examples\\sample_report.py_from_gpt_researcher_impor_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\examples\\sample_report.py", "file_name": "sample_report.py", "file_type": "text/x-python", "category": "test", "tokens": 142, "span_ids": ["imports", "main", "impl"], "start_line": 1, "end_line": 27, "community": null}, "content": "from gpt_researcher import GPTResearcher\nimport asyncio\n\n\nasync def main():\n    \"\"\"\r\n    This is a sample script that shows how to run a research report.\r\n    \"\"\"\n    # Query\r\n    query = \"What happened in the latest burning man floods?\"\n\n    # Report Type\r\n    report_type = \"research_report\"\n\n    # Initialize the researcher\r\n    researcher = GPTResearcher(query=query, report_type=report_type, config_path=None)\n    # Conduct research on the given query\r\n    await researcher.conduct_research()\n    # Write the report\r\n    report = await researcher.write_report()\n\n    return report\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "kind": "Chunk", "id": "examples/sample_report.py#25.26"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\__init__.py__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 28, "span_ids": ["imports"], "start_line": 1, "end_line": 5, "community": null}, "content": "from .master import GPTResearcher\nfrom .config import Config\n\n__all__ = ['GPTResearcher', 'Config']", "kind": "Chunk", "id": "gpt_researcher/__init__.py#26.4"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\config\\__init__.py____all__._Config_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\config\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 13, "span_ids": ["imports"], "start_line": 1, "end_line": 3, "community": null}, "content": "from .config import Config\n\n__all__ = ['Config']", "kind": "Chunk", "id": "config/__init__.py#27.2"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\config\\config.py__config_file_Config.__init__.if_self_doc_path_.self_validate_doc_path_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\config\\config.py", "file_name": "config.py", "file_type": "text/x-python", "category": "test", "tokens": 626, "span_ids": ["docstring", "Config.__init__", "Config"], "start_line": 1, "end_line": 52, "community": null}, "content": "# config file\r\nimport json\nimport os\n\n\nclass Config:\n    \"\"\"Config class for GPT Researcher.\"\"\"\n\n    def __init__(self, config_file: str = None):\n        \"\"\"Initialize the config class.\"\"\"\n        self.config_file = (\r\n            os.path.expanduser(config_file) if config_file else os.getenv(\"CONFIG_FILE\")\r\n        )\n        self.retrievers = self.parse_retrievers(os.getenv(\"RETRIEVER\", \"tavily\"))\n        self.embedding_provider = os.getenv(\"EMBEDDING_PROVIDER\", \"openai\")\n        self.similarity_threshold = int(os.getenv(\"SIMILARITY_THRESHOLD\", 0.42))\n        self.llm_provider = os.getenv(\"LLM_PROVIDER\", \"openai\")\n        self.ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\", None)\n        self.llm_model = os.getenv(\"DEFAULT_LLM_MODEL\", \"gpt-4o-mini\")\n        self.fast_llm_model = os.getenv(\"FAST_LLM_MODEL\", \"gpt-4o-mini\")\n        self.smart_llm_model = os.getenv(\"SMART_LLM_MODEL\", \"gpt-4o-2024-08-06\")\n        self.fast_token_limit = int(os.getenv(\"FAST_TOKEN_LIMIT\", 2000))\n        self.smart_token_limit = int(os.getenv(\"SMART_TOKEN_LIMIT\", 4000))\n        self.browse_chunk_max_length = int(os.getenv(\"BROWSE_CHUNK_MAX_LENGTH\", 8192))\n        self.summary_token_limit = int(os.getenv(\"SUMMARY_TOKEN_LIMIT\", 700))\n        self.temperature = float(os.getenv(\"TEMPERATURE\", 0.4))\n        self.llm_temperature = float(os.getenv(\"LLM_TEMPERATURE\", 0.55))\n        self.user_agent = os.getenv(\r\n            \"USER_AGENT\",\r\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\r\n            \"(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\",\r\n        )\n        self.max_search_results_per_query = int(\r\n            os.getenv(\"MAX_SEARCH_RESULTS_PER_QUERY\", 5)\r\n        )\n        self.memory_backend = os.getenv(\"MEMORY_BACKEND\", \"local\")\n        self.total_words = int(os.getenv(\"TOTAL_WORDS\", 900))\n        self.report_format = os.getenv(\"REPORT_FORMAT\", \"APA\")\n        self.max_iterations = int(os.getenv(\"MAX_ITERATIONS\", 3))\n        self.agent_role = os.getenv(\"AGENT_ROLE\", None)\n        self.scraper = os.getenv(\"SCRAPER\", \"bs\")\n        self.max_subtopics = os.getenv(\"MAX_SUBTOPICS\", 3)\n        self.report_source = os.getenv(\"REPORT_SOURCE\", None)\n        self.doc_path = os.getenv(\"DOC_PATH\", \"\")\n        self.llm_kwargs = {}\n\n        self.load_config_file()\n        if not hasattr(self, \"llm_kwargs\"):\n            self.llm_kwargs = {}\n\n        if self.doc_path:\n            self.validate_doc_path()", "kind": "Chunk", "id": "config/config.py#28.51"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\config\\config.py_Config.parse_retrievers_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\config\\config.py", "file_name": "config.py", "file_type": "text/x-python", "category": "test", "tokens": 307, "span_ids": ["Config.parse_retrievers", "Config.load_config_file", "Config.validate_doc_path"], "start_line": 54, "end_line": 91, "community": null}, "content": "class Config:\n\n    def parse_retrievers(self, retriever_str: str):\n        \"\"\"Parse the retriever string into a list of retrievers and validate them.\"\"\"\n        VALID_RETRIEVERS = [\r\n            \"arxiv\",\r\n            \"bing\",\r\n            \"custom\",\r\n            \"duckduckgo\",\r\n            \"exa\",\r\n            \"google\",\r\n            \"searx\",\r\n            \"semantic_scholar\",\r\n            \"serpapi\",\r\n            \"serper\",\r\n            \"tavily\",\r\n            \"pubmed_central\",\r\n        ]\n        retrievers = [retriever.strip() for retriever in retriever_str.split(\",\")]\n        invalid_retrievers = [r for r in retrievers if r not in VALID_RETRIEVERS]\n        if invalid_retrievers:\n            raise ValueError(\r\n                f\"Invalid retriever(s) found: {', '.join(invalid_retrievers)}. \"\r\n                f\"Valid options are: {', '.join(VALID_RETRIEVERS)}.\"\r\n            )\n        return retrievers\n\n    def validate_doc_path(self):\n        \"\"\"Ensure that the folder exists at the doc path\"\"\"\n        os.makedirs(self.doc_path, exist_ok=True)\n\n    def load_config_file(self) -> None:\n        \"\"\"Load the config file.\"\"\"\n        if self.config_file is None:\n            return None\n        with open(self.config_file, \"r\") as f:\n            config = json.load(f)\n        for key, value in config.items():\n            setattr(self, key.lower(), value)", "kind": "Chunk", "id": "config/config.py#29.37"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\__init__.py__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 36, "span_ids": ["imports"], "start_line": 1, "end_line": 5, "community": null}, "content": "from .compression import ContextCompressor\nfrom .retriever import SearchAPIRetriever\n\n__all__ = ['ContextCompressor', 'SearchAPIRetriever']", "kind": "Chunk", "id": "context/__init__.py#30.4"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py_os_from_gpt_researcher_memor", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py", "file_name": "compression.py", "file_type": "text/x-python", "category": "test", "tokens": 112, "span_ids": ["imports"], "start_line": 1, "end_line": 14, "community": null}, "content": "import os\nimport asyncio\nfrom typing import Optional\nfrom .retriever import SearchAPIRetriever, SectionRetriever\nfrom langchain.retrievers import (\r\n    ContextualCompressionRetriever,\r\n)\nfrom langchain.retrievers.document_compressors import (\r\n    DocumentCompressorPipeline,\r\n    EmbeddingsFilter,\r\n)\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom gpt_researcher.utils.costs import estimate_embedding_cost\nfrom gpt_researcher.memory.embeddings import OPENAI_EMBEDDING_MODEL", "kind": "Chunk", "id": "context/compression.py#31.13"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py_VectorstoreCompressor_VectorstoreCompressor.async_get_context.return.self___pretty_print_docs_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py", "file_name": "compression.py", "file_type": "text/x-python", "category": "test", "tokens": 171, "span_ids": ["VectorstoreCompressor.__init__", "VectorstoreCompressor.__pretty_print_docs", "VectorstoreCompressor", "VectorstoreCompressor.async_get_context"], "start_line": 17, "end_line": 32, "community": null}, "content": "class VectorstoreCompressor:\n    def __init__(self, vector_store, max_results=5, filter: Optional[dict] = None, **kwargs):\n        self.vector_store = vector_store\n        self.max_results = max_results\n        self.filter = filter\n        self.kwargs = kwargs\n\n    def __pretty_print_docs(self, docs):\n        return f\"\\n\".join(f\"Source: {d.metadata.get('source')}\\n\"\r\n                          f\"Title: {d.metadata.get('title')}\\n\"\r\n                          f\"Content: {d.page_content}\\n\"\r\n                          for d in docs)\n\n    async def async_get_context(self, query, max_results=5):\n        results = await self.vector_store.asimilarity_search(query=query, k=max_results, filter=self.filter)\n        return self.__pretty_print_docs(results)", "kind": "Chunk", "id": "context/compression.py#32.15"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py_ContextCompressor_ContextCompressor.__get_contextual_retriever.return.contextual_retriever", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py", "file_name": "compression.py", "file_type": "text/x-python", "category": "test", "tokens": 203, "span_ids": ["ContextCompressor", "ContextCompressor.__get_contextual_retriever", "ContextCompressor.__init__"], "start_line": 35, "end_line": 56, "community": null}, "content": "class ContextCompressor:\n    def __init__(self, documents, embeddings, max_results=5, **kwargs):\n        self.max_results = max_results\n        self.documents = documents\n        self.kwargs = kwargs\n        self.embeddings = embeddings\n        self.similarity_threshold = os.environ.get(\"SIMILARITY_THRESHOLD\", 0.38)\n\n    def __get_contextual_retriever(self):\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n        relevance_filter = EmbeddingsFilter(embeddings=self.embeddings,\r\n                                            similarity_threshold=self.similarity_threshold)\n        pipeline_compressor = DocumentCompressorPipeline(\r\n            transformers=[splitter, relevance_filter]\r\n        )\n        base_retriever = SearchAPIRetriever(\r\n            pages=self.documents\r\n        )\n        contextual_retriever = ContextualCompressionRetriever(\r\n            base_compressor=pipeline_compressor, base_retriever=base_retriever\r\n        )\n        return contextual_retriever", "kind": "Chunk", "id": "context/compression.py#33.21"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py_ContextCompressor.__pretty_print_docs_ContextCompressor.async_get_context.return.self___pretty_print_docs_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py", "file_name": "compression.py", "file_type": "text/x-python", "category": "test", "tokens": 247, "span_ids": ["ContextCompressor.__pretty_print_docs", "ContextCompressor.async_get_context", "ContextCompressor.get_context"], "start_line": 58, "end_line": 76, "community": null}, "content": "class ContextCompressor:\n\n    def __pretty_print_docs(self, docs, top_n):\n        return f\"\\n\".join(f\"Source: {d.metadata.get('source')}\\n\"\r\n                          f\"Title: {d.metadata.get('title')}\\n\"\r\n                          f\"Content: {d.page_content}\\n\"\r\n                          for i, d in enumerate(docs) if i < top_n)\n\n    def get_context(self, query, max_results=5, cost_callback=None):\n        compressed_docs = self.__get_contextual_retriever()\n        if cost_callback:\n            cost_callback(estimate_embedding_cost(model=OPENAI_EMBEDDING_MODEL, docs=self.documents))\n        relevant_docs = compressed_docs.invoke(query)\n        return self.__pretty_print_docs(relevant_docs, max_results)\n\n    async def async_get_context(self, query, max_results=5, cost_callback=None):\n        compressed_docs = self.__get_contextual_retriever()\n        if cost_callback:\n            cost_callback(estimate_embedding_cost(model=OPENAI_EMBEDDING_MODEL, docs=self.documents))\n        relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query)\n        return self.__pretty_print_docs(relevant_docs, max_results)", "kind": "Chunk", "id": "context/compression.py#34.18"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py_WrittenContentCompressor_WrittenContentCompressor.__get_contextual_retriever.return.contextual_retriever", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py", "file_name": "compression.py", "file_type": "text/x-python", "category": "test", "tokens": 181, "span_ids": ["WrittenContentCompressor.__get_contextual_retriever", "WrittenContentCompressor.__init__", "WrittenContentCompressor"], "start_line": 79, "end_line": 99, "community": null}, "content": "class WrittenContentCompressor:\n    def __init__(self, documents, embeddings, similarity_threshold, **kwargs):\n        self.documents = documents\n        self.kwargs = kwargs\n        self.embeddings = embeddings\n        self.similarity_threshold = similarity_threshold\n\n    def __get_contextual_retriever(self):\n        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n        relevance_filter = EmbeddingsFilter(embeddings=self.embeddings,\r\n                                            similarity_threshold=self.similarity_threshold)\n        pipeline_compressor = DocumentCompressorPipeline(\r\n            transformers=[splitter, relevance_filter]\r\n        )\n        base_retriever = SectionRetriever(\r\n            sections=self.documents\r\n        )\n        contextual_retriever = ContextualCompressionRetriever(\r\n            base_compressor=pipeline_compressor, base_retriever=base_retriever\r\n        )\n        return contextual_retriever", "kind": "Chunk", "id": "context/compression.py#35.20"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py_WrittenContentCompressor.__pretty_docs_list_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\compression.py", "file_name": "compression.py", "file_type": "text/x-python", "category": "test", "tokens": 225, "span_ids": ["WrittenContentCompressor.__pretty_docs_list", "WrittenContentCompressor.get_context", "WrittenContentCompressor.async_get_context"], "start_line": 101, "end_line": 117, "community": null}, "content": "class WrittenContentCompressor:\n\n    def __pretty_docs_list(self, docs, top_n):\n        return [f\"Title: {d.metadata.get('section_title')}\\nContent: {d.page_content}\\n\" for i, d in enumerate(docs) if i < top_n]\n\n    def get_context(self, query, max_results=5, cost_callback=None):\n        compressed_docs = self.__get_contextual_retriever()\n        if cost_callback:\n            cost_callback(estimate_embedding_cost(model=OPENAI_EMBEDDING_MODEL, docs=self.documents))\n        relevant_docs = compressed_docs.invoke(query)\n        return self.__pretty_print_docs(relevant_docs, max_results)\n\n    async def async_get_context(self, query, max_results=5, cost_callback=None):\n        compressed_docs = self.__get_contextual_retriever()\n        if cost_callback:\n            cost_callback(estimate_embedding_cost(model=OPENAI_EMBEDDING_MODEL, docs=self.documents))\n        relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query)\n        return self.__pretty_docs_list(relevant_docs, max_results)", "kind": "Chunk", "id": "context/compression.py#36.16"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\retriever.py_os_SearchAPIRetriever._get_relevant_documents.return.docs", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\retriever.py", "file_name": "retriever.py", "file_type": "text/x-python", "category": "test", "tokens": 175, "span_ids": ["imports", "SearchAPIRetriever._get_relevant_documents", "SearchAPIRetriever"], "start_line": 1, "end_line": 29, "community": null}, "content": "import os\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain.callbacks.manager import CallbackManagerForRetrieverRun\nfrom langchain.schema import Document\nfrom langchain.schema.retriever import BaseRetriever\n\n\nclass SearchAPIRetriever(BaseRetriever):\n    \"\"\"Search API retriever.\"\"\"\n    pages: List[Dict] = []\n\n    def _get_relevant_documents(\r\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\r\n    ) -> List[Document]:\n\n        docs = [\r\n            Document(\r\n                page_content=page.get(\"raw_content\", \"\"),\r\n                metadata={\r\n                    \"title\": page.get(\"title\", \"\"),\r\n                    \"source\": page.get(\"url\", \"\"),\r\n                },\r\n            )\r\n            for page in self.pages\r\n        ]\n\n        return docs", "kind": "Chunk", "id": "context/retriever.py#37.28"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\retriever.py_SectionRetriever_SectionRetriever._get_relevant_documents.return.docs", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\context\\retriever.py", "file_name": "retriever.py", "file_type": "text/x-python", "category": "test", "tokens": 176, "span_ids": ["SectionRetriever._get_relevant_documents", "SectionRetriever"], "start_line": 31, "end_line": 62, "community": null}, "content": "class SectionRetriever(BaseRetriever):\n    \"\"\"\r\n    SectionRetriever:\r\n    This class is used to retrieve sections while avoiding redundant subtopics.\r\n    \"\"\"\n    sections: List[Dict] = []\n    \"\"\"\r\n    sections example:\r\n    [\r\n        {\r\n            \"section_title\": \"Example Title\",\r\n            \"written_content\": \"Example content\"\r\n        },\r\n        ...\r\n    ]\r\n    \"\"\"\n\n    def _get_relevant_documents(\r\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\r\n    ) -> List[Document]:\n\n        docs = [\r\n            Document(\r\n                page_content=page.get(\"written_content\", \"\"),\r\n                metadata={\r\n                    \"section_title\": page.get(\"section_title\", \"\"),\r\n                },\r\n            )\r\n            for page in self.sections  # Changed 'self.pages' to 'self.sections'\r\n        ]\n\n        return docs", "kind": "Chunk", "id": "context/retriever.py#38.31"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\document\\__init__.py__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\document\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 32, "span_ids": ["imports"], "start_line": 1, "end_line": 5, "community": null}, "content": "from .document import DocumentLoader\nfrom .langchain_document import LangChainDocumentLoader\n\n__all__ = ['DocumentLoader', 'LangChainDocumentLoader']", "kind": "Chunk", "id": "document/__init__.py#39.4"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\document\\document.py_asyncio_DocumentLoader.load.return.docs", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\document\\document.py", "file_name": "document.py", "file_type": "text/x-python", "category": "test", "tokens": 238, "span_ids": ["imports", "DocumentLoader.__init__", "DocumentLoader.load", "DocumentLoader"], "start_line": 1, "end_line": 41, "community": null}, "content": "import asyncio\nimport os\n\nfrom langchain_community.document_loaders import (\r\n    PyMuPDFLoader, \r\n    TextLoader, \r\n    UnstructuredCSVLoader, \r\n    UnstructuredExcelLoader,\r\n    UnstructuredMarkdownLoader, \r\n    UnstructuredPowerPointLoader,\r\n    UnstructuredWordDocumentLoader\r\n)\n\n\nclass DocumentLoader:\n\n    def __init__(self, path):\n        self.path = path\n\n    async def load(self) -> list:\n        tasks = []\n        for root, dirs, files in os.walk(self.path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                file_name, file_extension_with_dot = os.path.splitext(file_path)\n                file_extension = file_extension_with_dot.strip(\".\")\n                tasks.append(self._load_document(file_path, file_extension))\n\n        docs = []\n        for pages in await asyncio.gather(*tasks):\n            for page in pages:\n                if page.page_content:\n                    docs.append({\r\n                        \"raw_content\": page.page_content,\r\n                        \"url\": os.path.basename(page.metadata['source'])\r\n                    })\n\n        if not docs:\n            raise ValueError(\"\ud83e\udd37 Failed to load any documents!\")\n\n        return docs", "kind": "Chunk", "id": "document/document.py#40.40"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\document\\document.py_DocumentLoader._load_document_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\document\\document.py", "file_name": "document.py", "file_type": "text/x-python", "category": "test", "tokens": 203, "span_ids": ["DocumentLoader._load_document"], "start_line": 43, "end_line": 67, "community": null}, "content": "class DocumentLoader:\n\n    async def _load_document(self, file_path: str, file_extension: str) -> list:\n        ret_data = []\n        try:\n            loader_dict = {\r\n                \"pdf\": PyMuPDFLoader(file_path),\r\n                \"txt\": TextLoader(file_path),\r\n                \"doc\": UnstructuredWordDocumentLoader(file_path),\r\n                \"docx\": UnstructuredWordDocumentLoader(file_path),\r\n                \"pptx\": UnstructuredPowerPointLoader(file_path),\r\n                \"csv\": UnstructuredCSVLoader(file_path, mode=\"elements\"),\r\n                \"xls\": UnstructuredExcelLoader(file_path, mode=\"elements\"),\r\n                \"xlsx\": UnstructuredExcelLoader(file_path, mode=\"elements\"),\r\n                \"md\": UnstructuredMarkdownLoader(file_path)\r\n            }\n\n            loader = loader_dict.get(file_extension, None)\n            if loader:\n                ret_data = loader.load()\n\n        except Exception as e:\n            print(f\"Failed to load document : {file_path}\")\n            print(e)\n\n        return ret_data", "kind": "Chunk", "id": "document/document.py#41.24"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\document\\langchain_document.py_asyncio_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\document\\langchain_document.py", "file_name": "langchain_document.py", "file_type": "text/x-python", "category": "test", "tokens": 145, "span_ids": ["LangChainDocumentLoader", "imports", "LangChainDocumentLoader.load", "LangChainDocumentLoader.__init__"], "start_line": 1, "end_line": 25, "community": null}, "content": "import asyncio\nimport os\n\nfrom langchain_core.documents import Document\nfrom typing import List, Dict\n\n\n# Supports the base Document class from langchain\r\n# - https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/documents/base.py\r\nclass LangChainDocumentLoader:\n\n    def __init__(self, documents: List[Document]):\n        self.documents = documents\n\n    async def load(self, metadata_source_index=\"title\") -> List[Dict[str, str]]:\n        docs = []\n        for document in self.documents:\n            docs.append(\r\n                {\r\n                    \"raw_content\": document.page_content,\r\n                    \"url\": document.metadata.get(metadata_source_index, \"\"),\r\n                }\r\n            )\n        return docs", "kind": "Chunk", "id": "document/langchain_document.py#42.24"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\__init__.py__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 22, "span_ids": ["imports"], "start_line": 1, "end_line": 6, "community": null}, "content": "from .generic import GenericLLMProvider\n\n__all__ = [\r\n    \"GenericLLMProvider\",\r\n]", "kind": "Chunk", "id": "llm_provider/__init__.py#43.5"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\generic\\__init__.py____all__._GenericLLMProvider_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\generic\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 19, "span_ids": ["imports"], "start_line": 1, "end_line": 3, "community": null}, "content": "from .base import GenericLLMProvider\n\n__all__ = [\"GenericLLMProvider\"]", "kind": "Chunk", "id": "generic/__init__.py#44.2"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\generic\\base.py_importlib_GenericLLMProvider.from_provider.return.cls_llm_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\generic\\base.py", "file_name": "base.py", "file_type": "text/x-python", "category": "test", "tokens": 749, "span_ids": ["imports", "GenericLLMProvider.from_provider", "GenericLLMProvider.__init__", "GenericLLMProvider"], "start_line": 1, "end_line": 90, "community": null}, "content": "import importlib\nfrom typing import Any\nfrom colorama import Fore, Style, init\n\n\nclass GenericLLMProvider:\n\n    def __init__(self, llm):\n        self.llm = llm\n\n    @classmethod\r\n    def from_provider(cls, provider: str, **kwargs: Any):\n        if provider == \"openai\":\n            _check_pkg(\"langchain_openai\")\n            from langchain_openai import ChatOpenAI\n\n            llm = ChatOpenAI(**kwargs)\n        elif provider == \"anthropic\":\n            _check_pkg(\"langchain_anthropic\")\n            from langchain_anthropic import ChatAnthropic\n\n            llm = ChatAnthropic(**kwargs)\n        elif provider == \"azure_openai\":\n            _check_pkg(\"langchain_openai\")\n            from langchain_openai import AzureChatOpenAI\n\n            llm = AzureChatOpenAI(**kwargs)\n        elif provider == \"cohere\":\n            _check_pkg(\"langchain_cohere\")\n            from langchain_cohere import ChatCohere\n\n            llm = ChatCohere(**kwargs)\n        elif provider == \"google_vertexai\":\n            _check_pkg(\"langchain_google_vertexai\")\n            from langchain_google_vertexai import ChatVertexAI\n\n            llm = ChatVertexAI(**kwargs)\n        elif provider == \"google_genai\":\n            _check_pkg(\"langchain_google_genai\")\n            from langchain_google_genai import ChatGoogleGenerativeAI\n\n            llm = ChatGoogleGenerativeAI(**kwargs)\n        elif provider == \"fireworks\":\n            _check_pkg(\"langchain_fireworks\")\n            from langchain_fireworks import ChatFireworks\n\n            llm = ChatFireworks(**kwargs)\n        elif provider == \"ollama\":\n            _check_pkg(\"langchain_community\")\n            from langchain_community.chat_models import ChatOllama\n\n            llm = ChatOllama(**kwargs)\n        elif provider == \"together\":\n            _check_pkg(\"langchain_together\")\n            from langchain_together import ChatTogether\n\n            llm = ChatTogether(**kwargs)\n        elif provider == \"mistralai\":\n            _check_pkg(\"langchain_mistralai\")\n            from langchain_mistralai import ChatMistralAI\n\n            llm = ChatMistralAI(**kwargs)\n        elif provider == \"huggingface\":\n            _check_pkg(\"langchain_huggingface\")\n            from langchain_huggingface import ChatHuggingFace\n\n            if \"model\" in kwargs or \"model_name\" in kwargs:\n                model_id = kwargs.pop(\"model\", None) or kwargs.pop(\"model_name\", None)\n                kwargs = {\"model_id\": model_id, **kwargs}\n            llm = ChatHuggingFace(**kwargs)\n        elif provider == \"groq\":\n            _check_pkg(\"langchain_groq\")\n            from langchain_groq import ChatGroq\n\n            llm = ChatGroq(**kwargs)\n        elif provider == \"bedrock\":\n            _check_pkg(\"langchain_aws\")\n            from langchain_aws import ChatBedrock\n\n            if \"model\" in kwargs or \"model_name\" in kwargs:\n                model_id = kwargs.pop(\"model\", None) or kwargs.pop(\"model_name\", None)\n                kwargs = {\"model_id\": model_id, **kwargs}\n            llm = ChatBedrock(**kwargs)\n        else:\n            supported = \", \".join(_SUPPORTED_PROVIDERS)\n            raise ValueError(\r\n                f\"Unsupported {provider=}.\\n\\nSupported model providers are: \"\r\n                f\"{supported}\"\r\n            )\n        return cls(llm)", "kind": "Chunk", "id": "generic/base.py#45.89"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\generic\\base.py_GenericLLMProvider.get_chat_response_GenericLLMProvider.stream_response.return.response", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\generic\\base.py", "file_name": "base.py", "file_type": "text/x-python", "category": "test", "tokens": 195, "span_ids": ["GenericLLMProvider.get_chat_response", "GenericLLMProvider.stream_response"], "start_line": 93, "end_line": 120, "community": null}, "content": "class GenericLLMProvider:\n\n\n    async def get_chat_response(self, messages, stream, websocket=None):\n        if not stream:\r\n            # Getting output from the model chain using ainvoke for asynchronous invoking\r\n            output = await self.llm.ainvoke(messages)\n\n            return output.content\n\n        else:\n            return await self.stream_response(messages, websocket)\n\n    async def stream_response(self, messages, websocket=None):\n        paragraph = \"\"\n        response = \"\"\n\n        # Streaming the response using the chain astream method from langchain\r\n        async for chunk in self.llm.astream(messages):\n            content = chunk.content\n            if content is not None:\n                response += content\n                paragraph += content\n                if \"\\n\" in paragraph:\n                    if websocket is not None:\n                        await websocket.send_json({\"type\": \"report\", \"output\": paragraph})\n                    else:\n                        print(f\"{Fore.GREEN}{paragraph}{Style.RESET_ALL}\")\n                    paragraph = \"\"\n\n        return response", "kind": "Chunk", "id": "generic/base.py#46.27"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\generic\\base.py__SUPPORTED_PROVIDERS_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\llm_provider\\generic\\base.py", "file_name": "base.py", "file_type": "text/x-python", "category": "test", "tokens": 171, "span_ids": ["_check_pkg", "impl"], "start_line": 124, "end_line": 150, "community": null}, "content": "_SUPPORTED_PROVIDERS = {\r\n    \"openai\",\r\n    \"anthropic\",\r\n    \"azure_openai\",\r\n    \"cohere\",\r\n    \"google_vertexai\",\r\n    \"google_genai\",\r\n    \"fireworks\",\r\n    \"ollama\",\r\n    \"together\",\r\n    \"mistralai\",\r\n    \"huggingface\",\r\n    \"groq\",\r\n    \"bedrock\",\r\n}\n\ndef _check_pkg(pkg: str) -> None:\n    if not importlib.util.find_spec(pkg):\n        pkg_kebab = pkg.replace(\"_\", \"-\")\n        # Import colorama and initialize it\r\n        init(autoreset=True)\n        # Use Fore.RED to color the error message\r\n        raise ImportError(\r\n            Fore.RED + f\"Unable to import {pkg_kebab}. Please install with \"\r\n            f\"`pip install -U {pkg_kebab}`\"\r\n        )", "kind": "Chunk", "id": "generic/base.py#47.26"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\__init__.py____all__._GPTResearcher_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 19, "span_ids": ["imports"], "start_line": 1, "end_line": 3, "community": null}, "content": "from .agent import GPTResearcher\n\n__all__ = ['GPTResearcher']", "kind": "Chunk", "id": "master/__init__.py#48.2"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_asyncio_get_retriever.return.retriever", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 492, "span_ids": ["imports", "get_retriever"], "start_line": 1, "end_line": 78, "community": null}, "content": "import asyncio\nimport json\nimport re\nfrom typing import Dict, List\n\nimport json_repair\nimport markdown\n\nfrom gpt_researcher.master.prompts import *\nfrom gpt_researcher.scraper.scraper import Scraper\nfrom gpt_researcher.utils.enum import Tone\nfrom gpt_researcher.utils.llm import *\n\n\ndef get_retriever(retriever):\n    \"\"\"\r\n    Gets the retriever\r\n    Args:\r\n        retriever: retriever name\r\n\r\n    Returns:\r\n        retriever: Retriever class\r\n\r\n    \"\"\"\n    match retriever:\n        case \"google\":\n            from gpt_researcher.retrievers import GoogleSearch\n\n            retriever = GoogleSearch\n        case \"searx\":\n            from gpt_researcher.retrievers import SearxSearch\n\n            retriever = SearxSearch\n        case \"serpapi\":\n            from gpt_researcher.retrievers import SerpApiSearch\n\n            retriever = SerpApiSearch\n        case \"googleSerp\":\n            from gpt_researcher.retrievers import SerperSearch\n\n            retriever = SerperSearch\n        case \"duckduckgo\":\n            from gpt_researcher.retrievers import Duckduckgo\n\n            retriever = Duckduckgo\n        case \"bing\":\n            from gpt_researcher.retrievers import BingSearch\n\n            retriever = BingSearch\n        case \"arxiv\":\n            from gpt_researcher.retrievers import ArxivSearch\n\n            retriever = ArxivSearch\n        case \"tavily\":\n            from gpt_researcher.retrievers import TavilySearch\n\n            retriever = TavilySearch\n        case \"exa\":\n            from gpt_researcher.retrievers import ExaSearch\n\n            retriever = ExaSearch\n        case \"semantic_scholar\":\n            from gpt_researcher.retrievers import SemanticScholarSearch\n\n            retriever = SemanticScholarSearch\n        case \"pubmed_central\":\n            from gpt_researcher.retrievers import PubMedCentralSearch\n\n            retriever = PubMedCentralSearch\n        case \"custom\":\n            from gpt_researcher.retrievers import CustomRetriever\n\n            retriever = CustomRetriever\n\n        case _:\n            retriever = None\n\n    return retriever", "kind": "Chunk", "id": "master/actions.py#49.77"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_get_retrievers_get_default_retriever.return.TavilySearch", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 312, "span_ids": ["get_retrievers", "get_default_retriever"], "start_line": 81, "end_line": 116, "community": null}, "content": "def get_retrievers(headers, cfg):\n    \"\"\"\r\n    Determine which retriever(s) to use based on headers, config, or default.\r\n\r\n    Args:\r\n        headers (dict): The headers dictionary\r\n        cfg (Config): The configuration object\r\n\r\n    Returns:\r\n        list: A list of retriever classes to be used for searching.\r\n    \"\"\"\n    # Check headers first for multiple retrievers\r\n    if headers.get(\"retrievers\"):\n        retrievers = headers.get(\"retrievers\").split(\",\")\n    # If not found, check headers for a single retriever\r\n    elif headers.get(\"retriever\"):\n        retrievers = [headers.get(\"retriever\")]\n    # If not in headers, check config for multiple retrievers\r\n    elif cfg.retrievers:\n        retrievers = cfg.retrievers\n    # If not found, check config for a single retriever\r\n    elif cfg.retriever:\n        retrievers = [cfg.retriever]\n    # If still not set, use default retriever\r\n    else:\n        retrievers = [get_default_retriever().__name__]\n\n    # Convert retriever names to actual retriever classes\r\n    # Use get_default_retriever() as a fallback for any invalid retriever names\r\n    return [get_retriever(r) or get_default_retriever() for r in retrievers]\n\n\ndef get_default_retriever(retriever):\n    from gpt_researcher.retrievers import TavilySearch\n\n    return TavilySearch", "kind": "Chunk", "id": "master/actions.py#50.35"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_choose_agent_choose_agent.try_.except_Exception_as_e_.return.await_handle_json_error_r", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 297, "span_ids": ["choose_agent"], "start_line": 119, "end_line": 156, "community": null}, "content": "async def choose_agent(\r\n    query, cfg, parent_query=None, cost_callback: callable = None, headers=None\r\n):\n    \"\"\"\r\n    Chooses the agent automatically\r\n    Args:\r\n        parent_query: In some cases the research is conducted on a subtopic from the main query.\r\n        The parent query allows the agent to know the main context for better reasoning.\r\n        query: original query\r\n        cfg: Config\r\n        cost_callback: callback for calculating llm costs\r\n\r\n    Returns:\r\n        agent: Agent name\r\n        agent_role_prompt: Agent role prompt\r\n    \"\"\"\n    query = f\"{parent_query} - {query}\" if parent_query else f\"{query}\"\n    response = None  # Initialize response to ensure it's defined\r\n\n    try:\n        response = await create_chat_completion(\r\n            model=cfg.smart_llm_model,\r\n            messages=[\r\n                {\"role\": \"system\", \"content\": f\"{auto_agent_instructions()}\"},\r\n                {\"role\": \"user\", \"content\": f\"task: {query}\"},\r\n            ],\r\n            temperature=0,\r\n            llm_provider=cfg.llm_provider,\r\n            llm_kwargs=cfg.llm_kwargs,\r\n            cost_callback=cost_callback,\r\n        )\n\n        agent_dict = json.loads(response)\n        return agent_dict[\"server\"], agent_dict[\"agent_role_prompt\"]\n\n    except Exception as e:\n        print(\"\u26a0\ufe0f Error in reading JSON, attempting to repair JSON\")\n        return await handle_json_error(response)", "kind": "Chunk", "id": "master/actions.py#51.37"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_handle_json_error_extract_json_with_regex.return.None", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 231, "span_ids": ["handle_json_error", "extract_json_with_regex"], "start_line": 159, "end_line": 186, "community": null}, "content": "async def handle_json_error(response):\n    try:\n        agent_dict = json_repair.loads(response)\n        if agent_dict.get(\"server\") and agent_dict.get(\"agent_role_prompt\"):\n            return agent_dict[\"server\"], agent_dict[\"agent_role_prompt\"]\n    except Exception as e:\n        print(f\"Error using json_repair: {e}\")\n\n    json_string = extract_json_with_regex(response)\n    if json_string:\n        try:\n            json_data = json.loads(json_string)\n            return json_data[\"server\"], json_data[\"agent_role_prompt\"]\n        except json.JSONDecodeError as e:\n            print(f\"Error decoding JSON: {e}\")\n\n    print(\"No JSON found in the string. Falling back to Default Agent.\")\n    return \"Default Agent\", (\r\n        \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, \"\r\n        \"critically acclaimed, objective and structured reports on given text.\"\r\n    )\n\n\ndef extract_json_with_regex(response):\n    json_match = re.search(r\"{.*?}\", response, re.DOTALL)\n    if json_match:\n        return json_match.group(0)\n    return None", "kind": "Chunk", "id": "master/actions.py#52.27"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_get_sub_queries_get_sub_queries.return.sub_queries", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 245, "span_ids": ["get_sub_queries"], "start_line": 189, "end_line": 234, "community": null}, "content": "async def get_sub_queries(\r\n    query: str,\r\n    agent_role_prompt: str,\r\n    cfg,\r\n    parent_query: str,\r\n    report_type: str,\r\n    cost_callback: callable = None,\r\n):\n    \"\"\"\r\n    Gets the sub queries\r\n    Args:\r\n        query: original query\r\n        agent_role_prompt: agent role prompt\r\n        cfg: Config\r\n        parent_query:\r\n        report_type:\r\n        cost_callback:\r\n\r\n    Returns:\r\n        sub_queries: List of sub queries\r\n\r\n    \"\"\"\n    max_research_iterations = cfg.max_iterations if cfg.max_iterations else 1\n    response = await create_chat_completion(\r\n        model=cfg.smart_llm_model,\r\n        messages=[\r\n            {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": generate_search_queries_prompt(\r\n                    query,\r\n                    parent_query,\r\n                    report_type,\r\n                    max_iterations=max_research_iterations,\r\n                ),\r\n            },\r\n        ],\r\n        temperature=0,\r\n        llm_provider=cfg.llm_provider,\r\n        llm_kwargs=cfg.llm_kwargs,\r\n        cost_callback=cost_callback,\r\n    )\n\n    sub_queries = json_repair.loads(response)\n\n    return sub_queries", "kind": "Chunk", "id": "master/actions.py#53.45"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_scrape_urls_scrape_urls.return.content", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 168, "span_ids": ["scrape_urls"], "start_line": 237, "end_line": 258, "community": null}, "content": "def scrape_urls(urls, cfg=None):\n    \"\"\"\r\n    Scrapes the urls\r\n    Args:\r\n        urls: List of urls\r\n        cfg: Config (optional)\r\n\r\n    Returns:\r\n        text: str\r\n\r\n    \"\"\"\n    content = []\n    user_agent = (\r\n        cfg.user_agent\r\n        if cfg\r\n        else \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\"\r\n    )\n    try:\n        content = Scraper(urls, user_agent, cfg.scraper).run()\n    except Exception as e:\n        print(f\"{Fore.RED}Error in scrape_urls: {e}{Style.RESET_ALL}\")\n    return content", "kind": "Chunk", "id": "master/actions.py#54.21"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py__Deprecated_Instead_of__summarize.return.concatenated_summaries", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 465, "span_ids": ["scrape_urls", "summarize"], "start_line": 261, "end_line": 319, "community": null}, "content": "# Deprecated: Instead of summaries using ContextualRetriever embedding.\r\n# This exists in case we decide to modify in the future\r\nasync def summarize(\r\n    query,\r\n    content,\r\n    agent_role_prompt,\r\n    cfg,\r\n    websocket=None,\r\n    cost_callback: callable = None,\r\n):\n    \"\"\"\r\n    Asynchronously summarizes a list of URLs.\r\n\r\n    Args:\r\n        query (str): The search query.\r\n        content (list): List of dictionaries with 'url' and 'raw_content'.\r\n        agent_role_prompt (str): The role prompt for the agent.\r\n        cfg (object): Configuration object.\r\n\r\n    Returns:\r\n        list: A list of dictionaries with 'url' and 'summary'.\r\n    \"\"\"\n\n    # Function to handle each summarization task for a chunk\r\n    async def handle_task(url, chunk):\n        summary = await summarize_url(\r\n            query, chunk, agent_role_prompt, cfg, cost_callback\r\n        )\n        if summary:\n            await stream_output(\r\n                \"logs\", \"url_summary_coming_up\", f\"\ud83c\udf10 Summarizing url: {url}\", websocket\r\n            )\n            await stream_output(\"logs\", \"url_summary\", f\"\ud83d\udcc3 {summary}\", websocket)\n        return url, summary\n\n    # Function to split raw content into chunks of 10,000 words\r\n    def chunk_content(raw_content, chunk_size=10000):\n        words = raw_content.split()\n        for i in range(0, len(words), chunk_size):\n            yield \" \".join(words[i : i + chunk_size])\n\n    # Process each item one by one, but process chunks in parallel\r\n    concatenated_summaries = []\n    for item in content:\n        url = item[\"url\"]\n        raw_content = item[\"raw_content\"]\n\n        # Create tasks for all chunks of the current URL\r\n        chunk_tasks = [handle_task(url, chunk) for chunk in chunk_content(raw_content)]\n\n        # Run chunk tasks concurrently\r\n        chunk_summaries = await asyncio.gather(*chunk_tasks)\n\n        # Aggregate and concatenate summaries for the current URL\r\n        summaries = [summary for _, summary in chunk_summaries if summary]\n        concatenated_summary = \" \".join(summaries)\n        concatenated_summaries.append({\"url\": url, \"summary\": concatenated_summary})\n\n    return concatenated_summaries", "kind": "Chunk", "id": "master/actions.py#55.58"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_summarize_url_summarize_url.return.summary", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 200, "span_ids": ["summarize_url"], "start_line": 322, "end_line": 356, "community": null}, "content": "async def summarize_url(\r\n    query, raw_data, agent_role_prompt, cfg, cost_callback: callable = None\r\n):\n    \"\"\"\r\n    Summarizes the text\r\n    Args:\r\n        query:\r\n        raw_data:\r\n        agent_role_prompt:\r\n        cfg:\r\n        cost_callback\r\n\r\n    Returns:\r\n        summary: str\r\n\r\n    \"\"\"\n    summary = \"\"\n    try:\n        summary = await create_chat_completion(\r\n            model=cfg.fast_llm_model,\r\n            messages=[\r\n                {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\r\n                {\r\n                    \"role\": \"user\",\r\n                    \"content\": f\"{generate_summary_prompt(query, raw_data)}\",\r\n                },\r\n            ],\r\n            temperature=0,\r\n            llm_provider=cfg.llm_provider,\r\n            llm_kwargs=cfg.llm_kwargs,\r\n            cost_callback=cost_callback,\r\n        )\n    except Exception as e:\n        print(f\"{Fore.RED}Error in summarize: {e}{Style.RESET_ALL}\")\n    return summary", "kind": "Chunk", "id": "master/actions.py#56.34"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_generate_draft_section_titles_generate_draft_section_titles.return.draft_section_titles", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 216, "span_ids": ["generate_draft_section_titles"], "start_line": 358, "end_line": 386, "community": null}, "content": "async def generate_draft_section_titles(\r\n    query: str,\r\n    context,\r\n    agent_role_prompt: str,\r\n    report_type: str,\r\n    websocket,\r\n    cfg,\r\n    main_topic: str = \"\",\r\n    cost_callback: callable = None,\r\n    headers=None\r\n) -> str:\n    assert report_type == \"subtopic_report\", \"This function is only for subtopic reports\"\n    content = f\"{generate_draft_titles_prompt(query, main_topic, context)}\"\n    try:\n        draft_section_titles = await create_chat_completion(\r\n            model=cfg.fast_llm_model,\r\n            messages=[\r\n                {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\r\n                {\"role\": \"user\", \"content\": content},\r\n            ],\r\n            temperature=0,\r\n            llm_provider=cfg.llm_provider,\r\n            llm_kwargs=cfg.llm_kwargs,\r\n            cost_callback=cost_callback,\r\n        )\n    except Exception as e:\n        print(f\"{Fore.RED}Error in generate_draft_section_titles: {e}{Style.RESET_ALL}\")\n\n    return draft_section_titles", "kind": "Chunk", "id": "master/actions.py#57.28"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_generate_report_generate_report.return.report", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 373, "span_ids": ["generate_report"], "start_line": 388, "end_line": 447, "community": null}, "content": "async def generate_report(\r\n    query: str,\r\n    context,\r\n    agent_role_prompt: str,\r\n    report_type: str,\r\n    tone: Tone,\r\n    report_source: str,\r\n    websocket,\r\n    cfg,\r\n    main_topic: str = \"\",\r\n    existing_headers: list = [],\r\n    relevant_written_contents: list = [],\r\n    cost_callback: callable = None,\r\n    headers=None,\r\n):\n    \"\"\"\r\n    generates the final report\r\n    Args:\r\n        query:\r\n        context:\r\n        agent_role_prompt:\r\n        report_type:\r\n        websocket:\r\n        tone:\r\n        cfg:\r\n        main_topic:\r\n        existing_headers:\r\n        relevant_written_contents:\r\n        cost_callback:\r\n\r\n    Returns:\r\n        report:\r\n\r\n    \"\"\"\n    generate_prompt = get_prompt_by_report_type(report_type)\n    report = \"\"\n\n    if report_type == \"subtopic_report\":\n        content = f\"{generate_prompt(query, existing_headers, relevant_written_contents, main_topic, context, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words)}\"\n    else:\n        content = f\"{generate_prompt(query, context, report_source, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words)}\"\n    try:\n        report = await create_chat_completion(\r\n            model=cfg.smart_llm_model,\r\n            messages=[\r\n                {\"role\": \"system\", \"content\": f\"{agent_role_prompt}\"},\r\n                {\"role\": \"user\", \"content\": content},\r\n            ],\r\n            temperature=0,\r\n            llm_provider=cfg.llm_provider,\r\n            stream=True,\r\n            websocket=websocket,\r\n            max_tokens=cfg.smart_token_limit,\r\n            llm_kwargs=cfg.llm_kwargs,\r\n            cost_callback=cost_callback,\r\n        )\n    except Exception as e:\n        print(f\"{Fore.RED}Error in generate_report: {e}{Style.RESET_ALL}\")\n\n    return report", "kind": "Chunk", "id": "master/actions.py#58.59"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_stream_output_stream_output.if_websocket_.await_websocket_send_json", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 136, "span_ids": ["stream_output"], "start_line": 450, "end_line": 473, "community": null}, "content": "async def stream_output(\r\n    type, content, output, websocket=None, logging=True, metadata=None\r\n):\n    \"\"\"\r\n    Streams output to the websocket\r\n    Args:\r\n        type:\r\n        content:\r\n        output:\r\n\r\n    Returns:\r\n        None\r\n    \"\"\"\n    if not websocket or logging:\n        try:\n            print(output)\n        except UnicodeEncodeError:\r\n            # Option 1: Replace problematic characters with a placeholder\r\n            print(output.encode('cp1252', errors='replace').decode('cp1252'))\n\n    if websocket:\n        await websocket.send_json(\r\n            {\"type\": type, \"content\": content, \"output\": output, \"metadata\": metadata}\r\n        )", "kind": "Chunk", "id": "master/actions.py#59.23"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_get_report_introduction_get_report_introduction.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 177, "span_ids": ["get_report_introduction"], "start_line": 476, "end_line": 504, "community": null}, "content": "async def get_report_introduction(\r\n    query, context, role, config, websocket=None, cost_callback: callable = None\r\n):\n    try:\n        introduction = await create_chat_completion(\r\n            model=config.smart_llm_model,\r\n            messages=[\r\n                {\"role\": \"system\", \"content\": f\"{role}\"},\r\n                {\r\n                    \"role\": \"user\",\r\n                    \"content\": generate_report_introduction(query, context),\r\n                },\r\n            ],\r\n            temperature=0,\r\n            llm_provider=config.llm_provider,\r\n            stream=True,\r\n            websocket=websocket,\r\n            max_tokens=config.smart_token_limit,\r\n            llm_kwargs=config.llm_kwargs,\r\n            cost_callback=cost_callback,\r\n        )\n\n        return introduction\n    except Exception as e:\n        print(\r\n            f\"{Fore.RED}Error in generating report introduction: {e}{Style.RESET_ALL}\"\r\n        )\n\n    return \"\"", "kind": "Chunk", "id": "master/actions.py#60.28"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_extract_headers_extract_headers._Return_the_list_of_head", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 284, "span_ids": ["extract_headers"], "start_line": 507, "end_line": 541, "community": null}, "content": "def extract_headers(markdown_text: str):\n    # Function to extract headers from markdown text\r\n\n    headers = []\n    parsed_md = markdown.markdown(markdown_text)  # Parse markdown text\r\n    lines = parsed_md.split(\"\\n\")  # Split text into lines\r\n\n    stack = []  # Initialize stack to keep track of nested headers\r\n    for line in lines:\r\n        # Check if the line starts with an HTML header tag\r\n        if line.startswith(\"<h\") and len(line) > 2 and line[2].isdigit():\n            level = int(line[2])  # Extract header level\r\n            header_text = line[\r\n                line.index(\">\") + 1 : line.rindex(\"<\")\r\n            ]  # Extract header text\r\n\n            # Pop headers from the stack with higher or equal level\r\n            while stack and stack[-1][\"level\"] >= level:\n                stack.pop()\n\n            header = {\r\n                \"level\": level,\r\n                \"text\": header_text,\r\n            }  # Create header dictionary\r\n            if stack:\n                stack[-1].setdefault(\"children\", []).append(\r\n                    header\r\n                )  # Append as child if parent exists\r\n            else:\r\n                # Append as top-level header if no parent exists\r\n                headers.append(header)\n\n            stack.append(header)  # Push header onto the stack\r\n\n    return headers  # Return the list of headers\r", "kind": "Chunk", "id": "master/actions.py#61.34"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_extract_sections_extract_sections.return.sections", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 218, "span_ids": ["extract_sections"], "start_line": 543, "end_line": 573, "community": null}, "content": "def extract_sections(markdown_text: str) -> List[Dict[str, str]]:\n    \"\"\"\r\n    Extract all written sections from subtopic report\r\n    Args:\r\n        markdown_text: subtopic report text\r\n    Returns:\r\n        List of sections, each section is dictionary and contain following information\r\n        [\r\n            {\r\n                \"section_title\": \"Pruning\",\r\n                \"written_content\": \"Pruning involves removing redundant or less ...\"\r\n            },\r\n        ]\r\n    \"\"\"\n    sections = []\n    parsed_md = markdown.markdown(markdown_text)\n\n    # Use regex to find all headers and their content\r\n    pattern = r'<h\\d>(.*?)</h\\d>(.*?)(?=<h\\d>|$)'\n    matches = re.findall(pattern, parsed_md, re.DOTALL)\n\n    for title, content in matches:\r\n        # Clean up the content\r\n        clean_content = re.sub(r'<.*?>', '', content).strip()\n        if clean_content:\n            sections.append({\r\n                \"section_title\": title.strip(),\r\n                \"written_content\": clean_content\r\n            })\n\n    return sections", "kind": "Chunk", "id": "master/actions.py#62.30"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_table_of_contents_table_of_contents.try_.except_Exception_as_e_._Return_original_markdow", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 244, "span_ids": ["table_of_contents"], "start_line": 575, "end_line": 600, "community": null}, "content": "def table_of_contents(markdown_text: str):\n    try:\n        # Function to generate table of contents recursively\r\n        def generate_table_of_contents(headers, indent_level=0):\n            toc = \"\"  # Initialize table of contents string\r\n            for header in headers:\n                toc += (\r\n                    \" \" * (indent_level * 4) + \"- \" + header[\"text\"] + \"\\n\"\r\n                )  # Add header text with indentation\r\n                if \"children\" in header:  # If header has children\r\n                    toc += generate_table_of_contents(\r\n                        header[\"children\"], indent_level + 1\r\n                    )  # Generate TOC for children\r\n            return toc  # Return the generated table of contents\r\n\n        # Extract headers from markdown text\r\n        headers = extract_headers(markdown_text)\n        toc = \"## Table of Contents\\n\\n\" + generate_table_of_contents(\r\n            headers\r\n        )  # Generate table of contents\r\n\n        return toc  # Return the generated table of contents\r\n\n    except Exception as e:\n        print(\"table_of_contents Exception : \", e)  # Print exception if any\r\n        return markdown_text  # Return original markdown text if an exception occurs\r", "kind": "Chunk", "id": "master/actions.py#63.25"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py_add_source_urls_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\actions.py", "file_name": "actions.py", "file_type": "text/x-python", "category": "test", "tokens": 195, "span_ids": ["add_source_urls"], "start_line": 603, "end_line": 625, "community": null}, "content": "def add_source_urls(report_markdown: str, visited_urls: set):\n    \"\"\"\r\n    This function takes a Markdown report and a set of visited URLs as input parameters.\r\n\r\n    Args:\r\n      report_markdown (str): The `add_source_urls` function takes in two parameters:\r\n      visited_urls (set): Visited_urls is a set that contains URLs that have already been visited. This\r\n    parameter is used to keep track of which URLs have already been included in the report_markdown to\r\n    avoid duplication.\r\n    \"\"\"\n    try:\n        url_markdown = \"\\n\\n\\n## References\\n\\n\"\n\n        url_markdown += \"\".join(f\"- [{url}]({url})\\n\" for url in visited_urls)\n\n        updated_markdown_report = report_markdown + url_markdown\n\n        return updated_markdown_report\n\n    except Exception as e:\n        print(f\"Encountered exception in adding source urls : {e}\")\n        return report_markdown", "kind": "Chunk", "id": "master/actions.py#64.22"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_asyncio_GPTResearcher.__init__.self.subtopics.subtopics", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 643, "span_ids": ["imports", "GPTResearcher", "GPTResearcher.__init__"], "start_line": 1, "end_line": 94, "community": null}, "content": "import asyncio\nimport random\nimport time\n\nfrom typing import Set\n\nfrom gpt_researcher.config import Config\nfrom gpt_researcher.context.compression import ContextCompressor, WrittenContentCompressor, VectorstoreCompressor\nfrom gpt_researcher.document import DocumentLoader, LangChainDocumentLoader\nfrom gpt_researcher.master.actions import *\nfrom gpt_researcher.memory import Memory\nfrom gpt_researcher.utils.enum import ReportSource, ReportType, Tone\n\n\nclass GPTResearcher:\n    \"\"\"\r\n    GPT Researcher\r\n    \"\"\"\n\n    def __init__(\r\n        self,\r\n        query: str,\r\n        report_type: str = ReportType.ResearchReport.value,\r\n        report_source=ReportSource.Web.value,\r\n        tone: Tone = Tone.Objective,\r\n        source_urls=None,\r\n        documents=None,\r\n        vector_store=None,\r\n        vector_store_filter=None,\r\n        config_path=None,\r\n        websocket=None,\r\n        agent=None,\r\n        role=None,\r\n        parent_query: str = \"\",\r\n        subtopics: list = [],\r\n        visited_urls: set = set(),\r\n        verbose: bool = True,\r\n        context=[],\r\n        headers: dict = None,  # Add headers parameter\r\n    ):\n        \"\"\"\r\n        Initialize the GPT Researcher class.\r\n        Args:\r\n            query: str,\r\n            report_type: str\r\n            source_urls\r\n            tone\r\n            config_path\r\n            websocket\r\n            agent\r\n            role\r\n            parent_query: str\r\n            subtopics: list\r\n            visited_urls: set\r\n        \"\"\"\n        self.headers = headers or {}\n        self.query: str = query\n        self.agent: str = agent\n        self.role: str = role\n        self.report_type: str = report_type\n        self.report_prompt: str = get_prompt_by_report_type(\r\n            self.report_type\r\n        )  # this validates the report type\r\n        self.research_costs: float = 0.0\n        self.cfg = Config(config_path)\n        self.report_source: str = self.cfg.report_source or report_source\n        self.retrievers = get_retrievers(self.headers, self.cfg)\n        self.context = context\n        self.source_urls = source_urls\n        self.documents = documents\n        self.vector_store = vector_store\n        self.vector_store_filter = vector_store_filter\n        self.memory = Memory(self.cfg.embedding_provider, self.headers)\n        self.visited_urls: set[str] = visited_urls\n        self.verbose: bool = verbose\n        self.websocket = websocket\n        self.headers = headers or {}\n        # Ensure tone is an instance of Tone enum\r\n        if isinstance(tone, dict):\n            print(f\"Invalid tone format: {tone}. Setting to default Tone.Objective.\")\n            self.tone = Tone.Objective\n        elif isinstance(tone, str):\n            self.tone = Tone[tone]\n        else:\n            self.tone = tone\n\n        # Only relevant for DETAILED REPORTS\r\n        # --------------------------------------\r\n\n        # Stores the main query of the detailed report\r\n        self.parent_query = parent_query\n\n        # Stores all the user provided subtopics\r\n        self.subtopics = subtopics", "kind": "Chunk", "id": "master/agent.py#65.93"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.conduct_research_GPTResearcher.conduct_research.return.self_context", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 541, "span_ids": ["GPTResearcher.conduct_research"], "start_line": 96, "end_line": 164, "community": null}, "content": "class GPTResearcher:\n\n    async def conduct_research(self):\n        \"\"\"\r\n        Runs the GPT Researcher to conduct research\r\n        \"\"\"\n        # Reset visited_urls and source_urls at the start of each research task\r\n        self.visited_urls.clear()\n        if self.report_source != ReportSource.Sources.value:\n            self.source_urls = []\n\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"starting_research\",\r\n                f\"\ud83d\udd0e Starting the research task for '{self.query}'...\",\r\n                self.websocket,\r\n            )\n\n        # Generate Agent\r\n        if not (self.agent and self.role):\n            self.agent, self.role = await choose_agent(\r\n                query=self.query,\r\n                cfg=self.cfg,\r\n                parent_query=self.parent_query,\r\n                cost_callback=self.add_costs,\r\n                headers=self.headers,\r\n            )\n\n        if self.verbose:\n            await stream_output(\"logs\", \"agent_generated\", self.agent, self.websocket)\n\n        # If specified, the researcher will use the given urls as the context for the research.\r\n        if self.source_urls:\n            self.context = await self.__get_context_by_urls(self.source_urls)\n\n        elif self.report_source == ReportSource.Local.value:\n            document_data = await DocumentLoader(self.cfg.doc_path).load()\n            self.context = await self.__get_context_by_search(self.query, document_data)\n\n        # Hybrid search including both local documents and web sources\r\n        elif self.report_source == ReportSource.Hybrid.value:\n            document_data = await DocumentLoader(self.cfg.doc_path).load()\n            docs_context = await self.__get_context_by_search(self.query, document_data)\n            web_context = await self.__get_context_by_search(self.query)\n            self.context = f\"Context from local documents: {docs_context}\\n\\nContext from web sources: {web_context}\"\n\n        elif self.report_source == ReportSource.LangChainDocuments.value:\n            langchain_documents_data = await LangChainDocumentLoader(\r\n                self.documents\r\n            ).load()\n            self.context = await self.__get_context_by_search(\r\n                self.query, langchain_documents_data\r\n            )\n\n        elif self.report_source == ReportSource.LangChainVectorStore.value:\n            self.context = await self.__get_context_by_vectorstore(self.query, self.vector_store_filter)\n        # Default web based research\r\n        else:\n            self.context = await self.__get_context_by_search(self.query)\n\n        time.sleep(2)\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"research_step_finalized\",\r\n                f\"Finalized research step.\\n\ud83d\udcb8 Total Research Costs: ${self.get_costs()}\",\r\n                self.websocket,\r\n            )\n\n        return self.context", "kind": "Chunk", "id": "master/agent.py#66.68"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.write_report_GPTResearcher.write_report.return.report", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 370, "span_ids": ["GPTResearcher.write_report"], "start_line": 166, "end_line": 226, "community": null}, "content": "class GPTResearcher:\n\n    async def write_report(self, existing_headers: list = [], relevant_written_contents: list = []):\n        \"\"\"\r\n        Writes the report based on research conducted\r\n\r\n        Returns:\r\n            str: The report\r\n        \"\"\"\n        report = \"\"\n\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"task_summary_coming_up\",\r\n                f\"\u270d\ufe0f Writing summary for research task: {self.query} (this may take a few minutes)...\",\r\n                self.websocket,\r\n            )\n\n        if self.report_type == \"custom_report\":\n            self.role = self.cfg.agent_role if self.cfg.agent_role else self.role\n            report = await generate_report(\r\n                query=self.query,\r\n                context=self.context,\r\n                agent_role_prompt=self.role,\r\n                report_type=self.report_type,\r\n                report_source=self.report_source,\r\n                tone=self.tone,\r\n                websocket=self.websocket,\r\n                cfg=self.cfg,\r\n                headers=self.headers,\r\n            )\n        elif self.report_type == \"subtopic_report\":\n            report = await generate_report(\r\n                query=self.query,\r\n                context=self.context,\r\n                agent_role_prompt=self.role,\r\n                report_type=self.report_type,\r\n                report_source=self.report_source,\r\n                websocket=self.websocket,\r\n                tone=self.tone,\r\n                cfg=self.cfg,\r\n                main_topic=self.parent_query,\r\n                existing_headers=existing_headers,\r\n                relevant_written_contents=relevant_written_contents,\r\n                cost_callback=self.add_costs,\r\n                headers=self.headers,\r\n            )\n        else:\n            report = await generate_report(\r\n                query=self.query,\r\n                context=self.context,\r\n                agent_role_prompt=self.role,\r\n                report_type=self.report_type,\r\n                report_source=self.report_source,\r\n                tone=self.tone,\r\n                websocket=self.websocket,\r\n                cfg=self.cfg,\r\n                cost_callback=self.add_costs,\r\n                headers=self.headers,\r\n            )\n\n        return report", "kind": "Chunk", "id": "master/agent.py#67.60"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__get_context_by_urls_GPTResearcher.__get_context_by_urls.return.await_self___get_similar_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 128, "span_ids": ["GPTResearcher.__get_context_by_urls"], "start_line": 228, "end_line": 242, "community": null}, "content": "class GPTResearcher:\n\n    async def __get_context_by_urls(self, urls):\n        \"\"\"\r\n        Scrapes and compresses the context from the given urls\r\n        \"\"\"\n        new_search_urls = await self.__get_new_urls(urls)\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"source_urls\",\r\n                f\"\ud83d\uddc2\ufe0f I will conduct my research based on the following urls: {new_search_urls}...\",\r\n                self.websocket,\r\n            )\n\n        scraped_sites = scrape_urls(new_search_urls, self.cfg)\n        return await self.__get_similar_content_by_query(self.query, scraped_sites)", "kind": "Chunk", "id": "master/agent.py#68.14"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__get_context_by_vectorstore_GPTResearcher.__get_context_by_vectorstore.return.context", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 230, "span_ids": ["GPTResearcher.__get_context_by_vectorstore"], "start_line": 244, "end_line": 274, "community": null}, "content": "class GPTResearcher:\n\n    async def __get_context_by_vectorstore(self, query, filter: Optional[dict] = None):\n        \"\"\"\r\n            Generates the context for the research task by searching the vectorstore\r\n        Returns:\r\n            context: List of context\r\n        \"\"\"\n        context = []\n        # Generate Sub-Queries including original query\r\n        sub_queries = await self.__get_sub_queries(query)\n        # If this is not part of a sub researcher, add original query to research for better results\r\n        if self.report_type != \"subtopic_report\":\n            sub_queries.append(query)\n\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"subqueries\",\r\n                f\"\ud83d\uddc2\ufe0f  I will conduct my research based on the following queries: {sub_queries}...\",\r\n                self.websocket,\r\n                True,\r\n                sub_queries,\r\n            )\n\n        # Using asyncio.gather to process the sub_queries asynchronously\r\n        context = await asyncio.gather(\r\n            *[\r\n                self.__process_sub_query_with_vectorstore(sub_query, filter)\r\n                for sub_query in sub_queries\r\n            ]\r\n        )\n        return context", "kind": "Chunk", "id": "master/agent.py#69.30"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__get_context_by_search_GPTResearcher.__get_context_by_search.return.context", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 227, "span_ids": ["GPTResearcher.__get_context_by_search"], "start_line": 276, "end_line": 306, "community": null}, "content": "class GPTResearcher:\n\n    async def __get_context_by_search(self, query, scraped_data: list = []):\n        \"\"\"\r\n           Generates the context for the research task by searching the query and scraping the results\r\n        Returns:\r\n            context: List of context\r\n        \"\"\"\n        context = []\n        # Generate Sub-Queries including original query\r\n        sub_queries = await self.__get_sub_queries(query)\n        # If this is not part of a sub researcher, add original query to research for better results\r\n        if self.report_type != \"subtopic_report\":\n            sub_queries.append(query)\n\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"subqueries\",\r\n                f\"\ud83d\uddc2\ufe0f I will conduct my research based on the following queries: {sub_queries}...\",\r\n                self.websocket,\r\n                True,\r\n                sub_queries,\r\n            )\n\n        # Using asyncio.gather to process the sub_queries asynchronously\r\n        context = await asyncio.gather(\r\n            *[\r\n                self.__process_sub_query(sub_query, scraped_data)\r\n                for sub_query in sub_queries\r\n            ]\r\n        )\n        return context", "kind": "Chunk", "id": "master/agent.py#70.30"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__process_sub_query_with_vectorstore_GPTResearcher.__process_sub_query_with_vectorstore.return.content", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 231, "span_ids": ["GPTResearcher.__process_sub_query_with_vectorstore"], "start_line": 308, "end_line": 338, "community": null}, "content": "class GPTResearcher:\n\n    async def __process_sub_query_with_vectorstore(self, sub_query: str, filter: Optional[dict] = None):\n        \"\"\"Takes in a sub query and gathers context from the user provided vector store\r\n\r\n        Args:\r\n            sub_query (str): The sub-query generated from the original query\r\n\r\n        Returns:\r\n            str: The context gathered from search\r\n        \"\"\"\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"running_subquery_with_vectorstore_research\",\r\n                f\"\\n\ud83d\udd0d Running research for '{sub_query}'...\",\r\n                self.websocket,\r\n            )\n\n        content = await self.__get_similar_content_by_query_with_vectorstore(sub_query, filter)\n\n        if content and self.verbose:\n            await stream_output(\r\n                \"logs\", \"subquery_context_window\", f\"\ud83d\udcc3 {content}\", self.websocket\r\n            )\n        elif self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"subquery_context_not_found\",\r\n                f\"\ud83e\udd37 No content found for '{sub_query}'...\",\r\n                self.websocket,\r\n            )\n        return content", "kind": "Chunk", "id": "master/agent.py#71.30"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__process_sub_query_GPTResearcher.__process_sub_query.return.content", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 255, "span_ids": ["GPTResearcher.__process_sub_query"], "start_line": 340, "end_line": 374, "community": null}, "content": "class GPTResearcher:\n\n    async def __process_sub_query(self, sub_query: str, scraped_data: list = []):\n        \"\"\"Takes in a sub query and scrapes urls based on it and gathers context.\r\n\r\n        Args:\r\n            sub_query (str): The sub-query generated from the original query\r\n            scraped_data (list): Scraped data passed in\r\n\r\n        Returns:\r\n            str: The context gathered from search\r\n        \"\"\"\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"running_subquery_research\",\r\n                f\"\\n\ud83d\udd0d Running research for '{sub_query}'...\",\r\n                self.websocket,\r\n            )\n\n        if not scraped_data:\n            scraped_data = await self.__scrape_data_by_query(sub_query)\n\n        content = await self.__get_similar_content_by_query(sub_query, scraped_data)\n\n        if content and self.verbose:\n            await stream_output(\r\n                \"logs\", \"subquery_context_window\", f\"\ud83d\udcc3 {content}\", self.websocket\r\n            )\n        elif self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"subquery_context_not_found\",\r\n                f\"\ud83e\udd37 No content found for '{sub_query}'...\",\r\n                self.websocket,\r\n            )\n        return content", "kind": "Chunk", "id": "master/agent.py#72.34"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__get_new_urls_GPTResearcher.__get_new_urls.return.new_urls", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 157, "span_ids": ["GPTResearcher.__get_new_urls"], "start_line": 376, "end_line": 397, "community": null}, "content": "class GPTResearcher:\n\n    async def __get_new_urls(self, url_set_input):\n        \"\"\"Gets the new urls from the given url set.\r\n        Args: url_set_input (set[str]): The url set to get the new urls from\r\n        Returns: list[str]: The new urls from the given url set\r\n        \"\"\"\n\n        new_urls = []\n        for url in url_set_input:\n            if url not in self.visited_urls:\n                self.visited_urls.add(url)\n                new_urls.append(url)\n                if self.verbose:\n                    await stream_output(\r\n                        \"logs\",\r\n                        \"added_source_url\",\r\n                        f\"\u2705 Added source url to research: {url}\\n\",\r\n                        self.websocket,\r\n                        True,\r\n                        url,\r\n                    )\n\n        return new_urls", "kind": "Chunk", "id": "master/agent.py#73.21"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__scrape_data_by_query_GPTResearcher.__scrape_data_by_query.return.scraped_content_results", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 303, "span_ids": ["GPTResearcher.__scrape_data_by_query"], "start_line": 399, "end_line": 443, "community": null}, "content": "class GPTResearcher:\n\n    async def __scrape_data_by_query(self, sub_query):\n        \"\"\"\r\n        Runs a sub-query across multiple retrievers and scrapes the resulting URLs.\r\n\r\n        Args:\r\n            sub_query (str): The sub-query to search for.\r\n\r\n        Returns:\r\n            list: A list of scraped content results.\r\n        \"\"\"\n        new_search_urls = []\n\n        # Iterate through all retrievers\r\n        for retriever_class in self.retrievers:\r\n            # Instantiate the retriever with the sub-query\r\n            retriever = retriever_class(sub_query)\n\n            # Perform the search using the current retriever\r\n            search_results = await asyncio.to_thread(\r\n                retriever.search, max_results=self.cfg.max_search_results_per_query\r\n            )\n\n            # Collect new URLs from search results\r\n            search_urls = [url.get(\"href\") for url in search_results]\n            new_search_urls.extend(search_urls)\n\n        # Get unique URLs\r\n        new_search_urls = await self.__get_new_urls(new_search_urls)\n        random.shuffle(new_search_urls)\n\n        # Log the research process if verbose mode is on\r\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"researching\",\r\n                f\"\ud83e\udd14 Researching for relevant information across multiple sources...\\n\",\r\n                self.websocket,\r\n            )\n\n        # Scrape the new URLs\r\n        scraped_content_results = await asyncio.to_thread(\r\n            scrape_urls, new_search_urls, self.cfg\r\n        )\n\n        return scraped_content_results", "kind": "Chunk", "id": "master/agent.py#74.44"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__get_similar_content_by_query_with_vectorstore_GPTResearcher.__get_similar_content_by_query_with_vectorstore.return.await_vectorstore_compres", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 119, "span_ids": ["GPTResearcher.__get_similar_content_by_query_with_vectorstore"], "start_line": 445, "end_line": 459, "community": null}, "content": "class GPTResearcher:\n\n    async def __get_similar_content_by_query_with_vectorstore(self, query, filter):\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"fetching_query_content\",\r\n                f\"\ud83d\udcda Getting relevant content based on query: {query}...\",\r\n                self.websocket,\r\n            )\n\n        # Summarize data fetched from vector store\r\n        vectorstore_compressor = VectorstoreCompressor(self.vector_store, filter)\n\n        return await vectorstore_compressor.async_get_context(\r\n            query=query, max_results=8\r\n        )", "kind": "Chunk", "id": "master/agent.py#75.14"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__get_similar_content_by_query_GPTResearcher.__get_similar_content_by_query.return.await_context_compressor_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 129, "span_ids": ["GPTResearcher.__get_similar_content_by_query"], "start_line": 461, "end_line": 477, "community": null}, "content": "class GPTResearcher:\n\n    async def __get_similar_content_by_query(self, query, pages):\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"fetching_query_content\",\r\n                f\"\ud83d\udcda Getting relevant content based on query: {query}...\",\r\n                self.websocket,\r\n            )\n\n        # Summarize Raw Data\r\n        context_compressor = ContextCompressor(\r\n            documents=pages, embeddings=self.memory.get_embeddings()\r\n        )\n        # Run Tasks\r\n        return await context_compressor.async_get_context(\r\n            query=query, max_results=8, cost_callback=self.add_costs\r\n        )", "kind": "Chunk", "id": "master/agent.py#76.16"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.None_1_GPTResearcher.write_introduction.return.introduction", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 202, "span_ids": ["GPTResearcher.add_costs", "GPTResearcher.write_introduction", "GPTResearcher.__get_similar_content_by_query", "GPTResearcher.get_costs", "GPTResearcher.get_source_urls", "GPTResearcher.get_research_context", "GPTResearcher.set_verbose"], "start_line": 479, "end_line": 514, "community": null}, "content": "class GPTResearcher:\n\n    ########################################################################################\r\n\n    # GETTERS & SETTERS\r\n    def get_source_urls(self) -> list:\n        return list(self.visited_urls)\n\n    def get_research_context(self) -> list:\n        return self.context\n\n    def get_costs(self) -> float:\n        return self.research_costs\n\n    def set_verbose(self, verbose: bool):\n        self.verbose = verbose\n\n    def add_costs(self, cost: int) -> None:\n        if not isinstance(cost, float) and not isinstance(cost, int):\n            raise ValueError(\"Cost must be an integer or float\")\n        self.research_costs += cost\n\n    ########################################################################################\r\n\n    # DETAILED REPORT\r\n\n    async def write_introduction(self):\n        # Construct Report Introduction from main topic research\r\n        introduction = await get_report_introduction(\r\n            self.query,\r\n            self.context,\r\n            self.role,\r\n            self.cfg,\r\n            self.websocket,\r\n            self.add_costs,\r\n        )\n\n        return introduction", "kind": "Chunk", "id": "master/agent.py#77.35"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.get_subtopics_GPTResearcher.get_subtopics.return.subtopics", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 187, "span_ids": ["GPTResearcher.get_subtopics"], "start_line": 516, "end_line": 545, "community": null}, "content": "class GPTResearcher:\n\n    async def get_subtopics(self):\n        \"\"\"\r\n        This async function generates subtopics based on user input and other parameters.\r\n\r\n        Returns:\r\n          The `get_subtopics` function is returning the `subtopics` that are generated by the\r\n        `construct_subtopics` function.\r\n        \"\"\"\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"generating_subtopics\",\r\n                f\"\ud83e\udd14 Generating subtopics...\",\r\n                self.websocket,\r\n            )\n\n        subtopics = await construct_subtopics(\r\n            task=self.query,\r\n            data=self.context,\r\n            config=self.cfg,\r\n            # This is a list of user provided subtopics\r\n            subtopics=self.subtopics,\r\n        )\n\n        if self.verbose:\n            await stream_output(\r\n                \"logs\", \"subtopics\", f\"\ud83d\udccbSubtopics: {subtopics}\", self.websocket\r\n            )\n\n        return subtopics", "kind": "Chunk", "id": "master/agent.py#78.29"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.get_draft_section_titles_GPTResearcher.get_draft_section_titles.return.draft_section_titles", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 174, "span_ids": ["GPTResearcher.get_draft_section_titles"], "start_line": 547, "end_line": 574, "community": null}, "content": "class GPTResearcher:\n\n    async def get_draft_section_titles(self):\n        \"\"\"\r\n        Writes the draft section titles based on research conducted. The draft section titles are used to retrieve the previous relevant written contents.\r\n\r\n        Returns:\r\n            str: The headers markdown text\r\n        \"\"\"\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"task_summary_coming_up\",\r\n                f\"\u270d\ufe0f Writing draft section titles for research task: {self.query}...\",\r\n                self.websocket,\r\n            )\n\n        draft_section_titles = await generate_draft_section_titles(\r\n            query=self.query,\r\n            context=self.context,\r\n            agent_role_prompt=self.role,\r\n            report_type=self.report_type,\r\n            websocket=self.websocket,\r\n            cfg=self.cfg,\r\n            main_topic=self.parent_query,\r\n            cost_callback=self.add_costs,\r\n            headers=self.headers,\r\n        )\n\n        return draft_section_titles", "kind": "Chunk", "id": "master/agent.py#79.27"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.__get_similar_written_contents_by_query_GPTResearcher.__get_sub_queries.return.await_get_sub_queries_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 380, "span_ids": ["GPTResearcher.__get_similar_written_contents_by_query", "GPTResearcher.__get_sub_queries"], "start_line": 576, "end_line": 621, "community": null}, "content": "class GPTResearcher:\n\n    async def __get_similar_written_contents_by_query(self,\r\n            query: str,\r\n            written_contents: List[Dict],\r\n            similarity_threshold: float = 0.5,\r\n            max_results: int = 10\r\n        ) -> List[str]:\n        \"\"\"\r\n        Asynchronously retrieves similar written contents based on a given query.\r\n\r\n        Args:\r\n            query (str): The query to search for similar written contents.\r\n            written_contents (List[Dict]): List of written contents to search through.\r\n            similarity_threshold (float, optional): The minimum similarity score for content to be considered relevant. \r\n                                                    Defaults to 0.5.\r\n            max_results (int, optional): The maximum number of similar contents to return. Defaults to 10.\r\n\r\n        Returns:\r\n            List[str]: A list of similar written contents, limited by max_results.\r\n        \"\"\"\n        if self.verbose:\n            await stream_output(\r\n                \"logs\",\r\n                \"fetching_relevant_written_content\",\r\n                f\"\ud83d\udd0e Getting relevant written content based on query: {query}...\",\r\n                self.websocket,\r\n            )\n\n        # Retrieve similar written contents based on the query\r\n        # Use a higher similarity threshold to ensure more relevant results and reduce irrelevant matches\r\n        written_content_compressor = WrittenContentCompressor(\r\n            documents=written_contents, embeddings=self.memory.get_embeddings(), similarity_threshold=similarity_threshold\r\n        )\n        return await written_content_compressor.async_get_context(\r\n            query=query, max_results=max_results, cost_callback=self.add_costs\r\n        )\n\n    async def __get_sub_queries(self, query):\n        # Generate Sub-Queries including original query\r\n        return await get_sub_queries(\r\n            query=query,\r\n            agent_role_prompt=self.role,\r\n            cfg=self.cfg,\r\n            parent_query=self.parent_query,\r\n            report_type=self.report_type,\r\n            cost_callback=self.add_costs,\r\n        )", "kind": "Chunk", "id": "master/agent.py#80.45"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py_GPTResearcher.get_similar_written_contents_by_draft_section_titles_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 322, "span_ids": ["GPTResearcher.get_similar_written_contents_by_draft_section_titles"], "start_line": 623, "end_line": 663, "community": null}, "content": "class GPTResearcher:\n\n    async def get_similar_written_contents_by_draft_section_titles(\r\n        self, \r\n        current_subtopic: str, \r\n        draft_section_titles: List[str],\r\n        written_contents: List[Dict],\r\n        max_results: int = 10\r\n    ) -> List[str]:\n        \"\"\"\r\n        Retrieve similar written contents based on current subtopic and draft section titles.\r\n        \r\n        Args:\r\n        current_subtopic (str): The current subtopic.\r\n        draft_section_titles (List[str]): List of draft section titles.\r\n        written_contents (List[Dict]): List of written contents to search through.\r\n        max_results (int): Maximum number of results to return. Defaults to 10.\r\n        \r\n        Returns:\r\n        List[str]: List of relevant written contents.\r\n        \"\"\"\n        all_queries = [current_subtopic] + draft_section_titles\n\n        async def process_query(query: str) -> Set[str]:\n            return set(await self.__get_similar_written_contents_by_query(query, written_contents))\n\n        # Run all queries in parallel\r\n        results = await asyncio.gather(*[process_query(query) for query in all_queries])\n\n        # Combine all results\r\n        relevant_contents = set().union(*results)\n\n        # Limit the number of results\r\n        relevant_contents = list(relevant_contents)[:max_results]\n\n        if relevant_contents and self.verbose:\n            prettier_contents = \"\\n\".join(relevant_contents)\n            await stream_output(\r\n                \"logs\", \"relevant_contents_context\", f\"\ud83d\udcc3 {prettier_contents}\", self.websocket\r\n            )\n\n        return relevant_contents", "kind": "Chunk", "id": "master/agent.py#81.40"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_warnings_generate_search_queries_prompt.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 275, "span_ids": ["generate_search_queries_prompt", "imports"], "start_line": 1, "end_line": 35, "community": null}, "content": "import warnings\nfrom datetime import date, datetime, timezone\n\nfrom gpt_researcher.utils.enum import ReportSource, ReportType, Tone\n\n\ndef generate_search_queries_prompt(\r\n    question: str,\r\n    parent_query: str,\r\n    report_type: str,\r\n    max_iterations: int = 3,\r\n):\n    \"\"\"Generates the search queries prompt for the given question.\r\n    Args:\r\n        question (str): The question to generate the search queries prompt for\r\n        parent_query (str): The main question (only relevant for detailed reports)\r\n        report_type (str): The report type\r\n        max_iterations (int): The maximum number of search queries to generate\r\n\r\n    Returns: str: The search queries prompt for the given question\r\n    \"\"\"\n\n    if (\r\n        report_type == ReportType.DetailedReport.value\r\n        or report_type == ReportType.SubtopicReport.value\r\n    ):\n        task = f\"{parent_query} - {question}\"\n    else:\n        task = question\n\n    return (\r\n        f'Write {max_iterations} google search queries to search online that form an objective opinion from the following task: \"{task}\"'\r\n        f'You must respond with a list of strings in the following format: [\"query 1\", \"query 2\", \"query 3\"].\\n'\r\n        f\"The response should contain ONLY the list.\"\r\n    )", "kind": "Chunk", "id": "master/prompts.py#82.34"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_generate_report_prompt_generate_report_prompt.return.f_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 540, "span_ids": ["generate_report_prompt"], "start_line": 38, "end_line": 87, "community": null}, "content": "def generate_report_prompt(\r\n    question: str,\r\n    context,\r\n    report_source: str,\r\n    report_format=\"apa\",\r\n    total_words=1000,\r\n    tone=None,\r\n):\n    \"\"\"Generates the report prompt for the given question and research summary.\r\n    Args: question (str): The question to generate the report prompt for\r\n            research_summary (str): The research summary to generate the report prompt for\r\n    Returns: str: The report prompt for the given question and research summary\r\n    \"\"\"\n\n    reference_prompt = \"\"\n    if report_source == ReportSource.Web.value:\n        reference_prompt = f\"\"\"\r\nYou MUST write all used source urls at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each.\r\nEvery url should be hyperlinked: [url website](url)\r\nAdditionally, you MUST include hyperlinks to the relevant URLs wherever they are referenced in the report: \r\n\r\neg: Author, A. A. (Year, Month Date). Title of web page. Website Name. [url website](url)\r\n\"\"\"\n    else:\n        reference_prompt = f\"\"\"\r\nYou MUST write all used source document names at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each.\"\r\n\"\"\"\n\n    tone_prompt = f\"Write the report in a {tone.value} tone.\" if tone else \"\"\n\n    return f\"\"\"\r\nInformation: \"{context}\"\r\n---\r\nUsing the above information, answer the following query or task: \"{question}\" in a detailed report --\r\nThe report should focus on the answer to the query, should be well structured, informative, \r\nin-depth, and comprehensive, with facts and numbers if available and a minimum of {total_words} words.\r\nYou should strive to write the report as long as you can using all relevant and necessary information provided.\r\n\r\nPlease follow all of the following guidelines in your report:\r\n- You MUST determine your own concrete and valid opinion based on the given information. Do NOT defer to general and meaningless conclusions.\r\n- You MUST write the report with markdown syntax and {report_format} format.\r\n- Use an unbiased and journalistic tone.\r\n- Use in-text citation references in {report_format} format and make it with markdown hyperlink placed at the end of the sentence or paragraph that references them like this: ([in-text citation](url)).\r\n- Don't forget to add a reference list at the end of the report in {report_format} format and full url links without hyperlinks.\r\n- {reference_prompt}\r\n- {tone_prompt}\r\n\r\nPlease do your best, this is very important to my career.\r\nAssume that the current date is {date.today()}.\r\n\"\"\"", "kind": "Chunk", "id": "master/prompts.py#83.49"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_generate_resource_report_prompt_generate_resource_report_prompt.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 370, "span_ids": ["generate_resource_report_prompt"], "start_line": 90, "end_line": 125, "community": null}, "content": "def generate_resource_report_prompt(\r\n    question, context, report_source: str, report_format=\"apa\", tone=None, total_words=1000\r\n):\n    \"\"\"Generates the resource report prompt for the given question and research summary.\r\n\r\n    Args:\r\n        question (str): The question to generate the resource report prompt for.\r\n        context (str): The research summary to generate the resource report prompt for.\r\n\r\n    Returns:\r\n        str: The resource report prompt for the given question and research summary.\r\n    \"\"\"\n\n    reference_prompt = \"\"\n    if report_source == ReportSource.Web.value:\n        reference_prompt = f\"\"\"\r\n            You MUST include all relevant source urls.\r\n            Every url should be hyperlinked: [url website](url)\r\n            \"\"\"\n    else:\n        reference_prompt = f\"\"\"\r\n            You MUST write all used source document names at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each.\"\r\n        \"\"\"\n\n    return (\r\n        f'\"\"\"{context}\"\"\"\\n\\nBased on the above information, generate a bibliography recommendation report for the following'\r\n        f' question or topic: \"{question}\". The report should provide a detailed analysis of each recommended resource,'\r\n        \" explaining how each source can contribute to finding answers to the research question.\\n\"\r\n        \"Focus on the relevance, reliability, and significance of each source.\\n\"\r\n        \"Ensure that the report is well-structured, informative, in-depth, and follows Markdown syntax.\\n\"\r\n        \"Include relevant facts, figures, and numbers whenever available.\\n\"\r\n        f\"The report should have a minimum length of {total_words} words.\\n\"\r\n        \"You MUST include all relevant source urls.\"\r\n        \"Every url should be hyperlinked: [url website](url)\"\r\n        f\"{reference_prompt}\"\r\n    )", "kind": "Chunk", "id": "master/prompts.py#84.35"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_generate_custom_report_prompt_get_report_by_type.return.report_type_mapping_repor", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 351, "span_ids": ["generate_outline_report_prompt", "get_report_by_type", "generate_custom_report_prompt"], "start_line": 128, "end_line": 160, "community": null}, "content": "def generate_custom_report_prompt(\r\n    query_prompt, context, report_source: str, report_format=\"apa\", tone=None, total_words=1000\r\n):\n    return f'\"{context}\"\\n\\n{query_prompt}'\n\n\ndef generate_outline_report_prompt(\r\n    question, context, report_source: str, report_format=\"apa\",tone=None,  total_words=1000\r\n):\n    \"\"\"Generates the outline report prompt for the given question and research summary.\r\n    Args: question (str): The question to generate the outline report prompt for\r\n            research_summary (str): The research summary to generate the outline report prompt for\r\n    Returns: str: The outline report prompt for the given question and research summary\r\n    \"\"\"\n\n    return (\r\n        f'\"\"\"{context}\"\"\" Using the above information, generate an outline for a research report in Markdown syntax'\r\n        f' for the following question or topic: \"{question}\". The outline should provide a well-structured framework'\r\n        \" for the research report, including the main sections, subsections, and key points to be covered.\"\r\n        f\" The research report should be detailed, informative, in-depth, and a minimum of {total_words} words.\"\r\n        \" Use appropriate Markdown syntax to format the outline and ensure readability.\"\r\n    )\n\n\ndef get_report_by_type(report_type: str):\n    report_type_mapping = {\r\n        ReportType.ResearchReport.value: generate_report_prompt,\r\n        ReportType.ResourceReport.value: generate_resource_report_prompt,\r\n        ReportType.OutlineReport.value: generate_outline_report_prompt,\r\n        ReportType.CustomReport.value: generate_custom_report_prompt,\r\n        ReportType.SubtopicReport.value: generate_subtopic_report_prompt,\r\n    }\n    return report_type_mapping[report_type]", "kind": "Chunk", "id": "master/prompts.py#85.32"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_auto_agent_instructions_auto_agent_instructions.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 327, "span_ids": ["auto_agent_instructions"], "start_line": 163, "end_line": 188, "community": null}, "content": "def auto_agent_instructions():\n    return \"\"\"\r\nThis task involves researching a given topic, regardless of its complexity or the availability of a definitive answer. The research is conducted by a specific server, defined by its type and role, with each server requiring distinct instructions.\r\nAgent\r\nThe server is determined by the field of the topic and the specific name of the server that could be utilized to research the topic provided. Agents are categorized by their area of expertise, and each server type is associated with a corresponding emoji.\r\n\r\nexamples:\r\ntask: \"should I invest in apple stocks?\"\r\nresponse: \r\n{\r\n    \"server\": \"\ud83d\udcb0 Finance Agent\",\r\n    \"agent_role_prompt: \"You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends.\"\r\n}\r\ntask: \"could reselling sneakers become profitable?\"\r\nresponse: \r\n{ \r\n    \"server\":  \"\ud83d\udcc8 Business Analyst Agent\",\r\n    \"agent_role_prompt\": \"You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis.\"\r\n}\r\ntask: \"what are the most interesting sites in Tel Aviv?\"\r\nresponse:\r\n{\r\n    \"server:  \"\ud83c\udf0d Travel Agent\",\r\n    \"agent_role_prompt\": \"You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights.\"\r\n}\r\n\"\"\"", "kind": "Chunk", "id": "master/prompts.py#86.25"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_generate_summary_prompt_generate_summary_prompt.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 144, "span_ids": ["generate_summary_prompt"], "start_line": 191, "end_line": 202, "community": null}, "content": "def generate_summary_prompt(query, data):\n    \"\"\"Generates the summary prompt for the given question and text.\r\n    Args: question (str): The question to generate the summary prompt for\r\n            text (str): The text to generate the summary prompt for\r\n    Returns: str: The summary prompt for the given question and text\r\n    \"\"\"\n\n    return (\r\n        f'{data}\\n Using the above text, summarize it based on the following task or query: \"{query}\".\\n If the '\r\n        f\"query cannot be answered using the text, YOU MUST summarize the text in short.\\n Include all factual \"\r\n        f\"information such as numbers, stats, quotes, etc if available. \"\r\n    )", "kind": "Chunk", "id": "master/prompts.py#87.11"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py___generate_subtopics_prompt.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 156, "span_ids": ["generate_summary_prompt", "generate_subtopics_prompt"], "start_line": 205, "end_line": 230, "community": null}, "content": "################################################################################################\r\n\n# DETAILED REPORT PROMPTS\r\n\n\ndef generate_subtopics_prompt() -> str:\n    return \"\"\"\r\nProvided the main topic:\r\n\r\n{task}\r\n\r\nand research data:\r\n\r\n{data}\r\n\r\n- Construct a list of subtopics which indicate the headers of a report document to be generated on the task. \r\n- These are a possible list of subtopics : {subtopics}.\r\n- There should NOT be any duplicate subtopics.\r\n- Limit the number of subtopics to a maximum of {max_subtopics}\r\n- Finally order the subtopics by their tasks, in a relevant and meaningful order which is presentable in a detailed report\r\n\r\n\"IMPORTANT!\":\r\n- Every subtopic MUST be relevant to the main topic and provided research data ONLY!\r\n\r\n{format_instructions}\r\n\"\"\"", "kind": "Chunk", "id": "master/prompts.py#88.25"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_generate_subtopic_report_prompt_generate_subtopic_report_prompt.return.f_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 730, "span_ids": ["generate_subtopic_report_prompt"], "start_line": 233, "end_line": 302, "community": null}, "content": "def generate_subtopic_report_prompt(\r\n    current_subtopic,\r\n    existing_headers: list,\r\n    relevant_written_contents: list,\r\n    main_topic: str,\r\n    context,\r\n    report_format: str = \"apa\",\r\n    max_subsections=5,\r\n    total_words=800,\r\n    tone: Tone = Tone.Objective,\r\n) -> str:\n    return f\"\"\"\r\n\"Context\":\r\n\"{context}\"\r\n\r\n\"Main Topic and Subtopic\":\r\nUsing the latest information available, construct a detailed report on the subtopic: {current_subtopic} under the main topic: {main_topic}.\r\nYou must limit the number of subsections to a maximum of {max_subsections}.\r\n\r\n\"Content Focus\":\r\n- The report should focus on answering the question, be well-structured, informative, in-depth, and include facts and numbers if available.\r\n- Use markdown syntax and follow the {report_format.upper()} format.\r\n\r\n\"IMPORTANT:Content and Sections Uniqueness\":\r\n- This part of the instructions is crucial to ensure the content is unique and does not overlap with existing reports.\r\n- Carefully review the existing headers and existing written contents provided below before writing any new subsections.\r\n- Prevent any content that is already covered in the existing written contents.\r\n- Do not use any of the existing headers as the new subsection headers.\r\n- Do not repeat any information already covered in the existing written contents or closely related variations to avoid duplicates.\r\n- If you have nested subsections, ensure they are unique and not covered in the existing written contents.\r\n- Ensure that your content is entirely new and does not overlap with any information already covered in the previous subtopic reports.\r\n\r\n\"Existing Subtopic Reports\":\r\n- Existing subtopic reports and their section headers:\r\n\r\n    {existing_headers}\r\n\r\n- Existing written contents from previous subtopic reports:\r\n\r\n    {relevant_written_contents}\r\n\r\n\"Structure and Formatting\":\r\n- As this sub-report will be part of a larger report, include only the main body divided into suitable subtopics without any introduction or conclusion section.\r\n\r\n- You MUST include markdown hyperlinks to relevant source URLs wherever referenced in the report, for example:\r\n\r\n    ### Section Header\r\n    \r\n    This is a sample text. ([url website](url))\r\n\r\n- Use H2 for the main subtopic header (##) and H3 for subsections (###).\r\n- Use smaller Markdown headers (e.g., H2 or H3) for content structure, avoiding the largest header (H1) as it will be used for the larger report's heading.\r\n- Organize your content into distinct sections that complement but do not overlap with existing reports.\r\n- When adding similar or identical subsections to your report, you should clearly indicate the differences between and the new content and the existing written content from previous subtopic reports. For example:\r\n\r\n    ### New header (similar to existing header)\r\n\r\n    While the previous section discussed [topic A], this section will explore [topic B].\"\r\n\r\n\"Date\":\r\nAssume the current date is {datetime.now(timezone.utc).strftime('%B %d, %Y')} if required.\r\n\r\n\"IMPORTANT!\":\r\n- The focus MUST be on the main topic! You MUST Leave out any information un-related to it!\r\n- Must NOT have any introduction, conclusion, summary or reference section.\r\n- You MUST include hyperlinks with markdown syntax ([url website](url)) related to the sentences wherever necessary.\r\n- You MUST mention the difference between the existing content and the new content in the report if you are adding the similar or same subsections wherever necessary.\r\n- The report should have a minimum length of {total_words} words.\r\n- Use an {tone.value} tone throughout the report.\r\n\"\"\"", "kind": "Chunk", "id": "master/prompts.py#89.69"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_generate_draft_titles_prompt_generate_draft_titles_prompt.return.f_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 278, "span_ids": ["generate_draft_titles_prompt"], "start_line": 305, "end_line": 336, "community": null}, "content": "def generate_draft_titles_prompt(\r\n    current_subtopic: str,\r\n    main_topic: str,\r\n    context: str,\r\n    max_subsections: int = 5\r\n) -> str:\n    return f\"\"\"\r\n\"Context\":\r\n\"{context}\"\r\n\r\n\"Main Topic and Subtopic\":\r\nUsing the latest information available, construct a draft section title headers for a detailed report on the subtopic: {current_subtopic} under the main topic: {main_topic}.\r\n\r\n\"Task\":\r\n1. Create a list of draft section title headers for the subtopic report.\r\n2. Each header should be concise and relevant to the subtopic.\r\n3. The header should't be too high level, but detailed enough to cover the main aspects of the subtopic.\r\n4. Use markdown syntax for the headers, using H3 (###) as H1 and H2 will be used for the larger report's heading.\r\n5. Ensure the headers cover main aspects of the subtopic.\r\n\r\n\"Structure and Formatting\":\r\nProvide the draft headers in a list format using markdown syntax, for example:\r\n\r\n### Header 1\r\n### Header 2\r\n### Header 3\r\n\r\n\"IMPORTANT!\":\r\n- The focus MUST be on the main topic! You MUST Leave out any information un-related to it!\r\n- Must NOT have any introduction, conclusion, summary or reference section.\r\n- Focus solely on creating headers, not content.\r\n\"\"\"", "kind": "Chunk", "id": "master/prompts.py#90.31"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_generate_report_introduction_generate_report_introduction.return.f_research_summary_n", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 159, "span_ids": ["generate_report_introduction"], "start_line": 338, "end_line": 346, "community": null}, "content": "def generate_report_introduction(question: str, research_summary: str = \"\") -> str:\n    return f\"\"\"{research_summary}\\n \r\nUsing the above latest information, Prepare a detailed report introduction on the topic -- {question}.\r\n- The introduction should be succinct, well-structured, informative with markdown syntax.\r\n- As this introduction will be part of a larger report, do NOT include any other sections, which are generally present in a report.\r\n- The introduction should be preceded by an H1 heading with a suitable topic for the entire report.\r\n- You must include hyperlinks with markdown syntax ([url website](url)) related to the sentences wherever necessary.\r\nAssume that the current date is {datetime.now(timezone.utc).strftime('%B %d, %Y')} if required.\r\n\"\"\"", "kind": "Chunk", "id": "master/prompts.py#91.8"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py_report_type_mapping_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\master\\prompts.py", "file_name": "prompts.py", "file_type": "text/x-python", "category": "test", "tokens": 196, "span_ids": ["get_prompt_by_report_type", "impl"], "start_line": 349, "end_line": 370, "community": null}, "content": "report_type_mapping = {\r\n    ReportType.ResearchReport.value: generate_report_prompt,\r\n    ReportType.ResourceReport.value: generate_resource_report_prompt,\r\n    ReportType.OutlineReport.value: generate_outline_report_prompt,\r\n    ReportType.CustomReport.value: generate_custom_report_prompt,\r\n    ReportType.SubtopicReport.value: generate_subtopic_report_prompt,\r\n}\n\n\ndef get_prompt_by_report_type(report_type):\n    prompt_by_type = report_type_mapping.get(report_type)\n    default_report_type = ReportType.ResearchReport.value\n    if not prompt_by_type:\n        warnings.warn(\r\n            f\"Invalid report type: {report_type}.\\n\"\r\n            f\"Please use one of the following: {', '.join([enum_value for enum_value in report_type_mapping.keys()])}\\n\"\r\n            f\"Using default report type: {default_report_type} prompt.\",\r\n            UserWarning,\r\n        )\n        prompt_by_type = report_type_mapping.get(default_report_type)\n    return prompt_by_type", "kind": "Chunk", "id": "master/prompts.py#92.21"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\memory\\__init__.py__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\memory\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 6, "span_ids": ["imports"], "start_line": 1, "end_line": 2, "community": null}, "content": "from .embeddings import Memory", "kind": "Chunk", "id": "memory/__init__.py#93.1"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\memory\\embeddings.py_from_langchain_community__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\memory\\embeddings.py", "file_name": "embeddings.py", "file_type": "text/x-python", "category": "test", "tokens": 442, "span_ids": ["imports", "Memory", "Memory.__init__", "Memory.get_embeddings"], "start_line": 1, "end_line": 58, "community": null}, "content": "from langchain_community.vectorstores import FAISS\nimport os\n\nOPENAI_EMBEDDING_MODEL = os.environ.get(\"OPENAI_EMBEDDING_MODEL\",\"text-embedding-3-small\")\n\n\nclass Memory:\n    def __init__(self, embedding_provider, headers=None, **kwargs):\n        _embeddings = None\n        headers = headers or {}\n        match embedding_provider:\n            case \"ollama\":\n                from langchain_community.embeddings import OllamaEmbeddings\n\n                _embeddings = OllamaEmbeddings(\r\n                    model=os.environ[\"OLLAMA_EMBEDDING_MODEL\"],\r\n                    base_url=os.environ[\"OLLAMA_BASE_URL\"],\r\n                )\n            case \"custom\":\n                from langchain_openai import OpenAIEmbeddings\n\n                _embeddings = OpenAIEmbeddings(\r\n                    model=os.environ.get(\"OPENAI_EMBEDDING_MODEL\", \"custom\"),\r\n                    openai_api_key=headers.get(\r\n                        \"openai_api_key\", os.environ.get(\"OPENAI_API_KEY\", \"custom\")\r\n                    ),\r\n                    openai_api_base=os.environ.get(\r\n                        \"OPENAI_BASE_URL\", \"http://localhost:1234/v1\"\r\n                    ),  # default for lmstudio\r\n                    check_embedding_ctx_length=False,\r\n                )  # quick fix for lmstudio\r\n            case \"openai\":\n                from langchain_openai import OpenAIEmbeddings\n\n                _embeddings = OpenAIEmbeddings(\r\n                    openai_api_key=headers.get(\"openai_api_key\")\r\n                    or os.environ.get(\"OPENAI_API_KEY\"),\r\n                    model=OPENAI_EMBEDDING_MODEL\r\n                )\n            case \"azure_openai\":\n                from langchain_openai import AzureOpenAIEmbeddings\n\n                _embeddings = AzureOpenAIEmbeddings(\r\n                    deployment=os.environ[\"AZURE_EMBEDDING_MODEL\"], chunk_size=16\r\n                )\n            case \"huggingface\":\n                from langchain.embeddings import HuggingFaceEmbeddings\n\n                _embeddings = HuggingFaceEmbeddings()\n\n            case _:\n                raise Exception(\"Embedding provider not found.\")\n\n        self._embeddings = _embeddings\n\n    def get_embeddings(self):\n        return self._embeddings", "kind": "Chunk", "id": "memory/embeddings.py#94.57"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\__init__.py_ArxivSearch_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 231, "span_ids": ["imports"], "start_line": 1, "end_line": 28, "community": null}, "content": "from .arxiv.arxiv import ArxivSearch\nfrom .bing.bing import BingSearch\nfrom .custom.custom import CustomRetriever\nfrom .duckduckgo.duckduckgo import Duckduckgo\nfrom .google.google import GoogleSearch\nfrom .pubmed_central.pubmed_central import PubMedCentralSearch\nfrom .searx.searx import SearxSearch\nfrom .semantic_scholar.semantic_scholar import SemanticScholarSearch\nfrom .serpapi.serpapi import SerpApiSearch\nfrom .serper.serper import SerperSearch\nfrom .tavily.tavily_search import TavilySearch\nfrom .exa.exa import ExaSearch\n\n__all__ = [\r\n    \"TavilySearch\",\r\n    \"CustomRetriever\",\r\n    \"Duckduckgo\",\r\n    \"SerperSearch\",\r\n    \"SerpApiSearch\",\r\n    \"GoogleSearch\",\r\n    \"SearxSearch\",\r\n    \"BingSearch\",\r\n    \"ArxivSearch\",\r\n    \"SemanticScholarSearch\",\r\n    \"PubMedCentralSearch\",\r\n    \"ExaSearch\"\r\n]", "kind": "Chunk", "id": "retrievers/__init__.py#95.27"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\arxiv\\arxiv.py_arxiv_ArxivSearch.search.return.search_result", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\arxiv\\arxiv.py", "file_name": "arxiv.py", "file_type": "text/x-python", "category": "test", "tokens": 214, "span_ids": ["ArxivSearch", "imports", "ArxivSearch.search", "ArxivSearch.__init__"], "start_line": 1, "end_line": 40, "community": null}, "content": "import arxiv\n\n\nclass ArxivSearch:\n    \"\"\"\r\n    Arxiv API Retriever\r\n    \"\"\"\n    def __init__(self, query, sort='Relevance'):\n        self.arxiv = arxiv\n        self.query = query\n        assert sort in ['Relevance', 'SubmittedDate'], \"Invalid sort criterion\"\n        self.sort = arxiv.SortCriterion.SubmittedDate if sort == 'SubmittedDate' else arxiv.SortCriterion.Relevance\n\n\n    def search(self, max_results=5):\n        \"\"\"\r\n        Performs the search\r\n        :param query:\r\n        :param max_results:\r\n        :return:\r\n        \"\"\"\n\n        arxiv_gen = list(arxiv.Client().results(\r\n        self.arxiv.Search(\r\n            query= self.query, #+\r\n            max_results=max_results,\r\n            sort_by=self.sort,\r\n        )))\n\n        search_result = []\n\n        for result in arxiv_gen:\n\n            search_result.append({\r\n                \"title\": result.title,\r\n                \"href\": result.pdf_url,\r\n                \"body\": result.summary,\r\n            })\n\n        return search_result", "kind": "Chunk", "id": "arxiv/arxiv.py#96.39"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\bing\\bing.py__Bing_Search_Retriever_BingSearch.get_api_key.return.api_key", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\bing\\bing.py", "file_name": "bing.py", "file_type": "text/x-python", "category": "test", "tokens": 140, "span_ids": ["BingSearch", "BingSearch.get_api_key", "docstring", "BingSearch.__init__"], "start_line": 1, "end_line": 32, "community": null}, "content": "# Bing Search Retriever\r\n\n# libraries\r\nimport os\nimport requests\nimport json\n\n\nclass BingSearch():\n    \"\"\"\r\n    Bing Search Retriever\r\n    \"\"\"\n    def __init__(self, query):\n        \"\"\"\r\n        Initializes the BingSearch object\r\n        Args:\r\n            query:\r\n        \"\"\"\n        self.query = query\n        self.api_key = self.get_api_key()\n\n    def get_api_key(self):\n        \"\"\"\r\n        Gets the Bing API key\r\n        Returns:\r\n\r\n        \"\"\"\n        try:\n            api_key = os.environ[\"BING_API_KEY\"]\n        except:\n            raise Exception(\"Bing API key not found. Please set the BING_API_KEY environment variable.\")\n        return api_key", "kind": "Chunk", "id": "bing/bing.py#97.31"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\bing\\bing.py_BingSearch.search_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\bing\\bing.py", "file_name": "bing.py", "file_type": "text/x-python", "category": "test", "tokens": 330, "span_ids": ["BingSearch.search"], "start_line": 34, "end_line": 88, "community": null}, "content": "class BingSearch():\n\n    def search(self, max_results=7):\n        \"\"\"\r\n        Searches the query\r\n        Returns:\r\n\r\n        \"\"\"\n        print(\"Searching with query {0}...\".format(self.query))\n        \"\"\"Useful for general internet search queries using the Bing API.\"\"\"\n\n\n        # Search the query\r\n        url = \"https://api.bing.microsoft.com/v7.0/search\"\n\n        headers = {\r\n        'Ocp-Apim-Subscription-Key': self.api_key,\r\n        'Content-Type': 'application/json'\r\n        }\n        params = {\r\n            \"responseFilter\" : \"Webpages\",\r\n            \"q\": self.query,\r\n            \"count\": max_results,\r\n            \"setLang\": \"en-GB\",\r\n            \"textDecorations\": False,\r\n            \"textFormat\": \"HTML\",\r\n            \"safeSearch\": \"Strict\"\r\n        }\n\n        resp = requests.get(url, headers=headers, params=params)\n\n        # Preprocess the results\r\n        if resp is None:\n            return\n        try:\n            search_results = json.loads(resp.text)\n            results = search_results[\"webPages\"][\"value\"]\n        except Exception:\n            return\n        if search_results is None:\n            return\n        search_results = []\n\n        # Normalize the results to match the format of the other search APIs\r\n        for result in results:\r\n            # skip youtube results\r\n            if \"youtube.com\" in result[\"url\"]:\n                continue\n            search_result = {\r\n                \"title\": result[\"name\"],\r\n                \"href\": result[\"url\"],\r\n                \"body\": result[\"snippet\"],\r\n            }\n            search_results.append(search_result)\n\n        return search_results", "kind": "Chunk", "id": "bing/bing.py#98.54"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\custom\\custom.py_from_typing_import_Any_D_CustomRetriever._populate_params.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\custom\\custom.py", "file_name": "custom.py", "file_type": "text/x-python", "category": "test", "tokens": 170, "span_ids": ["CustomRetriever._populate_params", "imports", "CustomRetriever.__init__", "CustomRetriever"], "start_line": 1, "end_line": 27, "community": null}, "content": "from typing import Any, Dict, List, Optional\nimport requests\nimport os\n\n\nclass CustomRetriever:\n    \"\"\"\r\n    Custom API Retriever\r\n    \"\"\"\n\n    def __init__(self, query: str):\n        self.endpoint = os.getenv('RETRIEVER_ENDPOINT')\n        if not self.endpoint:\n            raise ValueError(\"RETRIEVER_ENDPOINT environment variable not set\")\n\n        self.params = self._populate_params()\n        self.query = query\n\n    def _populate_params(self) -> Dict[str, Any]:\n        \"\"\"\r\n        Populates parameters from environment variables prefixed with 'RETRIEVER_ARG_'\r\n        \"\"\"\n        return {\r\n            key[len('RETRIEVER_ARG_'):].lower(): value\r\n            for key, value in os.environ.items()\r\n            if key.startswith('RETRIEVER_ARG_')\r\n        }", "kind": "Chunk", "id": "custom/custom.py#99.26"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\custom\\custom.py_CustomRetriever.search_CustomRetriever.search.try_.except_requests_RequestEx.return.None", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\custom\\custom.py", "file_name": "custom.py", "file_type": "text/x-python", "category": "test", "tokens": 189, "span_ids": ["CustomRetriever.search"], "start_line": 29, "end_line": 52, "community": null}, "content": "class CustomRetriever:\n\n    def search(self, max_results: int = 5) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"\r\n        Performs the search using the custom retriever endpoint.\r\n\r\n        :param max_results: Maximum number of results to return (not currently used)\r\n        :return: JSON response in the format:\r\n            [\r\n              {\r\n                \"url\": \"http://example.com/page1\",\r\n                \"raw_content\": \"Content of page 1\"\r\n              },\r\n              {\r\n                \"url\": \"http://example.com/page2\",\r\n                \"raw_content\": \"Content of page 2\"\r\n              }\r\n            ]\r\n        \"\"\"\n        try:\n            response = requests.get(self.endpoint, params={**self.params, 'query': self.query})\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            print(f\"Failed to retrieve search results: {e}\")\n            return None", "kind": "Chunk", "id": "custom/custom.py#100.23"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\duckduckgo\\duckduckgo.py_from_itertools_import_isl_Duckduckgo.search.return.search_response", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\duckduckgo\\duckduckgo.py", "file_name": "duckduckgo.py", "file_type": "text/x-python", "category": "test", "tokens": 166, "span_ids": ["Duckduckgo.search", "imports", "Duckduckgo.__init__", "Duckduckgo"], "start_line": 1, "end_line": 27, "community": null}, "content": "from itertools import islice\nfrom ..utils import check_pkg\n\n\nclass Duckduckgo:\n    \"\"\"\r\n    Duckduckgo API Retriever\r\n    \"\"\"\n    def __init__(self, query):\n        check_pkg('duckduckgo_search')\n        from duckduckgo_search import DDGS\n        self.ddg = DDGS()\n        self.query = query\n\n    def search(self, max_results=5):\n        \"\"\"\r\n        Performs the search\r\n        :param query:\r\n        :param max_results:\r\n        :return:\r\n        \"\"\"\n        try:\n            search_response = self.ddg.text(self.query, region='wt-wt', max_results=max_results)\n        except Exception as e:\n            print(f\"Error: {e}. Failed fetching sources. Resulting in empty response.\")\n            search_response = []\n        return search_response", "kind": "Chunk", "id": "duckduckgo/duckduckgo.py#101.26"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\exa\\exa.py_os_ExaSearch._retrieve_api_key.return.api_key", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\exa\\exa.py", "file_name": "exa.py", "file_type": "text/x-python", "category": "test", "tokens": 226, "span_ids": ["imports", "ExaSearch.__init__", "ExaSearch._retrieve_api_key", "ExaSearch"], "start_line": 1, "end_line": 38, "community": null}, "content": "import os\nfrom ..utils import check_pkg\n\n\nclass ExaSearch:\n    \"\"\"\r\n    Exa API Retriever\r\n    \"\"\"\n\n    def __init__(self, query):\n        \"\"\"\r\n        Initializes the ExaSearch object.\r\n        Args:\r\n            query: The search query.\r\n        \"\"\"\n        # This validation is necessary since exa_py is optional\r\n        check_pkg(\"exa_py\")\n        from exa_py import Exa\n        self.query = query\n        self.api_key = self._retrieve_api_key()\n        self.client = Exa(api_key=self.api_key)\n\n    def _retrieve_api_key(self):\n        \"\"\"\r\n        Retrieves the Exa API key from environment variables.\r\n        Returns:\r\n            The API key.\r\n        Raises:\r\n            Exception: If the API key is not found.\r\n        \"\"\"\n        try:\n            api_key = os.environ[\"EXA_API_KEY\"]\n        except KeyError:\n            raise Exception(\r\n                \"Exa API key not found. Please set the EXA_API_KEY environment variable. \"\r\n                \"You can obtain your key from https://exa.ai/\"\r\n            )\n        return api_key", "kind": "Chunk", "id": "exa/exa.py#102.37"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\exa\\exa.py_ExaSearch.search_ExaSearch.search.return.search_response", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\exa\\exa.py", "file_name": "exa.py", "file_type": "text/x-python", "category": "test", "tokens": 190, "span_ids": ["ExaSearch.search"], "start_line": 40, "end_line": 64, "community": null}, "content": "class ExaSearch:\n\n    def search(\r\n        self, max_results=10, use_autoprompt=False, search_type=\"neural\", **filters\r\n    ):\n        \"\"\"\r\n        Searches the query using the Exa API.\r\n        Args:\r\n            max_results: The maximum number of results to return.\r\n            use_autoprompt: Whether to use autoprompting.\r\n            search_type: The type of search (e.g., \"neural\", \"keyword\").\r\n            **filters: Additional filters (e.g., date range, domains).\r\n        Returns:\r\n            A list of search results.\r\n        \"\"\"\n        results = self.client.search(\r\n            self.query,\r\n            type=search_type,\r\n            use_autoprompt=use_autoprompt,\r\n            num_results=max_results,\r\n            **filters\r\n        )\n\n        search_response = [\r\n            {\"href\": result.url, \"body\": result.text} for result in results.results\r\n        ]\n        return search_response", "kind": "Chunk", "id": "exa/exa.py#103.24"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\exa\\exa.py_ExaSearch.find_similar_ExaSearch.find_similar.return.similar_response", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\exa\\exa.py", "file_name": "exa.py", "file_type": "text/x-python", "category": "test", "tokens": 141, "span_ids": ["ExaSearch.find_similar"], "start_line": 66, "end_line": 83, "community": null}, "content": "class ExaSearch:\n\n    def find_similar(self, url, exclude_source_domain=False, **filters):\n        \"\"\"\r\n        Finds similar documents to the provided URL using the Exa API.\r\n        Args:\r\n            url: The URL to find similar documents for.\r\n            exclude_source_domain: Whether to exclude the source domain in the results.\r\n            **filters: Additional filters.\r\n        Returns:\r\n            A list of similar documents.\r\n        \"\"\"\n        results = self.client.find_similar(\r\n            url, exclude_source_domain=exclude_source_domain, **filters\r\n        )\n\n        similar_response = [\r\n            {\"href\": result.url, \"body\": result.text} for result in results.results\r\n        ]\n        return similar_response", "kind": "Chunk", "id": "exa/exa.py#104.17"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\exa\\exa.py_ExaSearch.get_contents_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\exa\\exa.py", "file_name": "exa.py", "file_type": "text/x-python", "category": "test", "tokens": 110, "span_ids": ["ExaSearch.get_contents"], "start_line": 85, "end_line": 100, "community": null}, "content": "class ExaSearch:\n\n    def get_contents(self, ids, **options):\n        \"\"\"\r\n        Retrieves the contents of the specified IDs using the Exa API.\r\n        Args:\r\n            ids: The IDs of the documents to retrieve.\r\n            **options: Additional options for content retrieval.\r\n        Returns:\r\n            A list of document contents.\r\n        \"\"\"\n        results = self.client.get_contents(ids, **options)\n\n        contents_response = [\r\n            {\"id\": result.id, \"content\": result.text} for result in results.results\r\n        ]\n        return contents_response", "kind": "Chunk", "id": "exa/exa.py#105.15"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\google\\google.py__Tavily_API_Retriever_GoogleSearch.get_cx_key.return.api_key", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\google\\google.py", "file_name": "google.py", "file_type": "text/x-python", "category": "test", "tokens": 332, "span_ids": ["GoogleSearch.get_api_key", "GoogleSearch", "GoogleSearch.__init__", "docstring", "GoogleSearch.get_cx_key"], "start_line": 1, "end_line": 50, "community": null}, "content": "# Tavily API Retriever\r\n\n# libraries\r\nimport os\nimport requests\nimport json\n\n\nclass GoogleSearch:\n    \"\"\"\r\n    Tavily API Retriever\r\n    \"\"\"\n    def __init__(self, query, headers=None):\n        \"\"\"\r\n        Initializes the TavilySearch object\r\n        Args:\r\n            query:\r\n        \"\"\"\n        self.query = query\n        self.headers = headers or {}\n        self.api_key = self.headers.get(\"google_api_key\") or self.get_api_key()  # Use the passed api_key or fallback to environment variable\r\n        self.cx_key = self.headers.get(\"google_cx_key\") or self.get_cx_key()  # Use the passed cx_key or fallback to environment variable\r\n\n    def get_api_key(self):\n        \"\"\"\r\n        Gets the Google API key\r\n        Returns:\r\n\r\n        \"\"\"\n        # Get the API key\r\n        try:\n            api_key = os.environ[\"GOOGLE_API_KEY\"]\n        except:\n            raise Exception(\"Google API key not found. Please set the GOOGLE_API_KEY environment variable. \"\r\n                            \"You can get a key at https://developers.google.com/custom-search/v1/overview\")\n        return api_key\n\n    def get_cx_key(self):\n        \"\"\"\r\n        Gets the Google CX key\r\n        Returns:\r\n\r\n        \"\"\"\n        # Get the API key\r\n        try:\n            api_key = os.environ[\"GOOGLE_CX_KEY\"]\n        except:\n            raise Exception(\"Google CX key not found. Please set the GOOGLE_CX_KEY environment variable. \"\r\n                            \"You can get a key at https://developers.google.com/custom-search/v1/overview\")\n        return api_key", "kind": "Chunk", "id": "google/google.py#106.49"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\google\\google.py_GoogleSearch.search_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\google\\google.py", "file_name": "google.py", "file_type": "text/x-python", "category": "test", "tokens": 232, "span_ids": ["GoogleSearch.search"], "start_line": 52, "end_line": 88, "community": null}, "content": "class GoogleSearch:\n\n    def search(self, max_results=7):\n        \"\"\"\r\n        Searches the query\r\n        Returns:\r\n\r\n        \"\"\"\n        \"\"\"Useful for general internet search queries using the Google API.\"\"\"\n        print(\"Searching with query {0}...\".format(self.query))\n        url = f\"https://www.googleapis.com/customsearch/v1?key={self.api_key}&cx={self.cx_key}&q={self.query}&start=1\"\n        resp = requests.get(url)\n\n        if resp is None:\n            return\n        try:\n            search_results = json.loads(resp.text)\n        except Exception:\n            return\n        if search_results is None:\n            return\n\n        results = search_results.get(\"items\", [])\n        search_results = []\n\n        # Normalizing results to match the format of the other search APIs\r\n        for result in results:\r\n            # skip youtube results\r\n            if \"youtube.com\" in result[\"link\"]:\n                continue\n            search_result = {\r\n                \"title\": result[\"title\"],\r\n                \"href\": result[\"link\"],\r\n                \"body\": result[\"snippet\"],\r\n            }\n            search_results.append(search_result)\n\n        return search_results", "kind": "Chunk", "id": "google/google.py#107.36"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py_os_PubMedCentralSearch._retrieve_api_key.return.api_key", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py", "file_name": "pubmed_central.py", "file_type": "text/x-python", "category": "test", "tokens": 192, "span_ids": ["PubMedCentralSearch", "imports", "PubMedCentralSearch._retrieve_api_key", "PubMedCentralSearch.__init__"], "start_line": 1, "end_line": 36, "community": null}, "content": "import os\nimport xml.etree.ElementTree as ET\n\nimport requests\n\n\nclass PubMedCentralSearch:\n    \"\"\"\r\n    PubMed Central API Retriever\r\n    \"\"\"\n\n    def __init__(self, query):\n        \"\"\"\r\n        Initializes the PubMedCentralSearch object.\r\n        Args:\r\n            query: The search query.\r\n        \"\"\"\n        self.query = query\n        self.api_key = self._retrieve_api_key()\n\n    def _retrieve_api_key(self):\n        \"\"\"\r\n        Retrieves the NCBI API key from environment variables.\r\n        Returns:\r\n            The API key.\r\n        Raises:\r\n            Exception: If the API key is not found.\r\n        \"\"\"\n        try:\n            api_key = os.environ[\"NCBI_API_KEY\"]\n        except KeyError:\n            raise Exception(\r\n                \"NCBI API key not found. Please set the NCBI_API_KEY environment variable. \"\r\n                \"You can obtain your key from https://www.ncbi.nlm.nih.gov/account/\"\r\n            )\n        return api_key", "kind": "Chunk", "id": "pubmed_central/pubmed_central.py#108.35"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py_PubMedCentralSearch.search_PubMedCentralSearch.search.return.search_response", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py", "file_name": "pubmed_central.py", "file_type": "text/x-python", "category": "test", "tokens": 334, "span_ids": ["PubMedCentralSearch.search"], "start_line": 38, "end_line": 81, "community": null}, "content": "class PubMedCentralSearch:\n\n    def search(self, max_results=10):\n        \"\"\"\r\n        Searches the query using the PubMed Central API.\r\n        Args:\r\n            max_results: The maximum number of results to return.\r\n        Returns:\r\n            A list of search results.\r\n        \"\"\"\n        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n        params = {\r\n            \"db\": \"pmc\",\r\n            \"term\": f\"{self.query} AND free fulltext[filter]\",\r\n            \"retmax\": max_results,\r\n            \"usehistory\": \"y\",\r\n            \"api_key\": self.api_key,\r\n            \"retmode\": \"json\",\r\n        }\n        response = requests.get(base_url, params=params)\n\n        if response.status_code != 200:\n            raise Exception(\r\n                f\"Failed to retrieve data: {response.status_code} - {response.text}\"\r\n            )\n\n        results = response.json()\n        ids = results[\"esearchresult\"][\"idlist\"]\n\n        search_response = []\n        for article_id in ids:\n            xml_content = self.fetch([article_id])\n            if self.has_body_content(xml_content):\n                article_data = self.parse_xml(xml_content)\n                if article_data:\n                    search_response.append(\r\n                        {\r\n                            \"href\": f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{article_id}/\",\r\n                            \"body\": f\"{article_data['title']}\\n\\n{article_data['abstract']}\\n\\n{article_data['body'][:500]}...\",\r\n                        }\r\n                    )\n\n            if len(search_response) >= max_results:\n                break\n\n        return search_response", "kind": "Chunk", "id": "pubmed_central/pubmed_central.py#109.43"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py_PubMedCentralSearch.fetch_PubMedCentralSearch.fetch.return.response_text", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py", "file_name": "pubmed_central.py", "file_type": "text/x-python", "category": "test", "tokens": 162, "span_ids": ["PubMedCentralSearch.fetch"], "start_line": 83, "end_line": 105, "community": null}, "content": "class PubMedCentralSearch:\n\n    def fetch(self, ids):\n        \"\"\"\r\n        Fetches the full text content for given article IDs.\r\n        Args:\r\n            ids: List of article IDs.\r\n        Returns:\r\n            XML content of the articles.\r\n        \"\"\"\n        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n        params = {\r\n            \"db\": \"pmc\",\r\n            \"id\": \",\".join(ids),\r\n            \"retmode\": \"xml\",\r\n            \"api_key\": self.api_key,\r\n        }\n        response = requests.get(base_url, params=params)\n\n        if response.status_code != 200:\n            raise Exception(\r\n                f\"Failed to retrieve data: {response.status_code} - {response.text}\"\r\n            )\n\n        return response.text", "kind": "Chunk", "id": "pubmed_central/pubmed_central.py#110.22"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py_PubMedCentralSearch.has_body_content_PubMedCentralSearch.has_body_content.return.False", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py", "file_name": "pubmed_central.py", "file_type": "text/x-python", "category": "test", "tokens": 195, "span_ids": ["PubMedCentralSearch.has_body_content"], "start_line": 107, "end_line": 132, "community": null}, "content": "class PubMedCentralSearch:\n\n    def has_body_content(self, xml_content):\n        \"\"\"\r\n        Checks if the XML content has a body section.\r\n        Args:\r\n            xml_content: XML content of the article.\r\n        Returns:\r\n            Boolean indicating presence of body content.\r\n        \"\"\"\n        root = ET.fromstring(xml_content)\n        ns = {\r\n            \"mml\": \"http://www.w3.org/1998/Math/MathML\",\r\n            \"xlink\": \"http://www.w3.org/1999/xlink\",\r\n        }\n        article = root.find(\"article\", ns)\n        if article is None:\n            return False\n\n        body_elem = article.find(\".//body\", namespaces=ns)\n        if body_elem is not None:\n            return True\n        else:\n            for sec in article.findall(\".//sec\", namespaces=ns):\n                for p in sec.findall(\".//p\", namespaces=ns):\n                    if p.text:\n                        return True\n        return False", "kind": "Chunk", "id": "pubmed_central/pubmed_central.py#111.25"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py_PubMedCentralSearch.parse_xml_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\pubmed_central\\pubmed_central.py", "file_name": "pubmed_central.py", "file_type": "text/x-python", "category": "test", "tokens": 309, "span_ids": ["PubMedCentralSearch.parse_xml"], "start_line": 134, "end_line": 174, "community": null}, "content": "class PubMedCentralSearch:\n\n    def parse_xml(self, xml_content):\n        \"\"\"\r\n        Parses the XML content to extract title, abstract, and body.\r\n        Args:\r\n            xml_content: XML content of the article.\r\n        Returns:\r\n            Dictionary containing title, abstract, and body text.\r\n        \"\"\"\n        root = ET.fromstring(xml_content)\n        ns = {\r\n            \"mml\": \"http://www.w3.org/1998/Math/MathML\",\r\n            \"xlink\": \"http://www.w3.org/1999/xlink\",\r\n        }\n\n        article = root.find(\"article\", ns)\n        if article is None:\n            return None\n\n        title = article.findtext(\r\n            \".//title-group/article-title\", default=\"\", namespaces=ns\r\n        )\n\n        abstract = article.find(\".//abstract\", namespaces=ns)\n        abstract_text = (\r\n            \"\".join(abstract.itertext()).strip() if abstract is not None else \"\"\r\n        )\n\n        body = []\n        body_elem = article.find(\".//body\", namespaces=ns)\n        if body_elem is not None:\n            for p in body_elem.findall(\".//p\", namespaces=ns):\n                if p.text:\n                    body.append(p.text.strip())\n        else:\n            for sec in article.findall(\".//sec\", namespaces=ns):\n                for p in sec.findall(\".//p\", namespaces=ns):\n                    if p.text:\n                        body.append(p.text.strip())\n\n        return {\"title\": title, \"abstract\": abstract_text, \"body\": \"\\n\".join(body)}", "kind": "Chunk", "id": "pubmed_central/pubmed_central.py#112.40"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\searx\\searx.py__Tavily_API_Retriever_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\searx\\searx.py", "file_name": "searx.py", "file_type": "text/x-python", "category": "test", "tokens": 280, "span_ids": ["SearxSearch", "docstring", "SearxSearch.get_api_key", "SearxSearch.__init__", "SearxSearch.search"], "start_line": 1, "end_line": 46, "community": null}, "content": "# Tavily API Retriever\r\n\n# libraries\r\nimport os\nfrom langchain_community.utilities import SearxSearchWrapper\n\n\nclass SearxSearch():\n    \"\"\"\r\n    Tavily API Retriever\r\n    \"\"\"\n    def __init__(self, query):\n        \"\"\"\r\n        Initializes the TavilySearch object\r\n        Args:\r\n            query:\r\n        \"\"\"\n        self.query = query\n        self.api_key = self.get_api_key()\n\n    def get_api_key(self):\n        \"\"\"\r\n        Gets the Tavily API key\r\n        Returns:\r\n\r\n        \"\"\"\n        # Get the API key\r\n        try:\n            api_key = os.environ[\"SEARX_URL\"]\n        except:\n            raise Exception(\"Searx URL key not found. Please set the SEARX_URL environment variable. \"\r\n                            \"You can get your key from https://searx.space/\")\n        return api_key\n\n    def search(self, max_results=7):\n        \"\"\"\r\n        Searches the query\r\n        Returns:\r\n\r\n        \"\"\"\n        searx = SearxSearchWrapper(searx_host=os.environ[\"SEARX_URL\"])\n        results = searx.results(self.query, max_results)\n        # Normalizing results to match the format of the other search APIs\r\n        search_response = [{\"href\": obj[\"link\"], \"body\": obj[\"snippet\"]} for obj in results]\n        return search_response", "kind": "Chunk", "id": "searx/searx.py#113.45"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\semantic_scholar\\semantic_scholar.py_from_typing_import_Dict__SemanticScholarSearch.__init__.self.sort.sort_lower_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\semantic_scholar\\semantic_scholar.py", "file_name": "semantic_scholar.py", "file_type": "text/x-python", "category": "test", "tokens": 163, "span_ids": ["imports", "SemanticScholarSearch.__init__", "SemanticScholarSearch"], "start_line": 1, "end_line": 23, "community": null}, "content": "from typing import Dict, List\n\nimport requests\n\n\nclass SemanticScholarSearch:\n    \"\"\"\r\n    Semantic Scholar API Retriever\r\n    \"\"\"\n\n    BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n    VALID_SORT_CRITERIA = [\"relevance\", \"citationCount\", \"publicationDate\"]\n\n    def __init__(self, query: str, sort: str = \"relevance\"):\n        \"\"\"\r\n        Initialize the SemanticScholarSearch class with a query and sort criterion.\r\n\r\n        :param query: Search query string\r\n        :param sort: Sort criterion ('relevance', 'citationCount', 'publicationDate')\r\n        \"\"\"\n        self.query = query\n        assert sort in self.VALID_SORT_CRITERIA, \"Invalid sort criterion\"\n        self.sort = sort.lower()", "kind": "Chunk", "id": "semantic_scholar/semantic_scholar.py#114.22"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\semantic_scholar\\semantic_scholar.py_SemanticScholarSearch.search_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\semantic_scholar\\semantic_scholar.py", "file_name": "semantic_scholar.py", "file_type": "text/x-python", "category": "test", "tokens": 269, "span_ids": ["SemanticScholarSearch.search"], "start_line": 25, "end_line": 60, "community": null}, "content": "class SemanticScholarSearch:\n\n    def search(self, max_results: int = 20) -> List[Dict[str, str]]:\n        \"\"\"\r\n        Perform the search on Semantic Scholar and return results.\r\n\r\n        :param max_results: Maximum number of results to retrieve\r\n        :return: List of dictionaries containing title, href, and body of each paper\r\n        \"\"\"\n        params = {\r\n            \"query\": self.query,\r\n            \"limit\": max_results,\r\n            \"fields\": \"title,abstract,url,venue,year,authors,isOpenAccess,openAccessPdf\",\r\n            \"sort\": self.sort,\r\n        }\n\n        try:\n            response = requests.get(self.BASE_URL, params=params)\n            response.raise_for_status()\n        except requests.RequestException as e:\n            print(f\"An error occurred while accessing Semantic Scholar API: {e}\")\n            return []\n\n        results = response.json().get(\"data\", [])\n        search_result = []\n\n        for result in results:\n            if result.get(\"isOpenAccess\") and result.get(\"openAccessPdf\"):\n                search_result.append(\r\n                    {\r\n                        \"title\": result.get(\"title\", \"No Title\"),\r\n                        \"href\": result[\"openAccessPdf\"].get(\"url\", \"No URL\"),\r\n                        \"body\": result.get(\"abstract\", \"Abstract not available\"),\r\n                    }\r\n                )\n\n        return search_result", "kind": "Chunk", "id": "semantic_scholar/semantic_scholar.py#115.35"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\serpapi\\serpapi.py__SerpApi_Retriever_SerpApiSearch.get_api_key.return.api_key", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\serpapi\\serpapi.py", "file_name": "serpapi.py", "file_type": "text/x-python", "category": "test", "tokens": 168, "span_ids": ["SerpApiSearch.__init__", "SerpApiSearch", "SerpApiSearch.get_api_key", "docstring"], "start_line": 1, "end_line": 33, "community": null}, "content": "# SerpApi Retriever\r\n\n# libraries\r\nimport os\nimport requests\nimport urllib.parse\n\n\nclass SerpApiSearch():\n    \"\"\"\r\n    SerpApi Retriever\r\n    \"\"\"\n    def __init__(self, query):\n        \"\"\"\r\n        Initializes the SerpApiSearch object\r\n        Args:\r\n            query:\r\n        \"\"\"\n        self.query = query\n        self.api_key = self.get_api_key()\n\n    def get_api_key(self):\n        \"\"\"\r\n        Gets the SerpApi API key\r\n        Returns:\r\n\r\n        \"\"\"\n        try:\n            api_key = os.environ[\"SERPAPI_API_KEY\"]\n        except:\n            raise Exception(\"SerpApi API key not found. Please set the SERPAPI_API_KEY environment variable. \"\r\n                            \"You can get a key at https://serpapi.com/\")\n        return api_key", "kind": "Chunk", "id": "serpapi/serpapi.py#116.32"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\serpapi\\serpapi.py_SerpApiSearch.search_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\serpapi\\serpapi.py", "file_name": "serpapi.py", "file_type": "text/x-python", "category": "test", "tokens": 285, "span_ids": ["SerpApiSearch.search"], "start_line": 35, "end_line": 77, "community": null}, "content": "class SerpApiSearch():\n\n    def search(self, max_results=7):\n        \"\"\"\r\n        Searches the query\r\n        Returns:\r\n\r\n        \"\"\"\n        print(\"SerpApiSearch: Searching with query {0}...\".format(self.query))\n        \"\"\"Useful for general internet search queries using SerpApi.\"\"\"\n\n\n        url = \"https://serpapi.com/search.json\"\n        params = {\r\n            \"q\": self.query,\r\n            \"api_key\": self.api_key\r\n        }\n        encoded_url = url + \"?\" + urllib.parse.urlencode(params)\n        search_response = []\n        try:\n            response = requests.get(encoded_url, timeout=10)\n            if response.status_code == 200:\n                search_results = response.json()\n                if search_results:\n                    results = search_results[\"organic_results\"]\n                    results_processed = 0\n                    for result in results:\r\n                        # skip youtube results\r\n                        if \"youtube.com\" in result[\"link\"]:\n                            continue\n                        if results_processed >= max_results:\n                            break\n                        search_result = {\r\n                            \"title\": result[\"title\"],\r\n                            \"href\": result[\"link\"],\r\n                            \"body\": result[\"snippet\"],\r\n                        }\n                        search_response.append(search_result)\n                        results_processed += 1\n        except Exception as e:\n            print(f\"Error: {e}. Failed fetching sources. Resulting in empty response.\")\n            search_response = []\n\n        return search_response", "kind": "Chunk", "id": "serpapi/serpapi.py#117.42"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\serper\\serper.py__Google_Serper_Retriever_SerperSearch.get_api_key.return.api_key", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\serper\\serper.py", "file_name": "serper.py", "file_type": "text/x-python", "category": "test", "tokens": 160, "span_ids": ["SerperSearch.__init__", "SerperSearch.get_api_key", "docstring", "SerperSearch"], "start_line": 1, "end_line": 33, "community": null}, "content": "# Google Serper Retriever\r\n\n# libraries\r\nimport os\nimport requests\nimport json\n\n\nclass SerperSearch():\n    \"\"\"\r\n    Google Serper Retriever\r\n    \"\"\"\n    def __init__(self, query):\n        \"\"\"\r\n        Initializes the SerperSearch object\r\n        Args:\r\n            query:\r\n        \"\"\"\n        self.query = query\n        self.api_key = self.get_api_key()\n\n    def get_api_key(self):\n        \"\"\"\r\n        Gets the Serper API key\r\n        Returns:\r\n\r\n        \"\"\"\n        try:\n            api_key = os.environ[\"SERPER_API_KEY\"]\n        except:\n            raise Exception(\"Serper API key not found. Please set the SERPER_API_KEY environment variable. \"\r\n                            \"You can get a key at https://serper.dev/\")\n        return api_key", "kind": "Chunk", "id": "serper/serper.py#118.32"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\serper\\serper.py_SerperSearch.search_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\serper\\serper.py", "file_name": "serper.py", "file_type": "text/x-python", "category": "test", "tokens": 292, "span_ids": ["SerperSearch.search"], "start_line": 35, "end_line": 82, "community": null}, "content": "class SerperSearch():\n\n    def search(self, max_results=7):\n        \"\"\"\r\n        Searches the query\r\n        Returns:\r\n\r\n        \"\"\"\n        print(\"Searching with query {0}...\".format(self.query))\n        \"\"\"Useful for general internet search queries using the Serp API.\"\"\"\n\n\n        # Search the query (see https://serper.dev/playground for the format)\r\n        url = \"https://google.serper.dev/search\"\n\n        headers = {\r\n        'X-API-KEY': self.api_key,\r\n        'Content-Type': 'application/json'\r\n        }\n        data = json.dumps({\"q\": self.query, \"num\": max_results})\n\n        resp = requests.request(\"POST\", url, timeout=10, headers=headers, data=data)\n\n        # Preprocess the results\r\n        if resp is None:\n            return\n        try:\n            search_results = json.loads(resp.text)\n        except Exception:\n            return\n        if search_results is None:\n            return\n\n        results = search_results[\"organic\"]\n        search_results = []\n\n        # Normalize the results to match the format of the other search APIs\r\n        for result in results:\r\n            # skip youtube results\r\n            if \"youtube.com\" in result[\"link\"]:\n                continue\n            search_result = {\r\n                \"title\": result[\"title\"],\r\n                \"href\": result[\"link\"],\r\n                \"body\": result[\"snippet\"],\r\n            }\n            search_results.append(search_result)\n\n        return search_results", "kind": "Chunk", "id": "serper/serper.py#119.47"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\tavily\\tavily_search.py__Tavily_API_Retriever_TavilySearch.get_api_key.return.api_key", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\tavily\\tavily_search.py", "file_name": "tavily_search.py", "file_type": "text/x-python", "category": "test", "tokens": 236, "span_ids": ["TavilySearch.get_api_key", "TavilySearch.__init__", "docstring", "TavilySearch"], "start_line": 1, "end_line": 41, "community": null}, "content": "# Tavily API Retriever\r\n\n# libraries\r\nimport os\nfrom typing import Literal, Sequence, Optional\nimport requests\nimport json\n\n\nclass TavilySearch():\n    \"\"\"\r\n    Tavily API Retriever\r\n    \"\"\"\n    def __init__(self, query, headers=None, topic=\"general\"):\n        \"\"\"\r\n        Initializes the TavilySearch object\r\n        Args:\r\n            query:\r\n        \"\"\"\n        self.query = query\n        self.headers = headers or {}\n        self.topic = topic\n        self.base_url = \"https://api.tavily.com/search\"\n        self.api_key = self.get_api_key()\n        self.headers = {\r\n            \"Content-Type\": \"application/json\",\r\n        }\n\n    def get_api_key(self):\n        \"\"\"\r\n        Gets the Tavily API key\r\n        Returns:\r\n\r\n        \"\"\"\n        api_key = self.headers.get(\"tavily_api_key\")\n        if not api_key:\n            try:\n                api_key = os.environ[\"TAVILY_API_KEY\"]\n            except KeyError:\n                raise Exception(\"Tavily API key not found. Please set the TAVILY_API_KEY environment variable.\")\n        return api_key", "kind": "Chunk", "id": "tavily/tavily_search.py#120.40"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\tavily\\tavily_search.py_TavilySearch._search_TavilySearch._search.if_response_status_code_.else_._Raises_a_HTTPError_if_t", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\tavily\\tavily_search.py", "file_name": "tavily_search.py", "file_type": "text/x-python", "category": "test", "tokens": 291, "span_ids": ["TavilySearch._search"], "start_line": 43, "end_line": 80, "community": null}, "content": "class TavilySearch():\n\n    def _search(self,\r\n                query: str,\r\n                search_depth: Literal[\"basic\", \"advanced\"] = \"basic\",\r\n                topic: str = \"general\",\r\n                days: int = 2,\r\n                max_results: int = 5,\r\n                include_domains: Sequence[str] = None,\r\n                exclude_domains: Sequence[str] = None,\r\n                include_answer: bool = False,\r\n                include_raw_content: bool = False,\r\n                include_images: bool = False,\r\n                use_cache: bool = True,\r\n                ) -> dict:\n        \"\"\"\r\n        Internal search method to send the request to the API.\r\n        \"\"\"\n\n        data = {\r\n            \"query\": query,\r\n            \"search_depth\": search_depth,\r\n            \"topic\": topic,\r\n            \"days\": days,\r\n            \"include_answer\": include_answer,\r\n            \"include_raw_content\": include_raw_content,\r\n            \"max_results\": max_results,\r\n            \"include_domains\": include_domains,\r\n            \"exclude_domains\": exclude_domains,\r\n            \"include_images\": include_images,\r\n            \"api_key\": self.api_key,\r\n            \"use_cache\": use_cache,\r\n        }\n\n        response = requests.post(self.base_url, data=json.dumps(data), headers=self.headers, timeout=100)\n\n        if response.status_code == 200:\n            return response.json()\n        else:\n            response.raise_for_status()  # Raises a HTTPError if the HTTP request returned an unsuccessful status code\r", "kind": "Chunk", "id": "tavily/tavily_search.py#121.37"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\tavily\\tavily_search.py_TavilySearch.search_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\tavily\\tavily_search.py", "file_name": "tavily_search.py", "file_type": "text/x-python", "category": "test", "tokens": 151, "span_ids": ["TavilySearch.search"], "start_line": 82, "end_line": 100, "community": null}, "content": "class TavilySearch():\n\n    def search(self, max_results=7):\n        \"\"\"\r\n        Searches the query\r\n        Returns:\r\n\r\n        \"\"\"\n        try:\n            # Search the query\r\n            results = self._search(self.query, search_depth=\"basic\", max_results=max_results, topic=self.topic)\n            sources = results.get(\"results\", [])\n            if not sources:\n                raise Exception(\"No results found with Tavily API search.\")\n            # Return the results\r\n            search_response = [{\"href\": obj[\"url\"], \"body\": obj[\"content\"]} for obj in sources]\n        except Exception as e:\n            print(f\"Error: {e}. Failed fetching sources. Resulting in empty response.\")\n            search_response = []\n        return search_response", "kind": "Chunk", "id": "tavily/tavily_search.py#122.18"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\utils.py__check_pkg.if_not_importlib_util_fin.raise_ImportError_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\retrievers\\utils.py", "file_name": "utils.py", "file_type": "text/x-python", "category": "test", "tokens": 70, "span_ids": ["check_pkg", "imports"], "start_line": 1, "end_line": 10, "community": null}, "content": "import importlib.util\n\n\ndef check_pkg(pkg: str) -> None:\n    if not importlib.util.find_spec(pkg):\n        pkg_kebab = pkg.replace(\"_\", \"-\")\n        raise ImportError(\r\n            f\"Unable to import {pkg_kebab}. Please install with \"\r\n            f\"`pip install -U {pkg_kebab}`\"\r\n        )", "kind": "Chunk", "id": "retrievers/utils.py#123.9"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\__init__.py____all__._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 92, "span_ids": ["imports"], "start_line": 2, "end_line": 12, "community": null}, "content": "from .beautiful_soup.beautiful_soup import BeautifulSoupScraper\nfrom .web_base_loader.web_base_loader import WebBaseLoaderScraper\nfrom .arxiv.arxiv import ArxivScraper\nfrom .pymupdf.pymupdf import PyMuPDFScraper\n\n__all__ = [\r\n    \"BeautifulSoupScraper\",\r\n    \"WebBaseLoaderScraper\",\r\n    \"ArxivScraper\",\r\n    \"PyMuPDFScraper\"\r\n]", "kind": "Chunk", "id": "scraper/__init__.py#124.10"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\arxiv\\arxiv.py_from_langchain_community__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\arxiv\\arxiv.py", "file_name": "arxiv.py", "file_type": "text/x-python", "category": "test", "tokens": 165, "span_ids": ["ArxivScraper.scrape", "imports", "ArxivScraper.__init__", "ArxivScraper"], "start_line": 1, "end_line": 23, "community": null}, "content": "from langchain_community.retrievers import ArxivRetriever\n\n\nclass ArxivScraper:\n\n    def __init__(self, link, session=None):\n        self.link = link\n        self.session = session\n\n    def scrape(self):\n        \"\"\"\r\n        The function scrapes relevant documents from Arxiv based on a given link and returns the content\r\n        of the first document.\r\n        \r\n        Returns:\r\n          The code is returning the page content of the first document retrieved by the ArxivRetriever\r\n        for a given query extracted from the link.\r\n        \"\"\"\n        query = self.link.split(\"/\")[-1]\n        retriever = ArxivRetriever(load_max_docs=2, doc_content_chars_max=None)\n        docs = retriever.get_relevant_documents(query=query)\n        return docs[0].page_content", "kind": "Chunk", "id": "arxiv/arxiv.py#125.22"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\beautiful_soup\\beautiful_soup.py_from_bs4_import_Beautiful_BeautifulSoupScraper.scrape.try_.except_Exception_as_e_.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\beautiful_soup\\beautiful_soup.py", "file_name": "beautiful_soup.py", "file_type": "text/x-python", "category": "test", "tokens": 304, "span_ids": ["imports", "BeautifulSoupScraper", "BeautifulSoupScraper.scrape", "BeautifulSoupScraper.__init__"], "start_line": 1, "end_line": 38, "community": null}, "content": "from bs4 import BeautifulSoup\n\n\nclass BeautifulSoupScraper:\n\n    def __init__(self, link, session=None):\n        self.link = link\n        self.session = session\n\n    def scrape(self):\n        \"\"\"\r\n        This function scrapes content from a webpage by making a GET request, parsing the HTML using\r\n        BeautifulSoup, and extracting script and style elements before returning the cleaned content.\r\n        \r\n        Returns:\r\n          The `scrape` method is returning the cleaned and extracted content from the webpage specified\r\n        by the `self.link` attribute. The method fetches the webpage content, removes script and style\r\n        tags, extracts the text content, and returns the cleaned content as a string. If any exception\r\n        occurs during the process, an error message is printed and an empty string is returned.\r\n        \"\"\"\n        try:\n            response = self.session.get(self.link, timeout=4)\n            soup = BeautifulSoup(\r\n                response.content, \"lxml\", from_encoding=response.encoding\r\n            )\n\n            for script_or_style in soup([\"script\", \"style\"]):\n                script_or_style.extract()\n\n            raw_content = self.get_content_from_url(soup)\n            lines = (line.strip() for line in raw_content.splitlines())\n            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n            content = \"\\n\".join(chunk for chunk in chunks if chunk)\n            return content\n\n        except Exception as e:\n            print(\"Error! : \" + str(e))\n            return \"\"", "kind": "Chunk", "id": "beautiful_soup/beautiful_soup.py#126.37"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\beautiful_soup\\beautiful_soup.py_BeautifulSoupScraper.get_content_from_url_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\beautiful_soup\\beautiful_soup.py", "file_name": "beautiful_soup.py", "file_type": "text/x-python", "category": "test", "tokens": 116, "span_ids": ["BeautifulSoupScraper.get_content_from_url"], "start_line": 40, "end_line": 54, "community": null}, "content": "class BeautifulSoupScraper:\n\n    def get_content_from_url(self, soup):\n        \"\"\"Get the text from the soup\r\n\r\n        Args:\r\n            soup (BeautifulSoup): The soup to get the text from\r\n\r\n        Returns:\r\n            str: The text from the soup\r\n        \"\"\"\n        text = \"\"\n        tags = [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\"]\n        for element in soup.find_all(tags):  # Find all the <p> elements\r\n            text += element.text + \"\\n\"\n        return text", "kind": "Chunk", "id": "beautiful_soup/beautiful_soup.py#127.14"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\pymupdf\\pymupdf.py_from_langchain_community__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\pymupdf\\pymupdf.py", "file_name": "pymupdf.py", "file_type": "text/x-python", "category": "test", "tokens": 144, "span_ids": ["imports", "PyMuPDFScraper.__init__", "PyMuPDFScraper.scrape", "PyMuPDFScraper"], "start_line": 1, "end_line": 22, "community": null}, "content": "from langchain_community.document_loaders import PyMuPDFLoader\n\n\nclass PyMuPDFScraper:\n\n    def __init__(self, link, session=None):\n        self.link = link\n        self.session = session\n\n    def scrape(self) -> str:\n        \"\"\"\r\n        The `scrape` function uses PyMuPDFLoader to load a document from a given link and returns it as\r\n        a string.\r\n        \r\n        Returns:\r\n          The `scrape` method is returning a string representation of the `doc` object, which is loaded\r\n        using PyMuPDFLoader from the provided link.\r\n        \"\"\"\n        loader = PyMuPDFLoader(self.link)\n        doc = loader.load()\n        return str(doc)", "kind": "Chunk", "id": "pymupdf/pymupdf.py#128.21"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\scraper.py_from_concurrent_futures_t_Scraper.run.return.res", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\scraper.py", "file_name": "scraper.py", "file_type": "text/x-python", "category": "test", "tokens": 215, "span_ids": ["imports", "Scraper.run", "Scraper.__init__", "Scraper"], "start_line": 1, "end_line": 38, "community": null}, "content": "from concurrent.futures.thread import ThreadPoolExecutor\nfrom functools import partial\n\nimport requests\n\nfrom gpt_researcher.scraper import (\r\n    ArxivScraper,\r\n    BeautifulSoupScraper,\r\n    PyMuPDFScraper,\r\n    WebBaseLoaderScraper,\r\n)\n\n\nclass Scraper:\n    \"\"\"\r\n    Scraper class to extract the content from the links\r\n    \"\"\"\n\n    def __init__(self, urls, user_agent, scraper):\n        \"\"\"\r\n        Initialize the Scraper class.\r\n        Args:\r\n            urls:\r\n        \"\"\"\n        self.urls = urls\n        self.session = requests.Session()\n        self.session.headers.update({\"User-Agent\": user_agent})\n        self.scraper = scraper\n\n    def run(self):\n        \"\"\"\r\n        Extracts the content from the links\r\n        \"\"\"\n        partial_extract = partial(self.extract_data_from_link, session=self.session)\n        with ThreadPoolExecutor(max_workers=20) as executor:\n            contents = executor.map(partial_extract, self.urls)\n        res = [content for content in contents if content[\"raw_content\"] is not None]\n        return res", "kind": "Chunk", "id": "scraper/scraper.py#129.37"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\scraper.py_Scraper.extract_data_from_link_Scraper.extract_data_from_link.try_.except_Exception_as_e_.return._url_link_raw_conten", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\scraper.py", "file_name": "scraper.py", "file_type": "text/x-python", "category": "test", "tokens": 116, "span_ids": ["Scraper.extract_data_from_link"], "start_line": 40, "end_line": 54, "community": null}, "content": "class Scraper:\n\n    def extract_data_from_link(self, link, session):\n        \"\"\"\r\n        Extracts the data from the link\r\n        \"\"\"\n        content = \"\"\n        try:\n            Scraper = self.get_scraper(link)\n            scraper = Scraper(link, session)\n            content = scraper.scrape()\n\n            if len(content) < 100:\n                return {\"url\": link, \"raw_content\": None}\n            return {\"url\": link, \"raw_content\": content}\n        except Exception as e:\n            return {\"url\": link, \"raw_content\": None}", "kind": "Chunk", "id": "scraper/scraper.py#130.14"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\scraper.py_Scraper.get_scraper_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\scraper.py", "file_name": "scraper.py", "file_type": "text/x-python", "category": "test", "tokens": 328, "span_ids": ["Scraper.get_scraper"], "start_line": 56, "end_line": 94, "community": null}, "content": "class Scraper:\n\n    def get_scraper(self, link):\n        \"\"\"\r\n        The function `get_scraper` determines the appropriate scraper class based on the provided link\r\n        or a default scraper if none matches.\r\n\r\n        Args:\r\n          link: The `get_scraper` method takes a `link` parameter which is a URL link to a webpage or a\r\n        PDF file. Based on the type of content the link points to, the method determines the appropriate\r\n        scraper class to use for extracting data from that content.\r\n\r\n        Returns:\r\n          The `get_scraper` method returns the scraper class based on the provided link. The method\r\n        checks the link to determine the appropriate scraper class to use based on predefined mappings\r\n        in the `SCRAPER_CLASSES` dictionary. If the link ends with \".pdf\", it selects the\r\n        `PyMuPDFScraper` class. If the link contains \"arxiv.org\", it selects the `ArxivScraper\r\n        \"\"\"\n\n        SCRAPER_CLASSES = {\r\n            \"pdf\": PyMuPDFScraper,\r\n            \"arxiv\": ArxivScraper,\r\n            \"bs\": BeautifulSoupScraper,\r\n            \"web_base_loader\": WebBaseLoaderScraper,\r\n        }\n\n        scraper_key = None\n\n        if link.endswith(\".pdf\"):\n            scraper_key = \"pdf\"\n        elif \"arxiv.org\" in link:\n            scraper_key = \"arxiv\"\n        else:\n            scraper_key = self.scraper\n\n        scraper_class = SCRAPER_CLASSES.get(scraper_key)\n        if scraper_class is None:\n            raise Exception(\"Scraper not found.\")\n\n        return scraper_class", "kind": "Chunk", "id": "scraper/scraper.py#131.38"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\web_base_loader\\web_base_loader.py_WebBaseLoaderScraper_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\scraper\\web_base_loader\\web_base_loader.py", "file_name": "web_base_loader.py", "file_type": "text/x-python", "category": "test", "tokens": 211, "span_ids": ["WebBaseLoaderScraper.scrape", "WebBaseLoaderScraper", "WebBaseLoaderScraper.__init__"], "start_line": 2, "end_line": 33, "community": null}, "content": "class WebBaseLoaderScraper:\n\n    def __init__(self, link, session=None):\n        self.link = link\n        self.session = session\n\n    def scrape(self) -> str:\n        \"\"\"\r\n        This Python function scrapes content from a webpage using a WebBaseLoader object and returns the\r\n        concatenated page content.\r\n        \r\n        Returns:\r\n          The `scrape` method is returning a string variable named `content` which contains the\r\n        concatenated page content from the documents loaded by the `WebBaseLoader`. If an exception\r\n        occurs during the process, an error message is printed and an empty string is returned.\r\n        \"\"\"\n        try:\n            from langchain_community.document_loaders import WebBaseLoader\n            loader = WebBaseLoader(self.link)\n            loader.requests_kwargs = {\"verify\": False}\n            docs = loader.load()\n            content = \"\"\n\n            for doc in docs:\n                content += doc.page_content\n\n            return content\n\n        except Exception as e:\n            print(\"Error! : \" + str(e))\n            return \"\"", "kind": "Chunk", "id": "web_base_loader/web_base_loader.py#132.31"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\costs.py_tiktoken_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\costs.py", "file_name": "costs.py", "file_type": "text/x-python", "category": "test", "tokens": 234, "span_ids": ["estimate_llm_cost", "imports", "estimate_embedding_cost"], "start_line": 1, "end_line": 26, "community": null}, "content": "import tiktoken\n\n# Per OpenAI Pricing Page: https://openai.com/api/pricing/\r\nENCODING_MODEL = \"o200k_base\"\nINPUT_COST_PER_TOKEN = 0.000005\nOUTPUT_COST_PER_TOKEN = 0.000015\nIMAGE_INFERENCE_COST = 0.003825\nEMBEDDING_COST = 0.02 / 1000000 # Assumes new ada-3-small\r\n\n\n# Cost estimation is via OpenAI libraries and models. May vary for other models\r\ndef estimate_llm_cost(input_content: str, output_content: str) -> float:\n    encoding = tiktoken.get_encoding(ENCODING_MODEL)\n    input_tokens = encoding.encode(input_content)\n    output_tokens = encoding.encode(output_content)\n    input_costs = len(input_tokens) * INPUT_COST_PER_TOKEN\n    output_costs = len(output_tokens) * OUTPUT_COST_PER_TOKEN\n    return input_costs + output_costs\n\n\ndef estimate_embedding_cost(model, docs):\n    encoding = tiktoken.encoding_for_model(model)\n    total_tokens = sum(len(encoding.encode(str(doc))) for doc in docs)\n    return total_tokens * EMBEDDING_COST", "kind": "Chunk", "id": "utils/costs.py#133.25"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\enum.py_from_enum_import_Enum_ReportSource.Hybrid._hybrid_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\enum.py", "file_name": "enum.py", "file_type": "text/x-python", "category": "test", "tokens": 113, "span_ids": ["imports", "ReportSource", "ReportType"], "start_line": 1, "end_line": 19, "community": null}, "content": "from enum import Enum\n\n\nclass ReportType(Enum):\n    ResearchReport = \"research_report\"\n    ResourceReport = \"resource_report\"\n    OutlineReport = \"outline_report\"\n    CustomReport = \"custom_report\"\n    DetailedReport = \"detailed_report\"\n    SubtopicReport = \"subtopic_report\"\n\n\nclass ReportSource(Enum):\n    Web = \"web\"\n    Local = \"local\"\n    LangChainDocuments = \"langchain_documents\"\n    LangChainVectorStore = \"langchain_vectorstore\"\n    Sources = \"sources\"\n    Hybrid = \"hybrid\"", "kind": "Chunk", "id": "utils/enum.py#134.18"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\enum.py_Tone_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\enum.py", "file_name": "enum.py", "file_type": "text/x-python", "category": "test", "tokens": 323, "span_ids": ["Tone"], "start_line": 22, "end_line": 50, "community": null}, "content": "class Tone(Enum):\n    Objective = \"Objective (impartial and unbiased presentation of facts and findings)\"\n    Formal = \"Formal (adheres to academic standards with sophisticated language and structure)\"\n    Analytical = (\r\n        \"Analytical (critical evaluation and detailed examination of data and theories)\"\r\n    )\n    Persuasive = (\r\n        \"Persuasive (convincing the audience of a particular viewpoint or argument)\"\r\n    )\n    Informative = (\r\n        \"Informative (providing clear and comprehensive information on a topic)\"\r\n    )\n    Explanatory = \"Explanatory (clarifying complex concepts and processes)\"\n    Descriptive = (\r\n        \"Descriptive (detailed depiction of phenomena, experiments, or case studies)\"\r\n    )\n    Critical = \"Critical (judging the validity and relevance of the research and its conclusions)\"\n    Comparative = \"Comparative (juxtaposing different theories, data, or methods to highlight differences and similarities)\"\n    Speculative = \"Speculative (exploring hypotheses and potential implications or future research directions)\"\n    Reflective = \"Reflective (considering the research process and personal insights or experiences)\"\n    Narrative = (\r\n        \"Narrative (telling a story to illustrate research findings or methodologies)\"\r\n    )\n    Humorous = \"Humorous (light-hearted and engaging, usually to make the content more relatable)\"\n    Optimistic = \"Optimistic (highlighting positive findings and potential benefits)\"\n    Pessimistic = (\r\n        \"Pessimistic (focusing on limitations, challenges, or negative outcomes)\"\r\n    )", "kind": "Chunk", "id": "utils/enum.py#135.28"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\llm.py__libraries_get_llm.return.GenericLLMProvider_from_p", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\llm.py", "file_name": "llm.py", "file_type": "text/x-python", "category": "test", "tokens": 129, "span_ids": ["get_llm", "docstring"], "start_line": 1, "end_line": 19, "community": null}, "content": "# libraries\r\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom typing import Optional, Any, Dict\n\nfrom colorama import Fore, Style\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain.prompts import PromptTemplate\n\nfrom gpt_researcher.master.prompts import generate_subtopics_prompt\nfrom .costs import estimate_llm_cost\nfrom .validators import Subtopics\n\n\ndef get_llm(llm_provider, **kwargs):\n    from gpt_researcher.llm_provider import GenericLLMProvider\n    return GenericLLMProvider.from_provider(llm_provider, **kwargs)", "kind": "Chunk", "id": "utils/llm.py#136.18"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\llm.py_create_chat_completion_create_chat_completion.raise_RuntimeError_f_Fail", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\llm.py", "file_name": "llm.py", "file_type": "text/x-python", "category": "test", "tokens": 471, "span_ids": ["create_chat_completion"], "start_line": 22, "end_line": 70, "community": null}, "content": "async def create_chat_completion(\r\n        messages: list,  # type: ignore\r\n        model: Optional[str] = None,\r\n        temperature: float = 0.4,\r\n        max_tokens: Optional[int] = 4000,\r\n        llm_provider: Optional[str] = None,\r\n        stream: Optional[bool] = False,\r\n        websocket: Any | None = None,\r\n        llm_kwargs: Dict[str, Any] | None = None,\r\n        cost_callback: callable = None\r\n) -> str:\n    \"\"\"Create a chat completion using the OpenAI API\r\n    Args:\r\n        messages (list[dict[str, str]]): The messages to send to the chat completion\r\n        model (str, optional): The model to use. Defaults to None.\r\n        temperature (float, optional): The temperature to use. Defaults to 0.9.\r\n        max_tokens (int, optional): The max tokens to use. Defaults to None.\r\n        stream (bool, optional): Whether to stream the response. Defaults to False.\r\n        llm_provider (str, optional): The LLM Provider to use.\r\n        webocket (WebSocket): The websocket used in the currect request,\r\n        cost_callback: Callback function for updating cost\r\n    Returns:\r\n        str: The response from the chat completion\r\n    \"\"\"\n    # validate input\r\n    if model is None:\n        raise ValueError(\"Model cannot be None\")\n    if max_tokens is not None and max_tokens > 8001:\n        raise ValueError(\r\n            f\"Max tokens cannot be more than 8001, but got {max_tokens}\")\n\n    # Get the provider from supported providers\r\n    provider = get_llm(llm_provider, model=model, temperature=temperature, max_tokens=max_tokens, **(llm_kwargs or {}))\n\n    response = \"\"\n    # create response\r\n    for _ in range(10):  # maximum of 10 attempts\r\n        response = await provider.get_chat_response(\r\n            messages, stream, websocket\r\n        )\n\n        if cost_callback:\n            llm_costs = estimate_llm_cost(str(messages), response)\n            cost_callback(llm_costs)\n\n        return response\n\n    logging.error(f\"Failed to get response from {llm_provider} API\")\n    raise RuntimeError(f\"Failed to get response from {llm_provider} API\")", "kind": "Chunk", "id": "utils/llm.py#137.48"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\llm.py_construct_subtopics_construct_subtopics.try_.except_Exception_as_e_.return.subtopics", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\llm.py", "file_name": "llm.py", "file_type": "text/x-python", "category": "test", "tokens": 254, "span_ids": ["construct_subtopics"], "start_line": 73, "end_line": 105, "community": null}, "content": "async def construct_subtopics(task: str, data: str, config, subtopics: list = []) -> list:\n    try:\n        parser = PydanticOutputParser(pydantic_object=Subtopics)\n\n        prompt = PromptTemplate(\r\n            template=generate_subtopics_prompt(),\r\n            input_variables=[\"task\", \"data\", \"subtopics\", \"max_subtopics\"],\r\n            partial_variables={\r\n                \"format_instructions\": parser.get_format_instructions()},\r\n        )\n\n        print(f\"\\n\ud83e\udd16 Calling {config.smart_llm_model}...\\n\")\n\n        temperature = config.temperature\n        # temperature = 0 # Note: temperature throughout the code base is currently set to Zero\r\n        provider = get_llm(config.llm_provider, model=config.smart_llm_model, temperature=temperature, max_tokens=config.smart_token_limit, **config.llm_kwargs)\n        model = provider.llm\n\n\n        chain = prompt | model | parser\n\n        output = chain.invoke({\r\n            \"task\": task,\r\n            \"data\": data,\r\n            \"subtopics\": subtopics,\r\n            \"max_subtopics\": config.max_subtopics\r\n        })\n\n        return output\n\n    except Exception as e:\n        print(\"Exception in parsing subtopics : \", e)\n        return subtopics", "kind": "Chunk", "id": "utils/llm.py#138.32"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\validators.py__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\gpt_researcher\\utils\\validators.py", "file_name": "validators.py", "file_type": "text/x-python", "category": "test", "tokens": 53, "span_ids": ["imports", "Subtopic", "Subtopics"], "start_line": 1, "end_line": 10, "community": null}, "content": "from typing import List\n\nfrom pydantic import BaseModel, Field\n\nclass Subtopic(BaseModel):\n    task: str = Field(description=\"Task name\", min_length=1)\n\nclass Subtopics(BaseModel):\n    subtopics: List[Subtopic] = []", "kind": "Chunk", "id": "utils/validators.py#139.9"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\main.py__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\main.py", "file_name": "main.py", "file_type": "text/x-python", "category": "test", "tokens": 51, "span_ids": ["imports"], "start_line": 1, "end_line": 11, "community": null}, "content": "from dotenv import load_dotenv\n\nload_dotenv()\n\nfrom backend.server import app\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)", "kind": "Chunk", "id": "gpt-researcher/main.py#140.10"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\__init__.py__multi_agents___init___p___all__._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 114, "span_ids": ["docstring"], "start_line": 1, "end_line": 27, "community": null}, "content": "# multi_agents/__init__.py\r\n\nfrom .agents import (\r\n    ResearchAgent,\r\n    WriterAgent,\r\n    PublisherAgent,\r\n    ReviserAgent,\r\n    ReviewerAgent,\r\n    EditorAgent,\r\n    ChiefEditorAgent\r\n)\nfrom .memory import (\r\n    DraftState,\r\n    ResearchState\r\n)\n\n__all__ = [\r\n    \"ResearchAgent\",\r\n    \"WriterAgent\",\r\n    \"PublisherAgent\",\r\n    \"ReviserAgent\",\r\n    \"ReviewerAgent\",\r\n    \"EditorAgent\",\r\n    \"ChiefEditorAgent\",\r\n    \"DraftState\",\r\n    \"ResearchState\"\r\n]", "kind": "Chunk", "id": "multi_agents/__init__.py#141.26"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agent.py_from_multi_agents_agents__graph_2.graph_compile_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agent.py", "file_name": "agent.py", "file_type": "text/x-python", "category": "test", "tokens": 147, "span_ids": ["imports"], "start_line": 1, "end_line": 16, "community": null}, "content": "from multi_agents.agents import ChiefEditorAgent\n\nchief_editor = ChiefEditorAgent({\r\n  \"query\": \"Is AI in a hype cycle?\",\r\n  \"max_sections\": 3,\r\n  \"follow_guidelines\": False,\r\n  \"model\": \"gpt-4o\",\r\n  \"guidelines\": [\r\n    \"The report MUST be written in APA format\",\r\n    \"Each sub section MUST include supporting sources using hyperlinks. If none exist, erase the sub section or rewrite it to be a part of the previous section\",\r\n    \"The report MUST be written in spanish\"\r\n  ],\r\n  \"verbose\": False\r\n}, websocket=None, stream_output=None)\ngraph = chief_editor.init_research_team()\ngraph = graph.compile()", "kind": "Chunk", "id": "multi_agents/agent.py#142.15"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\__init__.py_ResearchAgent___all__._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 127, "span_ids": ["imports"], "start_line": 1, "end_line": 21, "community": null}, "content": "from .researcher import ResearchAgent\nfrom .writer import WriterAgent\nfrom .publisher import PublisherAgent\nfrom .reviser import ReviserAgent\nfrom .reviewer import ReviewerAgent\nfrom .editor import EditorAgent\nfrom .human import HumanAgent\n\n# Below import should remain last since it imports all of the above\r\nfrom .master import ChiefEditorAgent\n\n__all__ = [\r\n    \"ChiefEditorAgent\",\r\n    \"ResearchAgent\",\r\n    \"WriterAgent\",\r\n    \"EditorAgent\",\r\n    \"PublisherAgent\",\r\n    \"ReviserAgent\",\r\n    \"ReviewerAgent\",\r\n    \"HumanAgent\"\r\n]", "kind": "Chunk", "id": "agents/__init__.py#143.20"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\editor.py_from_datetime_import_date_EditorAgent.plan_research.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\editor.py", "file_name": "editor.py", "file_type": "text/x-python", "category": "test", "tokens": 583, "span_ids": ["imports", "EditorAgent.__init__", "EditorAgent", "EditorAgent.plan_research"], "start_line": 1, "end_line": 70, "community": null}, "content": "from datetime import datetime\nfrom .utils.views import print_agent_output\nfrom .utils.llms import call_model\nfrom langgraph.graph import StateGraph, END\nimport asyncio\nimport json\n\nfrom ..memory.draft import DraftState\nfrom . import ResearchAgent, ReviewerAgent, ReviserAgent\n\n\nclass EditorAgent:\n    def __init__(self, websocket=None, stream_output=None, headers=None):\n        self.websocket = websocket\n        self.stream_output = stream_output\n        self.headers = headers or {}\n\n    async def plan_research(self, research_state: dict):\n        \"\"\"\r\n        Curate relevant sources for a query\r\n        :param summary_report:\r\n        :return:\r\n        :param total_sub_headers:\r\n        :return:\r\n        \"\"\"\n\n        initial_research = research_state.get(\"initial_research\")\n        task = research_state.get(\"task\")\n        include_human_feedback = task.get(\"include_human_feedback\")\n        human_feedback = research_state.get(\"human_feedback\")\n        max_sections = task.get(\"max_sections\")\n\n        prompt = [\r\n            {\r\n                \"role\": \"system\",\r\n                \"content\": \"You are a research editor. Your goal is to oversee the research project\"\r\n                \" from inception to completion. Your main task is to plan the article section \"\r\n                \"layout based on an initial research summary.\\n \",\r\n            },\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": f\"\"\"Today's date is {datetime.now().strftime('%d/%m/%Y')}\r\n                                  Research summary report: '{initial_research}'\r\n                                  {f'Human feedback: {human_feedback}. You must plan the sections based on the human feedback.'\r\n            if include_human_feedback and human_feedback and human_feedback != 'no' else ''}\r\n                                  \\nYour task is to generate an outline of sections headers for the research project\r\n                                  based on the research summary report above.\r\n                                  You must generate a maximum of {max_sections} section headers.\r\n                                  You must focus ONLY on related research topics for subheaders and do NOT include introduction, conclusion and references.\r\n                                  You must return nothing but a JSON with the fields 'title' (str) and \r\n                                  'sections' (maximum {max_sections} section headers) with the following structure:\r\n                                  '{{title: string research title, date: today's date, \r\n                                  sections: ['section header 1', 'section header 2', 'section header 3' ...]}}.\"\"\",\r\n            },\r\n        ]\n\n        print_agent_output(\r\n            f\"Planning an outline layout based on initial research...\", agent=\"EDITOR\"\r\n        )\n        plan = await call_model(\r\n            prompt=prompt,\r\n            model=task.get(\"model\"),\r\n            response_format=\"json\",\r\n        )\n\n        return {\r\n            \"title\": plan.get(\"title\"),\r\n            \"date\": plan.get(\"date\"),\r\n            \"sections\": plan.get(\"sections\"),\r\n        }", "kind": "Chunk", "id": "agents/editor.py#144.69"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\editor.py_EditorAgent.run_parallel_research_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\editor.py", "file_name": "editor.py", "file_type": "text/x-python", "category": "test", "tokens": 452, "span_ids": ["EditorAgent.run_parallel_research"], "start_line": 72, "end_line": 127, "community": null}, "content": "class EditorAgent:\n\n    async def run_parallel_research(self, research_state: dict):\n        research_agent = ResearchAgent(self.websocket, self.stream_output, self.headers)\n        reviewer_agent = ReviewerAgent(self.websocket, self.stream_output, self.headers)\n        reviser_agent = ReviserAgent(self.websocket, self.stream_output, self.headers)\n        queries = research_state.get(\"sections\")\n        title = research_state.get(\"title\")\n        human_feedback = research_state.get(\"human_feedback\")\n        workflow = StateGraph(DraftState)\n\n        workflow.add_node(\"researcher\", research_agent.run_depth_research)\n        workflow.add_node(\"reviewer\", reviewer_agent.run)\n        workflow.add_node(\"reviser\", reviser_agent.run)\n\n        # set up edges researcher->reviewer->reviser->reviewer...\r\n        workflow.set_entry_point(\"researcher\")\n        workflow.add_edge(\"researcher\", \"reviewer\")\n        workflow.add_edge(\"reviser\", \"reviewer\")\n        workflow.add_conditional_edges(\r\n            \"reviewer\",\r\n            (lambda draft: \"accept\" if draft[\"review\"] is None else \"revise\"),\r\n            {\"accept\": END, \"revise\": \"reviser\"},\r\n        )\n\n        chain = workflow.compile()\n\n        # Execute the graph for each query in parallel\r\n        if self.websocket and self.stream_output:\n            await self.stream_output(\r\n                \"logs\",\r\n                \"parallel_research\",\r\n                f\"Running parallel research for the following queries: {queries}\",\r\n                self.websocket,\r\n            )\n        else:\n            print_agent_output(\r\n                f\"Running the following research tasks in parallel: {queries}...\",\r\n                agent=\"EDITOR\",\r\n            )\n\n        final_drafts = [\r\n            chain.ainvoke(\r\n                {\r\n                    \"task\": research_state.get(\"task\"),\r\n                    \"topic\": query,  # + (f\". Also: {human_feedback}\" if human_feedback is not None else \"\"),\r\n                    \"title\": title,\r\n                    \"headers\": self.headers,\r\n                }\r\n            )\r\n            for query in queries\r\n        ]\n        research_results = [\r\n            result[\"draft\"] for result in await asyncio.gather(*final_drafts)\r\n        ]\n\n        return {\"research_data\": research_results}", "kind": "Chunk", "id": "agents/editor.py#145.55"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\human.py_json_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\human.py", "file_name": "human.py", "file_type": "text/x-python", "category": "test", "tokens": 387, "span_ids": ["imports", "HumanAgent", "HumanAgent.__init__", "HumanAgent.review_plan"], "start_line": 1, "end_line": 52, "community": null}, "content": "import json\n\n\nclass HumanAgent:\n    def __init__(self, websocket=None, stream_output=None, headers=None):\n        self.websocket = websocket\n        self.stream_output = stream_output\n        self.headers = headers or {}\n\n    async def review_plan(self, research_state: dict):\n        print(f\"HumanAgent websocket: {self.websocket}\")\n        print(f\"HumanAgent stream_output: {self.stream_output}\")\n        task = research_state.get(\"task\")\n        layout = research_state.get(\"sections\")\n\n        user_feedback = None\n\n        if task.get(\"include_human_feedback\"):\r\n            # Stream response to the user if a websocket is provided (such as from web app)\r\n            if self.websocket and self.stream_output:\n                try:\n                    await self.stream_output(\r\n                        \"human_feedback\",\r\n                        \"request\",\r\n                        f\"Any feedback on this plan of topics to research? {layout}? If not, please reply with 'no'.\",\r\n                        self.websocket,\r\n                    )\n                    response = await self.websocket.receive_text()\n                    print(f\"Received response: {response}\", flush=True)\n                    response_data = json.loads(response)\n                    if response_data.get(\"type\") == \"human_feedback\":\n                        user_feedback = response_data.get(\"content\")\n                    else:\n                        print(\r\n                            f\"Unexpected response type: {response_data.get('type')}\",\r\n                            flush=True,\r\n                        )\n                except Exception as e:\n                    print(f\"Error receiving human feedback: {e}\", flush=True)\n            # Otherwise, prompt the user for feedback in the console\r\n            else:\n                user_feedback = input(\r\n                    f\"Any feedback on this plan? {layout}? If not, please reply with 'no'.\\n>> \"\r\n                )\n\n        if user_feedback and \"no\" in user_feedback.strip().lower():\n            user_feedback = None\n\n        print(f\"User feedback before return: {user_feedback}\")\n\n        return {\"human_feedback\": user_feedback}", "kind": "Chunk", "id": "agents/human.py#146.51"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\master.py_os_ChiefEditorAgent.__init__.os_makedirs_self_output_d", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\master.py", "file_name": "master.py", "file_type": "text/x-python", "category": "test", "tokens": 213, "span_ids": ["imports", "ChiefEditorAgent", "ChiefEditorAgent.__init__"], "start_line": 1, "end_line": 29, "community": null}, "content": "import os\nimport time\nfrom langgraph.graph import StateGraph, END\n#from langgraph.checkpoint.memory import MemorySaver\r\nimport datetime\nfrom .utils.views import print_agent_output\nfrom ..memory.research import ResearchState\nfrom .utils.utils import sanitize_filename\n\n\n# Import agent classes\r\nfrom . import \\\r\n    WriterAgent, \\\r\n    EditorAgent, \\\r\n    PublisherAgent, \\\r\n    ResearchAgent, \\\r\n    HumanAgent\n\n\nclass ChiefEditorAgent:\n    def __init__(self, task: dict, websocket=None, stream_output=None, tone=None, headers=None):\n        self.task_id = int(time.time()) # Currently time based, but can be any unique identifier\r\n        self.output_dir = \"./outputs/\" + sanitize_filename(f\"run_{self.task_id}_{task.get('query')[0:40]}\")\n        self.task = task\n        self.websocket = websocket\n        self.stream_output = stream_output\n        self.headers = headers or {}\n        self.tone = tone\n        os.makedirs(self.output_dir, exist_ok=True)", "kind": "Chunk", "id": "agents/master.py#147.28"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\master.py_ChiefEditorAgent.init_research_team_ChiefEditorAgent.init_research_team.return.workflow", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\master.py", "file_name": "master.py", "file_type": "text/x-python", "category": "test", "tokens": 340, "span_ids": ["ChiefEditorAgent.init_research_team"], "start_line": 31, "end_line": 64, "community": null}, "content": "class ChiefEditorAgent:\n\n    def init_research_team(self):\n        # Initialize agents\r\n        writer_agent = WriterAgent(self.websocket, self.stream_output, self.headers)\n        editor_agent = EditorAgent(self.websocket, self.stream_output, self.headers)\n        research_agent = ResearchAgent(self.websocket, self.stream_output, self.tone, self.headers)\n        publisher_agent = PublisherAgent(self.output_dir, self.websocket, self.stream_output, self.headers)\n        human_agent = HumanAgent(self.websocket, self.stream_output, self.headers)\n\n        # Define a Langchain StateGraph with the ResearchState\r\n        workflow = StateGraph(ResearchState)\n\n        # Add nodes for each agent\r\n        workflow.add_node(\"browser\", research_agent.run_initial_research)\n        workflow.add_node(\"planner\", editor_agent.plan_research)\n        workflow.add_node(\"researcher\", editor_agent.run_parallel_research)\n        workflow.add_node(\"writer\", writer_agent.run)\n        workflow.add_node(\"publisher\", publisher_agent.run)\n        workflow.add_node(\"human\", human_agent.review_plan)\n\n        workflow.add_edge('browser', 'planner')\n        workflow.add_edge('planner', 'human')\n        workflow.add_edge('researcher', 'writer')\n        workflow.add_edge('writer', 'publisher')\n\n        # set up start and end nodes\r\n        workflow.set_entry_point(\"browser\")\n        workflow.add_edge('publisher', END)\n\n        # Add human in the loop\r\n        workflow.add_conditional_edges('human',\r\n                                       (lambda review: \"accept\" if review['human_feedback'] is None else \"revise\"),\r\n                                       {\"accept\": \"researcher\", \"revise\": \"planner\"})\n\n        return workflow", "kind": "Chunk", "id": "agents/master.py#148.33"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\master.py_ChiefEditorAgent.run_research_task_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\master.py", "file_name": "master.py", "file_type": "text/x-python", "category": "test", "tokens": 169, "span_ids": ["ChiefEditorAgent.run_research_task"], "start_line": 66, "end_line": 87, "community": null}, "content": "class ChiefEditorAgent:\n\n    async def run_research_task(self, task_id=None):\n        research_team = self.init_research_team()\n\n        # compile the graph\r\n        #memory = MemorySaver()\r\n        chain = research_team.compile()\n        if self.websocket and self.stream_output:\n            await self.stream_output(\"logs\", \"starting_research\", f\"Starting the research process for query '{self.task.get('query')}'...\", self.websocket)\n        else:\n            print_agent_output(f\"Starting the research process for query '{self.task.get('query')}'...\", \"MASTER\")\n\n        config = {\r\n            \"configurable\": {\r\n                \"thread_id\": task_id,\r\n                \"thread_ts\": datetime.datetime.utcnow()\r\n            }\r\n        }\n\n        result = await chain.ainvoke({\"task\": self.task}, config=config)\n\n        return result", "kind": "Chunk", "id": "agents/master.py#149.21"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\publisher.py_write_md_to_pdf_PublisherAgent.generate_layout.return.layout", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\publisher.py", "file_name": "publisher.py", "file_type": "text/x-python", "category": "test", "tokens": 314, "span_ids": ["PublisherAgent.generate_layout", "imports", "PublisherAgent.publish_research_report", "PublisherAgent.__init__", "PublisherAgent"], "start_line": 1, "end_line": 45, "community": null}, "content": "from .utils.file_formats import \\\r\n    write_md_to_pdf, \\\r\n    write_md_to_word, \\\r\n    write_text_to_md\n\nfrom .utils.views import print_agent_output\n\n\nclass PublisherAgent:\n    def __init__(self, output_dir: str, websocket=None, stream_output=None, headers=None):\n        self.websocket = websocket\n        self.stream_output = stream_output\n        self.output_dir = output_dir\n        self.headers = headers or {}\n\n    async def publish_research_report(self, research_state: dict, publish_formats: dict):\n        layout = self.generate_layout(research_state)\n        await self.write_report_by_formats(layout, publish_formats)\n\n        return layout\n\n    def generate_layout(self, research_state: dict):\n        sections = '\\n\\n'.join(f\"{value}\"\r\n                                 for subheader in research_state.get(\"research_data\")\r\n                                 for key, value in subheader.items())\n        references = '\\n'.join(f\"{reference}\" for reference in research_state.get(\"sources\"))\n        headers = research_state.get(\"headers\")\n        layout = f\"\"\"# {headers.get('title')}\r\n#### {headers.get(\"date\")}: {research_state.get('date')}\r\n\r\n## {headers.get(\"introduction\")}\r\n{research_state.get('introduction')}\r\n\r\n## {headers.get(\"table_of_contents\")}\r\n{research_state.get('table_of_contents')}\r\n\r\n{sections}\r\n\r\n## {headers.get(\"conclusion\")}\r\n{research_state.get('conclusion')}\r\n\r\n## {headers.get(\"references\")}\r\n{references}\r\n\"\"\"\n        return layout", "kind": "Chunk", "id": "agents/publisher.py#150.44"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\publisher.py_PublisherAgent.write_report_by_formats_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\publisher.py", "file_name": "publisher.py", "file_type": "text/x-python", "category": "test", "tokens": 203, "span_ids": ["PublisherAgent.run", "PublisherAgent.write_report_by_formats"], "start_line": 47, "end_line": 64, "community": null}, "content": "class PublisherAgent:\n\n    async def write_report_by_formats(self, layout:str, publish_formats: dict):\n        if publish_formats.get(\"pdf\"):\n            await write_md_to_pdf(layout, self.output_dir)\n        if publish_formats.get(\"docx\"):\n            await write_md_to_word(layout, self.output_dir)\n        if publish_formats.get(\"markdown\"):\n            await write_text_to_md(layout, self.output_dir)\n\n    async def run(self, research_state: dict):\n        task = research_state.get(\"task\")\n        publish_formats = task.get(\"publish_formats\")\n        if self.websocket and self.stream_output:\n            await self.stream_output(\"logs\", \"publishing\", f\"Publishing final research report based on retrieved data...\", self.websocket)\n        else:\n            print_agent_output(output=\"Publishing final research report based on retrieved data...\", agent=\"PUBLISHER\")\n        final_research_report = await self.publish_research_report(research_state, publish_formats)\n        return {\"report\": final_research_report}", "kind": "Chunk", "id": "agents/publisher.py#151.17"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\researcher.py_from_gpt_researcher_impor_ResearchAgent.research.return.report", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\researcher.py", "file_name": "researcher.py", "file_type": "text/x-python", "category": "test", "tokens": 204, "span_ids": ["imports", "ResearchAgent", "ResearchAgent.research", "ResearchAgent.__init__"], "start_line": 1, "end_line": 23, "community": null}, "content": "from gpt_researcher import GPTResearcher\nfrom colorama import Fore, Style\nfrom .utils.views import print_agent_output\n\n\nclass ResearchAgent:\n    def __init__(self, websocket=None, stream_output=None, tone=None, headers=None):\n        self.websocket = websocket\n        self.stream_output = stream_output\n        self.headers = headers or {}\n        self.tone = tone\n\n    async def research(self, query: str, research_report: str = \"research_report\",\r\n                       parent_query: str = \"\", verbose=True, source=\"web\", tone=None, headers=None):\n        # Initialize the researcher\r\n        researcher = GPTResearcher(query=query, report_type=research_report, parent_query=parent_query,\r\n                                   verbose=verbose, report_source=source, tone=tone, websocket=self.websocket, headers=self.headers)\n        # Conduct research on the given query\r\n        await researcher.conduct_research()\n        # Write the report\r\n        report = await researcher.write_report()\n\n        return report", "kind": "Chunk", "id": "agents/researcher.py#152.22"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\researcher.py_ResearchAgent.run_subtopic_research_ResearchAgent.run_subtopic_research.return._subtopic_report_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\researcher.py", "file_name": "researcher.py", "file_type": "text/x-python", "category": "test", "tokens": 122, "span_ids": ["ResearchAgent.run_subtopic_research"], "start_line": 25, "end_line": 32, "community": null}, "content": "class ResearchAgent:\n\n    async def run_subtopic_research(self, parent_query: str, subtopic: str, verbose: bool = True, source=\"web\", headers=None):\n        try:\n            report = await self.research(parent_query=parent_query, query=subtopic,\r\n                                         research_report=\"subtopic_report\", verbose=verbose, source=source, tone=self.tone, headers=None)\n        except Exception as e:\n            print(f\"{Fore.RED}Error in researching topic {subtopic}: {e}{Style.RESET_ALL}\")\n            report = None\n        return {subtopic: report}", "kind": "Chunk", "id": "agents/researcher.py#153.7"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\researcher.py_ResearchAgent.run_initial_research_ResearchAgent.run_initial_research.return._task_task_initial_r", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\researcher.py", "file_name": "researcher.py", "file_type": "text/x-python", "category": "test", "tokens": 150, "span_ids": ["ResearchAgent.run_initial_research"], "start_line": 34, "end_line": 44, "community": null}, "content": "class ResearchAgent:\n\n    async def run_initial_research(self, research_state: dict):\n        task = research_state.get(\"task\")\n        query = task.get(\"query\")\n        source = task.get(\"source\", \"web\")\n\n        if self.websocket and self.stream_output:\n            await self.stream_output(\"logs\", \"initial_research\", f\"Running initial research on the following query: {query}\", self.websocket)\n        else:\n            print_agent_output(f\"Running initial research on the following query: {query}\", agent=\"RESEARCHER\")\n        return {\"task\": task, \"initial_research\": await self.research(query=query, verbose=task.get(\"verbose\"),\r\n                                                                      source=source, tone=self.tone, headers=self.headers)}", "kind": "Chunk", "id": "agents/researcher.py#154.10"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\researcher.py_ResearchAgent.run_depth_research_ResearchAgent.run_depth_research.return._draft_research_draft_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\researcher.py", "file_name": "researcher.py", "file_type": "text/x-python", "category": "test", "tokens": 175, "span_ids": ["ResearchAgent.run_depth_research"], "start_line": 46, "end_line": 58, "community": null}, "content": "class ResearchAgent:\n\n    async def run_depth_research(self, draft_state: dict):\n        task = draft_state.get(\"task\")\n        topic = draft_state.get(\"topic\")\n        parent_query = task.get(\"query\")\n        source = task.get(\"source\", \"web\")\n        verbose = task.get(\"verbose\")\n        if self.websocket and self.stream_output:\n            await self.stream_output(\"logs\", \"depth_research\", f\"Running in depth research on the following report topic: {topic}\", self.websocket)\n        else:\n            print_agent_output(f\"Running in depth research on the following report topic: {topic}\", agent=\"RESEARCHER\")\n        research_draft = await self.run_subtopic_research(parent_query=parent_query, subtopic=topic,\r\n                                                          verbose=verbose, source=source, headers=self.headers)\n        return {\"draft\": research_draft}", "kind": "Chunk", "id": "agents/researcher.py#155.12"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\reviewer.py_print_agent_output_ReviewerAgent.review_draft.return.response", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\reviewer.py", "file_name": "reviewer.py", "file_type": "text/x-python", "category": "test", "tokens": 489, "span_ids": ["ReviewerAgent", "imports", "ReviewerAgent.review_draft", "ReviewerAgent.__init__"], "start_line": 1, "end_line": 61, "community": null}, "content": "from .utils.views import print_agent_output\nfrom .utils.llms import call_model\n\nTEMPLATE = \"\"\"You are an expert research article reviewer. \\\r\nYour goal is to review research drafts and provide feedback to the reviser only based on specific guidelines. \\\r\n\"\"\"\n\n\nclass ReviewerAgent:\n    def __init__(self, websocket=None, stream_output=None, headers=None):\n        self.websocket = websocket\n        self.stream_output = stream_output\n        self.headers = headers or {}\n\n    async def review_draft(self, draft_state: dict):\n        \"\"\"\r\n        Review a draft article\r\n        :param draft_state:\r\n        :return:\r\n        \"\"\"\n        task = draft_state.get(\"task\")\n        guidelines = \"- \".join(guideline for guideline in task.get(\"guidelines\"))\n        revision_notes = draft_state.get(\"revision_notes\")\n\n        revise_prompt = f\"\"\"The reviser has already revised the draft based on your previous review notes with the following feedback:\r\n{revision_notes}\\n\r\nPlease provide additional feedback ONLY if critical since the reviser has already made changes based on your previous feedback.\r\nIf you think the article is sufficient or that non critical revisions are required, please aim to return None.\r\n\"\"\"\n\n        review_prompt = f\"\"\"You have been tasked with reviewing the draft which was written by a non-expert based on specific guidelines.\r\nPlease accept the draft if it is good enough to publish, or send it for revision, along with your notes to guide the revision.\r\nIf not all of the guideline criteria are met, you should send appropriate revision notes.\r\nIf the draft meets all the guidelines, please return None.\r\n{revise_prompt if revision_notes else \"\"}\r\n\r\nGuidelines: {guidelines}\\nDraft: {draft_state.get(\"draft\")}\\n\r\n\"\"\"\n        prompt = [\r\n            {\"role\": \"system\", \"content\": TEMPLATE},\r\n            {\"role\": \"user\", \"content\": review_prompt},\r\n        ]\n\n        response = await call_model(prompt, model=task.get(\"model\"))\n\n        if task.get(\"verbose\"):\n            if self.websocket and self.stream_output:\n                await self.stream_output(\r\n                    \"logs\",\r\n                    \"review_feedback\",\r\n                    f\"Review feedback is: {response}...\",\r\n                    self.websocket,\r\n                )\n            else:\n                print_agent_output(\r\n                    f\"Review feedback is: {response}...\", agent=\"REVIEWER\"\r\n                )\n\n        if \"None\" in response:\n            return None\n        return response", "kind": "Chunk", "id": "agents/reviewer.py#156.60"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\reviewer.py_ReviewerAgent.run_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\reviewer.py", "file_name": "reviewer.py", "file_type": "text/x-python", "category": "test", "tokens": 142, "span_ids": ["ReviewerAgent.run"], "start_line": 63, "end_line": 80, "community": null}, "content": "class ReviewerAgent:\n\n    async def run(self, draft_state: dict):\n        task = draft_state.get(\"task\")\n        guidelines = task.get(\"guidelines\")\n        to_follow_guidelines = task.get(\"follow_guidelines\")\n        review = None\n        if to_follow_guidelines:\n            print_agent_output(f\"Reviewing draft...\", agent=\"REVIEWER\")\n\n            if task.get(\"verbose\"):\n                print_agent_output(\r\n                    f\"Following guidelines {guidelines}...\", agent=\"REVIEWER\"\r\n                )\n\n            review = await self.review_draft(draft_state)\n        else:\n            print_agent_output(f\"Ignoring guidelines...\", agent=\"REVIEWER\")\n        return {\"review\": review}", "kind": "Chunk", "id": "agents/reviewer.py#157.17"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\reviser.py_print_agent_output_ReviserAgent.revise_draft.return.response", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\reviser.py", "file_name": "reviser.py", "file_type": "text/x-python", "category": "test", "tokens": 363, "span_ids": ["imports", "ReviserAgent", "ReviserAgent.revise_draft", "ReviserAgent.__init__"], "start_line": 1, "end_line": 52, "community": null}, "content": "from .utils.views import print_agent_output\nfrom .utils.llms import call_model\nimport json\n\nsample_revision_notes = \"\"\"\r\n{\r\n  \"draft\": { \r\n    draft title: The revised draft that you are submitting for review \r\n  },\r\n  \"revision_notes\": Your message to the reviewer about the changes you made to the draft based on their feedback\r\n}\r\n\"\"\"\n\n\nclass ReviserAgent:\n    def __init__(self, websocket=None, stream_output=None, headers=None):\n        self.websocket = websocket\n        self.stream_output = stream_output\n        self.headers = headers or {}\n\n    async def revise_draft(self, draft_state: dict):\n        \"\"\"\r\n        Review a draft article\r\n        :param draft_state:\r\n        :return:\r\n        \"\"\"\n        review = draft_state.get(\"review\")\n        task = draft_state.get(\"task\")\n        draft_report = draft_state.get(\"draft\")\n        prompt = [\r\n            {\r\n                \"role\": \"system\",\r\n                \"content\": \"You are an expert writer. Your goal is to revise drafts based on reviewer notes.\",\r\n            },\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": f\"\"\"Draft:\\n{draft_report}\" + \"Reviewer's notes:\\n{review}\\n\\n\r\nYou have been tasked by your reviewer with revising the following draft, which was written by a non-expert.\r\nIf you decide to follow the reviewer's notes, please write a new draft and make sure to address all of the points they raised.\r\nPlease keep all other aspects of the draft the same.\r\nYou MUST return nothing but a JSON in the following format:\r\n{sample_revision_notes}\r\n\"\"\",\r\n            },\r\n        ]\n\n        response = await call_model(\r\n            prompt,\r\n            model=task.get(\"model\"),\r\n            response_format=\"json\",\r\n        )\n        return response", "kind": "Chunk", "id": "agents/reviser.py#158.51"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\reviser.py_ReviserAgent.run_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\reviser.py", "file_name": "reviser.py", "file_type": "text/x-python", "category": "test", "tokens": 159, "span_ids": ["ReviserAgent.run"], "start_line": 54, "end_line": 75, "community": null}, "content": "class ReviserAgent:\n\n    async def run(self, draft_state: dict):\n        print_agent_output(f\"Rewriting draft based on feedback...\", agent=\"REVISOR\")\n        revision = await self.revise_draft(draft_state)\n\n        if draft_state.get(\"task\").get(\"verbose\"):\n            if self.websocket and self.stream_output:\n                await self.stream_output(\r\n                    \"logs\",\r\n                    \"revision_notes\",\r\n                    f\"Revision notes: {revision.get('revision_notes')}\",\r\n                    self.websocket,\r\n                )\n            else:\n                print_agent_output(\r\n                    f\"Revision notes: {revision.get('revision_notes')}\", agent=\"REVISOR\"\r\n                )\n\n        return {\r\n            \"draft\": revision.get(\"draft\"),\r\n            \"revision_notes\": revision.get(\"revision_notes\"),\r\n        }", "kind": "Chunk", "id": "agents/reviser.py#159.21"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\file_formats.py_aiofiles_write_to_file.async_with_aiofiles_open_.await_file_write_text_utf", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\file_formats.py", "file_name": "file_formats.py", "file_type": "text/x-python", "category": "test", "tokens": 135, "span_ids": ["imports", "write_to_file"], "start_line": 1, "end_line": 18, "community": null}, "content": "import aiofiles\nimport urllib\nimport uuid\nimport mistune\n\n\nasync def write_to_file(filename: str, text: str) -> None:\n    \"\"\"Asynchronously write text to a file in UTF-8 encoding.\r\n\r\n    Args:\r\n        filename (str): The filename to write to.\r\n        text (str): The text to write.\r\n    \"\"\"\n    # Convert text to UTF-8, replacing any problematic characters\r\n    text_utf8 = text.encode('utf-8', errors='replace').decode('utf-8')\n\n    async with aiofiles.open(filename, \"w\", encoding='utf-8') as file:\n        await file.write(text_utf8)", "kind": "Chunk", "id": "utils/file_formats.py#160.17"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\file_formats.py_write_text_to_md_write_text_to_md.return.file_path", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\file_formats.py", "file_name": "file_formats.py", "file_type": "text/x-python", "category": "test", "tokens": 110, "span_ids": ["write_text_to_md"], "start_line": 21, "end_line": 34, "community": null}, "content": "async def write_text_to_md(text: str, path: str) -> str:\n    \"\"\"Writes text to a Markdown file and returns the file path.\r\n\r\n    Args:\r\n        text (str): Text to write to the Markdown file.\r\n\r\n    Returns:\r\n        str: The file path of the generated Markdown file.\r\n    \"\"\"\n    task = uuid.uuid4().hex\n    file_path = f\"{path}/{task}.md\"\n    await write_to_file(file_path, text)\n    print(f\"Report written to {file_path}\")\n    return file_path", "kind": "Chunk", "id": "utils/file_formats.py#161.13"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\file_formats.py_write_md_to_pdf_write_md_to_pdf.return.encoded_file_path", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\file_formats.py", "file_name": "file_formats.py", "file_type": "text/x-python", "category": "test", "tokens": 216, "span_ids": ["write_md_to_pdf"], "start_line": 37, "end_line": 63, "community": null}, "content": "async def write_md_to_pdf(text: str, path: str) -> str:\n    \"\"\"Converts Markdown text to a PDF file and returns the file path.\r\n\r\n    Args:\r\n        text (str): Markdown text to convert.\r\n\r\n    Returns:\r\n        str: The encoded file path of the generated PDF.\r\n    \"\"\"\n    task = uuid.uuid4().hex\n    file_path = f\"{path}/{task}.pdf\"\n\n    try:\n        # Moved imports to inner function to avoid known import errors with gobject-2.0\r\n        from md2pdf.core import md2pdf\n        md2pdf(file_path,\r\n               md_content=text,\r\n               # md_file_path=f\"{file_path}.md\",\r\n               css_file_path=\"./multi_agents/agents/utils/pdf_styles.css\",  # Updated path\r\n               base_url=None)\n        print(f\"Report written to {file_path}\")\n    except Exception as e:\n        print(f\"Error in converting Markdown to PDF: {e}\")\n        return \"\"\n\n    encoded_file_path = urllib.parse.quote(file_path)\n    return encoded_file_path", "kind": "Chunk", "id": "utils/file_formats.py#162.26"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\file_formats.py_write_md_to_word_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\file_formats.py", "file_name": "file_formats.py", "file_type": "text/x-python", "category": "test", "tokens": 239, "span_ids": ["write_md_to_word"], "start_line": 66, "end_line": 99, "community": null}, "content": "async def write_md_to_word(text: str, path: str) -> str:\n    \"\"\"Converts Markdown text to a DOCX file and returns the file path.\r\n\r\n    Args:\r\n        text (str): Markdown text to convert.\r\n\r\n    Returns:\r\n        str: The encoded file path of the generated DOCX.\r\n    \"\"\"\n    task = uuid.uuid4().hex\n    file_path = f\"{path}/{task}.docx\"\n\n    try:\n        from htmldocx import HtmlToDocx\n        from docx import Document\n        # Convert report markdown to HTML\r\n        html = mistune.html(text)\n        # Create a document object\r\n        doc = Document()\n        # Convert the html generated from the report to document format\r\n        HtmlToDocx().add_html_to_document(html, doc)\n\n        # Saving the docx document to file_path\r\n        doc.save(file_path)\n\n        print(f\"Report written to {file_path}\")\n\n        encoded_file_path = urllib.parse.quote(f\"{file_path}.docx\")\n        return encoded_file_path\n\n    except Exception as e:\n        print(f\"Error in converting Markdown to DOCX: {e}\")\n        return \"\"", "kind": "Chunk", "id": "utils/file_formats.py#163.33"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\llms.py_json_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\llms.py", "file_name": "llms.py", "file_type": "text/x-python", "category": "test", "tokens": 317, "span_ids": ["imports", "call_model"], "start_line": 1, "end_line": 51, "community": null}, "content": "import json5 as json\nimport json_repair\nfrom langchain_community.adapters.openai import convert_openai_messages\n\nfrom gpt_researcher.config.config import Config\nfrom gpt_researcher.master.actions import handle_json_error\nfrom gpt_researcher.utils.llm import create_chat_completion\n\nfrom loguru import logger\n\n\nasync def call_model(\r\n    prompt: list,\r\n    model: str,\r\n    response_format: str = None,\r\n):\n\n    optional_params = {}\n    if response_format == \"json\":\n        optional_params = {\"response_format\": {\"type\": \"json_object\"}}\n\n    cfg = Config()\n    lc_messages = convert_openai_messages(prompt)\n\n    try:\n        response = await create_chat_completion(\r\n            model=model,\r\n            messages=lc_messages,\r\n            temperature=0,\r\n            llm_provider=cfg.llm_provider,\r\n            llm_kwargs=cfg.llm_kwargs,\r\n            # cost_callback=cost_callback,\r\n        )\n\n        if response_format == \"json\":\n            try:\n                cleaned_json_string = response.strip(\"```json\\n\")\n                return json.loads(cleaned_json_string)\n            except Exception as e:\n                print(\"\u26a0\ufe0f Error in reading JSON, attempting to repair JSON\")\n                logger.error(\r\n                    f\"Error in reading JSON, attempting to repair reponse: {response}\"\r\n                )\n                return json_repair.loads(response)\n        else:\n            return response\n\n    except Exception as e:\n        print(\"\u26a0\ufe0f Error in calling model\")\n        logger.error(f\"Error in calling model: {e}\")", "kind": "Chunk", "id": "utils/llms.py#164.50"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\utils.py_re_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\utils.py", "file_name": "utils.py", "file_type": "text/x-python", "category": "test", "tokens": 185, "span_ids": ["sanitize_filename", "imports"], "start_line": 1, "end_line": 27, "community": null}, "content": "import re\n\ndef sanitize_filename(filename: str) -> str:\n    \"\"\"\r\n    Sanitize a given filename by replacing characters that are invalid \r\n    in Windows file paths with an underscore ('_').\r\n\r\n    This function ensures that the filename is compatible with all \r\n    operating systems by removing or replacing characters that are \r\n    not allowed in Windows file paths. Specifically, it replaces \r\n    the following characters: < > : \" / \\ | ? *\r\n\r\n    Parameters:\r\n    filename (str): The original filename to be sanitized.\r\n\r\n    Returns:\r\n    str: The sanitized filename with invalid characters replaced by an underscore.\r\n    \r\n    Examples:\r\n    >>> sanitize_filename('invalid:file/name*example?.txt')\r\n    'invalid_file_name_example_.txt'\r\n    \r\n    >>> sanitize_filename('valid_filename.txt')\r\n    'valid_filename.txt'\r\n    \"\"\"\n    return re.sub(r'[<>:\"/\\\\|?*]', '_', filename)", "kind": "Chunk", "id": "utils/utils.py#165.26"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\views.py_from_colorama_import_Fore_print_agent_output.print_f_AgentColor_agent", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\utils\\views.py", "file_name": "views.py", "file_type": "text/x-python", "category": "test", "tokens": 121, "span_ids": ["imports", "print_agent_output", "AgentColor"], "start_line": 1, "end_line": 16, "community": null}, "content": "from colorama import Fore, Style\nfrom enum import Enum\n\n\nclass AgentColor(Enum):\n    RESEARCHER = Fore.LIGHTBLUE_EX\n    EDITOR = Fore.YELLOW\n    WRITER = Fore.LIGHTGREEN_EX\n    PUBLISHER = Fore.MAGENTA\n    REVIEWER = Fore.CYAN\n    REVISOR = Fore.LIGHTWHITE_EX\n    MASTER = Fore.LIGHTYELLOW_EX\n\n\ndef print_agent_output(output:str, agent: str=\"RESEARCHER\"):\n    print(f\"{AgentColor[agent].value}{agent}: {output}{Style.RESET_ALL}\")", "kind": "Chunk", "id": "utils/views.py#166.15"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\writer.py_from_datetime_import_date_sample_json._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\writer.py", "file_name": "writer.py", "file_type": "text/x-python", "category": "test", "tokens": 153, "span_ids": ["imports"], "start_line": 1, "end_line": 13, "community": null}, "content": "from datetime import datetime\nimport json5 as json\nfrom .utils.views import print_agent_output\nfrom .utils.llms import call_model\n\nsample_json = \"\"\"\r\n{\r\n  \"table_of_contents\": A table of contents in markdown syntax (using '-') based on the research headers and subheaders,\r\n  \"introduction\": An indepth introduction to the topic in markdown syntax and hyperlink references to relevant sources,\r\n  \"conclusion\": A conclusion to the entire research based on all research data in markdown syntax and hyperlink references to relevant sources,\r\n  \"sources\": A list with strings of all used source links in the entire research data in markdown syntax and apa citation format. For example: ['-  Title, year, Author [source url](source)', ...]\r\n}\r\n\"\"\"", "kind": "Chunk", "id": "agents/writer.py#167.12"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\writer.py_WriterAgent_WriterAgent.write_sections.return.response", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\writer.py", "file_name": "writer.py", "file_type": "text/x-python", "category": "test", "tokens": 457, "span_ids": ["WriterAgent.__init__", "WriterAgent.write_sections", "WriterAgent", "WriterAgent.get_headers"], "start_line": 16, "end_line": 67, "community": null}, "content": "class WriterAgent:\n    def __init__(self, websocket=None, stream_output=None, headers=None):\n        self.websocket = websocket\n        self.stream_output = stream_output\n        self.headers = headers\n\n    def get_headers(self, research_state: dict):\n        return {\r\n            \"title\": research_state.get(\"title\"),\r\n            \"date\": \"Date\",\r\n            \"introduction\": \"Introduction\",\r\n            \"table_of_contents\": \"Table of Contents\",\r\n            \"conclusion\": \"Conclusion\",\r\n            \"references\": \"References\",\r\n        }\n\n    async def write_sections(self, research_state: dict):\n        query = research_state.get(\"title\")\n        data = research_state.get(\"research_data\")\n        task = research_state.get(\"task\")\n        follow_guidelines = task.get(\"follow_guidelines\")\n        guidelines = task.get(\"guidelines\")\n\n        prompt = [\r\n            {\r\n                \"role\": \"system\",\r\n                \"content\": \"You are a research writer. Your sole purpose is to write a well-written \"\r\n                \"research reports about a \"\r\n                \"topic based on research findings and information.\\n \",\r\n            },\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": f\"Today's date is {datetime.now().strftime('%d/%m/%Y')}\\n.\"\r\n                f\"Query or Topic: {query}\\n\"\r\n                f\"Research data: {str(data)}\\n\"\r\n                f\"Your task is to write an in depth, well written and detailed \"\r\n                f\"introduction and conclusion to the research report based on the provided research data. \"\r\n                f\"Do not include headers in the results.\\n\"\r\n                f\"You MUST include any relevant sources to the introduction and conclusion as markdown hyperlinks -\"\r\n                f\"For example: 'This is a sample text. ([url website](url))'\\n\\n\"\r\n                f\"{f'You must follow the guidelines provided: {guidelines}' if follow_guidelines else ''}\\n\"\r\n                f\"You MUST return nothing but a JSON in the following format (without json markdown):\\n\"\r\n                f\"{sample_json}\\n\\n\",\r\n            },\r\n        ]\n\n        response = await call_model(\r\n            prompt,\r\n            task.get(\"model\"),\r\n            response_format=\"json\",\r\n        )\n        return response", "kind": "Chunk", "id": "agents/writer.py#168.51"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\writer.py_WriterAgent.revise_headers_WriterAgent.revise_headers.return._headers_response_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\writer.py", "file_name": "writer.py", "file_type": "text/x-python", "category": "test", "tokens": 187, "span_ids": ["WriterAgent.revise_headers"], "start_line": 69, "end_line": 92, "community": null}, "content": "class WriterAgent:\n\n    async def revise_headers(self, task: dict, headers: dict):\n        prompt = [\r\n            {\r\n                \"role\": \"system\",\r\n                \"content\": \"\"\"You are a research writer. \r\nYour sole purpose is to revise the headers data based on the given guidelines.\"\"\",\r\n            },\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": f\"\"\"Your task is to revise the given headers JSON based on the guidelines given.\r\nYou are to follow the guidelines but the values should be in simple strings, ignoring all markdown syntax.\r\nYou must return nothing but a JSON in the same format as given in headers data.\r\nGuidelines: {task.get(\"guidelines\")}\\n\r\nHeaders Data: {headers}\\n\r\n\"\"\",\r\n            },\r\n        ]\n\n        response = await call_model(\r\n            prompt,\r\n            task.get(\"model\"),\r\n            response_format=\"json\",\r\n        )\n        return {\"headers\": response}", "kind": "Chunk", "id": "agents/writer.py#169.23"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\writer.py_WriterAgent.run_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\agents\\writer.py", "file_name": "writer.py", "file_type": "text/x-python", "category": "test", "tokens": 319, "span_ids": ["WriterAgent.run"], "start_line": 94, "end_line": 143, "community": null}, "content": "class WriterAgent:\n\n    async def run(self, research_state: dict):\n        if self.websocket and self.stream_output:\n            await self.stream_output(\r\n                \"logs\",\r\n                \"writing_report\",\r\n                f\"Writing final research report based on research data...\",\r\n                self.websocket,\r\n            )\n        else:\n            print_agent_output(\r\n                f\"Writing final research report based on research data...\",\r\n                agent=\"WRITER\",\r\n            )\n\n        research_layout_content = await self.write_sections(research_state)\n\n        if research_state.get(\"task\").get(\"verbose\"):\n            if self.websocket and self.stream_output:\n                research_layout_content_str = json.dumps(\r\n                    research_layout_content, indent=2\r\n                )\n                await self.stream_output(\r\n                    \"logs\",\r\n                    \"research_layout_content\",\r\n                    research_layout_content_str,\r\n                    self.websocket,\r\n                )\n            else:\n                print_agent_output(research_layout_content, agent=\"WRITER\")\n\n        headers = self.get_headers(research_state)\n        if research_state.get(\"task\").get(\"follow_guidelines\"):\n            if self.websocket and self.stream_output:\n                await self.stream_output(\r\n                    \"logs\",\r\n                    \"rewriting_layout\",\r\n                    \"Rewriting layout based on guidelines...\",\r\n                    self.websocket,\r\n                )\n            else:\n                print_agent_output(\r\n                    \"Rewriting layout based on guidelines...\", agent=\"WRITER\"\r\n                )\n            headers = await self.revise_headers(\r\n                task=research_state.get(\"task\"), headers=headers\r\n            )\n            headers = headers.get(\"headers\")\n\n        return {**research_layout_content, \"headers\": headers}", "kind": "Chunk", "id": "agents/writer.py#170.49"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\main.py_from_dotenv_import_load_d_if___name_____main___.asyncio_run_main_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\main.py", "file_name": "main.py", "file_type": "text/x-python", "category": "test", "tokens": 356, "span_ids": ["imports", "run_research_task", "impl:5", "open_task", "main"], "start_line": 1, "end_line": 52, "community": null}, "content": "from dotenv import load_dotenv\nimport sys\nimport os\nimport uuid\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom multi_agents.agents import ChiefEditorAgent\nimport asyncio\nimport json\nfrom gpt_researcher.utils.enum import Tone\n\n# Run with LangSmith if API key is set\r\nif os.environ.get(\"LANGCHAIN_API_KEY\"):\n    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nload_dotenv()\n\ndef open_task():\n    # Get the directory of the current script\r\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    # Construct the absolute path to task.json\r\n    task_json_path = os.path.join(current_dir, 'task.json')\n\n    with open(task_json_path, 'r') as f:\n        task = json.load(f)\n\n    if not task:\n        raise Exception(\"No task provided. Please include a task.json file in the multi_agents directory.\")\n\n    return task\n\nasync def run_research_task(query, websocket=None, stream_output=None, tone=Tone.Objective, headers=None):\n    task = open_task()\n    task[\"query\"] = query\n\n    chief_editor = ChiefEditorAgent(task, websocket, stream_output, tone, headers)\n    research_report = await chief_editor.run_research_task()\n\n    if websocket and stream_output:\n        await stream_output(\"logs\", \"research_report\", research_report, websocket)\n\n    return research_report\n\nasync def main():\n    task = open_task()\n\n    chief_editor = ChiefEditorAgent(task)\n    research_report = await chief_editor.run_research_task(task_id=uuid.uuid4())\n\n    return research_report\n\nif __name__ == \"__main__\":\n    asyncio.run(main())", "kind": "Chunk", "id": "multi_agents/main.py#171.51"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\memory\\__init__.py____all__._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\memory\\__init__.py", "file_name": "__init__.py", "file_type": "text/x-python", "category": "test", "tokens": 30, "span_ids": ["imports"], "start_line": 1, "end_line": 7, "community": null}, "content": "from .draft import DraftState\nfrom .research import ResearchState\n\n__all__ = [\r\n    \"DraftState\",\r\n    \"ResearchState\"\r\n]", "kind": "Chunk", "id": "memory/__init__.py#172.6"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\memory\\draft.py__DraftState.revision_notes", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\memory\\draft.py", "file_name": "draft.py", "file_type": "text/x-python", "category": "test", "tokens": 46, "span_ids": ["DraftState", "imports"], "start_line": 1, "end_line": 10, "community": null}, "content": "from typing import TypedDict, List, Annotated\nimport operator\n\n\nclass DraftState(TypedDict):\n    task: dict\n    topic: str\n    draft: dict\n    review: str\n    revision_notes: str", "kind": "Chunk", "id": "memory/draft.py#173.9"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\memory\\research.py__", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\multi_agents\\memory\\research.py", "file_name": "research.py", "file_type": "text/x-python", "category": "test", "tokens": 100, "span_ids": ["ResearchState", "imports"], "start_line": 1, "end_line": 22, "community": null}, "content": "from typing import TypedDict, List, Annotated\nimport operator\n\n\nclass ResearchState(TypedDict):\n    task: dict\n    initial_research: str\n    sections: List[str]\n    research_data: List[dict]\n    human_feedback: str\n    # Report layout\r\n    title: str\n    headers: dict\n    date: str\n    table_of_contents: str\n    introduction: str\n    conclusion: str\n    sources: List[str]\n    report: str", "kind": "Chunk", "id": "memory/research.py#174.21"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\html.py__HTML_processing_functi_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\html.py", "file_name": "html.py", "file_type": "text/x-python", "category": "test", "tokens": 221, "span_ids": ["extract_hyperlinks", "docstring", "format_hyperlinks"], "start_line": 1, "end_line": 34, "community": null}, "content": "\"\"\"HTML processing functions\"\"\"\nfrom __future__ import annotations\n\nfrom bs4 import BeautifulSoup\nfrom requests.compat import urljoin\n\n\ndef extract_hyperlinks(soup: BeautifulSoup, base_url: str) -> list[tuple[str, str]]:\n    \"\"\"Extract hyperlinks from a BeautifulSoup object\r\n\r\n    Args:\r\n        soup (BeautifulSoup): The BeautifulSoup object\r\n        base_url (str): The base URL\r\n\r\n    Returns:\r\n        List[Tuple[str, str]]: The extracted hyperlinks\r\n    \"\"\"\n    return [\r\n        (link.text, urljoin(base_url, link[\"href\"]))\r\n        for link in soup.find_all(\"a\", href=True)\r\n    ]\n\n\ndef format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]:\n    \"\"\"Format hyperlinks to be displayed to the user\r\n\r\n    Args:\r\n        hyperlinks (List[Tuple[str, str]]): The hyperlinks to format\r\n\r\n    Returns:\r\n        List[str]: The formatted hyperlinks\r\n    \"\"\"\n    return [f\"{link_text} ({link_url})\" for link_text, link_url in hyperlinks]", "kind": "Chunk", "id": "processing/html.py#175.33"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py__Text_processing_functi_split_text.if_current_chunk_.yield_n_join_current_c", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py", "file_name": "text.py", "file_type": "text/x-python", "category": "test", "tokens": 267, "span_ids": ["docstring", "split_text"], "start_line": 1, "end_line": 40, "community": null}, "content": "\"\"\"Text processing functions\"\"\"\nimport urllib\nfrom typing import Dict, Generator, Optional\n\nfrom selenium.webdriver.remote.webdriver import WebDriver\n\nfrom config import Config\nfrom gpt_researcher_old.retriever.llm_utils import create_chat_completion\nimport os\nfrom md2pdf.core import md2pdf\n\n\ndef split_text(text: str, max_length: int = 8192) -> Generator[str, None, None]:\n    \"\"\"Split text into chunks of a maximum length\r\n\r\n    Args:\r\n        text (str): The text to split\r\n        max_length (int, optional): The maximum length of each chunk. Defaults to 8192.\r\n\r\n    Yields:\r\n        str: The next chunk of text\r\n\r\n    Raises:\r\n        ValueError: If the text is longer than the maximum length\r\n    \"\"\"\n    paragraphs = text.split(\"\\n\")\n    current_length = 0\n    current_chunk = []\n\n    for paragraph in paragraphs:\n        if current_length + len(paragraph) + 1 <= max_length:\n            current_chunk.append(paragraph)\n            current_length += len(paragraph) + 1\n        else:\n            yield \"\\n\".join(current_chunk)\n            current_chunk = [paragraph]\n            current_length = len(paragraph) + 1\n\n    if current_chunk:\n        yield \"\\n\".join(current_chunk)", "kind": "Chunk", "id": "processing/text.py#176.39"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py_summarize_text_summarize_text.return.final_summary", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py", "file_name": "text.py", "file_type": "text/x-python", "category": "test", "tokens": 485, "span_ids": ["summarize_text"], "start_line": 43, "end_line": 101, "community": null}, "content": "def summarize_text(\r\n    fast_llm_model: str, summary_token_limit: int, llm_provider: str, url: str, text: str, question: str, driver: Optional[WebDriver] = None\r\n) -> str:\n    \"\"\"Summarize text using the OpenAI API\r\n\r\n    Args:\r\n        fast_llm_model (str): The fast LLM model e.g gpt-4o-mini\r\n        summary_token_limit (int): The summary token limit\r\n        llm_provider (str): The llm provider\r\n        url (str): The url of the text\r\n        text (str): The text to summarize\r\n        question (str): The question to ask the model\r\n        driver (WebDriver): The webdriver to use to scroll the page\r\n\r\n    Returns:\r\n        str: The summary of the text\r\n    \"\"\"\n    if not text:\n        return \"Error: No text to summarize\"\n\n    summaries = []\n    chunks = list(split_text(text))\n    scroll_ratio = 1 / len(chunks)\n\n    print(f\"Summarizing url: {url} with total chunks: {len(chunks)}\")\n    for i, chunk in enumerate(chunks):\n        if driver:\n            scroll_to_percentage(driver, scroll_ratio * i)\n\n        #memory_to_add = f\"Source: {url}\\n\" f\"Raw content part#{i + 1}: {chunk}\"\r\n\n        #MEMORY.add_documents([Document(page_content=memory_to_add)])\r\n\n        messages = [create_message(chunk, question)]\n\n        summary = create_chat_completion(\r\n            model=fast_llm_model,\r\n            messages=messages,\r\n            max_tokens=summary_token_limit,\r\n            llm_provider=llm_provider\r\n        )\n        summaries.append(summary)\n        #memory_to_add = f\"Source: {url}\\n\" f\"Content summary part#{i + 1}: {summary}\"\r\n\n        #MEMORY.add_documents([Document(page_content=memory_to_add)])\r\n\n    combined_summary = \"\\n\".join(summaries)\n    messages = [create_message(combined_summary, question)]\n\n    final_summary = create_chat_completion(\r\n        model=fast_llm_model,\r\n        messages=messages,\r\n        max_tokens=summary_token_limit,\r\n        llm_provider=llm_provider,\r\n    )\n    print(\"Final summary length: \", len(combined_summary))\n    print(final_summary)\n\n    return final_summary", "kind": "Chunk", "id": "processing/text.py#177.58"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py_scroll_to_percentage_scroll_to_percentage.driver_execute_script_f_w", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py", "file_name": "text.py", "file_type": "text/x-python", "category": "test", "tokens": 114, "span_ids": ["scroll_to_percentage"], "start_line": 104, "end_line": 116, "community": null}, "content": "def scroll_to_percentage(driver: WebDriver, ratio: float) -> None:\n    \"\"\"Scroll to a percentage of the page\r\n\r\n    Args:\r\n        driver (WebDriver): The webdriver to use\r\n        ratio (float): The percentage to scroll to\r\n\r\n    Raises:\r\n        ValueError: If the ratio is not between 0 and 1\r\n    \"\"\"\n    if ratio < 0 or ratio > 1:\n        raise ValueError(\"Percentage should be between 0 and 1\")\n    driver.execute_script(f\"window.scrollTo(0, document.body.scrollHeight * {ratio});\")", "kind": "Chunk", "id": "processing/text.py#178.12"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py_create_message_create_message.return._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py", "file_name": "text.py", "file_type": "text/x-python", "category": "test", "tokens": 164, "span_ids": ["create_message"], "start_line": 119, "end_line": 135, "community": null}, "content": "def create_message(chunk: str, question: str) -> Dict[str, str]:\n    \"\"\"Create a message for the chat completion\r\n\r\n    Args:\r\n        chunk (str): The chunk of text to summarize\r\n        question (str): The question to answer\r\n\r\n    Returns:\r\n        Dict[str, str]: The message to send to the chat completion\r\n    \"\"\"\n    return {\r\n        \"role\": \"user\",\r\n        \"content\": f'\"\"\"{chunk}\"\"\"\\n'\r\n        f'Using the above text, summarize it based on the following task or query: \"{question}\".\\n'\r\n        f'If the query cannot be answered using the text, YOU MUST summarize the text in short.\\n'\r\n        f'Include all factual information such as numbers, stats, quotes, etc if available.',\r\n    }", "kind": "Chunk", "id": "processing/text.py#179.16"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py_write_to_file_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\processing\\text.py", "file_name": "text.py", "file_type": "text/x-python", "category": "test", "tokens": 260, "span_ids": ["write_md_to_pdf", "read_txt_files", "write_to_file", "md_to_pdf"], "start_line": 137, "end_line": 174, "community": null}, "content": "def write_to_file(filename: str, text: str) -> None:\n    \"\"\"Write text to a file\r\n\r\n    Args:\r\n        text (str): The text to write\r\n        filename (str): The filename to write to\r\n    \"\"\"\n    with open(filename, \"w\") as file:\n        file.write(text)\n\nasync def write_md_to_pdf(task: str, path: str, text: str) -> None:\n    file_path = f\"{path}/{task}\"\n    write_to_file(f\"{file_path}.md\", text)\n    md_to_pdf(f\"{file_path}.md\", f\"{file_path}.pdf\")\n    print(f\"{task} written to {file_path}.pdf\")\n\n    encoded_file_path = urllib.parse.quote(f\"{file_path}.pdf\")\n\n    return encoded_file_path\n\ndef read_txt_files(directory):\n    all_text = ''\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory, filename), 'r') as file:\n                all_text += file.read() + '\\n'\n\n    return all_text\n\n\ndef md_to_pdf(input_file, output_file):\n    md2pdf(output_file,\r\n           md_content=None,\r\n           md_file_path=input_file,\r\n           css_file_path=None,\r\n           base_url=None)", "kind": "Chunk", "id": "processing/text.py#180.37"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\scrape_skills.py_from_langchain_document_l_scrape_pdf_with_arxiv.return.docs_0_page_content", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\scrape_skills.py", "file_name": "scrape_skills.py", "file_type": "text/x-python", "category": "test", "tokens": 209, "span_ids": ["imports", "scrape_pdf_with_arxiv", "scrape_pdf_with_pymupdf"], "start_line": 1, "end_line": 31, "community": null}, "content": "from langchain.document_loaders import PyMuPDFLoader\nfrom langchain.retrievers import ArxivRetriever\n\n\ndef scrape_pdf_with_pymupdf(url) -> str:\n    \"\"\"Scrape a pdf with pymupdf\r\n\r\n    Args:\r\n        url (str): The url of the pdf to scrape\r\n\r\n    Returns:\r\n        str: The text scraped from the pdf\r\n    \"\"\"\n    loader = PyMuPDFLoader(url)\n    doc = loader.load()\n    return str(doc)\n\n\ndef scrape_pdf_with_arxiv(query) -> str:\n    \"\"\"Scrape a pdf with arxiv\r\n    default document length of 70000 about ~15 pages or None for no limit\r\n\r\n    Args:\r\n        query (str): The query to search for\r\n\r\n    Returns:\r\n        str: The text scraped from the pdf\r\n    \"\"\"\n    retriever = ArxivRetriever(load_max_docs=2, doc_content_chars_max=None)\n    docs = retriever.get_relevant_documents(query=query)\n    return docs[0].page_content", "kind": "Chunk", "id": "scraping/scrape_skills.py#181.30"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py__Selenium_web_scraping__FILE_DIR.Path___file___parent_par", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py", "file_name": "web_scrape.py", "file_type": "text/x-python", "category": "test", "tokens": 171, "span_ids": ["docstring"], "start_line": 1, "end_line": 29, "community": null}, "content": "\"\"\"Selenium web scraping module.\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport asyncio\nfrom pathlib import Path\nfrom sys import platform\n\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options as ChromeOptions\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nfrom selenium.webdriver.remote.webdriver import WebDriver\nfrom selenium.webdriver.safari.options import Options as SafariOptions\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom fastapi import WebSocket\n\nfrom scraping import scrape_skills, processing as summary\nfrom scraping.processing.html import extract_hyperlinks, format_hyperlinks\n\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom scraping.processing.text import summarize_text\n\nexecutor = ThreadPoolExecutor()\n\nFILE_DIR = Path(__file__).parent.parent", "kind": "Chunk", "id": "scraping/web_scrape.py#182.28"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py_async_browse_async_browse.try_.except_Exception_as_e_.return.f_Error_processing_the_ur", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py", "file_name": "web_scrape.py", "file_type": "text/x-python", "category": "test", "tokens": 465, "span_ids": ["async_browse"], "start_line": 32, "end_line": 88, "community": null}, "content": "async def async_browse(\r\n        selenium_web_browser: str,\r\n        user_agent: str,\r\n        fast_llm_model: str,\r\n        summary_token_limit: str,\r\n        llm_provider: str,\r\n        url: str, question: str,\r\n        websocket: WebSocket\r\n) -> str:\n    \"\"\"Browse a website and return the answer and links to the user\r\n\r\n    Args:\r\n        selenium_web_browser (str): The web browser used for scraping\r\n        user_agent (str): The user agent used when scraping\r\n        url (str): The url of the website to browse\r\n        question (str): The question asked by the user\r\n        websocket (WebSocketManager): The websocket manager\r\n\r\n    Returns:\r\n        str: The answer and links to the user\r\n    \"\"\"\n    loop = asyncio.get_event_loop()\n    executor = ThreadPoolExecutor(max_workers=8)\n\n    print(f\"Scraping url {url} with question {question}\")\n    if websocket:\n        await websocket.send_json(\r\n            {\r\n                \"type\": \"logs\",\r\n                \"output\": f\"\ud83d\udd0e Browsing the {url} for relevant about: {question}...\",\r\n            }\r\n        )\n    else:\n        print(f\"\ud83d\udd0e Browsing the {url} for relevant about: {question}...\")\n\n    try:\n        driver, text = await loop.run_in_executor(\r\n            executor, scrape_text_with_selenium, selenium_web_browser, user_agent, url\r\n        )\n        await loop.run_in_executor(executor, add_header, driver)\n        summary_text = await loop.run_in_executor(\r\n            executor, summarize_text, fast_llm_model, summary_token_limit, llm_provider, url, text, question, driver\r\n        )\n        if websocket:\n            await websocket.send_json(\r\n                {\r\n                    \"type\": \"logs\",\r\n                    \"output\": f\"\ud83d\udcdd Information gathered from url {url}: {summary_text}\",\r\n                }\r\n            )\n        else:\n            print(f\"\ud83d\udcdd Information gathered from url {url}: {summary_text}\")\n\n        return f\"Information gathered from url {url}: {summary_text}\"\n    except Exception as e:\n        print(f\"An error occurred while processing the url {url}: {e}\")\n        return f\"Error processing the url {url}: {e}\"", "kind": "Chunk", "id": "scraping/web_scrape.py#183.56"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py_browse_website_browse_website.return.f_Answer_gathered_from_we", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py", "file_name": "web_scrape.py", "file_type": "text/x-python", "category": "test", "tokens": 235, "span_ids": ["browse_website"], "start_line": 91, "end_line": 118, "community": null}, "content": "def browse_website(url: str, question: str) -> tuple[str, WebDriver]:\n    \"\"\"Browse a website and return the answer and links to the user\r\n\r\n    Args:\r\n        url (str): The url of the website to browse\r\n        question (str): The question asked by the user\r\n\r\n    Returns:\r\n        Tuple[str, WebDriver]: The answer and links to the user and the webdriver\r\n    \"\"\"\n\n    if not url:\n        return \"A URL was not specified, cancelling request to browse website.\", None\n\n    driver, text = scrape_text_with_selenium(url)\n    add_header(driver)\n    summary_text = summary.summarize_text(url, text, question, driver)\n\n    links = scrape_links_with_selenium(driver, url)\n\n    # Limit links to 5\r\n    if len(links) > 5:\n        links = links[:5]\n\n    # write_to_file('research-{0}.txt'.format(url), summary_text + \"\\nSource Links: {0}\\n\\n\".format(links))\r\n\n    close_browser(driver)\n    return f\"Answer gathered from website: {summary_text} \\n \\n Links: {links}\", driver", "kind": "Chunk", "id": "scraping/web_scrape.py#184.27"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py_scrape_text_with_selenium_scrape_text_with_selenium.return.driver_text", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py", "file_name": "web_scrape.py", "file_type": "text/x-python", "category": "test", "tokens": 577, "span_ids": ["scrape_text_with_selenium"], "start_line": 121, "end_line": 187, "community": null}, "content": "def scrape_text_with_selenium(selenium_web_browser: str, user_agent: str, url: str) -> tuple[WebDriver, str]:\n    \"\"\"Scrape text from a website using selenium\r\n\r\n    Args:\r\n        url (str): The url of the website to scrape\r\n        selenium_web_browser (str): The web browser used to scrape\r\n        user_agent (str): The user agent used when scraping\r\n\r\n    Returns:\r\n        Tuple[WebDriver, str]: The webdriver and the text scraped from the website\r\n    \"\"\"\n    logging.getLogger(\"selenium\").setLevel(logging.CRITICAL)\n\n    options_available = {\r\n        \"chrome\": ChromeOptions,\r\n        \"safari\": SafariOptions,\r\n        \"firefox\": FirefoxOptions,\r\n    }\n\n    options = options_available[selenium_web_browser]()\n    options.add_argument(f\"user-agent={user_agent}\")\n    options.add_argument(\"--headless\")\n    options.add_argument(\"--enable-javascript\")\n\n    if selenium_web_browser == \"firefox\":\n        driver = webdriver.Firefox(options=options)\n    elif selenium_web_browser == \"safari\":\r\n        # Requires a bit more setup on the users end\r\n        # See https://developer.apple.com/documentation/webkit/testing_with_webdriver_in_safari\r\n        driver = webdriver.Safari(options=options)\n    else:\n        if platform == \"linux\" or platform == \"linux2\":\n            options.add_argument(\"--disable-dev-shm-usage\")\n            options.add_argument(\"--remote-debugging-port=9222\")\n        options.add_argument(\"--no-sandbox\")\n        options.add_experimental_option(\"prefs\", {\"download_restrictions\": 3})\n        driver = webdriver.Chrome(options=options)\n\n    print(f\"scraping url {url}...\")\n    driver.get(url)\n\n    WebDriverWait(driver, 10).until(\r\n        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\r\n    )\n\n    # check if url is a pdf or arxiv link\r\n    if url.endswith(\".pdf\"):\n        text = scrape_skills.scrape_pdf_with_pymupdf(url)\n    elif \"arxiv\" in url:\r\n        # parse the document number from the url\r\n        doc_num = url.split(\"/\")[-1]\n        text = scrape_skills.scrape_pdf_with_arxiv(doc_num)\n    else:\r\n        # Get the HTML content directly from the browser's DOM\r\n        page_source = driver.execute_script(\"return document.body.outerHTML;\")\n        soup = BeautifulSoup(page_source, \"html.parser\")\n\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n\n        # text = soup.get_text()\r\n        text = get_text(soup)\n\n    lines = (line.strip() for line in text.splitlines())\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    text = \"\\n\".join(chunk for chunk in chunks if chunk)\n    return driver, text", "kind": "Chunk", "id": "scraping/web_scrape.py#185.66"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py_get_text_get_text.return.text", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py", "file_name": "web_scrape.py", "file_type": "text/x-python", "category": "test", "tokens": 108, "span_ids": ["get_text"], "start_line": 190, "end_line": 203, "community": null}, "content": "def get_text(soup):\n    \"\"\"Get the text from the soup\r\n\r\n    Args:\r\n        soup (BeautifulSoup): The soup to get the text from\r\n\r\n    Returns:\r\n        str: The text from the soup\r\n    \"\"\"\n    text = \"\"\n    tags = [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"p\"]\n    for element in soup.find_all(tags):  # Find all the <p> elements\r\n        text += element.text + \"\\n\\n\"\n    return text", "kind": "Chunk", "id": "scraping/web_scrape.py#186.13"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py_scrape_links_with_selenium_scrape_links_with_selenium.return.format_hyperlinks_hyperli", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py", "file_name": "web_scrape.py", "file_type": "text/x-python", "category": "test", "tokens": 118, "span_ids": ["scrape_links_with_selenium"], "start_line": 206, "end_line": 223, "community": null}, "content": "def scrape_links_with_selenium(driver: WebDriver, url: str) -> list[str]:\n    \"\"\"Scrape links from a website using selenium\r\n\r\n    Args:\r\n        driver (WebDriver): The webdriver to use to scrape the links\r\n\r\n    Returns:\r\n        List[str]: The links scraped from the website\r\n    \"\"\"\n    page_source = driver.page_source\n    soup = BeautifulSoup(page_source, \"html.parser\")\n\n    for script in soup([\"script\", \"style\"]):\n        script.extract()\n\n    hyperlinks = extract_hyperlinks(soup, url)\n\n    return format_hyperlinks(hyperlinks)", "kind": "Chunk", "id": "scraping/web_scrape.py#187.17"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py_close_browser_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\scraping\\web_scrape.py", "file_name": "web_scrape.py", "file_type": "text/x-python", "category": "test", "tokens": 105, "span_ids": ["add_header", "close_browser"], "start_line": 226, "end_line": 248, "community": null}, "content": "def close_browser(driver: WebDriver) -> None:\n    \"\"\"Close the browser\r\n\r\n    Args:\r\n        driver (WebDriver): The webdriver to close\r\n\r\n    Returns:\r\n        None\r\n    \"\"\"\n    driver.quit()\n\n\ndef add_header(driver: WebDriver) -> None:\n    \"\"\"Add a header to the website\r\n\r\n    Args:\r\n        driver (WebDriver): The webdriver to use to add the header\r\n\r\n    Returns:\r\n        None\r\n    \"\"\"\n    driver.execute_script(open(f\"{FILE_DIR}/js/overlay.js\", \"r\").read())", "kind": "Chunk", "id": "scraping/web_scrape.py#188.22"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\setup.py_from_setuptools_import_fi_setup_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\setup.py", "file_name": "setup.py", "file_type": "text/x-python", "category": "test", "tokens": 333, "span_ids": ["imports"], "start_line": 1, "end_line": 44, "community": null}, "content": "from setuptools import find_packages, setup\n\nexclude_packages = [\r\n    \"selenium\",\r\n    \"webdriver\",\r\n    \"fastapi\",\r\n    \"fastapi.*\",\r\n    \"uvicorn\",\r\n    \"jinja2\",\r\n    \"gpt-researcher\",\r\n    \"langgraph\"\r\n]\n\nwith open(r\"README.md\", \"r\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\nwith open(\"requirements.txt\", \"r\") as f:\n    reqs = [line.strip() for line in f if not any(pkg in line for pkg in exclude_packages)]\n\nsetup(\r\n    name=\"gpt-researcher\",\r\n    version=\"0.9.1\",\r\n    description=\"GPT Researcher is an autonomous agent designed for comprehensive online research on a variety of tasks.\",\r\n    package_dir={'gpt_researcher': 'gpt_researcher'},\r\n    packages=find_packages(exclude=exclude_packages),\r\n    long_description=long_description,\r\n    long_description_content_type=\"text/markdown\",\r\n    url=\"https://github.com/assafelovic/gpt-researcher\",\r\n    author=\"Assaf Elovic\",\r\n    author_email=\"assaf.elovic@gmail.com\",\r\n    license=\"MIT\",\r\n    classifiers=[\r\n        \"License :: OSI Approved :: MIT License\",\r\n        \"Intended Audience :: Developers\",\r\n        \"Intended Audience :: Education\",\r\n        \"Intended Audience :: Science/Research\",\r\n        \"Programming Language :: Python :: 3.11\",\r\n        \"Programming Language :: Python :: 3.12\",\r\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\r\n    ],\r\n    install_requires=reqs,\r\n\r\n\r\n)", "kind": "Chunk", "id": "gpt-researcher/setup.py#189.43"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\documents-report-source.py_os_output_dir._outputs_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\documents-report-source.py", "file_name": "documents-report-source.py", "file_type": "text/x-python", "category": "test", "tokens": 123, "span_ids": ["imports"], "start_line": 1, "end_line": 22, "community": null}, "content": "import os\nimport asyncio\nimport pytest\nfrom gpt_researcher.master.agent import GPTResearcher  # Ensure this path is correct\r\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Define the report types to test\r\nreport_types = [\r\n    \"research_report\",\r\n    \"custom_report\",\r\n    \"subtopic_report\",\r\n    \"summary_report\",\r\n    \"detailed_report\",\r\n    \"quick_report\"\r\n]\n\n# Define a common query and sources for testing\r\nquery = \"What can you tell me about myself based on my documents?\"\n\n# Define the output directory\r\noutput_dir = \"./outputs\"", "kind": "Chunk", "id": "tests/documents-report-source.py#190.21"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\documents-report-source.py_test_gpt_researcher_if___name_____main___.pytest_main_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\documents-report-source.py", "file_name": "documents-report-source.py", "file_type": "text/x-python", "category": "test", "tokens": 258, "span_ids": ["test_gpt_researcher", "impl:8"], "start_line": 24, "end_line": 51, "community": null}, "content": "@pytest.mark.asyncio\r\n@pytest.mark.parametrize(\"report_type\", report_types)\r\nasync def test_gpt_researcher(report_type):\n    # Ensure the output directory exists\r\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create an instance of GPTResearcher with report_source set to \"documents\"\r\n    researcher = GPTResearcher(query=query, report_type=report_type, report_source=\"documents\")\n\n    # Conduct research and write the report\r\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n\n    # Define the expected output filenames\r\n    pdf_filename = os.path.join(output_dir, f\"{report_type}.pdf\")\n    docx_filename = os.path.join(output_dir, f\"{report_type}.docx\")\n\n    # Check if the PDF and DOCX files are created\r\n    # assert os.path.exists(pdf_filename), f\"PDF file not found for report type: {report_type}\"\r\n    # assert os.path.exists(docx_filename), f\"DOCX file not found for report type: {report_type}\"\r\n\n    # Clean up the generated files (optional)\r\n    # os.remove(pdf_filename)\r\n    # os.remove(docx_filename)\r\n\nif __name__ == \"__main__\":\n    pytest.main()", "kind": "Chunk", "id": "tests/documents-report-source.py#191.27"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\report-types.py_os_output_dir._outputs_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\report-types.py", "file_name": "report-types.py", "file_type": "text/x-python", "category": "test", "tokens": 107, "span_ids": ["imports"], "start_line": 1, "end_line": 17, "community": null}, "content": "import os\nimport asyncio\nimport pytest\nfrom gpt_researcher import GPTResearcher\n\n# Define the report types to test\r\nreport_types = [\r\n    \"research_report\",\r\n    \"subtopic_report\"\r\n]\n\n# Define a common query and sources for testing\r\nquery = \"What are the latest advancements in AI?\"\n# sources = [\"https://en.wikipedia.org/wiki/Artificial_intelligence\", \"https://www.ibm.com/watson/ai\"]\r\n\n# Define the output directory\r\noutput_dir = \"./outputs\"", "kind": "Chunk", "id": "tests/report-types.py#192.16"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\report-types.py_test_gpt_researcher_if___name_____main___.pytest_main_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\report-types.py", "file_name": "report-types.py", "file_type": "text/x-python", "category": "test", "tokens": 246, "span_ids": ["impl:7", "test_gpt_researcher"], "start_line": 19, "end_line": 46, "community": null}, "content": "@pytest.mark.asyncio\r\n@pytest.mark.parametrize(\"report_type\", report_types)\r\nasync def test_gpt_researcher(report_type):\n    # Ensure the output directory exists\r\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Create an instance of GPTResearcher\r\n    researcher = GPTResearcher(query=query, report_type=report_type)\n\n    # Conduct research and write the report\r\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n\n    # Define the expected output filenames\r\n    pdf_filename = os.path.join(output_dir, f\"{report_type}.pdf\")\n    docx_filename = os.path.join(output_dir, f\"{report_type}.docx\")\n\n    # Check if the PDF and DOCX files are created\r\n    # assert os.path.exists(pdf_filename), f\"PDF file not found for report type: {report_type}\"\r\n    # assert os.path.exists(docx_filename), f\"DOCX file not found for report type: {report_type}\"\r\n\n    # Clean up the generated files (optional)\r\n    # os.remove(pdf_filename)\r\n    # os.remove(docx_filename)\r\n\nif __name__ == \"__main__\":\n    pytest.main()", "kind": "Chunk", "id": "tests/report-types.py#193.27"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\test-loaders.py_from_langchain_community__None_1.except_Exception_as_e_.print_Failed_to_load_CSV", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\test-loaders.py", "file_name": "test-loaders.py", "file_type": "text/x-python", "category": "test", "tokens": 147, "span_ids": ["imports"], "start_line": 1, "end_line": 17, "community": null}, "content": "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredCSVLoader\n\n# # Test PyMuPDFLoader\r\npdf_loader = PyMuPDFLoader(\"my-docs/Elisha - Coding Career.pdf\")\ntry:\n    pdf_data = pdf_loader.load()\n    print(\"PDF Data:\", pdf_data)\nexcept Exception as e:\n    print(\"Failed to load PDF:\", e)\n\n# Test UnstructuredCSVLoader\r\ncsv_loader = UnstructuredCSVLoader(\"my-docs/active_braze_protocols_from_bq.csv\", mode=\"elements\")\ntry:\n    csv_data = csv_loader.load()\n    print(\"CSV Data:\", csv_data)\nexcept Exception as e:\n    print(\"Failed to load CSV:\", e)", "kind": "Chunk", "id": "tests/test-loaders.py#194.16"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\vector-store.py_asyncio__taken_from_https_paul", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\vector-store.py", "file_name": "vector-store.py", "file_type": "text/x-python", "category": "test", "tokens": 79, "span_ids": ["imports"], "start_line": 1, "end_line": 12, "community": null}, "content": "import asyncio\nimport pytest\nfrom typing import List\nfrom gpt_researcher import GPTResearcher\n\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\n\n# taken from https://paulgraham.com/persistence.html\r", "kind": "Chunk", "id": "tests/vector-store.py#195.11"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\vector-store.py_essay_essay._", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\vector-store.py", "file_name": "vector-store.py", "file_type": "text/x-python", "category": "test", "tokens": 2417, "span_ids": ["imports"], "start_line": 13, "end_line": 99, "community": null}, "content": "essay = \"\"\"\r\nThe right kind of Stubborn\r\n\r\nJuly 2024\r\n\r\nSuccessful people tend to be persistent. New ideas often don't work at first, but they're not deterred. They keep trying and eventually find something that does.\r\n\r\nMere obstinacy, on the other hand, is a recipe for failure. Obstinate people are so annoying. They won't listen. They beat their heads against a wall and get nowhere.\r\n\r\nBut is there any real difference between these two cases? Are persistent and obstinate people actually behaving differently? Or are they doing the same thing, and we just label them later as persistent or obstinate depending on whether they turned out to be right or not?\r\n\r\nIf that's the only difference then there's nothing to be learned from the distinction. Telling someone to be persistent rather than obstinate would just be telling them to be right rather than wrong, and they already know that. Whereas if persistence and obstinacy are actually different kinds of behavior, it would be worthwhile to tease them apart. [1]\r\n\r\nI've talked to a lot of determined people, and it seems to me that they're different kinds of behavior. I've often walked away from a conversation thinking either \"Wow, that guy is determined\" or \"Damn, that guy is stubborn,\" and I don't think I'm just talking about whether they seemed right or not. That's part of it, but not all of it.\r\n\r\nThere's something annoying about the obstinate that's not simply due to being mistaken. They won't listen. And that's not true of all determined people. I can't think of anyone more determined than the Collison brothers, and when you point out a problem to them, they not only listen, but listen with an almost predatory intensity. Is there a hole in the bottom of their boat? Probably not, but if there is, they want to know about it.\r\n\r\nIt's the same with most successful people. They're never more engaged than when you disagree with them. Whereas the obstinate don't want to hear you. When you point out problems, their eyes glaze over, and their replies sound like ideologues talking about matters of doctrine. [2]\r\n\r\nThe reason the persistent and the obstinate seem similar is that they're both hard to stop. But they're hard to stop in different senses. The persistent are like boats whose engines can't be throttled back. The obstinate are like boats whose rudders can't be turned. [3]\r\n\r\nIn the degenerate case they're indistinguishable: when there's only one way to solve a problem, your only choice is whether to give up or not, and persistence and obstinacy both say no. This is presumably why the two are so often conflated in popular culture. It assumes simple problems. But as problems get more complicated, we can see the difference between them. The persistent are much more attached to points high in the decision tree than to minor ones lower down, while the obstinate spray \"don't give up\" indiscriminately over the whole tree.\r\n\r\nThe persistent are attached to the goal. The obstinate are attached to their ideas about how to reach it.\r\n\r\nWorse still, that means they'll tend to be attached to their first ideas about how to solve a problem, even though these are the least informed by the experience of working on it. So the obstinate aren't merely attached to details, but disproportionately likely to be attached to wrong ones.\r\n\r\n\r\n\r\nWhy are they like this? Why are the obstinate obstinate? One possibility is that they're overwhelmed. They're not very capable. They take on a hard problem. They're immediately in over their head. So they grab onto ideas the way someone on the deck of a rolling ship might grab onto the nearest handhold.\r\n\r\nThat was my initial theory, but on examination it doesn't hold up. If being obstinate were simply a consequence of being in over one's head, you could make persistent people become obstinate by making them solve harder problems. But that's not what happens. If you handed the Collisons an extremely hard problem to solve, they wouldn't become obstinate. If anything they'd become less obstinate. They'd know they had to be open to anything.\r\n\r\nSimilarly, if obstinacy were caused by the situation, the obstinate would stop being obstinate when solving easier problems. But they don't. And if obstinacy isn't caused by the situation, it must come from within. It must be a feature of one's personality.\r\n\r\nObstinacy is a reflexive resistance to changing one's ideas. This is not identical with stupidity, but they're closely related. A reflexive resistance to changing one's ideas becomes a sort of induced stupidity as contrary evidence mounts. And obstinacy is a form of not giving up that's easily practiced by the stupid. You don't have to consider complicated tradeoffs; you just dig in your heels. It even works, up to a point.\r\n\r\nThe fact that obstinacy works for simple problems is an important clue. Persistence and obstinacy aren't opposites. The relationship between them is more like the relationship between the two kinds of respiration we can do: aerobic respiration, and the anaerobic respiration we inherited from our most distant ancestors. Anaerobic respiration is a more primitive process, but it has its uses. When you leap suddenly away from a threat, that's what you're using.\r\n\r\nThe optimal amount of obstinacy is not zero. It can be good if your initial reaction to a setback is an unthinking \"I won't give up,\" because this helps prevent panic. But unthinking only gets you so far. The further someone is toward the obstinate end of the continuum, the less likely they are to succeed in solving hard problems. [4]\r\n\r\n\r\n\r\nObstinacy is a simple thing. Animals have it. But persistence turns out to have a fairly complicated internal structure.\r\n\r\nOne thing that distinguishes the persistent is their energy. At the risk of putting too much weight on words, they persist rather than merely resisting. They keep trying things. Which means the persistent must also be imaginative. To keep trying things, you have to keep thinking of things to try.\r\n\r\nEnergy and imagination make a wonderful combination. Each gets the best out of the other. Energy creates demand for the ideas produced by imagination, which thus produces more, and imagination gives energy somewhere to go. [5]\r\n\r\nMerely having energy and imagination is quite rare. But to solve hard problems you need three more qualities: resilience, good judgement, and a focus on some kind of goal.\r\n\r\nResilience means not having one's morale destroyed by setbacks. Setbacks are inevitable once problems reach a certain size, so if you can't bounce back from them, you can only do good work on a small scale. But resilience is not the same as obstinacy. Resilience means setbacks can't change your morale, not that they can't change your mind.\r\n\r\nIndeed, persistence often requires that one change one's mind. That's where good judgement comes in. The persistent are quite rational. They focus on expected value. It's this, not recklessness, that lets them work on things that are unlikely to succeed.\r\n\r\nThere is one point at which the persistent are often irrational though: at the very top of the decision tree. When they choose between two problems of roughly equal expected value, the choice usually comes down to personal preference. Indeed, they'll often classify projects into deliberately wide bands of expected value in order to ensure that the one they want to work on still qualifies.\r\n\r\nEmpirically this doesn't seem to be a problem. It's ok to be irrational near the top of the decision tree. One reason is that we humans will work harder on a problem we love. But there's another more subtle factor involved as well: our preferences among problems aren't random. When we love a problem that other people don't, it's often because we've unconsciously noticed that it's more important than they realize.\r\n\r\nWhich leads to our fifth quality: there needs to be some overall goal. If you're like me you began, as a kid, merely with the desire to do something great. In theory that should be the most powerful motivator of all, since it includes everything that could possibly be done. But in practice it's not much use, precisely because it includes too much. It doesn't tell you what to do at this moment.\r\n\r\nSo in practice your energy and imagination and resilience and good judgement have to be directed toward some fairly specific goal. Not too specific, or you might miss a great discovery adjacent to what you're searching for, but not too general, or it won't work to motivate you. [6]\r\n\r\nWhen you look at the internal structure of persistence, it doesn't resemble obstinacy at all. It's so much more complex. Five distinct qualities \u2014 energy, imagination, resilience, good judgement, and focus on a goal \u2014 combine to produce a phenomenon that seems a bit like obstinacy in the sense that it causes you not to give up. But the way you don't give up is completely different. Instead of merely resisting change, you're driven toward a goal by energy and resilience, through paths discovered by imagination and optimized by judgement. You'll give way on any point low down in the decision tree, if its expected value drops sufficiently, but energy and resilience keep pushing you toward whatever you chose higher up.\r\n\r\nConsidering what it's made of, it's not surprising that the right kind of stubbornness is so much rarer than the wrong kind, or that it gets so much better results. Anyone can do obstinacy. Indeed, kids and drunks and fools are best at it. Whereas very few people have enough of all five of the qualities that produce the right kind of stubbornness, but when they do the results are magical.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNotes\r\n\r\n[1] I'm going to use \"persistent\" for the good kind of stubborn and \"obstinate\" for the bad kind, but I can't claim I'm simply following current usage. Conventional opinion barely distinguishes between good and bad kinds of stubbornness, and usage is correspondingly promiscuous. I could have invented a new word for the good kind, but it seemed better just to stretch \"persistent.\"\r\n\r\n[2] There are some domains where one can succeed by being obstinate. Some political leaders have been notorious for it. But it won't work in situations where you have to pass external tests. And indeed the political leaders who are famous for being obstinate are famous for getting power, not for using it well.\r\n\r\n[3] There will be some resistance to turning the rudder of a persistent person, because there's some cost to changing direction.\r\n\r\n[4] The obstinate do sometimes succeed in solving hard problems. One way is through luck: like the stopped clock that's right twice a day, they seize onto some arbitrary idea, and it turns out to be right. Another is when their obstinacy cancels out some other form of error. For example, if a leader has overcautious subordinates, their estimates of the probability of success will always be off in the same direction. So if he mindlessly says \"push ahead regardless\" in every borderline case, he'll usually turn out to be right.\r\n\r\n[5] If you stop there, at just energy and imagination, you get the conventional caricature of an artist or poet.\r\n\r\n[6] Start by erring on the small side. If you're inexperienced you'll inevitably err on one side or the other, and if you err on the side of making the goal too broad, you won't get anywhere. Whereas if you err on the small side you'll at least be moving forward. Then, once you're moving, you expand the goal.\r\n\"\"\"", "kind": "Chunk", "id": "tests/vector-store.py#196.86"}, {"og_id": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\vector-store.py_test_gpt_researcher_with_vector_store_", "metadata": {"file_path": "C:\\Users\\jpeng\\Documents\\projects\\test_rtfs\\tests\\repos\\gpt-researcher\\tests\\vector-store.py", "file_name": "vector-store.py", "file_type": "text/x-python", "category": "test", "tokens": 258, "span_ids": ["create_vectorstore", "load_document", "test_gpt_researcher_with_vector_store"], "start_line": 101, "end_line": 140, "community": null}, "content": "@pytest.mark.asyncio\r\nasync def test_gpt_researcher_with_vector_store():\n    docs = load_document()\n    vectorstore = create_vectorstore(docs)\n\n    query = \"\"\"\r\n        Summarize the essay into 3 or 4 succint sections.\r\n        Make sure to include key points regarding the differences between\r\n        persistance vs obstinate.\r\n\r\n        Include some recommendations for entrepeneurs in the conclusion.\r\n        Recommend some ways to increase persistance in a healthy way.\r\n    \"\"\"\n\n\n    # Create an instance of GPTResearcher\r\n    researcher = GPTResearcher(\r\n        query=query,\r\n        report_type=\"research_report\",\r\n        report_source=\"langchain_vectorstore\",\r\n        vector_store=vectorstore,\r\n    )\n\n    # Conduct research and write the report\r\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n\n    assert report is not None\n\n\ndef load_document():\n    document = [Document(page_content=essay)]\n    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=30, separator=\"\\n\")\n    return text_splitter.split_documents(documents=document)\n\n\ndef create_vectorstore(documents: List[Document]):\n    embeddings = OpenAIEmbeddings()\n    return FAISS.from_documents(documents, embeddings)", "kind": "Chunk", "id": "tests/vector-store.py#197.39"}, {"id": 9, "kind": "Cluster", "title": "Research and Draft Workflow Execution", "summary": "This code defines an `EditorAgent` class that coordinates an asynchronous workflow involving research, review, and revision agents to handle research tasks in parallel.", "key_variables": ["DraftState", "EditorAgent", "run_parallel_research", "StateGraph"]}, {"id": 10, "kind": "Cluster", "title": "Research Workflow Management", "summary": "This code defines a research workflow system using various agents to handle different stages, orchestrated by a state graph, to produce a comprehensive report.", "key_variables": ["ResearchState", "ChiefEditorAgent", "StateGraph", "workflow"]}, {"id": 2, "kind": "Cluster", "title": "WebSocket-Driven Research Report Generation", "summary": "Classes and functions for generating various types of research reports through WebSocket connections, leveraging GPT models for a variety of tones and formats.", "key_variables": ["BasicReport", "DetailedReport", "WebSocketManager", "GPTResearcher"]}, {"id": 5, "kind": "Cluster", "title": "Detailed Report Generation and Management", "summary": "This code handles the generation of detailed reports by conducting research, extracting headers and sections, constructing a table of contents, and appending source URLs.", "key_variables": ["DetailedReport", "extract_headers", "extract_sections", "table_of_contents"]}, {"id": 6, "kind": "Cluster", "title": "Report Generation and WebSocket Communication", "summary": "This feature handles WebSocket connections to receive tasks and generate reports in different formats (PDF, DOCX, MD) while also processing human feedback.", "key_variables": ["websocket_endpoint", "run_multi_agents", "write_text_to_md", "write_md_to_word"]}, {"id": 11, "kind": "Cluster", "title": "File Management and Document Loading", "summary": "This feature handles file upload, listing, and deletion, along with loading and processing documents within a specified directory.", "key_variables": ["upload_file", "list_files", "delete_file", "DocumentLoader"]}, {"id": 1, "kind": "Cluster", "title": "Command Line Interface and GPTResearcher Integration", "summary": "This feature provides a command line interface for interacting with the GPTResearcher class to generate various types of research reports based on user queries and specified report types.", "key_variables": ["GPTResearcher", "LangChainDocumentLoader", "ReportType", "get_report_by_type"]}, {"id": 4, "kind": "Cluster", "title": "Configuration, Cost Estimation, and Agent Operations", "summary": "This feature handles the configuration settings, cost estimation for API usage, and agents' operational tasks such as planning research, revising drafts, and generating research sections.", "key_variables": ["Config", "estimate_llm_cost", "EditorAgent", "WriterAgent"]}, {"id": 12, "kind": "Cluster", "title": "Vector Store Content Retrieval and Compression", "summary": "These chunks of code handle the retrieval of relevant content from a vector store based on a given query and compress the results into a readable format.", "key_variables": ["VectorstoreCompressor", "GPTResearcher", "async_get_context", "__get_similar_content_by_query_with_vectorstore"]}, {"id": 8, "kind": "Cluster", "title": "Contextual Document Retrieval and Compression", "summary": "These chunks of code implement a system for retrieving and compressing documents based on their contextual relevance using embeddings and search API integration.", "key_variables": ["ContextCompressor", "SearchAPIRetriever", "GPTResearcher", "DocumentCompressorPipeline"]}, {"id": 7, "kind": "Cluster", "title": "Content Retrieval and Compression System", "summary": "The code segments deal with extracting relevant sections of written content based on similarity queries and compressing them to ensure efficient storage and retrieval.", "key_variables": ["WrittenContentCompressor", "SectionRetriever", "GPTResearcher", "__get_similar_written_contents_by_query"]}, {"id": 17, "kind": "Cluster", "title": "Dynamic Retriever Selection and Tavily API Integration", "summary": "This feature dynamically determines which retriever(s) to utilize based on provided headers, configuration settings, or defaults. It includes specific implementation for integrating with the Tavily API.", "key_variables": ["get_retrievers", "get_default_retriever", "TavilySearch", "get_api_key"]}, {"id": 18, "kind": "Cluster", "title": "Web Content Scraping Feature", "summary": "This feature is responsible for scraping and extracting content from a list of URLs using different scraping strategies and running the extraction process concurrently.", "key_variables": ["scrape_urls", "Scraper", "run", "extract_data_from_link"]}, {"id": 13, "kind": "Cluster", "title": "Detailed Report Subtopics Generation", "summary": "These functions and classes are responsible for generating subtopics for a detailed report based on a main topic and research data. They ensure that the subtopics are non-duplicative, relevant, and ordered meaningfully while adhering to given constraints.", "key_variables": ["generate_subtopics_prompt", "construct_subtopics", "Subtopic", "Subtopics"]}, {"id": 14, "kind": "Cluster", "title": "Chief Editor Agent Initialization and Utilities", "summary": "This feature initializes the ChiefEditorAgent class, sets up the task environment, and includes utility functions for file name sanitization and agent outputs.", "key_variables": ["ChiefEditorAgent", "WriterAgent", "EditorAgent", "sanitize_filename"]}, {"id": 3, "kind": "Cluster", "title": "Research Workflow Automation", "summary": "This feature automates the workflow for orchestrating, reviewing, revising, and writing research tasks, enabling collaborative and structured management of research data.", "key_variables": ["ChiefEditorAgent", "ReviewerAgent", "ReviserAgent", "WriterAgent"]}, {"id": 15, "kind": "Cluster", "title": "Research Report Publishing Module", "summary": "This module handles the conversion and publishing of research reports in various formats such as PDF, DOCX, and Markdown.", "key_variables": ["PublisherAgent", "write_report_by_formats", "run", "write_md_to_pdf"]}, {"id": 16, "kind": "Cluster", "title": "HTML Hyperlink Extraction and Formatting", "summary": "Functions to extract hyperlinks from HTML content using BeautifulSoup, format them for display, and scrape links with Selenium.", "key_variables": ["BeautifulSoup", "extract_hyperlinks", "format_hyperlinks", "scrape_links_with_selenium"]}, {"id": 19, "kind": "Cluster", "title": "Web Scraping and Text Summarization Using LLMs", "summary": "These chunks of code handle web browsing to extract text from a URL and summarize it using a language model, integrating both synchronous and asynchronous programming techniques.", "key_variables": ["summarize_text", "async_browse", "create_chat_completion", "scrape_text_with_selenium"]}], "links": [{"kind": "ChunkToCluster", "source": "memory/draft.py#2.9", "target": 9}, {"kind": "ChunkToCluster", "source": "memory/research.py#3.20", "target": 10}, {"kind": "ImportFrom", "ref": "Tone", "source": "basic_report/basic_report.py#5.46", "target": "utils/enum.py#135.28"}, {"kind": "CallTo", "ref": "GPTResearcher", "source": "basic_report/basic_report.py#5.46", "target": "master/agent.py#65.93"}, {"kind": "ImportFrom", "ref": "GPTResearcher", "source": "basic_report/basic_report.py#5.46", "target": "master/agent.py#65.93"}, {"kind": "ChunkToCluster", "source": "basic_report/basic_report.py#5.46", "target": 2}, {"kind": "CallTo", "ref": "Tone", "source": "detailed_report/detailed_report.py#6.57", "target": "utils/enum.py#135.28"}, {"kind": "ImportFrom", "ref": "Tone", "source": "detailed_report/detailed_report.py#6.57", "target": "utils/enum.py#135.28"}, {"kind": "CallTo", "ref": "GPTResearcher", "source": "detailed_report/detailed_report.py#6.57", "target": "master/agent.py#65.93"}, {"kind": "ImportFrom", "ref": "GPTResearcher", "source": "detailed_report/detailed_report.py#6.57", "target": "master/agent.py#65.93"}, {"kind": "ChunkToCluster", "source": "detailed_report/detailed_report.py#6.57", "target": 2}, {"kind": "CallTo", "ref": "GPTResearcher", "source": "detailed_report/detailed_report.py#10.61", "target": "master/agent.py#65.93"}, {"kind": "ImportFrom", "ref": "GPTResearcher", "source": "detailed_report/detailed_report.py#10.61", "target": "master/agent.py#65.93"}, {"kind": "ImportFrom", "ref": "extract_headers", "source": "detailed_report/detailed_report.py#10.61", "target": "master/actions.py#61.34"}, {"kind": "ImportFrom", "ref": "extract_headers", "source": "detailed_report/detailed_report.py#10.61", "target": "master/actions.py#61.34"}, {"kind": "ImportFrom", "ref": "extract_sections", "source": "detailed_report/detailed_report.py#10.61", "target": "master/actions.py#62.30"}, {"kind": "CallTo", "ref": "table_of_contents", "source": "detailed_report/detailed_report.py#10.61", "target": "master/actions.py#63.25"}, {"kind": "ImportFrom", "ref": "table_of_contents", "source": "detailed_report/detailed_report.py#10.61", "target": "master/actions.py#63.25"}, {"kind": "ImportFrom", "ref": "add_source_urls", "source": "detailed_report/detailed_report.py#10.61", "target": "master/actions.py#64.22"}, {"kind": "ChunkToCluster", "source": "detailed_report/detailed_report.py#10.61", "target": 5}, {"kind": "ImportFrom", "ref": "WebSocketManager", "source": "backend/server.py#13.26", "target": "backend/websocket_manager.py#21.60"}, {"kind": "ChunkToCluster", "source": "backend/server.py#13.26", "target": 2}, {"kind": "ImportFrom", "ref": "write_md_to_pdf", "source": "backend/server.py#14.61", "target": "backend/utils.py#19.24"}, {"kind": "ImportFrom", "ref": "write_md_to_word", "source": "backend/server.py#14.61", "target": "backend/utils.py#20.31"}, {"kind": "CallTo", "ref": "write_text_to_md", "source": "backend/server.py#14.61", "target": "backend/utils.py#18.32"}, {"kind": "ImportFrom", "ref": "write_text_to_md", "source": "backend/server.py#14.61", "target": "backend/utils.py#18.32"}, {"kind": "ImportFrom", "ref": "run_research_task", "source": "backend/server.py#14.61", "target": "multi_agents/main.py#171.51"}, {"kind": "ImportFrom", "ref": "stream_output", "source": "backend/server.py#14.61", "target": "master/actions.py#59.23"}, {"kind": "ChunkToCluster", "source": "backend/server.py#14.61", "target": 6}, {"kind": "CallTo", "ref": "DocumentLoader", "source": "backend/server.py#17.45", "target": "document/document.py#40.40"}, {"kind": "ImportFrom", "ref": "DocumentLoader", "source": "backend/server.py#17.45", "target": "document/document.py#40.40"}, {"kind": "ChunkToCluster", "source": "backend/server.py#17.45", "target": 11}, {"kind": "ChunkToCluster", "source": "backend/utils.py#18.32", "target": 6}, {"kind": "ChunkToCluster", "source": "backend/utils.py#19.24", "target": 6}, {"kind": "ChunkToCluster", "source": "backend/utils.py#20.31", "target": 6}, {"kind": "CallTo", "ref": "Tone", "source": "backend/websocket_manager.py#21.60", "target": "utils/enum.py#135.28"}, {"kind": "ImportFrom", "ref": "Tone", "source": "backend/websocket_manager.py#21.60", "target": "utils/enum.py#135.28"}, {"kind": "ChunkToCluster", "source": "backend/websocket_manager.py#21.60", "target": 2}, {"kind": "ImportFrom", "ref": "Tone", "source": "backend/websocket_manager.py#22.41", "target": "utils/enum.py#135.28"}, {"kind": "CallTo", "ref": "run_research_task", "source": "backend/websocket_manager.py#22.41", "target": "multi_agents/main.py#171.51"}, {"kind": "ImportFrom", "ref": "run_research_task", "source": "backend/websocket_manager.py#22.41", "target": "multi_agents/main.py#171.51"}, {"kind": "CallTo", "ref": "stream_output", "source": "backend/websocket_manager.py#22.41", "target": "master/actions.py#59.23"}, {"kind": "ImportFrom", "ref": "stream_output", "source": "backend/websocket_manager.py#22.41", "target": "master/actions.py#59.23"}, {"kind": "CallTo", "ref": "ReportType", "source": "backend/websocket_manager.py#22.41", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "backend/websocket_manager.py#22.41", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "DetailedReport", "source": "backend/websocket_manager.py#22.41", "target": "detailed_report/detailed_report.py#6.57"}, {"kind": "ImportFrom", "ref": "DetailedReport", "source": "backend/websocket_manager.py#22.41", "target": "detailed_report/detailed_report.py#6.57"}, {"kind": "CallTo", "ref": "BasicReport", "source": "backend/websocket_manager.py#22.41", "target": "basic_report/basic_report.py#5.46"}, {"kind": "ImportFrom", "ref": "BasicReport", "source": "backend/websocket_manager.py#22.41", "target": "basic_report/basic_report.py#5.46"}, {"kind": "ChunkToCluster", "source": "backend/websocket_manager.py#22.41", "target": 2}, {"kind": "ImportFrom", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "gpt-researcher/cli.py#23.62", "target": "utils/enum.py#134.18"}, {"kind": "ChunkToCluster", "source": "gpt-researcher/cli.py#23.62", "target": 1}, {"kind": "ChunkToCluster", "source": "config/config.py#28.51", "target": 4}, {"kind": "ChunkToCluster", "source": "context/compression.py#32.15", "target": 12}, {"kind": "CallTo", "ref": "SearchAPIRetriever", "source": "context/compression.py#33.21", "target": "context/retriever.py#37.28"}, {"kind": "ImportFrom", "ref": "SearchAPIRetriever", "source": "context/compression.py#33.21", "target": "context/retriever.py#37.28"}, {"kind": "ChunkToCluster", "source": "context/compression.py#33.21", "target": 8}, {"kind": "CallTo", "ref": "SectionRetriever", "source": "context/compression.py#35.20", "target": "context/retriever.py#38.31"}, {"kind": "ImportFrom", "ref": "SectionRetriever", "source": "context/compression.py#35.20", "target": "context/retriever.py#38.31"}, {"kind": "ChunkToCluster", "source": "context/compression.py#35.20", "target": 7}, {"kind": "ChunkToCluster", "source": "context/retriever.py#37.28", "target": 8}, {"kind": "ChunkToCluster", "source": "context/retriever.py#38.31", "target": 7}, {"kind": "ChunkToCluster", "source": "document/document.py#40.40", "target": 11}, {"kind": "ChunkToCluster", "source": "document/langchain_document.py#42.24", "target": 1}, {"kind": "ImportFrom", "ref": "TavilySearch", "source": "master/actions.py#50.35", "target": "tavily/tavily_search.py#120.40"}, {"kind": "ChunkToCluster", "source": "master/actions.py#50.35", "target": 17}, {"kind": "ImportFrom", "ref": "Scraper", "source": "master/actions.py#54.21", "target": "scraper/scraper.py#129.37"}, {"kind": "ChunkToCluster", "source": "master/actions.py#54.21", "target": 18}, {"kind": "ImportFrom", "ref": "Tone", "source": "master/actions.py#58.59", "target": "utils/enum.py#135.28"}, {"kind": "ChunkToCluster", "source": "master/actions.py#58.59", "target": 2}, {"kind": "ChunkToCluster", "source": "master/actions.py#59.23", "target": 2}, {"kind": "ChunkToCluster", "source": "master/actions.py#61.34", "target": 5}, {"kind": "ChunkToCluster", "source": "master/actions.py#62.30", "target": 5}, {"kind": "ChunkToCluster", "source": "master/actions.py#63.25", "target": 5}, {"kind": "ChunkToCluster", "source": "master/actions.py#64.22", "target": 5}, {"kind": "CallTo", "ref": "ReportType", "source": "master/agent.py#65.93", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/agent.py#65.93", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportSource", "source": "master/agent.py#65.93", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportSource", "source": "master/agent.py#65.93", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "Tone", "source": "master/agent.py#65.93", "target": "utils/enum.py#135.28"}, {"kind": "ImportFrom", "ref": "Tone", "source": "master/agent.py#65.93", "target": "utils/enum.py#135.28"}, {"kind": "ImportFrom", "ref": "Tone", "source": "master/agent.py#65.93", "target": "utils/enum.py#135.28"}, {"kind": "CallTo", "ref": "Memory", "source": "master/agent.py#65.93", "target": "memory/embeddings.py#94.57"}, {"kind": "ImportFrom", "ref": "Memory", "source": "master/agent.py#65.93", "target": "memory/embeddings.py#94.57"}, {"kind": "ChunkToCluster", "source": "master/agent.py#65.93", "target": 2}, {"kind": "CallTo", "ref": "ReportSource", "source": "master/agent.py#66.68", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportSource", "source": "master/agent.py#66.68", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportSource", "source": "master/agent.py#66.68", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportSource", "source": "master/agent.py#66.68", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportSource", "source": "master/agent.py#66.68", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportSource", "source": "master/agent.py#66.68", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportSource", "source": "master/agent.py#66.68", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "LangChainDocumentLoader", "source": "master/agent.py#66.68", "target": "document/langchain_document.py#42.24"}, {"kind": "ImportFrom", "ref": "LangChainDocumentLoader", "source": "master/agent.py#66.68", "target": "document/langchain_document.py#42.24"}, {"kind": "ChunkToCluster", "source": "master/agent.py#66.68", "target": 1}, {"kind": "CallTo", "ref": "VectorstoreCompressor", "source": "master/agent.py#75.14", "target": "context/compression.py#32.15"}, {"kind": "ImportFrom", "ref": "VectorstoreCompressor", "source": "master/agent.py#75.14", "target": "context/compression.py#32.15"}, {"kind": "ChunkToCluster", "source": "master/agent.py#75.14", "target": 12}, {"kind": "ImportFrom", "ref": "ContextCompressor", "source": "master/agent.py#76.16", "target": "context/compression.py#33.21"}, {"kind": "ChunkToCluster", "source": "master/agent.py#76.16", "target": 8}, {"kind": "CallTo", "ref": "WrittenContentCompressor", "source": "master/agent.py#80.45", "target": "context/compression.py#35.20"}, {"kind": "ImportFrom", "ref": "WrittenContentCompressor", "source": "master/agent.py#80.45", "target": "context/compression.py#35.20"}, {"kind": "ChunkToCluster", "source": "master/agent.py#80.45", "target": 7}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#82.34", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#82.34", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#82.34", "target": "utils/enum.py#134.18"}, {"kind": "ChunkToCluster", "source": "master/prompts.py#82.34", "target": 1}, {"kind": "ImportFrom", "ref": "ReportSource", "source": "master/prompts.py#83.49", "target": "utils/enum.py#134.18"}, {"kind": "ChunkToCluster", "source": "master/prompts.py#83.49", "target": 1}, {"kind": "ImportFrom", "ref": "ReportSource", "source": "master/prompts.py#84.35", "target": "utils/enum.py#134.18"}, {"kind": "ChunkToCluster", "source": "master/prompts.py#84.35", "target": 1}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#85.32", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#85.32", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#85.32", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#85.32", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#85.32", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#85.32", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#85.32", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#85.32", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#85.32", "target": "utils/enum.py#134.18"}, {"kind": "ChunkToCluster", "source": "master/prompts.py#85.32", "target": 1}, {"kind": "ChunkToCluster", "source": "master/prompts.py#88.25", "target": 13}, {"kind": "ImportFrom", "ref": "Tone", "source": "master/prompts.py#89.69", "target": "utils/enum.py#135.28"}, {"kind": "ChunkToCluster", "source": "master/prompts.py#89.69", "target": 2}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "CallTo", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "ImportFrom", "ref": "ReportType", "source": "master/prompts.py#92.21", "target": "utils/enum.py#134.18"}, {"kind": "ChunkToCluster", "source": "master/prompts.py#92.21", "target": 1}, {"kind": "ChunkToCluster", "source": "memory/embeddings.py#94.57", "target": 2}, {"kind": "ChunkToCluster", "source": "tavily/tavily_search.py#120.40", "target": 17}, {"kind": "ChunkToCluster", "source": "scraper/scraper.py#129.37", "target": 18}, {"kind": "ChunkToCluster", "source": "utils/costs.py#133.25", "target": 4}, {"kind": "ChunkToCluster", "source": "utils/enum.py#134.18", "target": 1}, {"kind": "ChunkToCluster", "source": "utils/enum.py#135.28", "target": 2}, {"kind": "CallTo", "ref": "estimate_llm_cost", "source": "utils/llm.py#137.48", "target": "utils/costs.py#133.25"}, {"kind": "ImportFrom", "ref": "estimate_llm_cost", "source": "utils/llm.py#137.48", "target": "utils/costs.py#133.25"}, {"kind": "ChunkToCluster", "source": "utils/llm.py#137.48", "target": 4}, {"kind": "ImportFrom", "ref": "Subtopics", "source": "utils/llm.py#138.32", "target": "utils/validators.py#139.9"}, {"kind": "ImportFrom", "ref": "generate_subtopics_prompt", "source": "utils/llm.py#138.32", "target": "master/prompts.py#88.25"}, {"kind": "ChunkToCluster", "source": "utils/llm.py#138.32", "target": 13}, {"kind": "ChunkToCluster", "source": "utils/validators.py#139.9", "target": 13}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/editor.py#144.69", "target": "utils/views.py#166.15"}, {"kind": "CallTo", "ref": "call_model", "source": "agents/editor.py#144.69", "target": "utils/llms.py#164.50"}, {"kind": "ImportFrom", "ref": "call_model", "source": "agents/editor.py#144.69", "target": "utils/llms.py#164.50"}, {"kind": "ChunkToCluster", "source": "agents/editor.py#144.69", "target": 4}, {"kind": "CallTo", "ref": "DraftState", "source": "agents/editor.py#145.55", "target": "memory/draft.py#2.9"}, {"kind": "ImportFrom", "ref": "DraftState", "source": "agents/editor.py#145.55", "target": "memory/draft.py#2.9"}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/editor.py#145.55", "target": "utils/views.py#166.15"}, {"kind": "ChunkToCluster", "source": "agents/editor.py#145.55", "target": 9}, {"kind": "CallTo", "ref": "sanitize_filename", "source": "agents/master.py#147.28", "target": "utils/utils.py#165.26"}, {"kind": "ImportFrom", "ref": "sanitize_filename", "source": "agents/master.py#147.28", "target": "utils/utils.py#165.26"}, {"kind": "ChunkToCluster", "source": "agents/master.py#147.28", "target": 14}, {"kind": "CallTo", "ref": "ResearchState", "source": "agents/master.py#148.33", "target": "memory/research.py#3.20"}, {"kind": "ImportFrom", "ref": "ResearchState", "source": "agents/master.py#148.33", "target": "memory/research.py#3.20"}, {"kind": "ChunkToCluster", "source": "agents/master.py#148.33", "target": 10}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/master.py#149.21", "target": "utils/views.py#166.15"}, {"kind": "ChunkToCluster", "source": "agents/master.py#149.21", "target": 3}, {"kind": "CallTo", "ref": "write_md_to_pdf", "source": "agents/publisher.py#151.17", "target": "utils/file_formats.py#162.26"}, {"kind": "ImportFrom", "ref": "write_md_to_pdf", "source": "agents/publisher.py#151.17", "target": "utils/file_formats.py#162.26"}, {"kind": "ChunkToCluster", "source": "agents/publisher.py#151.17", "target": 15}, {"kind": "ImportFrom", "ref": "call_model", "source": "agents/reviewer.py#156.60", "target": "utils/llms.py#164.50"}, {"kind": "CallTo", "ref": "print_agent_output", "source": "agents/reviewer.py#156.60", "target": "utils/views.py#166.15"}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/reviewer.py#156.60", "target": "utils/views.py#166.15"}, {"kind": "ChunkToCluster", "source": "agents/reviewer.py#156.60", "target": 3}, {"kind": "CallTo", "ref": "print_agent_output", "source": "agents/reviewer.py#157.17", "target": "utils/views.py#166.15"}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/reviewer.py#157.17", "target": "utils/views.py#166.15"}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/reviewer.py#157.17", "target": "utils/views.py#166.15"}, {"kind": "ChunkToCluster", "source": "agents/reviewer.py#157.17", "target": 3}, {"kind": "CallTo", "ref": "call_model", "source": "agents/reviser.py#158.51", "target": "utils/llms.py#164.50"}, {"kind": "ImportFrom", "ref": "call_model", "source": "agents/reviser.py#158.51", "target": "utils/llms.py#164.50"}, {"kind": "ChunkToCluster", "source": "agents/reviser.py#158.51", "target": 4}, {"kind": "CallTo", "ref": "print_agent_output", "source": "agents/reviser.py#159.21", "target": "utils/views.py#166.15"}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/reviser.py#159.21", "target": "utils/views.py#166.15"}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/reviser.py#159.21", "target": "utils/views.py#166.15"}, {"kind": "ChunkToCluster", "source": "agents/reviser.py#159.21", "target": 3}, {"kind": "ChunkToCluster", "source": "utils/file_formats.py#162.26", "target": 15}, {"kind": "CallTo", "ref": "Config", "source": "utils/llms.py#164.50", "target": "config/config.py#28.51"}, {"kind": "ImportFrom", "ref": "Config", "source": "utils/llms.py#164.50", "target": "config/config.py#28.51"}, {"kind": "CallTo", "ref": "create_chat_completion", "source": "utils/llms.py#164.50", "target": "utils/llm.py#137.48"}, {"kind": "ImportFrom", "ref": "create_chat_completion", "source": "utils/llms.py#164.50", "target": "utils/llm.py#137.48"}, {"kind": "ChunkToCluster", "source": "utils/llms.py#164.50", "target": 4}, {"kind": "ChunkToCluster", "source": "utils/utils.py#165.26", "target": 14}, {"kind": "ChunkToCluster", "source": "utils/views.py#166.15", "target": 3}, {"kind": "CallTo", "ref": "call_model", "source": "agents/writer.py#168.51", "target": "utils/llms.py#164.50"}, {"kind": "ImportFrom", "ref": "call_model", "source": "agents/writer.py#168.51", "target": "utils/llms.py#164.50"}, {"kind": "ChunkToCluster", "source": "agents/writer.py#168.51", "target": 4}, {"kind": "ImportFrom", "ref": "call_model", "source": "agents/writer.py#169.23", "target": "utils/llms.py#164.50"}, {"kind": "ChunkToCluster", "source": "agents/writer.py#169.23", "target": 4}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/writer.py#170.49", "target": "utils/views.py#166.15"}, {"kind": "CallTo", "ref": "print_agent_output", "source": "agents/writer.py#170.49", "target": "utils/views.py#166.15"}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/writer.py#170.49", "target": "utils/views.py#166.15"}, {"kind": "CallTo", "ref": "print_agent_output", "source": "agents/writer.py#170.49", "target": "utils/views.py#166.15"}, {"kind": "ImportFrom", "ref": "print_agent_output", "source": "agents/writer.py#170.49", "target": "utils/views.py#166.15"}, {"kind": "ChunkToCluster", "source": "agents/writer.py#170.49", "target": 3}, {"kind": "CallTo", "ref": "Tone", "source": "multi_agents/main.py#171.51", "target": "utils/enum.py#135.28"}, {"kind": "ImportFrom", "ref": "Tone", "source": "multi_agents/main.py#171.51", "target": "utils/enum.py#135.28"}, {"kind": "ChunkToCluster", "source": "multi_agents/main.py#171.51", "target": 2}, {"kind": "ChunkToCluster", "source": "processing/html.py#175.33", "target": 16}, {"kind": "ChunkToCluster", "source": "processing/text.py#177.58", "target": 19}, {"kind": "ImportFrom", "ref": "summarize_text", "source": "scraping/web_scrape.py#183.56", "target": "processing/text.py#177.58"}, {"kind": "ChunkToCluster", "source": "scraping/web_scrape.py#183.56", "target": 19}, {"kind": "ImportFrom", "ref": "extract_hyperlinks", "source": "scraping/web_scrape.py#187.17", "target": "processing/html.py#175.33"}, {"kind": "ImportFrom", "ref": "format_hyperlinks", "source": "scraping/web_scrape.py#187.17", "target": "processing/html.py#175.33"}, {"kind": "ChunkToCluster", "source": "scraping/web_scrape.py#187.17", "target": 16}, {"kind": "ImportFrom", "ref": "GPTResearcher", "source": "tests/documents-report-source.py#191.27", "target": "master/agent.py#65.93"}, {"kind": "ChunkToCluster", "source": "tests/documents-report-source.py#191.27", "target": 2}]}}