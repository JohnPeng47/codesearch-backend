{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "bin_diff.py b'#!/usr/bin/python\\r\\n\\r\\n# purpose of this is to incrementally make file b the same as a in order to isolate the bytes that\\r\\n# triggered the crash\\r\\nimport sys\\r\\nimport subprocess\\r\\nfrom unittest import TextTestResult\\r\\nfrom crashwalk import GDBJob, Exploitable, NoCrashException, run_GDBWorker\\r\\nfrom concurrent.futures import as_completed\\r\\nfrom utils import bytes_to_hex_str, hex_str_to_bytes, add_bytes, hex_str_to_int, serialize_exploitables, CustomThreadPoolExecutor, GDBExecutor, replaceBytes, replaceBytesDiff\\r\\nfrom constants import DWORD, QWORD\\r\\nimport argparse\\r\\nimport os\\r\\nimport logging\\r\\nimport math\\r\\nimport glob\\r\\nimport pickle\\r\\nimport re\\r\\nfrom multiprocessing import cpu_count\\r\\nimport datetime\\r\\nfrom typing import List\\r\\nimport shutil\\r\\n\\r\\nGDB_PROCS = cpu_count()\\r\\n# disable logging from pwn\\r\\nlogging.getLogger(\"pwnlib\").setLevel(logging.WARNING)\\r\\n\\r\\nclass CrashedBytes:\\r\\n    def __init__(self, child_crash, child_sgsev, parent_crash):\\r\\n        self.child_crash = child_crash\\r\\n        self.child_sgsev = child_sgsev\\r\\n        # self.executable = executable => don\\'t need since defined as a global\\r\\n        self.tmp_dir = \"/tmp/modified\"\\r\\n        try:\\r\\n            os.mkdir(\"/tmp/modified\")\\r\\n        except Exception:\\r\\n            pass\\r\\n        # executing gdb jobs to get the segfault address\\r\\n        self.executor = GDBExecutor(executable)\\r\\n        # get diff of child and parent via radiff2\\r\\n        diff = diff_crash(child_crash, parent_crash)\\r\\n        if len(diff) > 30:\\r\\n            print(\"Skipping too big..\")\\r\\n            # TODO: change this\\r\\n            raise NoCrashException\\r\\n\\r\\n        # find crashing offset\\r\\n        self.offset, self.modified_bytes, self.modified_parent = self.get_crashing_offset(parent_crash, diff)\\r\\n\\r\\n    def get_crashing_offset(self, parent, diff):\\r\\n        child_file = self.child_crash[self.child_crash.rindex(\"/\") + 1:]\\r\\n        t_pool = CustomThreadPoolExecutor(max_workers=GDB_PROCS)\\r\\n        pending_futures = []\\r\\n        modified_parents = {}\\r\\n        # copy a newly modified file for each line in the diff\\r\\n        for i, (parent_off, child_bytes) in enumerate(diff):\\r\\n            print(f\"{parent_off}: {child_bytes}\")\\r\\n            modified_parent = os.path.join(self.tmp_dir, child_file + str(i) + \".modified\")\\r\\n            modified_parents[modified_parent] = (parent_off, child_bytes)\\r\\n            partial_diff = diff[:i+1]\\r\\n            pending_futures.append(t_pool.submit( replaceBytesDiff, parent, modified_parent, partial_diff))\\r\\n\\r\\n        for f in as_completed(pending_futures):\\r\\n            if not f.result():\\r\\n                print(\"Print exiting...\")\\r\\n                sys.exit()\\r\\n\\r\\n        offset = None\\r\\n        # execute subprocesses to determine which diff lines crashes the input\\r\\n        future_jobs = self.executor.run_jobs(modified_parents.keys())\\r\\n        offset, modified_bytes, parent_crash = None, None, None\\r\\n        try:\\r\\n            for f in future_jobs:\\r\\n                # Attempt #1:\\r\\n                # this doesn\\'t work because all futures are iterated for in the beginning, without the chance for one to complete execution\\r\\n                # if finished:\\r\\n                #     print(\\'Cancelling..\\')\\r\\n                #     t_pool.shutdown(wait=False)\\r\\n                res = f.result()\\r\\n                parent_sgsev = res.segfault\\r\\n                parent_crash = res.crash_file\\r\\n                if parent_sgsev == self.child_sgsev:\\r\\n                    print(\"PARENT CRASH: >>>>> \", res.crash_file)\\r\\n                    offset = modified_parents[parent_crash][0]\\r\\n                    modified_bytes = modified_parents[parent_crash][1]\\r\\n                    print(\"Found crash triggering input fileoffset @ {}, segfaulting addr: {}, parent crash original: {}\"\\r\\n                        .format(offset, parent_sgsev, parent_crash))\\r\\n                    raise ValueError\\r\\n        # Attempt #2: This will execute unlike attempt #1, but will not cancel pending jobs\\r\\n        # wait=False just allows the function to return earlier rather than waiting for completion, but neither cancels pending jobs\\r\\n        except ValueError:\\r\\n            print(\"Canceling\")\\r\\n            t_pool.shutdown(wait=True, cancel_futures=True)\\r\\n    \\r\\n        # should be returning the unmodified parent here instead of parent_crash\\r\\n        return offset, modified_bytes, parent_crash\\r\\n\\r\\n# Create files for modification\\r\\nclass PrepFiles:\\r\\n    def __init__(self, modified_file, new_bytes, offset):\\r\\n        self.modified_file = modified_file\\r\\n        self.new_bytes = new_bytes\\r\\n        self.offset = offset\\r\\n        self.linearity = False\\r\\n        self.tmp_dir = \"/tmp/modified/linearity\"\\r\\n        # is this thread safe?\\r\\n        self.f_index = 0\\r\\n        self.filenames = []\\r\\n        try:\\r\\n            os.rmdir(\"/tmp/modified/linearity\")\\r\\n            os.mkdir(\"/tmp/modified/linearity\")\\r\\n        except Exception:\\r\\n            os.mkdir(\"/tmp/modified/linearity\")\\r\\n\\r\\n    def __enter__(self):\\r\\n        for b in self.new_bytes:\\r\\n            # if byte is zero, increment it\\r\\n            b = b if b != b\"\\\\x00\" else bytes([int(b) + 1])\\r\\n            # use offset to mark out unique files\\r\\n            filename = os.path.join(self.tmp_dir, \"_offset:{}_\".format(str(self.offset)) + str(self.f_index))\\r\\n            shutil.copy(self.modified_file, filename)\\r\\n            print(\"byte: {}\".format(b))\\r\\n            with open(filename, \"rb+\") as handle:\\r\\n                replaceBytes(handle, self.offset, b)\\r\\n            self.filenames.append(filename)\\r\\n            self.f_index += 1\\r\\n        return self\\r\\n\\r\\n    def __exit__(self, exc_type, exc_value, exc_tb):\\r\\n        if exc_type:\\r\\n            print(\"Exception occured of type: {} occurred, value: {}, trace: {}\".format(exc_type, exc_value, exc_tb))\\r\\n        # dont bother saving non-linear modified crashes\\r\\n        if not self.linearity:\\r\\n            for f in self.filenames:\\r\\n                os.remove(f)\\r\\n\\r\\n    def get_linear_crashes(self):\\r\\n        return glob.glob(os.path.join(self.tmp_dir, \"*\"))\\r\\n\\r\\nclass BinDiff:\\r\\n    def __init__(self, child_crash, parent_crash):\\r\\n        self.executor = GDBExecutor(executable)\\r\\n\\r\\n        self.child_sgsev, self.parent_sgsev = self._check_segfault(child_crash, parent_crash)\\r\\n        if not self.parent_sgsev and not self.child_sgsev:\\r\\n            raise NoCrashException\\r\\n        \\r\\n        crashed = CrashedBytes(child_crash, self.child_sgsev, parent_crash)\\r\\n        offset = crashed.offset\\r\\n        modified_bytes = crashed.modified_bytes\\r\\n        modified_parent = crashed.modified_parent\\r\\n        self.linearity, self.bytes_controlled = self._find_control_width(offset, modified_bytes, modified_parent)\\r\\n\\r\\n    def _dword_byte_pos(self, delta):\\r\\n        return int(math.log(delta,2))\\r\\n\\r\\n    def _check_segfault(self, child_crash, parent_crash):\\r\\n        # check that parent and child have diff crash sites\\r\\n        try:\\r\\n            futures = self.executor.run_jobs([child_crash, parent_crash], ordered=True)\\r\\n            child_sgsev = next(futures).segfault\\r\\n            parent_sgsev = next(futures).segfault\\r\\n            # child_sgsev, parent_sgsev = futures[0].result().segfault, futures[1].result().segfault\\r\\n        except AttributeError:\\r\\n            raise NoCrashException(\"Check segfault crashed\")\\r\\n        if not child_sgsev or not parent_sgsev:\\r\\n            print(\"Either the child or the parent did not crash\")\\r\\n            return None, None\\r\\n        if child_sgsev == parent_sgsev:\\r\\n            print(\"Child segfault == Parent segfault, skipping {}\".format(child_crash))\\r\\n            return None, None\\r\\n        return child_sgsev, parent_sgsev\\r\\n\\r\\n    def _is_linear(self, segfaults: List[int]):\\r\\n        linearity = False\\r\\n        byte_pos = None\\r\\n\\r\\n        segfaults = map(hex_str_to_int, segfaults)\\r\\n        segfaults = list(segfaults)\\r\\n\\r\\n        # check if segfaults resulting from modifying the same byte remains the same ie. there is a linear relationship between that byte and the segfault\\r\\n        if (segfaults[2] - segfaults[1]) == (segfaults[1] - segfaults[0]) and ((segfaults[1] - segfaults[0]) != 0):\\r\\n            linearity = True\\r\\n            segfault_delta = abs(segfaults[2] - segfaults[1])\\r\\n            if segfault_delta:\\r\\n                byte_pos = self._dword_byte_pos(segfault_delta)\\r\\n                # bytes_controlled[i+3] = byte_pos\\r\\n                # if segfault_delta < smallest_diff:\\r\\n                #     smallest_diff = segfault_delta\\r\\n                #     closest_segfaults = segfaults\\r\\n            print(\"Linear relationship found: {}\", segfaults)\\r\\n        return linearity, byte_pos\\r\\n\\r\\n    def _get_byte_n_offset(self, mod_bytes, offset, modified_file, struct_size=DWORD):\\r\\n        possible_bytes = bytes()\\r\\n        off_start = offset - struct_size + 1\\r\\n        off_end = offset + len(mod_bytes)\\r\\n        with open(modified_file, \"rb\") as handle:\\r\\n            handle.seek(off_start)\\r\\n            possible_bytes += handle.read(struct_size - 1)\\r\\n            possible_bytes += mod_bytes\\r\\n            handle.seek(off_end)\\r\\n            possible_bytes += handle.read(struct_size - 1)\\r\\n        print(type(possible_bytes))\\r\\n        return zip(possible_bytes, range(off_start, off_end + struct_size))\\r\\n\\r\\n    def _find_control_width(self, mod_offset, mod_bytes, modified_file):\\r\\n        linear_relationship = False\\r\\n        bytes_controlled = [None] * (len(mod_bytes) + 2 * (DWORD - 1))\\r\\n        # Test 1: subtract/add n bytes to the mod_bytes @ offset, then compare segfaulting addresses\\r\\n        # to detect if linear relationship exists\\r\\n        # For now we treat all crashing bytes as an integer offset stored in a DWORD\\r\\n        if len(mod_bytes) <= 4:\\r\\n            # get the bytes that come before/after the DWORD/QWORD in memory\\r\\n            bytes_n_offsets = self._get_byte_n_offset(mod_bytes, mod_offset, modified_file, struct_size=DWORD)\\r\\n\\r\\n            # Apparently iterating over bytes in Python will yield ints\\r\\n            for byte, offset in bytes_n_offsets:\\r\\n                assert(type(byte) == int)\\r\\n                # Note: add_bytes adds a wraparound behaviour that is not currently accounted for in the linearity calculation\\r\\n                inc_bytes =  [add_bytes(byte, i) for i in range(0, 3)]\\r\\n                inc_bytes =  bytes(inc_bytes)\\r\\n                with PrepFiles(modified_file, inc_bytes, offset) as files:\\r\\n                    exploitables = self.executor.run_jobs(files.filenames, ordered=True)\\r\\n                    # if any one of the files do not crash, just skip this batch of files\\r\\n                    try:\\r\\n                        segfaults = [e.segfault for e in exploitables]\\r\\n                        print(segfaults)\\r\\n                    except Exception as e:\\r\\n                        print(e)\\r\\n                        continue\\r\\n                # check if segfaults resulting from modifying the same byte remains the same ie. there is a linear relationship between that byte and the segfault\\r\\n                linearity, byte_pos = self._is_linear(segfaults)\\r\\n                if linearity:\\r\\n                    # which bytes in the segfault are controllable\\r\\n                    bytes_controlled[offset - mod_offset + DWORD - 1] = byte_pos\\r\\n                linear_relationship = linearity if not linear_relationship else True\\r\\n        \\r\\n            print(\"bytes_controlled: \", bytes_controlled)\\r\\n        # Test 2: Random tests\\r\\n        return linear_relationship, bytes_controlled\\r\\n\\r\\n    def get_crash_analysis(self):\\r\\n        try:\\r\\n            return self.linearity, self.bytes_controlled\\r\\n        except AttributeError:\\r\\n            return None, None, None\\r\\n\\r\\n# Basic algorithm\\r\\n# 1. Find the most popular crash site\\r\\n# 2. Find the least different crash file with a different segfaulting address\\r\\n# 3. Find the fileoffset that triggers crash by replacing successive byte ranges between the input file diffs\\r\\n# -> Greedy strategy: to only change bytes that that differ for the current comparison; this decreases the likelihood of\\r\\n# a false positive in identifying the input file offset for controlling the crash/segfault address (crashing offset)\\r\\n# -> Non-greedy strategy: replace all the bytes, but this\\r\\n# 4. When byte range(s) have been identified, then apply control_width_discovery algorithm for finding the control width\\r\\n# Prior research by CSE Group used a Metasploit style unique byte ranges strategy, where the identified byte ranges were replaced\\r\\n# with unqiue byte sequences, which allowed for easy correlation between the segfaulting address and the input file offset. However,\\r\\n# this strategy only works in the case where the input file bytes are directly accessed as a memory address, without any transformation\\r\\n# In the case where bytes could be transformed before accessed as memory (ie. some basic linear transformation), this strategy will not\\r\\n# be able to identify the crashing offset, since bytes in the crash could be very different than their representation in the input file\\r\\n# 5. Repeat with a less optimal crash file (reason for this may be due to complex operations)\\r\\n\\r\\n# TODO: USE THIS FUNCTION\\r\\ndef get_afl_queue_dir(crash_filepath):\\r\\n    crash_name = crash_filepath[crash_filepath.rindex(\"/\") + 1:]\\r\\n    crash_dir = crash_filepath[:crash_filepath.rindex(\"/\")]\\r\\n    parent_id = crash_name.split(\",\")[0]\\r\\n    queue_dir = os.path.join(crash_dir[:crash_dir.rindex(\"/\")], \"queue\")\\r\\n\\r\\n# handle\\r\\ndef get_parent_id(crash_file):\\r\\n    # delimiters = [\":\", \"_\"]\\r\\n    # print(\"crashing_file:\", crash_file)\\r\\n    delimiters = [\":\"]\\r\\n    try:\\r\\n        crash_name = crash_file[crash_file.rindex(\"/\"):]\\r\\n    except IndexError:\\r\\n        crash_name = crash_file\\r\\n    except ValueError:\\r\\n        crash_name = crash_file\\r\\n    # afl have different path delimiters\\r\\n    parent_id = re.search(\"src:([0-9]*)\", crash_file).group(1)\\r\\n    # id:000000 is the seed corpus, so at this point we stop the search\\r\\n    if parent_id == \"000000\":\\r\\n        return None\\r\\n    for d in delimiters:\\r\\n        return \"id:\" + parent_id\\r\\n\\r\\ndef radiff2(a, b):\\r\\n    res, err = subprocess.Popen([\"radiff2\", a, b], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL).communicate()\\r\\n    # remove extra line at the end of the file\\r\\n    return res.decode(\\'utf-8\\').split(\"\\\\n\")[:-1]\\r\\n \\r\\ndef get_ancestor_crashes(crash_name, queue_dir, ancestor_tree:list):\\r\\n    parent_id = get_parent_id(crash_name)\\r\\n    # we have reached the end of the parent tree\\r\\n    if not parent_id:\\r\\n        return\\r\\n    # queue_dir needs to bemanually specified if the crash_file isn\\'t using AFL\\'s canonical crash path\\r\\n    try:\\r\\n        parent = glob.glob(os.path.join(queue_dir, parent_id + \"*\"))[0]\\r\\n        ancestor_tree.append(parent)\\r\\n        return get_ancestor_crashes(parent, queue_dir, ancestor_tree)\\r\\n    except IndexError:\\r\\n        print(\"No ancestors found, check that queue directory is correct: \", ancestor_tree)\\r\\n        return\\r\\n\\r\\ndef find_closest_ancestor(crash_file, ancestors):\\r\\n    print(crash_file)\\r\\n    diff_len = 99999999999\\r\\n    closest_ancestor = ancestors[0]\\r\\n    for ancestor_crash in ancestors:\\r\\n        # get bytes from diff\\r\\n        #TODO: reimplement radiff in python\\r\\n        diff = diff_crash(crash_file, ancestor_crash)\\r\\n        print(len(diff), ancestor_crash)\\r\\n        if len(diff) < diff_len:\\r\\n            diff_len = len(diff)\\r\\n            print(\"Closest ancestor: \", ancestor_crash, \"diff_bytes: \", len(diff))\\r\\n            closest_ancestor = ancestor_crash\\r\\n    print(\"AAClosest ancestor: \", ancestor_crash, \"diff_bytes: \", diff_len)\\r\\n    # start\\r\\n    return closest_ancestor\\r\\n\\r\\ndef diff_crash(crash_file, ancestor_crash):\\r\\n    diff = []\\r\\n    for l in radiff2(crash_file, ancestor_crash):\\r\\n        try:\\r\\n            child_off, child_bytes, _, parent_bytes, parent_off = l.split(\" \")\\r\\n            child_bytes = hex_str_to_bytes(child_bytes)\\r\\n            diff.append((int(parent_off, 16), child_bytes))\\r\\n        except Exception as e:\\r\\n            logging.exception(e)\\r\\n    return diff\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    args = argparse.ArgumentParser()\\r\\n    args.add_argument(\"crash_file\", help=\"The AFL canonical crash file path ie. the filepath of the crash generated directly by AFL\", nargs=\"?\")\\r\\n    args.add_argument(\"--queue\", help=\"Directory of the afl queue\", required=True)\\r\\n    args.add_argument(\"--debug\", help=\"DebugMode\", action=\"store_true\")\\r\\n    args.add_argument(\"--executable\", help=\"The executable for the binary, can be set using the environment variable CRASHWALK_BINARY\")\\r\\n    args.add_argument(\"--pickle\", help=\"(IMPORTANT: This is the most used mode) A pickled file that holds a list of executables\")\\r\\n\\r\\n    arguments = args.parse_args()\\r\\n    default_usage = \"Usage information: \\\\n\" \\\\\\r\\n            + \"With a pickled crashwalk file: bin_diff --pickle <pickles_exploitable>\\\\n\" \\\\\\r\\n            + \"With a single crash file:      bin_diff <crash_file> \\\\n\"\\r\\n\\r\\n    debug = arguments.debug\\r\\n    executable = arguments.executable\\r\\n    crash_file = os.path.abspath(arguments.crash_file) if arguments.crash_file else None\\r\\n    pickle_exploitables = os.path.abspath(arguments.pickle) if arguments.pickle else None\\r\\n    queue_dir = os.path.abspath(arguments.queue) if arguments.queue else None\\r\\n\\r\\n    print(default_usage)\\r\\n    start = datetime.datetime.now()\\r\\n\\r\\n    if not executable:\\r\\n        executable = os.environ[\"CRASHWALK_BINARY\"] if os.environ[\"CRASHWALK_BINARY\"] else None\\r\\n\\r\\n    print(\"EXECUTABLE: \", executable)\\r\\n    # single crash file mode\\r\\n    if not pickle_exploitables:\\r\\n        if not os.path.isfile(crash_file):\\r\\n            print(\"Crash file {} does not exist or is a directory\".format(crash_file))\\r\\n            sys.exit(-1)\\r\\n\\r\\n        # find parent queue_file, assuming that crash_file is a AFL canonical crash path\\r\\n        crash_name = crash_file[crash_file.rindex(\"/\") + 1:]\\r\\n        crash_dir = crash_file[:crash_file.rindex(\"/\")]\\r\\n        parent_id = get_parent_id(crash_name)\\r\\n        queue_dir = queue_dir if queue_dir else os.path.join(crash_dir[:crash_dir.rindex(\"/\")], \"queue\")\\r\\n        try:\\r\\n            parent_file = glob.glob(os.path.join(queue_dir, parent_id + \"*\"))[0]\\r\\n        except IndexError:\\r\\n            print(\"Parent ID not found, check if queue_dir is specified corectly\")\\r\\n\\r\\n        diff = BinDiff(crash_file, parent_file)\\r\\n\\r\\n        # linearity, affected_bytes, crash_offset = diff.get_crash_analysis()\\r\\n        # print(linearity, affected_bytes, crash_offset)\\r\\n\\r\\n    # multiple crashes serialized into pickle mode\\r\\n    else:\\r\\n        new_exploitables = []\\r\\n        with open(pickle_exploitables, \"rb\") as pickled:\\r\\n            exploitables = pickle.load(pickled)\\r\\n            for e in exploitables:\\r\\n                try:\\r\\n                    crash_file = os.path.abspath(e.crash_file)\\r\\n                    crash_name = crash_file[crash_file.rindex(\"/\") + 1:]\\r\\n                    crash_dir = crash_file[:crash_file.rindex(\"/\")]\\r\\n                    print(\"crash_file\", crash_file)\\r\\n                    # grab the queue src id from crash name\\r\\n                    # ie. id:000136,sig:11,src:000642,time:5534110,op:havoc,rep:4.pickle\\r\\n                    # TODO: what if you have more than 100k files in the queue\\r\\n                    ancestors = []\\r\\n                    queue_dir = queue_dir if queue_dir else os.path.join(crash_dir[:crash_dir.rindex(\"/\")], \"queue\")\\r\\n\\r\\n                    get_ancestor_crashes(crash_name, queue_dir, ancestors)\\r\\n                    parent = ancestors[0]\\r\\n\\r\\n                    # find ancestor with the smallest diff; the immediate parent is not guranteed to be the smallest diff\\r\\n                    # Actually, maybe we dont want to do this, since the ancestor crashes may not have directly led to our crash\\r\\n                    # find_closest_ancestor(crash_file, ancestors)\\r\\n                    # try:\\r\\n                    diff = BinDiff(crash_file, parent)\\r\\n                    # except Exception as e:\\r\\n                    #     print(\"{}\".format(e))\\r\\n                    #     pass\\r\\n\\r\\n                    linearity, affected_bytes = diff.get_crash_analysis()\\r\\n                    if not linearity:\\r\\n                        e.set_linearity(None)\\r\\n                        e.set_crash_bytes(None)\\r\\n\\r\\n                    e.set_linearity(linearity)\\r\\n                    e.set_crash_bytes(affected_bytes)\\r\\n                    new_exploitables.append(e)\\r\\n                except NoCrashException:\\r\\n                    continue\\r\\n    end = datetime.datetime.now()\\r\\n    print(\"time: \", end - start)\\r\\n    # with open(pickle_exploitables + \".bin_diff\", \"wb\") as write_pickled:\\r\\n    #     write_pickled.write(pickle.dumps(new_exploitables))\\r\\n'\n",
      "callstack.py b'import argparse\\r\\nfrom collections import defaultdict, OrderedDict\\r\\nfrom os import lseek\\r\\nfrom crashwalk import Exploitable\\r\\nimport pickle\\r\\nimport sys\\r\\n\\r\\n# TODO: need to differentiate between location of crash within fauting frame; this is going to effect comparison of stack traces\\r\\n# ^^^ THIS IS ACTUALLY KINDA IMPORTANT\\r\\n# TODO: map frame to a source line\\r\\nclass ExploitableCallstack:\\r\\n    def __init__(self, filename, display_frames):\\r\\n        self.total_exploitables = 0\\r\\n        self.callstacks = defaultdict(dict)\\r\\n        self.exploitables = []\\r\\n        display_frames = int(display_frames) if display_frames else 3\\r\\n        with open(filename, \"rb\") as pickled:\\r\\n            pickled_output = pickle.load(pickled)\\r\\n\\r\\n        print(len(pickled_output))\\r\\n        for e in pickled_output:\\r\\n            print(e.segfault)\\r\\n        #     # if e and e.exploitable:     \\r\\n        #     # really dependent on this script to minimize false negatives\\r\\n        #     index = len(self.exploitables)\\r\\n        #     callstack_hash = e.get_call_hash(display_frames)\\r\\n        #     if not self.callstacks.get(callstack_hash):\\r\\n        #         self.callstacks[callstack_hash][\"index\"] = []\\r\\n        #         self.callstacks[callstack_hash][\"callstack\"] = e.get_callstack()[:display_frames]\\r\\n        #     self.callstacks[callstack_hash][\"index\"].append(index)\\r\\n        #     self.exploitables.append(e)\\r\\n        #     self.total_exploitables += 1\\r\\n\\r\\n        # # callstacks.items() returns a 2-tuple instead\\r\\n        # self.callstacks = sorted(self.callstacks.values(), key=lambda x: len(x[\"index\"]), reverse=True)\\r\\n    \\r\\n    def get_most_popular(self):\\r\\n        return self.callstacks[0]\\r\\n\\r\\n    def get_exploitable(self, i):\\r\\n        return self.exploitables[i]\\r\\n\\r\\n    def pprint_crashing_callstacks(self):\\r\\n        # take the three most popular crashing sites\\r\\n        accounted_for = 0\\r\\n        for stack in self.callstacks[:3]:\\r\\n            print(\"Same Crash Sites: {}/{}\".format(len(stack[\"index\"]), self.total_exploitables))\\r\\n            print(\"Callstack :\")\\r\\n            for frame in stack[\"callstack\"]:\\r\\n                print(frame)\\r\\n            accounted_for += len(stack[\"index\"])\\r\\n        print(\"{} in the top 3 crash sites, which is {}% of the total crashes\".format(accounted_for, int(accounted_for/self.total_exploitables * 100)))\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    args = argparse.ArgumentParser()\\r\\n    args.add_argument(\"--search\")\\r\\n    args.add_argument(\"--frames\", help=\"Number of frames in the callstack to compare\", default=None)\\r\\n    args.add_argument(\"--out\", help=\"Stores in an output file\")\\r\\n    args.add_argument(\"--filter\", metavar=\"KEY=VALUE\", nargs=\"+\", help=\"\"\"\\r\\n        Accepts key-value pairs for filtering output. The following are supported: \\r\\n        \"\"\")\\r\\n    args.add_argument(\"pickle\", help=\"The pickled file of Exploitables\")\\r\\n\\r\\n    arguments = args.parse_args()\\r\\n    search = arguments.search\\r\\n    frames = arguments.frames\\r\\n    pickle_filename = arguments.pickle\\r\\n    filter = arguments.filter\\r\\n\\r\\n    callstack = ExploitableCallstack(pickle_filename, frames)\\r\\n    # callstack.pprint_crashing_callstacks()\\r\\n'\n",
      "constants.py b'DWORD = 4\\r\\nQWORD = 8'\n",
      "crashwalk.py b'#!/usr/bin/python\\r\\n\\r\\nfrom posixpath import pathsep\\r\\nfrom typing_extensions import runtime\\r\\nfrom pwn import process, context\\r\\nimport glob\\r\\nimport sys\\r\\nimport argparse\\r\\nimport os\\r\\nimport glob\\r\\nimport re\\r\\nimport hashlib\\r\\nimport threading\\r\\nimport multiprocessing\\r\\nfrom time import sleep\\r\\nfrom datetime import datetime\\r\\nimport pickle\\r\\nimport logging\\r\\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\\r\\nimport functools\\r\\nimport time\\r\\n\\r\\nlogging.getLogger(\"pwnlib\").setLevel(logging.WARNING)\\r\\n\\r\\nclass Timer:\\r\\n    def __init__(self):\\r\\n        if os.path.exists(\"perf.log\"):\\r\\n            os.remove(\"perf.log\")\\r\\n    @staticmethod\\r\\n    def timer(func):\\r\\n        \"\"\"Print the runtime of the decorated function\"\"\"\\r\\n        @functools.wraps(func)\\r\\n        def wrapper_timer(*args, **kwargs):\\r\\n            start_time = time.perf_counter()    # 1\\r\\n            value = func(*args, **kwargs)\\r\\n            end_time = time.perf_counter()      # 2\\r\\n            run_time = end_time - start_time    # 3\\r\\n            print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\\r\\n            with open(\"perf.log\", \"a\") as times:\\r\\n                times.write(str(run_time) + \"\\\\n\")\\r\\n            return value\\r\\n        return wrapper_timer\\r\\nt = Timer()\\r\\n\\r\\n# Exceptions\\r\\nclass TimeoutException(Exception):\\r\\n    pass\\r\\nclass NoCrashException(Exception):\\r\\n    pass\\r\\nclass CrashwalkError(Exception):\\r\\n    pass\\r\\n\\r\\n# TODO: should not have written this as two separate classes\\r\\nclass GDBJob:\\r\\n    def __init__(self, proc_name, filename, timeout=20):\\r\\n        START_PARSE = \"---START_HERE---\"\\r\\n        END_PARSE = \"---END_HERE---\"\\r\\n        self.filename = filename\\r\\n        self.crashed = True\\r\\n        self.timedout = False\\r\\n        exploitable_path = \"/mnt/c/Users/pengjohn/Documents/tools/exploit/exploitable/exploitable/exploitable.py\"\\r\\n        if env_exploitable := os.environ.get(\"EXPLOITABLE_PATH\", None):\\r\\n            self.exploitable_path = env_exploitable\\r\\n        context.log_level = \"error\"\\r\\n        gdb = process([\"gdb\", \"--args\", proc_name, filename], stdin=process.PTY, stdout=process.PTY, timeout=timeout)\\r\\n        # PWN complains when string encoding is not explicit\\r\\n        # Need this or GDB will require user keystroke to display rest of output\\r\\n        gdb.sendline(\"set height unlimited\".encode(\"utf-8\"))\\r\\n        gdb.sendline(\"gef config context False\".encode(\"utf-8\"))\\r\\n        gdb.sendline(\"r\".encode(\"utf-8\"))\\r\\n        if not os.path.isfile(exploitable_path):\\r\\n            raise Exception(f\"Exploitable not found at {exploitable_path}\".encode(\"utf-8\"))\\r\\n        gdb.sendline(f\"source {exploitable_path}\".encode(\"utf-8\"))\\r\\n        gdb.sendline(f\"p \\'{START_PARSE}\\'\".encode(\"utf-8\"))\\r\\n        gdb.sendline(\"exploitable -v\".encode(\"utf-8\"))\\r\\n        actions = [\\r\\n            \"frame 2\",\\r\\n            \"p *next\"\\r\\n        ]\\r\\n        # segfaulting address\\r\\n        gdb.sendline(\"SegfaultAddy\".encode(\"utf-8\"))\\r\\n        gdb.sendline(\"p $_siginfo._sifields._sigfault.si_addr\".encode(\"utf-8\"))\\r\\n        self.send(actions, gdb)\\r\\n        gdb.sendline(f\"p \\'{END_PARSE}\\'\".encode(\\'utf-8\\'))\\r\\n        self.output = gdb.recvuntil(f\"{END_PARSE}\".encode(\"utf-8\")).decode(\\'utf-8\\').split(\\'\\\\n\\')\\r\\n        gdb.close()\\r\\n        \\r\\n        if self.timedout == True:\\r\\n            return\\r\\n        # check if process actually crashed\\r\\n        for line in self.output:\\r\\n            if \"exited normally\" in line or \"exited with\" in line:\\r\\n                self.crashed = False\\r\\n\\r\\n    def send(self, actions, gdb):\\r\\n        for action in actions:\\r\\n            gdb.sendline(action.encode(\"utf-8\"))\\r\\n\\r\\n    def generate_exploitable(self):\\r\\n        if not self.crashed:\\r\\n            print(\"{} did not crash\".format(self.filename))\\r\\n            raise NoCrashException\\r\\n        elif self.timedout == True:\\r\\n            print(\"{} timed out\".format(self.filename))\\r\\n            raise TimeoutException\\r\\n        elif not self.output:\\r\\n            print(\"no output\")\\r\\n            raise Exception\\r\\n        return Exploitable(self.output, self.filename)\\r\\n\\r\\nclass Exploitable:\\r\\n    def __init__(self, output, crash_file):\\r\\n        try:\\r\\n            START_PARSE = \"---START_HERE---\"\\r\\n            self.classification = []\\r\\n            self.exploitable = False\\r\\n            self.crash_file = crash_file\\r\\n            self._output = iter(output)\\r\\n            self.raw_output = output\\r\\n            not_start = True\\r\\n            line = next(self._output, None)\\r\\n            while line or not_start:\\r\\n                if f\"{START_PARSE}\" in line:\\r\\n                    not_start = False\\r\\n                if \"Nearby code:\" in line:\\r\\n                    self.disassembly, line = self.parse_until(\"Stack trace:\")\\r\\n                    # Dont need this line since the iterator from the prev parse_until call will consume this line\\r\\n                    # if \"Stack trace:\" in line:\\r\\n                    self.stack_trace, line = self.parse_until(\"Faulting frame:\")\\r\\n                    self.faulting_frame = line.split(\" \")[5]\\r\\n                if \"Description:\" in line:\\r\\n                    self.classification, line = self.parse_until(\"gef\")\\r\\n                if \"SegfaultAddy\" in line:\\r\\n                    self.segfault = self.parse_segfault()\\r\\n                line = next(self._output, None)\\r\\n            self.assert_correctness()\\r\\n        except Exception:\\r\\n            print(f\"Crashwalk error, self.output: \")\\r\\n            for l in self.raw_output:\\r\\n                print(l)\\r\\n            raise CrashwalkError\\r\\n\\r\\n    def parse_segfault(self):\\r\\n        segfault = next(self._output, None)\\r\\n        if not segfault:\\r\\n            raise Exception(\"Error parsing segfault\")\\r\\n        match = re.search(\"(0x.*)\", segfault)\\r\\n        if match:\\r\\n            return match.group(1)\\r\\n\\r\\n    # hash the first n callstacks\\r\\n    def get_call_hash(self, n):\\r\\n        callstack_string = \"\".join(self.get_callstack()[:n])\\r\\n        return hashlib.md5(callstack_string.encode(\"utf-8\")).hexdigest()\\r\\n\\r\\n    def parse_until(self, stop_parse):\\r\\n        trace = []\\r\\n        line = next(self._output, None)\\r\\n        if not line:\\r\\n            raise Exception(\"Error parsing stacktrace\")\\r\\n        while line and stop_parse not in line:\\r\\n            trace.append(line)\\r\\n            line = next(self._output, None)\\r\\n        return trace, line\\r\\n\\r\\n    def get_callstack(self):\\r\\n        # normalize the spaces for the split call\\r\\n        #  0 Umbra::BlockMemoryManager<4096>::removeFreeAlloc at 0x7ffff7a6957d in /mnt/c/Users/pengjohn/Documents/umbra/umbra3/bin/linux64/libumbraoptimizer64.so\\r\\n        callstack = [frame.replace(\"  \", \" \").split(\" \")[2] for frame in self.stack_trace]\\r\\n        return callstack\\r\\n\\r\\n    def get_callstack_raw(self):\\r\\n        return self.stack_trace\\r\\n\\r\\n    def assert_correctness(self):\\r\\n        assert self.disassembly\\r\\n        assert self.get_callstack_raw()\\r\\n        assert self.classification\\r\\n\\r\\n    # output functions\\r\\n    def print_raw(self):\\r\\n        print(\"Disassembly: \")\\r\\n        for line in self.disassembly:\\r\\n            print(line)\\r\\n        print(\"CallStack: \")\\r\\n        for frame in self.get_callstack_raw():\\r\\n            print(frame)\\r\\n        for descr in self.classification:\\r\\n            print(descr)\\r\\n        print(\"Segmentation Fault: \", self.segfault)\\r\\n\\r\\n    def set_linearity(self, linearity):\\r\\n        self.linearity = linearity\\r\\n\\r\\n    def set_crash_offset(self, crash_offset):\\r\\n        self.crash_offset = crash_offset\\r\\n\\r\\n    def set_crash_bytes(self, crash_bytes):\\r\\n        self.crash_bytes = crash_bytes\\r\\n\\r\\n@t.timer\\r\\ndef run_GDBWorker(filepath):\\r\\n    try:\\r\\n        print(\"Checking crash for {}\".format(filepath))\\r\\n        exploitable = GDBJob(executable, filepath).generate_exploitable()\\r\\n        # why doesn\\'t python complain about explotiables not being declared as global variable\\r\\n        return exploitable\\r\\n    except NoCrashException as e:\\r\\n        print(\"No crash\")\\r\\n\\r\\ndef get_pickle_fname(pickle_path):\\r\\n    pickle_fname = os.path.normpath(pickle_path)\\r\\n    if \"/\" in pickle_fname:\\r\\n        pickle_fname = pickle_fname.replace(\\'/\\', \"_\")\\r\\n    return pickle_fname\\r\\n\\r\\ndef write_pickle(pickle_path, exploitables):\\r\\n    if os.path.isdir(pickle_path):\\r\\n        pickle_path += datetime.now().strftime(\"%m-%d-%Y_%H_%M_%S\")\\r\\n    with open(\"{}.pickle\".format(pickle_path), \"wb\") as cw_pickle:\\r\\n        # only exploitable crashes are going to be serialized\\r\\n        # exploitables = [e for e in exploitables if e != None and e.exploitable]\\r\\n        exploitables = [e for e in exploitables if e]\\r\\n        pickle.dump(exploitables, cw_pickle)\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    argParse = argparse.ArgumentParser()\\r\\n    argParse.add_argument(\"--executable\", help=\"Path to the executable, if not provided via cmdline, will be read from CRASHWALK_BINARY env variable\")\\r\\n    argParse.add_argument(\"path\", help=\"Path to the crash file\")\\r\\n    argParse.add_argument(\"--pickle-name\", help=\"Optionally specify the name of the pickle file\")\\r\\n    argParse.add_argument(\"--verbose\", help=\"Print output to stdout\", action=\"store_true\")\\r\\n\\r\\n    arguments = argParse.parse_args()\\r\\n\\r\\n    try:\\r\\n        executable = arguments.executable if arguments.executable else os.environ[\"CRASHWALK_BINARY\"]\\r\\n    except KeyError:\\r\\n        print(\"Please specify the executable binary via env variables or cmd line arguments\")\\r\\n        sys.exit(-1)\\r\\n    pickle_name = arguments.pickle_name\\r\\n    path = arguments.path\\r\\n    verbose = arguments.verbose if arguments.verbose else False\\r\\n\\r\\n    GDB_PROCS = multiprocessing.cpu_count()\\r\\n    crash_files = [path]\\r\\n\\r\\n    # no recursive search for crash files and all files present are crash files\\r\\n    if os.path.isdir(path):\\r\\n        crash_files = glob.glob(os.path.join(path, \"*\"))\\r\\n    total_files = len(crash_files)\\r\\n\\r\\n    # initialize length so each thread can individually update its index without locking\\r\\n    exploitables = []\\r\\n    # updates the exit status of the GDB job: 1 for success, 2 for an exception raised\\r\\n    run_status = [0] * len(crash_files)\\r\\n\\r\\n    # TODO: fix this\\r\\n    # try:\\r\\n    #     # read files previously seen files and skip them\\r\\n    #     seen_crashes = [s.strip() for s in open(\".prev_files.db\", \"r\").readlines()]\\r\\n    #     crash_files = [crash for crash in crash_files if crash not in seen_crashes]\\r\\n    #     print(\"Restarting, using {}, {}/{} files to look through\".format(crash_files[0], len(crash_files), total_files))\\r\\n    # except FileNotFoundError as e:\\r\\n    #     pass\\r\\n    # except IndexError:\\r\\n    #     print(\"{} already processed in previous run\")\\r\\n    #     sys.exit(-1)\\r\\n\\r\\n    seen_crashes = open(\".prev_files.db\", \"a\")\\r\\n    pending_futures = []\\r\\n    try:\\r\\n        with ThreadPoolExecutor(max_workers=GDB_PROCS) as executor:\\r\\n            for i, crash in enumerate(crash_files):\\r\\n                print(\"Launching job {}\".format(i))\\r\\n                pending_futures.append( executor.submit(run_GDBWorker, crash) )\\r\\n\\r\\n            # as_completed registers a callback event that gets called for each thread that\\'s current waiting on a exploitable object\\r\\n            # https://stackoverflow.com/questions/51239251/how-does-concurrent-futures-as-completed-work\\r\\n            for future in as_completed(pending_futures):\\r\\n                exploitable = future.result()\\r\\n                if verbose:\\r\\n                    exploitable.print_raw()\\r\\n                exploitables.append(future.result())\\r\\n\\r\\n    except KeyboardInterrupt:\\r\\n        if not pickle_name:\\r\\n            pickle_name = get_pickle_fname(path)\\r\\n        print(\"Serializing pickle\")\\r\\n        write_pickle(pickle_name, exploitables)\\r\\n\\r\\n    if not pickle_name:\\r\\n        pickle_name = get_pickle_fname(path)\\r\\n    write_pickle(pickle_name, exploitables)\\r\\n'\n",
      "test2.py b'\"hello\" \\r\\n'\n",
      "utils.py b'import os\\r\\nimport pickle\\r\\nfrom concurrent.futures import ThreadPoolExecutor\\r\\nimport queue\\r\\nimport functools\\r\\nimport time\\r\\nimport shutil\\r\\nfrom multiprocessing import cpu_count\\r\\nfrom crashwalk import GDBJob, NoCrashException\\r\\n\\r\\n# utils\\r\\ndef bytes_to_hex_str(b: bytes, endianess=\"little\")-> str:\\r\\n    hex_str = \"\"\\r\\n    b = b if endianess == \"big\" else b[::-1]\\r\\n    for byte in b:\\r\\n        hex_str += hex(byte).replace(\"0x\",\"\")\\r\\n    return \"0x\" + hex_str\\r\\n\\r\\ndef hex_str_to_bytes(hex_bytes: str) -> bytes:\\r\\n    byte_str_array = [int(hex_bytes[i:i+2], 16) for i in range(0, len(hex_bytes)-1, 2)]\\r\\n    return bytes(byte_str_array)\\r\\n\\r\\ndef hex_str_to_int(hex_bytes: str) -> int:\\r\\n    return int(hex_bytes.replace(\"0x\", \"\"), 16)\\r\\n\\r\\n\\r\\ndef add_bytes(a:int, b:int) -> int:\\r\\n    return (a + b) % 256\\r\\n\\r\\ndef serialize_exploitables(path, exploitables):\\r\\n    pickle_fname = os.path.normpath(path)\\r\\n    if \"/\" in path:\\r\\n        # Note: Python trick\\r\\n        # Find reverse index\\r\\n        # pickle_fname = path[len(path) - path[::-1].index(\\'/\\'):]\\r\\n        pickle_fname = pickle_fname.replace(\\'/\\', \"_\")\\r\\n        print(\"Pickled filename: {}\".format(pickle_fname))\\r\\n    with open(\"{}.pickle\".format(pickle_fname), \"wb\") as cw_pickle:\\r\\n        # only exploitable crashes are going to be serialized\\r\\n        # exploitables = [e for e in exploitables if e != None and e.exploitable]\\r\\n        exploitables = [e for e in exploitables if e]\\r\\n        pickle.dump(exploitables, cw_pickle)\\r\\n\\r\\n# multithread stuff\\r\\nGDB_PROCS = cpu_count()\\r\\nclass CustomThreadPoolExecutor(ThreadPoolExecutor):\\r\\n    def shutdown(self, wait=True, *, cancel_futures=False):\\r\\n        with self._shutdown_lock:\\r\\n            self._shutdown = True\\r\\n            if cancel_futures:\\r\\n                # Drain all work items from the queue, and then cancel their\\r\\n                # associated futures.\\r\\n                while True:\\r\\n                    try:\\r\\n                        work_item = self._work_queue.get_nowait()\\r\\n                    except queue.Empty:\\r\\n                        break\\r\\n                    if work_item is not None:\\r\\n                        work_item.future.cancel()\\r\\n\\r\\n            # Send a wake-up to prevent threads calling\\r\\n            # _work_queue.get(block=True) from permanently blocking.\\r\\n            self._work_queue.put(None)\\r\\n        if wait:\\r\\n            for t in self._threads:\\r\\n                t.join()\\r\\n\\r\\n# class UtilsDecorator:\\r\\n#     def __init__(self):\\r\\n#         self.decorators = [\\r\\n#             Timer,\\r\\n            \\r\\n#         ]\\r\\n\\r\\nclass Timer:\\r\\n    def __init__(self):\\r\\n        if os.path.exists(\"perf.log\"):\\r\\n            os.remove(\"perf.log\")\\r\\n\\r\\n    def wrap(self, func):\\r\\n        self.timer(func)\\r\\n\\r\\n    @staticmethod\\r\\n    def timer(func):\\r\\n        \"\"\"Print the runtime of the decorated function\"\"\"\\r\\n        @functools.wraps(func)\\r\\n        def wrapper_timer(*args, **kwargs):\\r\\n            start_time = time.perf_counter()    # 1\\r\\n            value = func(*args, **kwargs)\\r\\n            end_time = time.perf_counter()      # 2\\r\\n            run_time = end_time - start_time    # 3\\r\\n            with open(\"perf.log\", \"a\") as times:\\r\\n                times.write(str(run_time) + \"\\\\n\")\\r\\n            return value\\r\\n        return wrapper_timer\\r\\n    \\r\\nt = Timer()\\r\\n\\r\\n\\r\\n# Function overloading python?\\r\\n@t.timer\\r\\ndef replaceBytesDiff(parent, modified_parent, diff):\\r\\n    res = shutil.copyfile(parent, modified_parent)\\r\\n    with open(modified_parent, \"r+b\") as file_handle:\\r\\n        # TODO:\\r\\n        # write back old bytes after GDB call, so we don\\'t make any inadvertent changes to execution trace\\r\\n        # old_bytes = file_handle.read(len(bytes))\\r\\n        # double seeking required since advance moves the file pointer\\r\\n        for offset, bytes in diff:\\r\\n            file_handle.seek(offset)\\r\\n            # print(\"Writing {} at {} @ file: {}\".format(bytes, offset, modified_parent))\\r\\n            b = file_handle.write(bytes)\\r\\n            if b != len(bytes):\\r\\n                return False\\r\\n            file_handle.flush()\\r\\n        return True\\r\\n\\r\\ndef replaceBytes(file_handle, offset, b):\\r\\n    if type(b) != bytes:\\r\\n        b = bytes([b])\\r\\n    # TODO:\\r\\n    # write back old bytes after GDB call, so we don\\'t make any inadvertent changes to execution trace\\r\\n    # old_bytes = file_handle.read(len(bytes))\\r\\n    # double seeking required since advance moves the file pointer\\r\\n    file_handle.seek(offset)\\r\\n    # print(\"Writing {} at {} @ file: {}\".format(bytes, offset, modified_parent))\\r\\n    written = file_handle.write(b)\\r\\n    if written != len(b):\\r\\n        return False\\r\\n    file_handle.flush()\\r\\n    return True\\r\\n\\r\\nclass GDBExecutor:\\r\\n    def __init__(self, executable):\\r\\n        self.t_pool = CustomThreadPoolExecutor(max_workers=GDB_PROCS)\\r\\n        self.executable = executable\\r\\n        self.inc_me = 0\\r\\n\\r\\n    def run_jobs(self, crashes, ordered=False):\\r\\n        if not ordered:\\r\\n            jobs = []\\r\\n            for crash in crashes:\\r\\n                job = self.t_pool.submit(self.runGDBJob, crash)\\r\\n                jobs.append(job)\\r\\n            return jobs\\r\\n        # map returns ordered results (not promises)\\r\\n        else:\\r\\n            jobs = self.t_pool.map( self.runGDBJob, crashes) \\r\\n            return jobs\\r\\n\\r\\n    @t.timer\\r\\n    def runGDBJob(self, filepath):\\r\\n        try:\\r\\n            # print(f\"running {filepath}\")\\r\\n            exploitable = GDBJob(self.executable, filepath).generate_exploitable()\\r\\n            # why doesn\\'t python complain about explotiables not being declared as global variable\\r\\n            return exploitable\\r\\n        except NoCrashException:\\r\\n            print(f\"No crash {filepath}\")\\r\\n            return None\\r\\n            \\r\\nif __name__ == \"__main__\":\\r\\n    pass'\n",
      "__init__.py b''\n",
      "bin_diff.py\n",
      "b'#!/usr/bin/python\\r\\n\\r\\n# purpose of this is to incrementally make file b the same as a in order to isolate the bytes that\\r\\n# triggered the crash\\r\\nimport sys\\r\\nimport subprocess\\r\\nfrom unittest import TextTestResult\\r\\nfrom crashwalk import GDBJob, Exploitable, NoCrashException, run_GDBWorker\\r\\nfrom concurrent.futures import as_completed\\r\\nfrom utils import bytes_to_hex_str, hex_str_to_bytes, add_bytes, hex_str_to_int, serialize_exploitables, CustomThreadPoolExecutor, GDBExecutor, replaceBytes, replaceBytesDiff\\r\\nfrom constants import DWORD, QWORD\\r\\nimport argparse\\r\\nimport os\\r\\nimport logging\\r\\nimport math\\r\\nimport glob\\r\\nimport pickle\\r\\nimport re\\r\\nfrom multiprocessing import cpu_count\\r\\nimport datetime\\r\\nfrom typing import List\\r\\nimport shutil\\r\\n\\r\\nGDB_PROCS = cpu_count()\\r\\n# disable logging from pwn\\r\\nlogging.getLogger(\"pwnlib\").setLevel(logging.WARNING)\\r\\n\\r\\nclass CrashedBytes:\\r\\n    def __init__(self, child_crash, child_sgsev, parent_crash):\\r\\n        self.child_crash = child_crash\\r\\n        self.child_sgsev = child_sgsev\\r\\n        # self.executable = executable => don\\'t need since defined as a global\\r\\n        self.tmp_dir = \"/tmp/modified\"\\r\\n        try:\\r\\n            os.mkdir(\"/tmp/modified\")\\r\\n        except Exception:\\r\\n            pass\\r\\n        # executing gdb jobs to get the segfault address\\r\\n        self.executor = GDBExecutor(executable)\\r\\n        # get diff of child and parent via radiff2\\r\\n        diff = diff_crash(child_crash, parent_crash)\\r\\n        if len(diff) > 30:\\r\\n            print(\"Skipping too big..\")\\r\\n            # TODO: change this\\r\\n            raise NoCrashException\\r\\n\\r\\n        # find crashing offset\\r\\n        self.offset, self.modified_bytes, self.modified_parent = self.get_crashing_offset(parent_crash, diff)\\r\\n\\r\\n    def get_crashing_offset(self, parent, diff):\\r\\n        child_file = self.child_crash[self.child_crash.rindex(\"/\") + 1:]\\r\\n        t_pool = CustomThreadPoolExecutor(max_workers=GDB_PROCS)\\r\\n        pending_futures = []\\r\\n        modified_parents = {}\\r\\n        # copy a newly modified file for each line in the diff\\r\\n        for i, (parent_off, child_bytes) in enumerate(diff):\\r\\n            print(f\"{parent_off}: {child_bytes}\")\\r\\n            modified_parent = os.path.join(self.tmp_dir, child_file + str(i) + \".modified\")\\r\\n            modified_parents[modified_parent] = (parent_off, child_bytes)\\r\\n            partial_diff = diff[:i+1]\\r\\n            pending_futures.append(t_pool.submit( replaceBytesDiff, parent, modified_parent, partial_diff))\\r\\n\\r\\n        for f in as_completed(pending_futures):\\r\\n            if not f.result():\\r\\n                print(\"Print exiting...\")\\r\\n                sys.exit()\\r\\n\\r\\n        offset = None\\r\\n        # execute subprocesses to determine which diff lines crashes the input\\r\\n        future_jobs = self.executor.run_jobs(modified_parents.keys())\\r\\n        offset, modified_bytes, parent_crash = None, None, None\\r\\n        try:\\r\\n            for f in future_jobs:\\r\\n                # Attempt #1:\\r\\n                # this doesn\\'t work because all futures are iterated for in the beginning, without the chance for one to complete execution\\r\\n                # if finished:\\r\\n                #     print(\\'Cancelling..\\')\\r\\n                #     t_pool.shutdown(wait=False)\\r\\n                res = f.result()\\r\\n                parent_sgsev = res.segfault\\r\\n                parent_crash = res.crash_file\\r\\n                if parent_sgsev == self.child_sgsev:\\r\\n                    print(\"PARENT CRASH: >>>>> \", res.crash_file)\\r\\n                    offset = modified_parents[parent_crash][0]\\r\\n                    modified_bytes = modified_parents[parent_crash][1]\\r\\n                    print(\"Found crash triggering input fileoffset @ {}, segfaulting addr: {}, parent crash original: {}\"\\r\\n                        .format(offset, parent_sgsev, parent_crash))\\r\\n                    raise ValueError\\r\\n        # Attempt #2: This will execute unlike attempt #1, but will not cancel pending jobs\\r\\n        # wait=False just allows the function to return earlier rather than waiting for completion, but neither cancels pending jobs\\r\\n        except ValueError:\\r\\n            print(\"Canceling\")\\r\\n            t_pool.shutdown(wait=True, cancel_futures=True)\\r\\n    \\r\\n        # should be returning the unmodified parent here instead of parent_crash\\r\\n        return offset, modified_bytes, parent_crash\\r\\n\\r\\n# Create files for modification\\r\\nclass PrepFiles:\\r\\n    def __init__(self, modified_file, new_bytes, offset):\\r\\n        self.modified_file = modified_file\\r\\n        self.new_bytes = new_bytes\\r\\n        self.offset = offset\\r\\n        self.linearity = False\\r\\n        self.tmp_dir = \"/tmp/modified/linearity\"\\r\\n        # is this thread safe?\\r\\n        self.f_index = 0\\r\\n        self.filenames = []\\r\\n        try:\\r\\n            os.rmdir(\"/tmp/modified/linearity\")\\r\\n            os.mkdir(\"/tmp/modified/linearity\")\\r\\n        except Exception:\\r\\n            os.mkdir(\"/tmp/modified/linearity\")\\r\\n\\r\\n    def __enter__(self):\\r\\n        for b in self.new_bytes:\\r\\n            # if byte is zero, increment it\\r\\n            b = b if b != b\"\\\\x00\" else bytes([int(b) + 1])\\r\\n            # use offset to mark out unique files\\r\\n            filename = os.path.join(self.tmp_dir, \"_offset:{}_\".format(str(self.offset)) + str(self.f_index))\\r\\n            shutil.copy(self.modified_file, filename)\\r\\n            print(\"byte: {}\".format(b))\\r\\n            with open(filename, \"rb+\") as handle:\\r\\n                replaceBytes(handle, self.offset, b)\\r\\n            self.filenames.append(filename)\\r\\n            self.f_index += 1\\r\\n        return self\\r\\n\\r\\n    def __exit__(self, exc_type, exc_value, exc_tb):\\r\\n        if exc_type:\\r\\n            print(\"Exception occured of type: {} occurred, value: {}, trace: {}\".format(exc_type, exc_value, exc_tb))\\r\\n        # dont bother saving non-linear modified crashes\\r\\n        if not self.linearity:\\r\\n            for f in self.filenames:\\r\\n                os.remove(f)\\r\\n\\r\\n    def get_linear_crashes(self):\\r\\n        return glob.glob(os.path.join(self.tmp_dir, \"*\"))\\r\\n\\r\\nclass BinDiff:\\r\\n    def __init__(self, child_crash, parent_crash):\\r\\n        self.executor = GDBExecutor(executable)\\r\\n\\r\\n        self.child_sgsev, self.parent_sgsev = self._check_segfault(child_crash, parent_crash)\\r\\n        if not self.parent_sgsev and not self.child_sgsev:\\r\\n            raise NoCrashException\\r\\n        \\r\\n        crashed = CrashedBytes(child_crash, self.child_sgsev, parent_crash)\\r\\n        offset = crashed.offset\\r\\n        modified_bytes = crashed.modified_bytes\\r\\n        modified_parent = crashed.modified_parent\\r\\n        self.linearity, self.bytes_controlled = self._find_control_width(offset, modified_bytes, modified_parent)\\r\\n\\r\\n    def _dword_byte_pos(self, delta):\\r\\n        return int(math.log(delta,2))\\r\\n\\r\\n    def _check_segfault(self, child_crash, parent_crash):\\r\\n        # check that parent and child have diff crash sites\\r\\n        try:\\r\\n            futures = self.executor.run_jobs([child_crash, parent_crash], ordered=True)\\r\\n            child_sgsev = next(futures).segfault\\r\\n            parent_sgsev = next(futures).segfault\\r\\n            # child_sgsev, parent_sgsev = futures[0].result().segfault, futures[1].result().segfault\\r\\n        except AttributeError:\\r\\n            raise NoCrashException(\"Check segfault crashed\")\\r\\n        if not child_sgsev or not parent_sgsev:\\r\\n            print(\"Either the child or the parent did not crash\")\\r\\n            return None, None\\r\\n        if child_sgsev == parent_sgsev:\\r\\n            print(\"Child segfault == Parent segfault, skipping {}\".format(child_crash))\\r\\n            return None, None\\r\\n        return child_sgsev, parent_sgsev\\r\\n\\r\\n    def _is_linear(self, segfaults: List[int]):\\r\\n        linearity = False\\r\\n        byte_pos = None\\r\\n\\r\\n        segfaults = map(hex_str_to_int, segfaults)\\r\\n        segfaults = list(segfaults)\\r\\n\\r\\n        # check if segfaults resulting from modifying the same byte remains the same ie. there is a linear relationship between that byte and the segfault\\r\\n        if (segfaults[2] - segfaults[1]) == (segfaults[1] - segfaults[0]) and ((segfaults[1] - segfaults[0]) != 0):\\r\\n            linearity = True\\r\\n            segfault_delta = abs(segfaults[2] - segfaults[1])\\r\\n            if segfault_delta:\\r\\n                byte_pos = self._dword_byte_pos(segfault_delta)\\r\\n                # bytes_controlled[i+3] = byte_pos\\r\\n                # if segfault_delta < smallest_diff:\\r\\n                #     smallest_diff = segfault_delta\\r\\n                #     closest_segfaults = segfaults\\r\\n            print(\"Linear relationship found: {}\", segfaults)\\r\\n        return linearity, byte_pos\\r\\n\\r\\n    def _get_byte_n_offset(self, mod_bytes, offset, modified_file, struct_size=DWORD):\\r\\n        possible_bytes = bytes()\\r\\n        off_start = offset - struct_size + 1\\r\\n        off_end = offset + len(mod_bytes)\\r\\n        with open(modified_file, \"rb\") as handle:\\r\\n            handle.seek(off_start)\\r\\n            possible_bytes += handle.read(struct_size - 1)\\r\\n            possible_bytes += mod_bytes\\r\\n            handle.seek(off_end)\\r\\n            possible_bytes += handle.read(struct_size - 1)\\r\\n        print(type(possible_bytes))\\r\\n        return zip(possible_bytes, range(off_start, off_end + struct_size))\\r\\n\\r\\n    def _find_control_width(self, mod_offset, mod_bytes, modified_file):\\r\\n        linear_relationship = False\\r\\n        bytes_controlled = [None] * (len(mod_bytes) + 2 * (DWORD - 1))\\r\\n        # Test 1: subtract/add n bytes to the mod_bytes @ offset, then compare segfaulting addresses\\r\\n        # to detect if linear relationship exists\\r\\n        # For now we treat all crashing bytes as an integer offset stored in a DWORD\\r\\n        if len(mod_bytes) <= 4:\\r\\n            # get the bytes that come before/after the DWORD/QWORD in memory\\r\\n            bytes_n_offsets = self._get_byte_n_offset(mod_bytes, mod_offset, modified_file, struct_size=DWORD)\\r\\n\\r\\n            # Apparently iterating over bytes in Python will yield ints\\r\\n            for byte, offset in bytes_n_offsets:\\r\\n                assert(type(byte) == int)\\r\\n                # Note: add_bytes adds a wraparound behaviour that is not currently accounted for in the linearity calculation\\r\\n                inc_bytes =  [add_bytes(byte, i) for i in range(0, 3)]\\r\\n                inc_bytes =  bytes(inc_bytes)\\r\\n                with PrepFiles(modified_file, inc_bytes, offset) as files:\\r\\n                    exploitables = self.executor.run_jobs(files.filenames, ordered=True)\\r\\n                    # if any one of the files do not crash, just skip this batch of files\\r\\n                    try:\\r\\n                        segfaults = [e.segfault for e in exploitables]\\r\\n                        print(segfaults)\\r\\n                    except Exception as e:\\r\\n                        print(e)\\r\\n                        continue\\r\\n                # check if segfaults resulting from modifying the same byte remains the same ie. there is a linear relationship between that byte and the segfault\\r\\n                linearity, byte_pos = self._is_linear(segfaults)\\r\\n                if linearity:\\r\\n                    # which bytes in the segfault are controllable\\r\\n                    bytes_controlled[offset - mod_offset + DWORD - 1] = byte_pos\\r\\n                linear_relationship = linearity if not linear_relationship else True\\r\\n        \\r\\n            print(\"bytes_controlled: \", bytes_controlled)\\r\\n        # Test 2: Random tests\\r\\n        return linear_relationship, bytes_controlled\\r\\n\\r\\n    def get_crash_analysis(self):\\r\\n        try:\\r\\n            return self.linearity, self.bytes_controlled\\r\\n        except AttributeError:\\r\\n            return None, None, None\\r\\n\\r\\n# Basic algorithm\\r\\n# 1. Find the most popular crash site\\r\\n# 2. Find the least different crash file with a different segfaulting address\\r\\n# 3. Find the fileoffset that triggers crash by replacing successive byte ranges between the input file diffs\\r\\n# -> Greedy strategy: to only change bytes that that differ for the current comparison; this decreases the likelihood of\\r\\n# a false positive in identifying the input file offset for controlling the crash/segfault address (crashing offset)\\r\\n# -> Non-greedy strategy: replace all the bytes, but this\\r\\n# 4. When byte range(s) have been identified, then apply control_width_discovery algorithm for finding the control width\\r\\n# Prior research by CSE Group used a Metasploit style unique byte ranges strategy, where the identified byte ranges were replaced\\r\\n# with unqiue byte sequences, which allowed for easy correlation between the segfaulting address and the input file offset. However,\\r\\n# this strategy only works in the case where the input file bytes are directly accessed as a memory address, without any transformation\\r\\n# In the case where bytes could be transformed before accessed as memory (ie. some basic linear transformation), this strategy will not\\r\\n# be able to identify the crashing offset, since bytes in the crash could be very different than their representation in the input file\\r\\n# 5. Repeat with a less optimal crash file (reason for this may be due to complex operations)\\r\\n\\r\\n# TODO: USE THIS FUNCTION\\r\\ndef get_afl_queue_dir(crash_filepath):\\r\\n    crash_name = crash_filepath[crash_filepath.rindex(\"/\") + 1:]\\r\\n    crash_dir = crash_filepath[:crash_filepath.rindex(\"/\")]\\r\\n    parent_id = crash_name.split(\",\")[0]\\r\\n    queue_dir = os.path.join(crash_dir[:crash_dir.rindex(\"/\")], \"queue\")\\r\\n\\r\\n# handle\\r\\ndef get_parent_id(crash_file):\\r\\n    # delimiters = [\":\", \"_\"]\\r\\n    # print(\"crashing_file:\", crash_file)\\r\\n    delimiters = [\":\"]\\r\\n    try:\\r\\n        crash_name = crash_file[crash_file.rindex(\"/\"):]\\r\\n    except IndexError:\\r\\n        crash_name = crash_file\\r\\n    except ValueError:\\r\\n        crash_name = crash_file\\r\\n    # afl have different path delimiters\\r\\n    parent_id = re.search(\"src:([0-9]*)\", crash_file).group(1)\\r\\n    # id:000000 is the seed corpus, so at this point we stop the search\\r\\n    if parent_id == \"000000\":\\r\\n        return None\\r\\n    for d in delimiters:\\r\\n        return \"id:\" + parent_id\\r\\n\\r\\ndef radiff2(a, b):\\r\\n    res, err = subprocess.Popen([\"radiff2\", a, b], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL).communicate()\\r\\n    # remove extra line at the end of the file\\r\\n    return res.decode(\\'utf-8\\').split(\"\\\\n\")[:-1]\\r\\n \\r\\ndef get_ancestor_crashes(crash_name, queue_dir, ancestor_tree:list):\\r\\n    parent_id = get_parent_id(crash_name)\\r\\n    # we have reached the end of the parent tree\\r\\n    if not parent_id:\\r\\n        return\\r\\n    # queue_dir needs to bemanually specified if the crash_file isn\\'t using AFL\\'s canonical crash path\\r\\n    try:\\r\\n        parent = glob.glob(os.path.join(queue_dir, parent_id + \"*\"))[0]\\r\\n        ancestor_tree.append(parent)\\r\\n        return get_ancestor_crashes(parent, queue_dir, ancestor_tree)\\r\\n    except IndexError:\\r\\n        print(\"No ancestors found, check that queue directory is correct: \", ancestor_tree)\\r\\n        return\\r\\n\\r\\ndef find_closest_ancestor(crash_file, ancestors):\\r\\n    print(crash_file)\\r\\n    diff_len = 99999999999\\r\\n    closest_ancestor = ancestors[0]\\r\\n    for ancestor_crash in ancestors:\\r\\n        # get bytes from diff\\r\\n        #TODO: reimplement radiff in python\\r\\n        diff = diff_crash(crash_file, ancestor_crash)\\r\\n        print(len(diff), ancestor_crash)\\r\\n        if len(diff) < diff_len:\\r\\n            diff_len = len(diff)\\r\\n            print(\"Closest ancestor: \", ancestor_crash, \"diff_bytes: \", len(diff))\\r\\n            closest_ancestor = ancestor_crash\\r\\n    print(\"AAClosest ancestor: \", ancestor_crash, \"diff_bytes: \", diff_len)\\r\\n    # start\\r\\n    return closest_ancestor\\r\\n\\r\\ndef diff_crash(crash_file, ancestor_crash):\\r\\n    diff = []\\r\\n    for l in radiff2(crash_file, ancestor_crash):\\r\\n        try:\\r\\n            child_off, child_bytes, _, parent_bytes, parent_off = l.split(\" \")\\r\\n            child_bytes = hex_str_to_bytes(child_bytes)\\r\\n            diff.append((int(parent_off, 16), child_bytes))\\r\\n        except Exception as e:\\r\\n            logging.exception(e)\\r\\n    return diff\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    args = argparse.ArgumentParser()\\r\\n    args.add_argument(\"crash_file\", help=\"The AFL canonical crash file path ie. the filepath of the crash generated directly by AFL\", nargs=\"?\")\\r\\n    args.add_argument(\"--queue\", help=\"Directory of the afl queue\", required=True)\\r\\n    args.add_argument(\"--debug\", help=\"DebugMode\", action=\"store_true\")\\r\\n    args.add_argument(\"--executable\", help=\"The executable for the binary, can be set using the environment variable CRASHWALK_BINARY\")\\r\\n    args.add_argument(\"--pickle\", help=\"(IMPORTANT: This is the most used mode) A pickled file that holds a list of executables\")\\r\\n\\r\\n    arguments = args.parse_args()\\r\\n    default_usage = \"Usage information: \\\\n\" \\\\\\r\\n            + \"With a pickled crashwalk file: bin_diff --pickle <pickles_exploitable>\\\\n\" \\\\\\r\\n            + \"With a single crash file:      bin_diff <crash_file> \\\\n\"\\r\\n\\r\\n    debug = arguments.debug\\r\\n    executable = arguments.executable\\r\\n    crash_file = os.path.abspath(arguments.crash_file) if arguments.crash_file else None\\r\\n    pickle_exploitables = os.path.abspath(arguments.pickle) if arguments.pickle else None\\r\\n    queue_dir = os.path.abspath(arguments.queue) if arguments.queue else None\\r\\n\\r\\n    print(default_usage)\\r\\n    start = datetime.datetime.now()\\r\\n\\r\\n    if not executable:\\r\\n        executable = os.environ[\"CRASHWALK_BINARY\"] if os.environ[\"CRASHWALK_BINARY\"] else None\\r\\n\\r\\n    print(\"EXECUTABLE: \", executable)\\r\\n    # single crash file mode\\r\\n    if not pickle_exploitables:\\r\\n        if not os.path.isfile(crash_file):\\r\\n            print(\"Crash file {} does not exist or is a directory\".format(crash_file))\\r\\n            sys.exit(-1)\\r\\n\\r\\n        # find parent queue_file, assuming that crash_file is a AFL canonical crash path\\r\\n        crash_name = crash_file[crash_file.rindex(\"/\") + 1:]\\r\\n        crash_dir = crash_file[:crash_file.rindex(\"/\")]\\r\\n        parent_id = get_parent_id(crash_name)\\r\\n        queue_dir = queue_dir if queue_dir else os.path.join(crash_dir[:crash_dir.rindex(\"/\")], \"queue\")\\r\\n        try:\\r\\n            parent_file = glob.glob(os.path.join(queue_dir, parent_id + \"*\"))[0]\\r\\n        except IndexError:\\r\\n            print(\"Parent ID not found, check if queue_dir is specified corectly\")\\r\\n\\r\\n        diff = BinDiff(crash_file, parent_file)\\r\\n\\r\\n        # linearity, affected_bytes, crash_offset = diff.get_crash_analysis()\\r\\n        # print(linearity, affected_bytes, crash_offset)\\r\\n\\r\\n    # multiple crashes serialized into pickle mode\\r\\n    else:\\r\\n        new_exploitables = []\\r\\n        with open(pickle_exploitables, \"rb\") as pickled:\\r\\n            exploitables = pickle.load(pickled)\\r\\n            for e in exploitables:\\r\\n                try:\\r\\n                    crash_file = os.path.abspath(e.crash_file)\\r\\n                    crash_name = crash_file[crash_file.rindex(\"/\") + 1:]\\r\\n                    crash_dir = crash_file[:crash_file.rindex(\"/\")]\\r\\n                    print(\"crash_file\", crash_file)\\r\\n                    # grab the queue src id from crash name\\r\\n                    # ie. id:000136,sig:11,src:000642,time:5534110,op:havoc,rep:4.pickle\\r\\n                    # TODO: what if you have more than 100k files in the queue\\r\\n                    ancestors = []\\r\\n                    queue_dir = queue_dir if queue_dir else os.path.join(crash_dir[:crash_dir.rindex(\"/\")], \"queue\")\\r\\n\\r\\n                    get_ancestor_crashes(crash_name, queue_dir, ancestors)\\r\\n                    parent = ancestors[0]\\r\\n\\r\\n                    # find ancestor with the smallest diff; the immediate parent is not guranteed to be the smallest diff\\r\\n                    # Actually, maybe we dont want to do this, since the ancestor crashes may not have directly led to our crash\\r\\n                    # find_closest_ancestor(crash_file, ancestors)\\r\\n                    # try:\\r\\n                    diff = BinDiff(crash_file, parent)\\r\\n                    # except Exception as e:\\r\\n                    #     print(\"{}\".format(e))\\r\\n                    #     pass\\r\\n\\r\\n                    linearity, affected_bytes = diff.get_crash_analysis()\\r\\n                    if not linearity:\\r\\n                        e.set_linearity(None)\\r\\n                        e.set_crash_bytes(None)\\r\\n\\r\\n                    e.set_linearity(linearity)\\r\\n                    e.set_crash_bytes(affected_bytes)\\r\\n                    new_exploitables.append(e)\\r\\n                except NoCrashException:\\r\\n                    continue\\r\\n    end = datetime.datetime.now()\\r\\n    print(\"time: \", end - start)\\r\\n    # with open(pickle_exploitables + \".bin_diff\", \"wb\") as write_pickled:\\r\\n    #     write_pickled.write(pickle.dumps(new_exploitables))\\r\\n'\n",
      "callstack.py\n",
      "b'import argparse\\r\\nfrom collections import defaultdict, OrderedDict\\r\\nfrom os import lseek\\r\\nfrom crashwalk import Exploitable\\r\\nimport pickle\\r\\nimport sys\\r\\n\\r\\n# TODO: need to differentiate between location of crash within fauting frame; this is going to effect comparison of stack traces\\r\\n# ^^^ THIS IS ACTUALLY KINDA IMPORTANT\\r\\n# TODO: map frame to a source line\\r\\nclass ExploitableCallstack:\\r\\n    def __init__(self, filename, display_frames):\\r\\n        self.total_exploitables = 0\\r\\n        self.callstacks = defaultdict(dict)\\r\\n        self.exploitables = []\\r\\n        display_frames = int(display_frames) if display_frames else 3\\r\\n        with open(filename, \"rb\") as pickled:\\r\\n            pickled_output = pickle.load(pickled)\\r\\n\\r\\n        print(len(pickled_output))\\r\\n        for e in pickled_output:\\r\\n            print(e.segfault)\\r\\n        #     # if e and e.exploitable:     \\r\\n        #     # really dependent on this script to minimize false negatives\\r\\n        #     index = len(self.exploitables)\\r\\n        #     callstack_hash = e.get_call_hash(display_frames)\\r\\n        #     if not self.callstacks.get(callstack_hash):\\r\\n        #         self.callstacks[callstack_hash][\"index\"] = []\\r\\n        #         self.callstacks[callstack_hash][\"callstack\"] = e.get_callstack()[:display_frames]\\r\\n        #     self.callstacks[callstack_hash][\"index\"].append(index)\\r\\n        #     self.exploitables.append(e)\\r\\n        #     self.total_exploitables += 1\\r\\n\\r\\n        # # callstacks.items() returns a 2-tuple instead\\r\\n        # self.callstacks = sorted(self.callstacks.values(), key=lambda x: len(x[\"index\"]), reverse=True)\\r\\n    \\r\\n    def get_most_popular(self):\\r\\n        return self.callstacks[0]\\r\\n\\r\\n    def get_exploitable(self, i):\\r\\n        return self.exploitables[i]\\r\\n\\r\\n    def pprint_crashing_callstacks(self):\\r\\n        # take the three most popular crashing sites\\r\\n        accounted_for = 0\\r\\n        for stack in self.callstacks[:3]:\\r\\n            print(\"Same Crash Sites: {}/{}\".format(len(stack[\"index\"]), self.total_exploitables))\\r\\n            print(\"Callstack :\")\\r\\n            for frame in stack[\"callstack\"]:\\r\\n                print(frame)\\r\\n            accounted_for += len(stack[\"index\"])\\r\\n        print(\"{} in the top 3 crash sites, which is {}% of the total crashes\".format(accounted_for, int(accounted_for/self.total_exploitables * 100)))\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    args = argparse.ArgumentParser()\\r\\n    args.add_argument(\"--search\")\\r\\n    args.add_argument(\"--frames\", help=\"Number of frames in the callstack to compare\", default=None)\\r\\n    args.add_argument(\"--out\", help=\"Stores in an output file\")\\r\\n    args.add_argument(\"--filter\", metavar=\"KEY=VALUE\", nargs=\"+\", help=\"\"\"\\r\\n        Accepts key-value pairs for filtering output. The following are supported: \\r\\n        \"\"\")\\r\\n    args.add_argument(\"pickle\", help=\"The pickled file of Exploitables\")\\r\\n\\r\\n    arguments = args.parse_args()\\r\\n    search = arguments.search\\r\\n    frames = arguments.frames\\r\\n    pickle_filename = arguments.pickle\\r\\n    filter = arguments.filter\\r\\n\\r\\n    callstack = ExploitableCallstack(pickle_filename, frames)\\r\\n    # callstack.pprint_crashing_callstacks()\\r\\n'\n",
      "constants.py\n",
      "b'DWORD = 4\\r\\nQWORD = 8'\n",
      "crashwalk.py\n",
      "b'#!/usr/bin/python\\r\\n\\r\\nfrom posixpath import pathsep\\r\\nfrom typing_extensions import runtime\\r\\nfrom pwn import process, context\\r\\nimport glob\\r\\nimport sys\\r\\nimport argparse\\r\\nimport os\\r\\nimport glob\\r\\nimport re\\r\\nimport hashlib\\r\\nimport threading\\r\\nimport multiprocessing\\r\\nfrom time import sleep\\r\\nfrom datetime import datetime\\r\\nimport pickle\\r\\nimport logging\\r\\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\\r\\nimport functools\\r\\nimport time\\r\\n\\r\\nlogging.getLogger(\"pwnlib\").setLevel(logging.WARNING)\\r\\n\\r\\nclass Timer:\\r\\n    def __init__(self):\\r\\n        if os.path.exists(\"perf.log\"):\\r\\n            os.remove(\"perf.log\")\\r\\n    @staticmethod\\r\\n    def timer(func):\\r\\n        \"\"\"Print the runtime of the decorated function\"\"\"\\r\\n        @functools.wraps(func)\\r\\n        def wrapper_timer(*args, **kwargs):\\r\\n            start_time = time.perf_counter()    # 1\\r\\n            value = func(*args, **kwargs)\\r\\n            end_time = time.perf_counter()      # 2\\r\\n            run_time = end_time - start_time    # 3\\r\\n            print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\\r\\n            with open(\"perf.log\", \"a\") as times:\\r\\n                times.write(str(run_time) + \"\\\\n\")\\r\\n            return value\\r\\n        return wrapper_timer\\r\\nt = Timer()\\r\\n\\r\\n# Exceptions\\r\\nclass TimeoutException(Exception):\\r\\n    pass\\r\\nclass NoCrashException(Exception):\\r\\n    pass\\r\\nclass CrashwalkError(Exception):\\r\\n    pass\\r\\n\\r\\n# TODO: should not have written this as two separate classes\\r\\nclass GDBJob:\\r\\n    def __init__(self, proc_name, filename, timeout=20):\\r\\n        START_PARSE = \"---START_HERE---\"\\r\\n        END_PARSE = \"---END_HERE---\"\\r\\n        self.filename = filename\\r\\n        self.crashed = True\\r\\n        self.timedout = False\\r\\n        exploitable_path = \"/mnt/c/Users/pengjohn/Documents/tools/exploit/exploitable/exploitable/exploitable.py\"\\r\\n        if env_exploitable := os.environ.get(\"EXPLOITABLE_PATH\", None):\\r\\n            self.exploitable_path = env_exploitable\\r\\n        context.log_level = \"error\"\\r\\n        gdb = process([\"gdb\", \"--args\", proc_name, filename], stdin=process.PTY, stdout=process.PTY, timeout=timeout)\\r\\n        # PWN complains when string encoding is not explicit\\r\\n        # Need this or GDB will require user keystroke to display rest of output\\r\\n        gdb.sendline(\"set height unlimited\".encode(\"utf-8\"))\\r\\n        gdb.sendline(\"gef config context False\".encode(\"utf-8\"))\\r\\n        gdb.sendline(\"r\".encode(\"utf-8\"))\\r\\n        if not os.path.isfile(exploitable_path):\\r\\n            raise Exception(f\"Exploitable not found at {exploitable_path}\".encode(\"utf-8\"))\\r\\n        gdb.sendline(f\"source {exploitable_path}\".encode(\"utf-8\"))\\r\\n        gdb.sendline(f\"p \\'{START_PARSE}\\'\".encode(\"utf-8\"))\\r\\n        gdb.sendline(\"exploitable -v\".encode(\"utf-8\"))\\r\\n        actions = [\\r\\n            \"frame 2\",\\r\\n            \"p *next\"\\r\\n        ]\\r\\n        # segfaulting address\\r\\n        gdb.sendline(\"SegfaultAddy\".encode(\"utf-8\"))\\r\\n        gdb.sendline(\"p $_siginfo._sifields._sigfault.si_addr\".encode(\"utf-8\"))\\r\\n        self.send(actions, gdb)\\r\\n        gdb.sendline(f\"p \\'{END_PARSE}\\'\".encode(\\'utf-8\\'))\\r\\n        self.output = gdb.recvuntil(f\"{END_PARSE}\".encode(\"utf-8\")).decode(\\'utf-8\\').split(\\'\\\\n\\')\\r\\n        gdb.close()\\r\\n        \\r\\n        if self.timedout == True:\\r\\n            return\\r\\n        # check if process actually crashed\\r\\n        for line in self.output:\\r\\n            if \"exited normally\" in line or \"exited with\" in line:\\r\\n                self.crashed = False\\r\\n\\r\\n    def send(self, actions, gdb):\\r\\n        for action in actions:\\r\\n            gdb.sendline(action.encode(\"utf-8\"))\\r\\n\\r\\n    def generate_exploitable(self):\\r\\n        if not self.crashed:\\r\\n            print(\"{} did not crash\".format(self.filename))\\r\\n            raise NoCrashException\\r\\n        elif self.timedout == True:\\r\\n            print(\"{} timed out\".format(self.filename))\\r\\n            raise TimeoutException\\r\\n        elif not self.output:\\r\\n            print(\"no output\")\\r\\n            raise Exception\\r\\n        return Exploitable(self.output, self.filename)\\r\\n\\r\\nclass Exploitable:\\r\\n    def __init__(self, output, crash_file):\\r\\n        try:\\r\\n            START_PARSE = \"---START_HERE---\"\\r\\n            self.classification = []\\r\\n            self.exploitable = False\\r\\n            self.crash_file = crash_file\\r\\n            self._output = iter(output)\\r\\n            self.raw_output = output\\r\\n            not_start = True\\r\\n            line = next(self._output, None)\\r\\n            while line or not_start:\\r\\n                if f\"{START_PARSE}\" in line:\\r\\n                    not_start = False\\r\\n                if \"Nearby code:\" in line:\\r\\n                    self.disassembly, line = self.parse_until(\"Stack trace:\")\\r\\n                    # Dont need this line since the iterator from the prev parse_until call will consume this line\\r\\n                    # if \"Stack trace:\" in line:\\r\\n                    self.stack_trace, line = self.parse_until(\"Faulting frame:\")\\r\\n                    self.faulting_frame = line.split(\" \")[5]\\r\\n                if \"Description:\" in line:\\r\\n                    self.classification, line = self.parse_until(\"gef\")\\r\\n                if \"SegfaultAddy\" in line:\\r\\n                    self.segfault = self.parse_segfault()\\r\\n                line = next(self._output, None)\\r\\n            self.assert_correctness()\\r\\n        except Exception:\\r\\n            print(f\"Crashwalk error, self.output: \")\\r\\n            for l in self.raw_output:\\r\\n                print(l)\\r\\n            raise CrashwalkError\\r\\n\\r\\n    def parse_segfault(self):\\r\\n        segfault = next(self._output, None)\\r\\n        if not segfault:\\r\\n            raise Exception(\"Error parsing segfault\")\\r\\n        match = re.search(\"(0x.*)\", segfault)\\r\\n        if match:\\r\\n            return match.group(1)\\r\\n\\r\\n    # hash the first n callstacks\\r\\n    def get_call_hash(self, n):\\r\\n        callstack_string = \"\".join(self.get_callstack()[:n])\\r\\n        return hashlib.md5(callstack_string.encode(\"utf-8\")).hexdigest()\\r\\n\\r\\n    def parse_until(self, stop_parse):\\r\\n        trace = []\\r\\n        line = next(self._output, None)\\r\\n        if not line:\\r\\n            raise Exception(\"Error parsing stacktrace\")\\r\\n        while line and stop_parse not in line:\\r\\n            trace.append(line)\\r\\n            line = next(self._output, None)\\r\\n        return trace, line\\r\\n\\r\\n    def get_callstack(self):\\r\\n        # normalize the spaces for the split call\\r\\n        #  0 Umbra::BlockMemoryManager<4096>::removeFreeAlloc at 0x7ffff7a6957d in /mnt/c/Users/pengjohn/Documents/umbra/umbra3/bin/linux64/libumbraoptimizer64.so\\r\\n        callstack = [frame.replace(\"  \", \" \").split(\" \")[2] for frame in self.stack_trace]\\r\\n        return callstack\\r\\n\\r\\n    def get_callstack_raw(self):\\r\\n        return self.stack_trace\\r\\n\\r\\n    def assert_correctness(self):\\r\\n        assert self.disassembly\\r\\n        assert self.get_callstack_raw()\\r\\n        assert self.classification\\r\\n\\r\\n    # output functions\\r\\n    def print_raw(self):\\r\\n        print(\"Disassembly: \")\\r\\n        for line in self.disassembly:\\r\\n            print(line)\\r\\n        print(\"CallStack: \")\\r\\n        for frame in self.get_callstack_raw():\\r\\n            print(frame)\\r\\n        for descr in self.classification:\\r\\n            print(descr)\\r\\n        print(\"Segmentation Fault: \", self.segfault)\\r\\n\\r\\n    def set_linearity(self, linearity):\\r\\n        self.linearity = linearity\\r\\n\\r\\n    def set_crash_offset(self, crash_offset):\\r\\n        self.crash_offset = crash_offset\\r\\n\\r\\n    def set_crash_bytes(self, crash_bytes):\\r\\n        self.crash_bytes = crash_bytes\\r\\n\\r\\n@t.timer\\r\\ndef run_GDBWorker(filepath):\\r\\n    try:\\r\\n        print(\"Checking crash for {}\".format(filepath))\\r\\n        exploitable = GDBJob(executable, filepath).generate_exploitable()\\r\\n        # why doesn\\'t python complain about explotiables not being declared as global variable\\r\\n        return exploitable\\r\\n    except NoCrashException as e:\\r\\n        print(\"No crash\")\\r\\n\\r\\ndef get_pickle_fname(pickle_path):\\r\\n    pickle_fname = os.path.normpath(pickle_path)\\r\\n    if \"/\" in pickle_fname:\\r\\n        pickle_fname = pickle_fname.replace(\\'/\\', \"_\")\\r\\n    return pickle_fname\\r\\n\\r\\ndef write_pickle(pickle_path, exploitables):\\r\\n    if os.path.isdir(pickle_path):\\r\\n        pickle_path += datetime.now().strftime(\"%m-%d-%Y_%H_%M_%S\")\\r\\n    with open(\"{}.pickle\".format(pickle_path), \"wb\") as cw_pickle:\\r\\n        # only exploitable crashes are going to be serialized\\r\\n        # exploitables = [e for e in exploitables if e != None and e.exploitable]\\r\\n        exploitables = [e for e in exploitables if e]\\r\\n        pickle.dump(exploitables, cw_pickle)\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    argParse = argparse.ArgumentParser()\\r\\n    argParse.add_argument(\"--executable\", help=\"Path to the executable, if not provided via cmdline, will be read from CRASHWALK_BINARY env variable\")\\r\\n    argParse.add_argument(\"path\", help=\"Path to the crash file\")\\r\\n    argParse.add_argument(\"--pickle-name\", help=\"Optionally specify the name of the pickle file\")\\r\\n    argParse.add_argument(\"--verbose\", help=\"Print output to stdout\", action=\"store_true\")\\r\\n\\r\\n    arguments = argParse.parse_args()\\r\\n\\r\\n    try:\\r\\n        executable = arguments.executable if arguments.executable else os.environ[\"CRASHWALK_BINARY\"]\\r\\n    except KeyError:\\r\\n        print(\"Please specify the executable binary via env variables or cmd line arguments\")\\r\\n        sys.exit(-1)\\r\\n    pickle_name = arguments.pickle_name\\r\\n    path = arguments.path\\r\\n    verbose = arguments.verbose if arguments.verbose else False\\r\\n\\r\\n    GDB_PROCS = multiprocessing.cpu_count()\\r\\n    crash_files = [path]\\r\\n\\r\\n    # no recursive search for crash files and all files present are crash files\\r\\n    if os.path.isdir(path):\\r\\n        crash_files = glob.glob(os.path.join(path, \"*\"))\\r\\n    total_files = len(crash_files)\\r\\n\\r\\n    # initialize length so each thread can individually update its index without locking\\r\\n    exploitables = []\\r\\n    # updates the exit status of the GDB job: 1 for success, 2 for an exception raised\\r\\n    run_status = [0] * len(crash_files)\\r\\n\\r\\n    # TODO: fix this\\r\\n    # try:\\r\\n    #     # read files previously seen files and skip them\\r\\n    #     seen_crashes = [s.strip() for s in open(\".prev_files.db\", \"r\").readlines()]\\r\\n    #     crash_files = [crash for crash in crash_files if crash not in seen_crashes]\\r\\n    #     print(\"Restarting, using {}, {}/{} files to look through\".format(crash_files[0], len(crash_files), total_files))\\r\\n    # except FileNotFoundError as e:\\r\\n    #     pass\\r\\n    # except IndexError:\\r\\n    #     print(\"{} already processed in previous run\")\\r\\n    #     sys.exit(-1)\\r\\n\\r\\n    seen_crashes = open(\".prev_files.db\", \"a\")\\r\\n    pending_futures = []\\r\\n    try:\\r\\n        with ThreadPoolExecutor(max_workers=GDB_PROCS) as executor:\\r\\n            for i, crash in enumerate(crash_files):\\r\\n                print(\"Launching job {}\".format(i))\\r\\n                pending_futures.append( executor.submit(run_GDBWorker, crash) )\\r\\n\\r\\n            # as_completed registers a callback event that gets called for each thread that\\'s current waiting on a exploitable object\\r\\n            # https://stackoverflow.com/questions/51239251/how-does-concurrent-futures-as-completed-work\\r\\n            for future in as_completed(pending_futures):\\r\\n                exploitable = future.result()\\r\\n                if verbose:\\r\\n                    exploitable.print_raw()\\r\\n                exploitables.append(future.result())\\r\\n\\r\\n    except KeyboardInterrupt:\\r\\n        if not pickle_name:\\r\\n            pickle_name = get_pickle_fname(path)\\r\\n        print(\"Serializing pickle\")\\r\\n        write_pickle(pickle_name, exploitables)\\r\\n\\r\\n    if not pickle_name:\\r\\n        pickle_name = get_pickle_fname(path)\\r\\n    write_pickle(pickle_name, exploitables)\\r\\n'\n",
      "test2.py\n",
      "b'\"hello\" \\r\\n'\n",
      "utils.py\n",
      "b'import os\\r\\nimport pickle\\r\\nfrom concurrent.futures import ThreadPoolExecutor\\r\\nimport queue\\r\\nimport functools\\r\\nimport time\\r\\nimport shutil\\r\\nfrom multiprocessing import cpu_count\\r\\nfrom crashwalk import GDBJob, NoCrashException\\r\\n\\r\\n# utils\\r\\ndef bytes_to_hex_str(b: bytes, endianess=\"little\")-> str:\\r\\n    hex_str = \"\"\\r\\n    b = b if endianess == \"big\" else b[::-1]\\r\\n    for byte in b:\\r\\n        hex_str += hex(byte).replace(\"0x\",\"\")\\r\\n    return \"0x\" + hex_str\\r\\n\\r\\ndef hex_str_to_bytes(hex_bytes: str) -> bytes:\\r\\n    byte_str_array = [int(hex_bytes[i:i+2], 16) for i in range(0, len(hex_bytes)-1, 2)]\\r\\n    return bytes(byte_str_array)\\r\\n\\r\\ndef hex_str_to_int(hex_bytes: str) -> int:\\r\\n    return int(hex_bytes.replace(\"0x\", \"\"), 16)\\r\\n\\r\\n\\r\\ndef add_bytes(a:int, b:int) -> int:\\r\\n    return (a + b) % 256\\r\\n\\r\\ndef serialize_exploitables(path, exploitables):\\r\\n    pickle_fname = os.path.normpath(path)\\r\\n    if \"/\" in path:\\r\\n        # Note: Python trick\\r\\n        # Find reverse index\\r\\n        # pickle_fname = path[len(path) - path[::-1].index(\\'/\\'):]\\r\\n        pickle_fname = pickle_fname.replace(\\'/\\', \"_\")\\r\\n        print(\"Pickled filename: {}\".format(pickle_fname))\\r\\n    with open(\"{}.pickle\".format(pickle_fname), \"wb\") as cw_pickle:\\r\\n        # only exploitable crashes are going to be serialized\\r\\n        # exploitables = [e for e in exploitables if e != None and e.exploitable]\\r\\n        exploitables = [e for e in exploitables if e]\\r\\n        pickle.dump(exploitables, cw_pickle)\\r\\n\\r\\n# multithread stuff\\r\\nGDB_PROCS = cpu_count()\\r\\nclass CustomThreadPoolExecutor(ThreadPoolExecutor):\\r\\n    def shutdown(self, wait=True, *, cancel_futures=False):\\r\\n        with self._shutdown_lock:\\r\\n            self._shutdown = True\\r\\n            if cancel_futures:\\r\\n                # Drain all work items from the queue, and then cancel their\\r\\n                # associated futures.\\r\\n                while True:\\r\\n                    try:\\r\\n                        work_item = self._work_queue.get_nowait()\\r\\n                    except queue.Empty:\\r\\n                        break\\r\\n                    if work_item is not None:\\r\\n                        work_item.future.cancel()\\r\\n\\r\\n            # Send a wake-up to prevent threads calling\\r\\n            # _work_queue.get(block=True) from permanently blocking.\\r\\n            self._work_queue.put(None)\\r\\n        if wait:\\r\\n            for t in self._threads:\\r\\n                t.join()\\r\\n\\r\\n# class UtilsDecorator:\\r\\n#     def __init__(self):\\r\\n#         self.decorators = [\\r\\n#             Timer,\\r\\n            \\r\\n#         ]\\r\\n\\r\\nclass Timer:\\r\\n    def __init__(self):\\r\\n        if os.path.exists(\"perf.log\"):\\r\\n            os.remove(\"perf.log\")\\r\\n\\r\\n    def wrap(self, func):\\r\\n        self.timer(func)\\r\\n\\r\\n    @staticmethod\\r\\n    def timer(func):\\r\\n        \"\"\"Print the runtime of the decorated function\"\"\"\\r\\n        @functools.wraps(func)\\r\\n        def wrapper_timer(*args, **kwargs):\\r\\n            start_time = time.perf_counter()    # 1\\r\\n            value = func(*args, **kwargs)\\r\\n            end_time = time.perf_counter()      # 2\\r\\n            run_time = end_time - start_time    # 3\\r\\n            with open(\"perf.log\", \"a\") as times:\\r\\n                times.write(str(run_time) + \"\\\\n\")\\r\\n            return value\\r\\n        return wrapper_timer\\r\\n    \\r\\nt = Timer()\\r\\n\\r\\n\\r\\n# Function overloading python?\\r\\n@t.timer\\r\\ndef replaceBytesDiff(parent, modified_parent, diff):\\r\\n    res = shutil.copyfile(parent, modified_parent)\\r\\n    with open(modified_parent, \"r+b\") as file_handle:\\r\\n        # TODO:\\r\\n        # write back old bytes after GDB call, so we don\\'t make any inadvertent changes to execution trace\\r\\n        # old_bytes = file_handle.read(len(bytes))\\r\\n        # double seeking required since advance moves the file pointer\\r\\n        for offset, bytes in diff:\\r\\n            file_handle.seek(offset)\\r\\n            # print(\"Writing {} at {} @ file: {}\".format(bytes, offset, modified_parent))\\r\\n            b = file_handle.write(bytes)\\r\\n            if b != len(bytes):\\r\\n                return False\\r\\n            file_handle.flush()\\r\\n        return True\\r\\n\\r\\ndef replaceBytes(file_handle, offset, b):\\r\\n    if type(b) != bytes:\\r\\n        b = bytes([b])\\r\\n    # TODO:\\r\\n    # write back old bytes after GDB call, so we don\\'t make any inadvertent changes to execution trace\\r\\n    # old_bytes = file_handle.read(len(bytes))\\r\\n    # double seeking required since advance moves the file pointer\\r\\n    file_handle.seek(offset)\\r\\n    # print(\"Writing {} at {} @ file: {}\".format(bytes, offset, modified_parent))\\r\\n    written = file_handle.write(b)\\r\\n    if written != len(b):\\r\\n        return False\\r\\n    file_handle.flush()\\r\\n    return True\\r\\n\\r\\nclass GDBExecutor:\\r\\n    def __init__(self, executable):\\r\\n        self.t_pool = CustomThreadPoolExecutor(max_workers=GDB_PROCS)\\r\\n        self.executable = executable\\r\\n        self.inc_me = 0\\r\\n\\r\\n    def run_jobs(self, crashes, ordered=False):\\r\\n        if not ordered:\\r\\n            jobs = []\\r\\n            for crash in crashes:\\r\\n                job = self.t_pool.submit(self.runGDBJob, crash)\\r\\n                jobs.append(job)\\r\\n            return jobs\\r\\n        # map returns ordered results (not promises)\\r\\n        else:\\r\\n            jobs = self.t_pool.map( self.runGDBJob, crashes) \\r\\n            return jobs\\r\\n\\r\\n    @t.timer\\r\\n    def runGDBJob(self, filepath):\\r\\n        try:\\r\\n            # print(f\"running {filepath}\")\\r\\n            exploitable = GDBJob(self.executable, filepath).generate_exploitable()\\r\\n            # why doesn\\'t python complain about explotiables not being declared as global variable\\r\\n            return exploitable\\r\\n        except NoCrashException:\\r\\n            print(f\"No crash {filepath}\")\\r\\n            return None\\r\\n            \\r\\nif __name__ == \"__main__\":\\r\\n    pass'\n",
      "__init__.py\n",
      "b''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import json\n",
    "from pathlib import Path\n",
    "from rtfs.fs import RepoFs\n",
    "\n",
    "REPO_BASE = Path(\"../../codesearch-data/repo\")\n",
    "\n",
    "repos = [\n",
    "    REPO_BASE / \"JohnPeng47_CrashOffsetFinder.git\",\n",
    "    # REPO_BASE / \"Aider-AI_aider.git\",\n",
    "]\n",
    "\n",
    "for repo in repos:\n",
    "    output = \"\"\n",
    "    for f, content in RepoFs(repo).get_files_content():\n",
    "        output += f\"{f}\\n{content}\\n\"\n",
    "\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
