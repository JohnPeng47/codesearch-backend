# What am I looking at?
https://cowboy.rocks/codesearch

The original idea behind this was: can AI help solve code understanding?
I approached this problem by:
1. Identifying clusters of code that represented distinct functional components in the code (ie. API Request Error Handling)
2. Generate summaries of these
3. Present a UI that allows you to browse the source code using the summaries as guidance
<br>
The goal here was not to replace looking at source code with summaries, but to present the user with summaries that represents a 10,000 ft. view of the code, quickly orienting them in the unfamiliar territory so they can better navigate it on their own

# How clustering works
1. Chunk using code aware chunking strategy (lifted from https://github.com/aorwall/moatless-tools)
    - "code aware" means for example chunking on function boundaries to keep variable scopes intact during retrieval
   (Example of non-code aware chunking) -> awareness of a,b as the function parameters is lost when the boundary is arbitrarily selected by a non-code aware chunking scheme:

   def func(a: T, b: S):
    ------- CHUNK BOUNDARY ------
        dostuff(a, b):
      
2. Generate dependency graph for references -> definitions (inspired by https://github.com/BloopAI/bloop.git)
   My depedency resolution algorithm works by assigning referenced/defined relationships to variables at the chunk level. So for each chunk, its outgoing edges are references to externally defined (in another chunk) symbols and the incoming edges are for when another chunk has referenced it definitions.
   The first problem we encounter is symbol resolution. How do we disambiguate variables that share the same name within a source file?
   The solution that I lifted from Bloop's rust code and optimized into python uses a scope graph structure to track all of the scopes in the source file. And to be specific, it is a DAG that starts with an origin at the root scope, and recursively traverses the scopes to uniquely identify variables across the entire source repository. 
   f1.py

   // scope 1, the global module scope
   // Scoped identifiers -> a::1, b::1
   a,b = 0,1
   // doing stuff with a,b
   def f(x, y):
      // scope 2, list comprehension
      // Scoped identifiers -> a::2, b::2
      a, b = 2,3
      global_var = 1

    
      -  ie. Network::request !== request
4. Cluster chunks together using a generic graph clustering/community detection algorithm (https://github.com/mapequation/infomap.git)
   - community detection here roughly translates to finding groups of nodes that maximizes the metric "of having more connections within group than outside of group" (roughly because apparently infomap uses a different "objective function")
   - pretty clustering algorithm agnostic, using a diff algorithm produces a similar result
5. Recluster using GPT
   - so reason for doing this is step 3. doesn't quite generate good enough clusters, and I use GPT here to reassign chunks between the clusters
   - .. and as to question why not just cluster in GPT in the first place:
      1. Context length
      2. Find that clusters generated by GPT tend to be biased towards selecting for chunks that close together in the directory layout
6. Generate summaries for chunks
7. (ROADMAP) Code search

# Honorable Mentions
The original idea was inspired for Microsoft's https://github.com/microsoft/graphrag; figured that code dependency edges probably should convey way more information than generic entity relation graphs
