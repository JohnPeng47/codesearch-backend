Chunk ID: build.py::2
Filepath: build.py
Content:
import os
import subprocess
import shutil
import toml
import sys

def run_command(command, cwd=None):
    result = subprocess.run(command, shell=True, cwd=cwd, check=True)
    return result

def npm_install():
    print("Running npm install")
    run_command("npm install", cwd="ell-studio")


def npm_build():
    print("Running npm build")
    run_command("npm run build", cwd="ell-studio")
    print("Copying static files")
    source_dir = os.path.join("ell-studio", "build")
    target_dir = os.path.join("src", "ell", "studio", "static")
    shutil.rmtree(target_dir, ignore_errors=True)
    shutil.copytree(source_dir, target_dir)
    print(f"Copied static files from {source_dir} to {target_dir}")


def get_ell_version():
    pyproject_path = "pyproject.toml"
    pyproject_data = toml.load(pyproject_path)
    return pyproject_data["tool"]["poetry"]["version"]


def run_pytest():
    print("Running pytest")
    try:
        run_command("pytest", cwd="tests")
    except subprocess.CalledProcessError:
        print("Pytest failed. Aborting build.")
        sys.exit(1)


def run_all_examples():
    print("Running all examples")
    try:
        run_command("python run_all_examples.py -w 16", cwd="tests")
    except subprocess.CalledProcessError:
        print("Some examples failed. Please review the output above.")
        user_input = input("Do you want to continue with the build? (y/n): ").lower()
        if user_input != 'y':
            print("Aborting build.")
            sys.exit(1)


def main():
    ell_version = get_ell_version()
    os.environ['REACT_APP_ELL_VERSION'] = ell_version
    npm_install()
    npm_build()
    run_pytest()
    run_all_examples()
    print("Build completed successfully.")


if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\autostreamprevention.py::1
Filepath: docs\ramblings\0.1.0\autostreamprevention.py
Content:
import openai
import os

# Define the function to stream the response
def stream_openai_response(prompt):
    try:
        # Make the API call
        response = openai.chat.completions.create(
            model="o1-mini",  # Specify the model
            messages=[{"role": "user", "content": prompt}],
            stream=True  # Enable streaming
        )

        # Stream the response
        for chunk in response:
            if chunk.choices[0].delta.get("content"):
                print(chunk.choices[0].delta.content, end="", flush=True)

        print()  # Print a newline at the end

    except Exception as e:
        print(f"An error occurred: {e}")

# Example usage
prompt = "Tell me a short joke."
stream_openai_response(prompt)
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cem.py::1
Filepath: docs\ramblings\0.1.0\cem.py
Content:
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from gym.vector import AsyncVectorEnv
import random

# Set random seeds for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# Hyperparameters
NUM_ENVIRONMENTS = 4           # Reduced for simplicity
NUM_ITERATIONS = 50            # Number of training iterations
TRAJECTORIES_PER_ITER = 100    # Total number of trajectories per iteration
ELITE_PERCENT = 10             # Top k% trajectories to select
LEARNING_RATE = 1e-3
BATCH_SIZE = 64
MAX_STEPS = 500                # Max steps per trajectory
ENV_NAME = 'CartPole-v1'
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cem.py::2
Filepath: docs\ramblings\0.1.0\cem.py
Content:
       # Gym environment

# Define the Policy Network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state):
        logits = self.fc(state)
        return logits

    def get_action(self, state):
        logits = self.forward(state)
        action_probs = torch.softmax(logits, dim=-1)
        action = torch.multinomial(action_probs, num_samples=1)
        return action.squeeze(-1)

# Function to create multiple environments
def make_env(env_name, seed):
    def _init():
        env = gym.make(env_name)
        return env
    return _init
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cem.py::3
Filepath: docs\ramblings\0.1.0\cem.py
Content:
def collect_trajectories(envs, policy, num_trajectories, max_steps):
    trajectories = []
    num_envs = envs.num_envs

    # Handle the return type of reset()
    reset_output = envs.reset()
    if isinstance(reset_output, tuple) or isinstance(reset_output, list):
        obs = reset_output[0]  # Extract observations
    else:
        obs = reset_output

    done_envs = [False] * num_envs
    steps = 0

    # Initialize storage for states, actions, and rewards per environment
    env_states = [[] for _ in range(num_envs)]
    env_actions = [[] for _ in range(num_envs)]
    env_rewards = [0.0 for _ in range(num_envs)]
    total_collected = 0
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cem.py::4
Filepath: docs\ramblings\0.1.0\cem.py
Content:
def collect_trajectories(envs, policy, num_trajectories, max_steps):
    # ... other code

    while total_collected < num_trajectories and steps < max_steps:
        # Convert observations to tensor efficiently
        try:
            # Ensure 'obs' is a NumPy array
            if not isinstance(obs, np.ndarray):
                print(f"Unexpected type for observations: {type(obs)}")
                raise ValueError("Observations are not a NumPy array.")

            # Convert observations to tensor using from_numpy for efficiency
            obs_tensor = torch.from_numpy(obs).float()
            # Ensure the observation dimension matches expected
            assert obs_tensor.shape[1] == 4, f"Expected observation dimension 4, got {obs_tensor.shape[1]}"
        except Exception as e:
            print(f"Error converting observations to tensor at step {steps}: {e}")
            print(f"Observations: {obs}")
            raise e

        with torch.no_grad():
            actions = policy.get_action(obs_tensor).cpu().numpy()

        # Unpack step based on Gym version
        try:
            # For Gym versions >=0.26, step returns five values
            next_obs, rewards, dones, truncs, infos = envs.step(actions)
        except ValueError:
            # For older Gym versions, step returns four values
            next_obs, rewards, dones, infos = envs.step(actions)
            truncs = [False] * len(dones)  # Assume no truncations if not provided

        # Handle the reset output of step()
        if isinstance(next_obs, tuple) or isinstance(next_obs, list):
            next_obs = next_obs[0]  # Extract observations

        # Ensure infos is a list
        if not isinstance(infos, list):
            infos = [{} for _ in range(num_envs)]  # Default to empty dicts

        for i in range(num_envs):
            if not done_envs[i]:
                # Check if obs[i] has the correct shape
                if len(obs[i]) != 4:
                    print(f"Unexpected observation shape for env {i}: {obs[i]}")
                    continue  # Skip this step for the problematic environment

                env_states[i].append(obs[i])
                env_actions[i].append(actions[i])
                env_rewards[i] += rewards[i]
                if dones[i] or truncs[i]:
                    # Extract reward from infos
                    if isinstance(infos[i], dict):
                        episode_info = infos[i].get('episode', {})
                        traj_reward = episode_info.get('r') if 'r' in episode_info else env_rewards[i]
                    else:
                        # Handle cases where infos[i] is not a dict
                        traj_reward = env_rewards[i]
                        print(f"Warning: infos[{i}] is not a dict. Received type: {type(infos[i])}")

                    trajectories.append({
                        'states': env_states[i],
                        'actions': env_actions[i],
                        'reward': traj_reward
                    })
                    total_collected += 1
                    env_states[i] = []
                    env_actions[i] = []
                    env_rewards[i] = 0.0
                    done_envs[i] = True

        obs = next_obs
        steps += 1

        # Reset environments that are done
        if any(done_envs):
            indices = [i for i, done in enumerate(done_envs) if done]
            if total_collected < num_trajectories:
                for i in indices:
                    try:
                        # Directly reset the environment
                        reset_output = envs.envs[i].reset()
                        if isinstance(reset_output, tuple) or isinstance(reset_output, list):
                            # For Gym versions where reset returns (obs, info)
                            obs[i] = reset_output[0]
                        else:
                            # For Gym versions where reset returns only obs
                            obs[i] = reset_output
                        done_envs[i] = False
                    except Exception as e:
                        print(f"Error resetting environment {i}: {e}")
                        # Optionally, handle the failure (e.g., retry, terminate the environment)
                        done_envs[i] = False  # Prevent infinite loop

    return trajectories
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cem.py::5
Filepath: docs\ramblings\0.1.0\cem.py
Content:
def select_elite(trajectories, percentile=ELITE_PERCENT):
    rewards = [traj['reward'] for traj in trajectories]
    if not rewards:
        return []
    reward_threshold = np.percentile(rewards, 100 - percentile)
    elite_trajectories = [traj for traj in trajectories if traj['reward'] >= reward_threshold]
    return elite_trajectories

# Function to create training dataset from elite trajectories
def create_training_data(elite_trajectories):
    states = []
    actions = []
    for traj in elite_trajectories:
        states.extend(traj['states'])
        actions.extend(traj['actions'])
    if not states or not actions:
        return None, None
    # Convert lists to NumPy arrays first for efficiency
    states = np.array(states, dtype=np.float32)
    actions = np.array(actions, dtype=np.int64)
    # Convert to PyTorch tensors
    states = torch.from_numpy(states)
    actions = torch.from_numpy(actions)
    return states, actions
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cem.py::6
Filepath: docs\ramblings\0.1.0\cem.py
Content:
# Main execution code
if __name__ == '__main__':
    # Initialize environments
    env_fns = [make_env(ENV_NAME, SEED + i) for i in range(NUM_ENVIRONMENTS)]
    envs = AsyncVectorEnv(env_fns)

    # Get environment details
    dummy_env = gym.make(ENV_NAME)
    state_dim = dummy_env.observation_space.shape[0]
    action_dim = dummy_env.action_space.n
    dummy_env.close()

    # Initialize policy network and optimizer
    policy = PolicyNetwork(state_dim, action_dim)
    optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    # Training Loop
    for iteration in range(1, NUM_ITERATIONS + 1):
        try:
            # Step 1: Collect Trajectories
            trajectories = collect_trajectories(envs, policy, TRAJECTORIES_PER_ITER, MAX_STEPS)
        except Exception as e:
            print(f"Error during trajectory collection at iteration {iteration}: {e}")
            break

        # Step 2: Select Elite Trajectories
        elite_trajectories = select_elite(trajectories, ELITE_PERCENT)

        if len(elite_trajectories) == 0:
            print(f"Iteration {iteration}: No elite trajectories found. Skipping update.")
            continue

        # Step 3: Create Training Data
        states, actions = create_training_data(elite_trajectories)

        if states is None or actions is None:
            print(f"Iteration {iteration}: No training data available. Skipping update.")
            continue

        # Step 4: Behavioral Cloning (Policy Update)
        dataset_size = states.size(0)
        indices = np.arange(dataset_size)
        np.random.shuffle(indices)

        for start in range(0, dataset_size, BATCH_SIZE):
            end = start + BATCH_SIZE
            batch_indices = indices[start:end]
            batch_states = states[batch_indices]
            batch_actions = actions[batch_indices]

            optimizer.zero_grad()
            logits = policy(batch_states)
            loss = criterion(logits, batch_actions)
            loss.backward()
            optimizer.step()

        # Step 5: Evaluate Current Policy
        avg_reward = np.mean([traj['reward'] for traj in elite_trajectories])
        print(f"Iteration {iteration}: Elite Trajectories: {len(elite_trajectories)}, Average Reward: {avg_reward:.2f}")

    # Close environments
    envs.close()

    # Testing the Trained Policy
    def test_policy(policy, env_name=ENV_NAME, episodes=5, max_steps=500):
        env = gym.make(env_name)
        total_rewards = []
        for episode in range(episodes):
            obs, _ = env.reset()
            done = False
            episode_reward = 0
            for _ in range(max_steps):
                obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)
                with torch.no_grad():
                    action = policy.get_action(obs_tensor).item()
                obs, reward, done, info, _ = env.step(action)
                episode_reward += reward
                if done:
                    break
            total_rewards.append(episode_reward)
            print(f"Test Episode {episode + 1}: Reward: {episode_reward}")
        env.close()
        print(f"Average Test Reward over {episodes} episodes: {np.mean(total_rewards):.2f}")

    # Run the test
    test_policy(policy)
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\context_versioning.py::1
Filepath: docs\ramblings\0.1.0\context_versioning.py
Content:
import inspect
import ast
from contextlib import contextmanager

@contextmanager
def context():
    # Get the current frame
    frame = inspect.currentframe()
    try:
        # Get the caller's frame
        caller_frame = frame.f_back.f_back
        # Get the filename and line number where the context manager is called
        filename = caller_frame.f_code.co_filename
        lineno = caller_frame.f_lineno

        # Read the source code from the file
        with open(filename, 'r') as f:
            source = f.read()

        # Parse the source code into an AST
        parsed = ast.parse(source, filename)
        # print(source)
        # Find the 'with' statement at the given line number
        class WithVisitor(ast.NodeVisitor):
            def __init__(self, target_lineno):
                self.target_lineno = target_lineno
                self.with_node = None

            def visit_With(self, node):
                if node.lineno <= self.target_lineno <= node.end_lineno:
                    self.with_node = node
                self.generic_visit(node)

        visitor = WithVisitor(lineno)
        visitor.visit(parsed)

        # print(parsed, source)
        if visitor.with_node:
            # Extract the source code of the block inside 'with'
            start = visitor.with_node.body[0].lineno
            end = visitor.with_node.body[-1].end_lineno
            block_source = '\n'.join(source.splitlines()[start-1:end])
            print("Source code inside 'with' block:")
            print(block_source)
        else:
            print("Could not find the 'with' block.")

        # Yield control to the block inside 'with'
        yield
    finally:
        # Any cleanup can be done here
        pass

from context_versioning import context
# Example usage
if __name__ == "__main__":
    with context():
        x = 10
        y = x * 2
        print(y)
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cpbo.py::1
Filepath: docs\ramblings\0.1.0\cpbo.py
Content:
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import namedtuple
from torch.utils.data import DataLoader, TensorDataset

# Define a simple policy network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)  # Output action probabilities
        )

    def forward(self, x):
        return self.network(x)
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cpbo.py::2
Filepath: docs\ramblings\0.1.0\cpbo.py
Content:
# Function to collect trajectories
def collect_trajectories(env, policy, num_episodes, device):
    trajectories = []
    Episode = namedtuple('Episode', ['states', 'actions', 'rewards'])

    for episode_num in range(num_episodes):
        states = []
        actions = []
        rewards = []
        # Handle Gym's updated reset() API
        state, info = env.reset(seed=42 + episode_num)  # Optional: set seed for reproducibility
        done = False

        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                action_probs = policy(state_tensor)
            action_dist = torch.distributions.Categorical(action_probs)
            action = action_dist.sample().item()

            # Handle Gym's updated step() API
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            states.append(state)
            actions.append(action)
            rewards.append(reward)

            state = next_state

        trajectories.append(Episode(states, actions, rewards))

    return trajectories

# Function to compute returns
def compute_returns(trajectories, gamma=0.99):
    all_returns = []
    for episode in trajectories:
        returns = []
        G = 0
        for reward in reversed(episode.rewards):
            G = reward + gamma * G
            returns.insert(0, G)
        all_returns.extend(returns)
    return all_returns
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cpbo.py::3
Filepath: docs\ramblings\0.1.0\cpbo.py
Content:
# Function to create labeled dataset
def create_labeled_dataset(trajectories, gamma=0.99, device='cpu'):
    states = []
    actions = []
    labels = []

    all_returns = compute_returns(trajectories, gamma)
    all_returns = np.array(all_returns)
    median_return = np.median(all_returns)

    for episode in trajectories:
        for t in range(len(episode.rewards)):
            # Compute return from timestep t
            G = sum([gamma**k * episode.rewards[t + k] for k in range(len(episode.rewards) - t)])
            label = 1 if G >= median_return else 0
            states.append(episode.states[t])
            actions.append(episode.actions[t])
            labels.append(label)

    # Convert lists to NumPy arrays first for efficiency
    states = np.array(states)
    actions = np.array(actions)
    labels = np.array(labels)

    # Convert to PyTorch tensors
    states = torch.FloatTensor(states).to(device)
    actions = torch.LongTensor(actions).to(device)
    labels = torch.FloatTensor(labels).to(device)

    return states, actions, labels
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cpbo.py::4
Filepath: docs\ramblings\0.1.0\cpbo.py
Content:
# Function to perform behavioral cloning update
def behavioral_cloning_update(policy, optimizer, dataloader, device):
    criterion = nn.BCELoss()
    policy.train()

    for states, actions, labels in dataloader:
        optimizer.zero_grad()
        action_probs = policy(states)
        # Gather the probability of the taken action
        selected_probs = action_probs.gather(1, actions.unsqueeze(1)).squeeze(1)
        # Labels are 1 for good actions, 0 for bad actions
        loss = criterion(selected_probs, labels)
        loss.backward()
        optimizer.step()
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cpbo.py::5
Filepath: docs\ramblings\0.1.0\cpbo.py
Content:
# Evaluation function
def evaluate_policy(env, policy, device, episodes=5):
    policy.eval()
    total_rewards = []
    for _ in range(episodes):
        state, info = env.reset()
        done = False
        ep_reward = 0
        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                action_probs = policy(state_tensor)
            action = torch.argmax(action_probs, dim=1).item()
            # Handle Gym's updated step() API
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            ep_reward += reward
            state = next_state
        total_rewards.append(ep_reward)
    average_reward = np.mean(total_rewards)
    return average_reward
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cpbo.py::6
Filepath: docs\ramblings\0.1.0\cpbo.py
Content:
# Main CBPO algorithm
def CBPO(env_name='CartPole-v1', num_epochs=10, num_episodes_per_epoch=100, gamma=0.99, 
         batch_size=64, learning_rate=1e-3, device='cpu'):

    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")

        # 1. Collect trajectories
        trajectories = collect_trajectories(env, policy, num_episodes_per_epoch, device)

        # 2. Create labeled dataset
        states, actions, labels = create_labeled_dataset(trajectories, gamma, device)

        # 3. Create DataLoader
        dataset = TensorDataset(states, actions, labels)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        # 4. Behavioral Cloning Update
        behavioral_cloning_update(policy, optimizer, dataloader, device)

        # 5. Evaluate current policy
        avg_reward = evaluate_policy(env, policy, device)
        print(f"Average Reward: {avg_reward}")

        # Early stopping if solved
        if avg_reward >= env.spec.reward_threshold:
            print(f"Environment solved in {epoch+1} epochs!")
            break

    env.close()
    return policy
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\cpbo.py::7
Filepath: docs\ramblings\0.1.0\cpbo.py
Content:
if __name__ == "__main__":
    # Check if GPU is available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Run CBPO
    trained_policy = CBPO(
        env_name='CartPole-v1',
        num_epochs=50,
        num_episodes_per_epoch=500,
        gamma=0.99,
        batch_size=64,
        learning_rate=1e-3,
        device=device
    )

    # Final Evaluation
    env = gym.make('CartPole-v1')
    final_avg_reward = evaluate_policy(env, trained_policy, device, episodes=20)
    print(f"Final Average Reward over 20 episodes: {final_avg_reward}")
    env.close()

    # Save the trained policy
    torch.save(trained_policy.state_dict(), "trained_cartpole_policy.pth")
    print("Trained policy saved to trained_cartpole_policy.pth")

    # Demo the trained policy with rendering
    env = gym.make('CartPole-v1', render_mode='human')
    state, _ = env.reset()
    done = False
    total_reward = 0

    while not done:
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        action = trained_policy(state_tensor).argmax().item()
        state, reward, terminated, truncated, _ = env.step(action)
        total_reward += reward
        done = terminated or truncated
        env.render()

    print(f"Demo episode finished with total reward: {total_reward}")
    env.close()
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\metapromptingtorch.py::1
Filepath: docs\ramblings\0.1.0\metapromptingtorch.py
Content:
import torch as th


weights = th.nn.Parameter(th.randn(10))


def forward(x):
    return x * weights


x = th.randn(10)

print(forward(x))
print(weights)

# OOOH WAHT IF WE DID MANY TYPES OF LEARNABLES in
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\mypytest.py::1
Filepath: docs\ramblings\0.1.0\mypytest.py
Content:
from typing import TypedDict


class Test(TypedDict):
    name: str
    age: int


def test(**t: Test):
    print(t)

# no type hinting like ts thats unfortunate.
test( )
--------------------------------------------------------------------------------
Chunk ID: 0.1.0\test.py::1
Filepath: docs\ramblings\0.1.0\test.py
Content:
from typing import Callable

# The follwoing works...



def decorator(fn : Callable):
    def wrapper(*args, **kwargs):
        print("before")
        result = fn(*args, **kwargs)
        print("after")
        return result
    return wrapper


class TestCallable:
    def __init__(self, fn : Callable):
        self.fn = fn

    def __call__(self, *args, **kwargs):
        return self.fn(*args, **kwargs)

def convert_to_test_callable(fn : Callable):
    return TestCallable(fn)

x = TestCallable(lambda : 1)

@decorator
@convert_to_test_callable
def test():
    print("test")

@decorator
class MyCallable:
    def __init__(self, fn : Callable):
        self.fn = fn

    def __call__(self, *args, **kwargs):
        return self.fn(*args, **kwargs)

# Oh so now ell.simples can actually be used as decorators on classes

--------------------------------------------------------------------------------
Chunk ID: src\conf.py::1
Filepath: docs\src\conf.py
Content:
# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

project = 'ell'
copyright = '2024, William Guss'
author = 'William Guss'

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.napoleon', 'sphinxawesome_theme', 'sphinxcontrib.autodoc_pydantic']

templates_path = ['_templates']
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

html_theme = "sphinxawesome_theme"


# Favicon configuration
html_favicon = '_static/favicon.ico'

# Configure syntax highlighting for Awesome Sphinx Theme
pygments_style = "default"
pygments_style_dark = "dracula"

# Additional theme configuration

--------------------------------------------------------------------------------
Chunk ID: src\conf.py::2
Filepath: docs\src\conf.py
Content:
html_theme_options = {
    "show_prev_next": True,
    "show_scrolltop": True,
    "main_nav_links": {
        "Docs": "index",
        "API Reference": "reference/index",
        "AI Jobs Board": "https://jobs.ell.so",
    },
    "extra_header_link_icons": {
        "Discord": {
        "link": "https://discord.gg/vWntgU52Xb",
            "icon": """<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" height="18" fill="currentColor"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M524.5 69.8a1.5 1.5 0 0 0 -.8-.7A485.1 485.1 0 0 0 404.1 32a1.8 1.8 0 0 0 -1.9 .9 337.5 337.5 0 0 0 -14.9 30.6 447.8 447.8 0 0 0 -134.4 0 309.5 309.5 0 0 0 -15.1-30.6 1.9 1.9 0 0 0 -1.9-.9A483.7 483.7 0 0 0 116.1 69.1a1.7 1.7 0 0 0 -.8 .7C39.1 183.7 18.2 294.7 28.4 404.4a2 2 0 0 0 .8 1.4A487.7 487.7 0 0 0 176 479.9a1.9 1.9 0 0 0 2.1-.7A348.2 348.2 0 0 0 208.1 430.4a1.9 1.9 0 0 0 -1-2.6 321.2 321.2 0 0 1 -45.9-21.9 1.9 1.9 0 0 1 -.2-3.1c3.1-2.3 6.2-4.7 9.1-7.1a1.8 1.8 0 0 1 1.9-.3c96.2 43.9 200.4 43.9 295.5 0a1.8 1.8 0 0 1 1.9 .2c2.9 2.4 6 4.9 9.1 7.2a1.9 1.9 0 0 1 -.2 3.1 301.4 301.4 0 0 1 -45.9 21.8 1.9 1.9 0 0 0 -1 2.6 391.1 391.1 0 0 0 30 48.8 1.9 1.9 0 0 0 2.1 .7A486 486 0 0 0 610.7 405.7a1.9 1.9 0 0 0 .8-1.4C623.7 277.6 590.9 167.5 524.5 69.8zM222.5 337.6c-29 0-52.8-26.6-52.8-59.2S193.1 219.1 222.5 219.1c29.7 0 53.3 26.8 52.8 59.2C275.3 311 251.9 337.6 222.5 337.6zm195.4 0c-29 0-52.8-26.6-52.8-59.2S388.4 219.1 417.9 219.1c29.7 0 53.3 26.8 52.8 59.2C470.7 311 447.5 337.6 417.9 337.6z"/></svg>""",
            "type": "font-awesome",
            "name": "Discord",
        },
    },

    "logo_light": "_static/ell-wide-light.png",
    "logo_dark": "_static/ell-wide-dark.png",
    
}

html_static_path = ['_static']



templates_path = ['_templates']
--------------------------------------------------------------------------------
Chunk ID: ell\__init__.py::1
Filepath: src\ell\__init__.py
Content:
"""
ell is a Python library for language model programming (LMP). It provides a simple
and intuitive interface for working with large language models.
"""


from ell.lmp.simple import simple
from ell.lmp.tool import tool
from ell.lmp.complex import complex
from ell.types.message import system, user, assistant, Message, ContentBlock
from ell.__version__ import __version__

# Import all models
import ell.providers
import ell.models


# Import everything from configurator
from ell.configurator import *
--------------------------------------------------------------------------------
Chunk ID: ell\__version__.py::1
Filepath: src\ell\__version__.py
Content:
try:
    from importlib.metadata import version
except ImportError:
    from importlib_metadata import version

__version__ = version("ell-ai")
--------------------------------------------------------------------------------
Chunk ID: ell\configurator.py::1
Filepath: src\ell\configurator.py
Content:
from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)
--------------------------------------------------------------------------------
Chunk ID: ell\configurator.py::2
Filepath: src\ell\configurator.py
Content:
class Config(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    registry: Dict[str, _Model] = Field(default_factory=dict, description="A dictionary mapping model names to their configurations.")
    verbose: bool = Field(default=False, description="If True, enables verbose logging.")
    wrapped_logging: bool = Field(default=True, description="If True, enables wrapped logging for better readability.")
    override_wrapped_logging_width: Optional[int] = Field(default=None, description="If set, overrides the default width for wrapped logging.")
    store: Optional[Store] = Field(default=None, description="An optional Store instance for persistence.")
    autocommit: bool = Field(default=False, description="If True, enables automatic committing of changes to the store.")
    lazy_versioning: bool = Field(default=True, description="If True, enables lazy versioning for improved performance.")
    default_api_params: Dict[str, Any] = Field(default_factory=dict, description="Default parameters for language models.")
    default_client: Optional[openai.Client] = Field(default=None, description="The default OpenAI client used when a specific model client is not found.")
    autocommit_model: str = Field(default="gpt-4o-mini", description="When set, changes the default autocommit model from GPT 4o mini.")
    providers: Dict[Type, Provider] = Field(default_factory=dict, description="A dictionary mapping client types to provider classes.")
    def __init__(self, **data):
        super().__init__(**data)
        self._lock = threading.Lock()
        self._local = threading.local()
--------------------------------------------------------------------------------
Chunk ID: ell\configurator.py::3
Filepath: src\ell\configurator.py
Content:
class Config(BaseModel):


    def register_model(
        self, 
        name: str,
        default_client: Optional[Union[openai.Client, Any]] = None,
        supports_streaming: Optional[bool] = None
    ) -> None:
        """
        Register a model with its configuration.
        """
        with self._lock:
            # XXX: Will be deprecated in 0.1.0
            self.registry[name] = _Model(
                name=name,
                default_client=default_client,
                supports_streaming=supports_streaming
            )
--------------------------------------------------------------------------------
Chunk ID: ell\configurator.py::4
Filepath: src\ell\configurator.py
Content:
class Config(BaseModel):



    @contextmanager
    def model_registry_override(self, overrides: Dict[str, _Model]):
        """
        Temporarily override the model registry with new model configurations.

        :param overrides: A dictionary of model names to ModelConfig instances to override.
        :type overrides: Dict[str, ModelConfig]
        """
        if not hasattr(self._local, 'stack'):
            self._local.stack = []

        with self._lock:
            current_registry = self._local.stack[-1] if self._local.stack else self.registry
            new_registry = current_registry.copy()
            new_registry.update(overrides)

        self._local.stack.append(new_registry)
        try:
            yield
        finally:
            self._local.stack.pop()
--------------------------------------------------------------------------------
Chunk ID: ell\configurator.py::5
Filepath: src\ell\configurator.py
Content:
class Config(BaseModel):

    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:
        """
        Get the OpenAI client for a specific model name.

        :param model_name: The name of the model to get the client for.
        :type model_name: str
        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.
        :rtype: Tuple[Optional[openai.Client], bool]
        """
        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry
        model_config = current_registry.get(model_name)
        fallback = False
        if not model_config:
            warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."
            if self.verbose:
                from colorama import Fore, Style
                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")
            else:
                _config_logger.debug(warning_message)
            client = self.default_client
            fallback = True
        else:
            client = model_config.default_client
        return client, fallback
--------------------------------------------------------------------------------
Chunk ID: ell\configurator.py::6
Filepath: src\ell\configurator.py
Content:
class Config(BaseModel):

    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:
        """
        Register a provider class for a specific client type.

        :param provider_class: The provider class to register.
        :type provider_class: Type[Provider]
        """
        assert isinstance(client_type, type), "client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))"
        with self._lock:
            self.providers[client_type] = provider
--------------------------------------------------------------------------------
Chunk ID: ell\configurator.py::7
Filepath: src\ell\configurator.py
Content:
class Config(BaseModel):

    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:
        """
        Get the provider instance for a specific client instance.

        :param client: The client instance to get the provider for.
        :type client: Any
        :return: The provider instance for the specified client, or None if not found.
        :rtype: Optional[Provider]
        """

        client_type = type(client) if not isinstance(client, type) else client
        for provider_type, provider in self.providers.items():
            if issubclass(client_type, provider_type) or client_type == provider_type:
                return provider
        return None

# Single* instance
# XXX: Make a singleton
config = Config()
--------------------------------------------------------------------------------
Chunk ID: ell\configurator.py::8
Filepath: src\ell\configurator.py
Content:

def init(
    store: Optional[Union[Store, str]] = None,
    verbose: bool = False,
    autocommit: bool = True,
    lazy_versioning: bool = True,
    default_api_params: Optional[Dict[str, Any]] = None,
    default_client: Optional[Any] = None,
    autocommit_model: str = "gpt-4o-mini"
) -> None:
    """
    Initialize the ELL configuration with various settings.

    :param verbose: Set verbosity of ELL operations.
    :type verbose: bool
    :param store: Set the store for ELL. Can be a Store instance or a string path for SQLiteStore.
    :type store: Union[Store, str], optional
    :param autocommit: Set autocommit for the store operations.
    :type autocommit: bool
    :param lazy_versioning: Enable or disable lazy versioning.
    :type lazy_versioning: bool
    :param default_api_params: Set default parameters for language models.
    :type default_api_params: Dict[str, Any], optional
    :param default_openai_client: Set the default OpenAI client.
    :type default_openai_client: openai.Client, optional
    :param autocommit_model: Set the model used for autocommitting.
    :type autocommit_model: str
    """
    # XXX: prevent double init
    config.verbose = verbose
    config.lazy_versioning = lazy_versioning

    if isinstance(store, str):
        from ell.stores.sql import SQLiteStore
        config.store = SQLiteStore(store)
    else:
        config.store = store
    config.autocommit = autocommit or config.autocommit

    if default_api_params is not None:
        config.default_api_params.update(default_api_params)

    if default_client is not None:
        config.default_client = default_client

    if autocommit_model is not None:
        config.autocommit_model = autocommit_model
--------------------------------------------------------------------------------
Chunk ID: ell\configurator.py::9
Filepath: src\ell\configurator.py
Content:
# Existing helper functions
def get_store() -> Union[Store, None]:
    return config.store

# Will be deprecated at 0.1.0 

# You can add more helper functions here if needed
def register_provider(provider: Provider, client_type: Type[Any]) -> None:
    return config.register_provider(provider, client_type)

# Deprecated now (remove at 0.1.0)
def set_store(*args, **kwargs) -> None:
    raise DeprecationWarning("The set_store function is deprecated and will be removed in a future version. Use ell.init(store=...) instead.")
--------------------------------------------------------------------------------
Chunk ID: lmp\__init__.py::1
Filepath: src\ell\lmp\__init__.py
Content:
from ell.lmp.simple import simple
from ell.lmp.complex import complex
--------------------------------------------------------------------------------
Chunk ID: lmp\_track.py::1
Filepath: src\ell\lmp\_track.py
Content:
import json
import logging
import threading
from ell.types import SerializedLMP, Invocation, InvocationTrace, InvocationContents
from ell.types.studio import LMPType, utc_now
from ell.util._warnings import _autocommit_warning
import ell.util.closure
from ell.configurator import config
from ell.types._lstr import _lstr

import inspect

import secrets
import time
from datetime import datetime
from functools import wraps
from typing import Any, Callable, Dict, Iterable, Optional, OrderedDict, Tuple

from ell.util.serialization import get_immutable_vars
from ell.util.serialization import compute_state_cache_key
from ell.util.serialization import prepare_invocation_params

logger = logging.getLogger(__name__)

# Thread-local storage for the invocation stack
_invocation_stack = threading.local()

def get_current_invocation() -> Optional[str]:
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    return _invocation_stack.stack[-1] if _invocation_stack.stack else None

def push_invocation(invocation_id: str):
    if not hasattr(_invocation_stack, 'stack'):
        _invocation_stack.stack = []
    _invocation_stack.stack.append(invocation_id)

def pop_invocation():
    if hasattr(_invocation_stack, 'stack') and _invocation_stack.stack:
        _invocation_stack.stack.pop()
--------------------------------------------------------------------------------
Chunk ID: lmp\_track.py::2
Filepath: src\ell\lmp\_track.py
Content:
def _track(func_to_track: Callable, *, forced_dependencies: Optional[Dict[str, Any]] = None) -> Callable:

    lmp_type = getattr(func_to_track, "__ell_type__", LMPType.OTHER)


    # see if it exists
    if not hasattr(func_to_track, "_has_serialized_lmp"):
        func_to_track._has_serialized_lmp = False

    if not hasattr(func_to_track, "__ell_hash__") and not config.lazy_versioning:
        ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)


    @wraps(func_to_track)
    def tracked_func(*fn_args, _get_invocation_id=False, **fn_kwargs) -> str:
        # XXX: Cache keys and global variable binding is not thread safe.
        # Compute the invocation id and hash the inputs for serialization.
        invocation_id = "invocation-" + secrets.token_hex(16)

        state_cache_key : str = None
        if not config.store:
            return func_to_track(*fn_args, **fn_kwargs, _invocation_origin=invocation_id)[0]

        parent_invocation_id = get_current_invocation()
        try:
            push_invocation(invocation_id)

            # Convert all positional arguments to named keyword arguments
            sig = inspect.signature(func_to_track)
            # Filter out kwargs that are not in the function signature
            filtered_kwargs = {k: v for k, v in fn_kwargs.items() if k in sig.parameters}

            bound_args = sig.bind(*fn_args, **filtered_kwargs)
            bound_args.apply_defaults()
            all_kwargs = dict(bound_args.arguments)

            # Get the list of consumed lmps and clean the invocation params for serialization.
            cleaned_invocation_params, ipstr, consumes = prepare_invocation_params( all_kwargs)

            try_use_cache = hasattr(func_to_track.__wrapper__, "__ell_use_cache__")

            if  try_use_cache:
                # Todo: add nice logging if verbose for when using a cahced invocaiton. IN a different color with thar args..
                if not hasattr(func_to_track, "__ell_hash__")  and config.lazy_versioning:
                    fn_closure, _ = ell.util.closure.lexically_closured_source(func_to_track)

                # compute the state cachekey
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

                cache_store = func_to_track.__wrapper__.__ell_use_cache__
                cached_invocations = cache_store.get_cached_invocations(func_to_track.__ell_hash__, state_cache_key)


                if len(cached_invocations) > 0:
                    # XXX: Fix caching.
                    results =  [d.deserialize() for  d in cached_invocations[0].results]

                    logger.info(f"Using cached result for {func_to_track.__qualname__} with state cache key: {state_cache_key}")
                    if len(results) == 1:
                        return results[0]
                    else:
                        return results
                    # Todo: Unfiy this with the non-cached case. We should go through the same code pathway.
                else:
                    logger.info(f"Attempted to use cache on {func_to_track.__qualname__} but it was not cached, or did not exist in the store. Refreshing cache...")


            _start_time = utc_now()

            # XXX: thread saftey note, if I prevent yielding right here and get the global context I should be fine re: cache key problem

            # get the prompt
            (result, invocation_api_params, metadata) = (
                (func_to_track(*fn_args, **fn_kwargs), {}, {})
                if lmp_type == LMPType.OTHER
                else func_to_track(*fn_args, _invocation_origin=invocation_id, **fn_kwargs, )
                )
            latency_ms = (utc_now() - _start_time).total_seconds() * 1000
            usage = metadata.get("usage", {"prompt_tokens": 0, "completion_tokens": 0})
            prompt_tokens= usage.get("prompt_tokens", 0) if usage else 0
            completion_tokens= usage.get("completion_tokens", 0) if usage else 0


            #XXX: cattrs add invocation origin here recursively on all pirmitive types within a message.
            #XXX: This will allow all objects to be traced automatically irrespective origin rather than relying on the API to do it, it will of vourse be expensive but unify track.
            #XXX: No other code will need to consider tracking after this point.

            if not hasattr(func_to_track, "__ell_hash__") and config.lazy_versioning:
                ell.util.closure.lexically_closured_source(func_to_track, forced_dependencies)
            _serialize_lmp(func_to_track)

            if not state_cache_key:
                state_cache_key = compute_state_cache_key(ipstr, func_to_track.__ell_closure__)

            _write_invocation(func_to_track, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                            state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id)

            if _get_invocation_id:
                return result, invocation_id
            else:
                return result
        finally:
            pop_invocation()


    func_to_track.__wrapper__  = tracked_func
    if hasattr(func_to_track, "__ell_api_params__"):
        tracked_func.__ell_api_params__ = func_to_track.__ell_api_params__
    if hasattr(func_to_track, "__ell_params_model__"):
        tracked_func.__ell_params_model__ = func_to_track.__ell_params_model__
    tracked_func.__ell_func__ = func_to_track
    tracked_func.__ell_track = True

    return tracked_func
--------------------------------------------------------------------------------
Chunk ID: lmp\_track.py::3
Filepath: src\ell\lmp\_track.py
Content:
def _serialize_lmp(func):
    # Serialize deptjh first all fo the used lmps.
    for f in func.__ell_uses__:
        _serialize_lmp(f)

    if getattr(func, "_has_serialized_lmp", False):
        return
    func._has_serialized_lmp = False
    fn_closure = func.__ell_closure__
    lmp_type = func.__ell_type__
    name = func.__qualname__
    api_params = getattr(func, "__ell_api_params__", None)

    lmps = config.store.get_versions_by_fqn(fqn=name)
    version = 0
    already_in_store = any(lmp.lmp_id == func.__ell_hash__ for lmp in lmps)

    if not already_in_store:
        commit = None
        if lmps:
            latest_lmp = max(lmps, key=lambda x: x.created_at)
            version = latest_lmp.version_number + 1
            if config.autocommit:
                # XXX: Move this out to autocommit itself.
                if not _autocommit_warning():
                    from ell.util.differ import write_commit_message_for_diff
                    commit = str(write_commit_message_for_diff(
                    f"{latest_lmp.dependencies}\n\n{latest_lmp.source}", 
                        f"{fn_closure[1]}\n\n{fn_closure[0]}")[0])

        serialized_lmp = SerializedLMP(
            lmp_id=func.__ell_hash__,
            name=name,
            created_at=utc_now(),
            source=fn_closure[0],
            dependencies=fn_closure[1],
            commit_message=commit,
            initial_global_vars=get_immutable_vars(fn_closure[2]),
            initial_free_vars=get_immutable_vars(fn_closure[3]),
            lmp_type=lmp_type,
            api_params=api_params if api_params else None,
            version_number=version,
        )
        config.store.write_lmp(serialized_lmp, [f.__ell_hash__ for f in func.__ell_uses__])
    func._has_serialized_lmp = True
--------------------------------------------------------------------------------
Chunk ID: lmp\_track.py::4
Filepath: src\ell\lmp\_track.py
Content:
def _write_invocation(func, invocation_id, latency_ms, prompt_tokens, completion_tokens, 
                     state_cache_key, invocation_api_params, cleaned_invocation_params, consumes, result, parent_invocation_id):

    invocation_contents = InvocationContents(
        invocation_id=invocation_id,
        params=cleaned_invocation_params,
        results=result,
        invocation_api_params=invocation_api_params,
        global_vars=get_immutable_vars(func.__ell_closure__[2]),
        free_vars=get_immutable_vars(func.__ell_closure__[3])
    )

    if invocation_contents.should_externalize and config.store.has_blob_storage:
        invocation_contents.is_external = True

        # Write to the blob store 
        blob_id = config.store.blob_store.store_blob(
            json.dumps(invocation_contents.model_dump(
            ), default=str, ensure_ascii=False).encode('utf-8'),
            invocation_id
        )
        invocation_contents = InvocationContents(
            invocation_id=invocation_id,
            is_external=True,
        )

    invocation = Invocation(
        id=invocation_id,
        lmp_id=func.__ell_hash__,
        created_at=utc_now(),
        latency_ms=latency_ms,
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        state_cache_key=state_cache_key,
        used_by_id=parent_invocation_id,
        contents=invocation_contents
    )

    config.store.write_invocation(invocation, consumes)
--------------------------------------------------------------------------------
Chunk ID: lmp\complex.py::1
Filepath: src\ell\lmp\complex.py
Content:
from ell.configurator import config
from ell.lmp._track import _track
from ell.provider import EllCallParams
from ell.types._lstr import _lstr
from ell.types import Message, ContentBlock
from ell.types.message import LMP, InvocableLM, LMPParams, MessageOrDict, _lstr_generic
from ell.types.studio import LMPType
from ell.util._warnings import _no_api_key_warning, _warnings
from ell.util.verbosity import compute_color, model_usage_logger_pre

from ell.util.verbosity import model_usage_logger_post_end, model_usage_logger_post_intermediate, model_usage_logger_post_start

from functools import wraps
from typing import Any, Dict, Optional, List, Callable, Tuple, Union
--------------------------------------------------------------------------------
Chunk ID: lmp\complex.py::2
Filepath: src\ell\lmp\complex.py
Content:
def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    default_client_from_decorator = client
    default_model_from_decorator = model
    default_api_params_from_decorator = api_params
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        _warnings(model, prompt, default_client_from_decorator)

        @wraps(prompt)
        def model_call(
            *prompt_args,
            _invocation_origin : Optional[str] = None,
            client: Optional[Any] = None,
            api_params: Optional[Dict[str, Any]] = None,
            lm_params: Optional[DeprecationWarning] = None,
            **prompt_kwargs,
        ) -> Tuple[Any, Any, Any]:
            # XXX: Deprecation in 0.1.0
            if lm_params:
                raise DeprecationWarning("lm_params is deprecated. Use api_params instead.")

            # promt -> str
            res = prompt(*prompt_args, **prompt_kwargs)
            # Convert prompt into ell messages
            messages = _get_messages(res, prompt)

            # XXX: move should log to a logger.
            should_log = not exempt_from_tracking and config.verbose
            # Cute verbose logging.
            if should_log: model_usage_logger_pre(prompt, prompt_args, prompt_kwargs, "[]", messages) #type: ignore

            # Call the model.
            # Merge API params
            merged_api_params = {**config.default_api_params, **default_api_params_from_decorator, **(api_params or {})}
            n = merged_api_params.get("n", 1)
            # Merge client overrides & client registry
            merged_client = _client_for_model(model, client or default_client_from_decorator)
            ell_call = EllCallParams(
                # XXX: Could change behaviour of overriding ell params for dyanmic tool calls.
                model=merged_api_params.pop("model", default_model_from_decorator),
                messages=messages,
                client = merged_client,
                api_params=merged_api_params,
                tools=tools or [],
            )
            # Get the provider for the model
            provider = config.get_provider_for(ell_call.client)
            assert provider is not None, f"No provider found for client {ell_call.client}."

            if should_log: model_usage_logger_post_start(n)
            with model_usage_logger_post_intermediate(n) as _logger:
                (result, final_api_params, metadata) = provider.call(ell_call, origin_id=_invocation_origin, logger=_logger if should_log else None)
                if isinstance(result, list) and len(result) == 1:
                    result = result[0]

            result = post_callback(result) if post_callback else result
            if should_log:
                model_usage_logger_post_end()
            #
            #  These get sent to track. This is wack.           
            return result, final_api_params, metadata
        # ... other code
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: lmp\complex.py::3
Filepath: src\ell\lmp\complex.py
Content:
def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        # ... other code



        model_call.__ell_api_params__ = default_api_params_from_decorator #type: ignore
        model_call.__ell_func__ = prompt #type: ignore
        model_call.__ell_type__ = LMPType.LM #type: ignore
        model_call.__ell_exempt_from_tracking = exempt_from_tracking #type: ignore


        if exempt_from_tracking:
            return model_call
        else:
            # XXX: Analyze decorators with AST instead.
            return _track(model_call, forced_dependencies=dict(tools=tools, response_format=api_params.get("response_format", {})))
    return parameterized_lm_decorator
--------------------------------------------------------------------------------
Chunk ID: lmp\complex.py::4
Filepath: src\ell\lmp\complex.py
Content:
def _get_messages(prompt_ret: Union[str, list[MessageOrDict]], prompt: LMP) -> list[Message]:
    """
    Helper function to convert the output of an LMP into a list of Messages.
    """
    if isinstance(prompt_ret, str):
        has_system_prompt = prompt.__doc__ is not None and prompt.__doc__.strip() != ""
        messages =     [Message(role="system", content=[ContentBlock(text=_lstr(prompt.__doc__ ) )])] if has_system_prompt else []
        return messages + [
            Message(role="user", content=[ContentBlock(text=prompt_ret)])
        ]
    else:
        assert isinstance(
            prompt_ret, list
        ), "Need to pass a list of Messages to the language model"
        return prompt_ret
--------------------------------------------------------------------------------
Chunk ID: lmp\complex.py::5
Filepath: src\ell\lmp\complex.py
Content:
def _client_for_model(
    model: str,
    client: Optional[Any] = None,
    _name: Optional[str] = None,
) -> Any:
    # XXX: Move to config to centralize api keys etc.
    if not client:
        client, was_fallback = config.get_client_for(model)

        # XXX: Wrong.
        if not client and not was_fallback:
            raise RuntimeError(_no_api_key_warning(model, _name, '', long=True, error=True))

    if client is None:
        raise ValueError(f"No client found for model '{model}'. Ensure the model is registered using 'register_model' in 'config.py' or specify a client directly using the 'client' argument in the decorator or function call.")
    return client


complex.__doc__ =
 # ... other code
--------------------------------------------------------------------------------
Chunk ID: lmp\simple.py::1
Filepath: src\ell\lmp\simple.py
Content:
from functools import wraps
from typing import Any, Optional

from ell.lmp.complex import complex


def simple(model: str, client: Optional[Any] = None,  exempt_from_tracking=False, **api_params):
    assert 'tools' not in api_params, "tools are not supported in lm decorator, use multimodal decorator instead"
    assert 'tool_choice' not in api_params, "tool_choice is not supported in lm decorator, use multimodal decorator instead"
    assert 'response_format' not in api_params or isinstance(api_params.get('response_format', None), dict), "response_format is not supported in lm decorator, use multimodal decorator instead"

    def convert_multimodal_response_to_lstr(response):
        return [x.content[0].text for x in response] if isinstance(response, list) else response.content[0].text
    return complex(model, client,  exempt_from_tracking=exempt_from_tracking, **api_params, post_callback=convert_multimodal_response_to_lstr)
--------------------------------------------------------------------------------
Chunk ID: lmp\simple.py::2
Filepath: src\ell\lmp\simple.py
Content:
simple.__doc__ = """The fundamental unit of language model programming in ell.

  This decorator simplifies the process of creating Language Model Programs (LMPs) 
  that return text-only outputs from language models, while supporting multimodal inputs.
  It wraps the more complex 'complex' decorator, providing a streamlined interface for common use cases.

  :param model: The name or identifier of the language model to use.
  :type model: str
  :param client: An optional OpenAI client instance. If not provided, a default client will be used.
  :type client: Optional[openai.Client]
  :param exempt_from_tracking: If True, the LMP usage won't be tracked. Default is False.
  :type exempt_from_tracking: bool
  :param api_params: Additional keyword arguments to pass to the underlying API call.
  :type api_params: Any

  Usage:
  The decorated function can return either a single prompt or a list of ell.Message objects:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def summarize_text(text: str) -> str:
          '''You are an expert at summarizing text.''' # System prompt
          return f"Please summarize the following text:\\n\\n{text}" # User prompt


      @ell.simple(model="gpt-4", temperature=0.7)
      def describe_image(image : PIL.Image.Image) -> List[ell.Message]:
          '''Describe the contents of an image.''' # unused because we're returning a list of Messages
          return [
              # helper function for ell.Message(text="...", role="system")
              ell.system("You are an AI trained to describe images."),
              # helper function for ell.Message(content="...", role="user")
              ell.user(["Describe this image in detail.", image]),
          ]


      image_description = describe_image(PIL.Image.open("https://example.com/image.jpg"))
      print(image_description) 
      # Output will be a string text-only description of the image

      summary = summarize_text("Long text to summarize...")
      print(summary)
      # Output will be a text-only summary

  Notes:

  - This decorator is designed for text-only model outputs, but supports multimodal inputs.
  - It simplifies complex responses from language models to text-only format, regardless of 
    the model's capability for structured outputs, function calling, or multimodal outputs.
  - For preserving complex model outputs (e.g., structured data, function calls, or multimodal 
    outputs), use the @ell.complex decorator instead. @ell.complex returns a Message object (role='assistant')
  - The decorated function can return a string or a list of ell.Message objects for more 
    complex prompts, including multimodal inputs.
  - If called with n > 1 in api_params, the wrapped LMP will return a list of strings for the n parallel outputs
    of the model instead of just one string. Otherwise, it will return a single string.
  - You can pass LM API parameters either in the decorator or when calling the decorated function.
    Parameters passed during the function call will override those set in the decorator.

  Example of passing LM API params:

  .. code-block:: python

      @ell.simple(model="gpt-4", temperature=0.7)
      def generate_story(prompt: str) -> str:
          return f"Write a short story based on this prompt: {prompt}"

      # Using default parameters
      story1 = generate_story("A day in the life of a time traveler")

      # Overriding parameters during function call
      story2 = generate_story("An AI's first day of consciousness", api_params={"temperature": 0.9, "max_tokens": 500})

  See Also:

  - :func:`ell.complex`: For LMPs that preserve full structure of model responses, including multimodal outputs.
  - :func:`ell.tool`: For defining tools that can be used within complex LMPs.
  - :mod:`ell.studio`: For visualizing and analyzing LMP executions.
    """
--------------------------------------------------------------------------------
Chunk ID: lmp\tool.py::1
Filepath: src\ell\lmp\tool.py
Content:
from functools import wraps
import json
from typing import Any, Callable, Optional

from pydantic import Field, create_model
from pydantic.fields import FieldInfo
from ell.lmp._track import _track
# from ell.types import ToolFunction, InvocableTool, ToolParams
# from ell.util.verbosity import compute_color, tool_usage_logger_pre
from ell.configurator import config
from ell.types._lstr import _lstr
from ell.types.studio import LMPType
import inspect

from ell.types.message import ContentBlock, InvocableTool, ToolResult, to_content_blocks
--------------------------------------------------------------------------------
Chunk ID: lmp\tool.py::2
Filepath: src\ell\lmp\tool.py
Content:
def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        _under_fn = fn

        @wraps(fn)
        def wrapper(
            *fn_args,
            _invocation_origin: str = None,
            _tool_call_id: str = None,
            **fn_kwargs
        ):
            #XXX: Post release, we need to wrap all tool arguments in type primitives for tracking I guess or change that tool makes the tool function inoperable.
            #XXX: Most people are not going to manually try and call the tool without a type primitive and if they do it will most likely be wrapped with l strs.

            if config.verbose and not exempt_from_tracking:
                pass
                # tool_usage_logger_pre(fn, fn_args, fn_kwargs, name, color)

            result = fn(*fn_args, **fn_kwargs)

            _invocation_api_params = dict(tool_kwargs=tool_kwargs)

            # Here you might want to add logic for tracking the tool usage
            # Similar to how it's done in the lm decorator # Use _invocation_origin

            if isinstance(result, str) and _invocation_origin:
                result = _lstr(result,origin_trace=_invocation_origin)

            #XXX: This _tool_call_id thing is a hack. Tracking should happen via params in the api
            # So if you call wiuth a _tool_callId
            if _tool_call_id:
                # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
                try:
                    if isinstance(result, ContentBlock):
                        content_results = [result]
                    elif isinstance(result, list) and all(isinstance(c, ContentBlock) for c in result):
                        content_results = result
                    else:
                        content_results = [ContentBlock(text=_lstr(json.dumps(result, ensure_ascii=False),origin_trace=_invocation_origin))]
                except TypeError as e:
                    raise TypeError(f"Failed to convert tool use result to ContentBlock: {e}. Tools must return json serializable objects. or a list of ContentBlocks.")
                # XXX: Need to support images and other content types somehow. We should look for images inside of the the result and then go from there.
                # try:
                #     content_results = coerce_content_list(result)
                # except ValueError as e:

                # TODO: poolymorphic validation here is important (cant have tool_call or formatted_response in the result)
                # XXX: Should we put this coercion here or in the tool call/result area.
                for c in content_results:
                    assert not c.tool_call, "Tool call in tool result"
                    # assert not c.formatted_response, "Formatted response in tool result"
                    if c.parsed:
                        # Warning: Formatted response in tool result will be converted to text
                        # TODO: Logging needs to produce not print.
                        print(f"Warning: Formatted response in tool result will be converted to text. Original: {c.parsed}")
                        c.text = _lstr(c.parsed.model_dump_json(),origin_trace=_invocation_origin)
                        c.parsed = None
                    assert not c.audio, "Audio in tool result"
                return ToolResult(tool_call_id=_tool_call_id, result=content_results), _invocation_api_params, {}
            else:
                return result, _invocation_api_params, {}
        # ... other code
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: lmp\tool.py::3
Filepath: src\ell\lmp\tool.py
Content:
def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        # ... other code


        wrapper.__ell_tool_kwargs__ = tool_kwargs
        wrapper.__ell_func__ = _under_fn
        wrapper.__ell_type__ = LMPType.TOOL
        wrapper.__ell_exempt_from_tracking = exempt_from_tracking

        # Construct the pydantic mdoel for the _under_fn's function signature parameters.
        # 1. Get the function signature.

        sig = inspect.signature(fn)

        # 2. Create a dictionary of field definitions for the Pydantic model
        fields = {}
        for param_name, param in sig.parameters.items():
            # Skip *args and **kwargs
            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                continue

            # Determine the type annotation
            if param.annotation == inspect.Parameter.empty:
                raise ValueError(f"Parameter {param_name} has no type annotation, and cannot be converted into a tool schema for OpenAI and other provisders. Should OpenAI produce a string or an integer, etc, for this parameter?")
            annotation = param.annotation

            # Determine the default value
            default = param.default

            # Check if the parameter has a Field with description
            if isinstance(param.default, FieldInfo):
                field = param.default
                fields[param_name] = (annotation, field)
            elif param.default != inspect.Parameter.empty:
                fields[param_name] = (annotation, param.default)
            else:
                # If no default value, use Field without default
                fields[param_name] = (annotation, Field(...))

        # 3. Create the Pydantic model
        model_name = f"{fn.__name__}"
        ParamsModel = create_model(model_name, **fields)

        # Attach the Pydantic model to the wrapper function
        wrapper.__ell_params_model__ = ParamsModel

        # handle tracking last.
        if exempt_from_tracking:
            ret = wrapper
        else:
            ret=  _track(wrapper)

        # Helper function to get the Pydantic model for the tool
        def get_params_model():
            return wrapper.__ell_params_model__

        # Attach the helper function to the wrapper
        wrapper.get_params_model = get_params_model
        ret.get_params_model = get_params_model
        return ret

    return tool_decorator
--------------------------------------------------------------------------------
Chunk ID: lmp\tool.py::4
Filepath: src\ell\lmp\tool.py
Content:
tool.__doc__ = """Defines a tool for use in language model programs (LMPs) that support tool use.

This decorator wraps a function, adding metadata and handling for tool invocations.
It automatically extracts the tool's description and parameters from the function's
docstring and type annotations, creating a structured representation for LMs to use.

:param exempt_from_tracking: If True, the tool usage won't be tracked. Default is False.
:type exempt_from_tracking: bool
:param tool_kwargs: Additional keyword arguments for tool configuration.
:return: A wrapped version of the original function, usable as a tool by LMs.
:rtype: Callable

Requirements:

- Function must have fully typed arguments (Pydantic-serializable).
- Return value must be one of: str, JSON-serializable object, Pydantic model, or List[ContentBlock].
- All parameters must have type annotations.
- Complex types should be Pydantic models.
- Function should have a descriptive docstring.
- Can only be used in LMPs with @ell.complex decorators

Functionality:

1. Metadata Extraction:
    - Uses function docstring as tool description.
    - Extracts parameter info from type annotations and docstring.
    - Creates a Pydantic model for parameter validation and schema generation.

2. Integration with LMs:
    - Can be passed to @ell.complex decorators.
    - Provides structured tool information to LMs.

3. Invocation Handling:
    - Manages tracking, logging, and result processing.
    - Wraps results in appropriate types (e.g., _lstr) for tracking.

Usage Modes:

1. Normal Function Call:
    - Behaves like a regular Python function.
    - Example: result = my_tool(arg1="value", arg2=123)

2. LMP Tool Call:
    - Used within LMPs or with explicit _tool_call_id.
    - Returns a ToolResult object.
    - Example: result = my_tool(arg1="value", arg2=123, _tool_call_id="unique_id")

Result Coercion:

- String → ContentBlock(text=result)
- Pydantic BaseModel → ContentBlock(parsed=result)
- List[ContentBlock] → Used as-is
- Other types → ContentBlock(text=json.dumps(result))

Example::

    @ell.tool()
    def create_claim_draft(
        claim_details: str,
        claim_type: str,
        claim_amount: float,
        claim_date: str = Field(description="Date format: YYYY-MM-DD")
    ) -> str:
        '''Create a claim draft. Returns the created claim ID.'''
        return "12345"

    # For use in a complex LMP:
    @ell.complex(model="gpt-4", tools=[create_claim_draft], temperature=0.1)
    def insurance_chatbot(message_history: List[Message]) -> List[Message]:
        # Chatbot implementation...

    x = insurance_chatbot([
        ell.user("I crashed my car into a tree."),
        ell.assistant("I'm sorry to hear that. Can you provide more details?"),
        ell.user("The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000")
    ]) 
    print(x)
    '''ell.Message(content=[
        ContentBlock(tool_call(
            tool_call_id="asdas4e",
            tool_fn=create_claim_draft,
            input=create_claim_draftParams({
                claim_details="The car is totaled and I need to file a claim. Happened on 2024-08-01. total value is like $5000",
                claim_type="car",
                claim_amount=5000,
                claim_date="2024-08-01"
            })
        ))
    ], role='assistant')'''
    
    if x.tool_calls:
        next_user_message = response_message.call_tools_and_collect_as_message()
        # This actually calls create_claim_draft
        print(next_user_message)
        '''
        ell.Message(content=[
            ContentBlock(tool_result=ToolResult(
                tool_call_id="asdas4e",
                result=[ContentBlock(text="12345")]
            ))
        ], role='user')
        '''
        y = insurance_chatbot(message_history + [x, next_user_message])
        print(y)
        '''
        ell.Message("I've filed that for you!", role='assistant')
        '''

Note:
- Tools are integrated into LMP calls via the 'tools' parameter in @ell.complex.
- LMs receive structured tool information, enabling understanding and usage within the conversation context.
    """
--------------------------------------------------------------------------------
Chunk ID: models\__init__.py::1
Filepath: src\ell\models\__init__.py
Content:
"""
Attempts to registeres model names with their respective API client bindings. This allows for the creation of a unified interface for interacting with different LLM providers.

For example, to register an OpenAI model:
@ell.simple(model='gpt-4o-mini') -> @ell.simple(model='gpt-4o-mini', client=openai.OpenAI())

"""

import ell.models.openai
import ell.models.anthropic
import ell.models.ollama
import ell.models.groq
import ell.models.bedrock
--------------------------------------------------------------------------------
Chunk ID: models\anthropic.py::1
Filepath: src\ell\models\anthropic.py
Content:
from ell.configurator import config
import logging

logger = logging.getLogger(__name__)


try:
    import anthropic

    def register(client: anthropic.Anthropic):
        """
        Register Anthropic models with the provided client.

        This function takes an Anthropic client and registers various Anthropic models
        with the global configuration. It allows the system to use these models
        for different AI tasks.

        Args:
            client (anthropic.Anthropic): An instance of the Anthropic client to be used
                                          for model registration.

        Note:
            The function doesn't return anything but updates the global
            configuration with the registered models.
        """
        model_data = [
            ('claude-3-opus-20240229', 'anthropic'),
            ('claude-3-sonnet-20240229', 'anthropic'),
            ('claude-3-haiku-20240307', 'anthropic'),
            ('claude-3-5-sonnet-20240620', 'anthropic'),
        ]
        for model_id, owned_by in model_data:
            config.register_model(model_id, client)

    try:
        default_client = anthropic.Anthropic()
        register(default_client)
    except Exception as e:
        # logger.warning(f"Failed to create default Anthropic client: {e}")
        pass


except ImportError:
    pass
--------------------------------------------------------------------------------
Chunk ID: models\bedrock.py::1
Filepath: src\ell\models\bedrock.py
Content:
from typing import Any
from ell.configurator import config
import logging

logger = logging.getLogger(__name__)


def register(client: Any):
    """
    Register Bedrock models with the provided client.

    This function takes an boto3 client and registers various Bedrock models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (boto3.client): An instance of the bedrock client to be used
                                        for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    model_data = [
        ('anthropic.claude-3-opus-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-sonnet-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-haiku-20240307-v1:0', 'bedrock'),
        ('anthropic.claude-3-5-sonnet-20240620-v1:0', 'bedrock'),

        ('mistral.mistral-7b-instruct-v0:2', 'bedrock'),
        ('mistral.mixtral-8x7b-instruct-v0:1', 'bedrock'),
        ('mistral.mistral-large-2402-v1:0', 'bedrock'),
        ('mistral.mistral-small-2402-v1:0', 'bedrock'),


        ('ai21.jamba-instruct-v1:0','bedrock'),
        ('ai21.j2-ultra-v1', 'bedrock'),
        ('ai21.j2-mid-v1', 'bedrock'),

        ('amazon.titan-embed-text-v1', 'bedrock'),
        ('amazon.titan-text-lite-v1', 'bedrock'),
        ('amazon.titan-text-express-v1', 'bedrock'),
        ('amazon.titan-image-generator-v2:0', 'bedrock'),
        ('amazon.titan-image-generator-v1', 'bedrock'),

        ('cohere.command-r-plus-v1:0', 'bedrock'),
        ('cohere.command-r-v1:0', 'bedrock'),
        ('cohere.embed-english-v3', 'bedrock'),
        ('cohere.embed-multilingual-v3', 'bedrock'),
        ('cohere.command-text-v14', 'bedrock'),

        ('meta.llama3-8b-instruct-v1:0', 'bedrock'),
        ('meta.llama3-70b-instruct-v1:0', 'bedrock'),
        ('meta.llama2-13b-chat-v1', 'bedrock'),
        ('meta.llama2-70b-chat-v1', 'bedrock'),
        ('meta.llama2-13b-v1', 'bedrock'),

    ]

    for model_id, owned_by in model_data:
        config.register_model(name=model_id, default_client=client, supports_streaming=True)

default_client = None
try:

    import boto3
    default_client = boto3.client('bedrock-runtime')
except Exception as e:
    pass

register(default_client)
--------------------------------------------------------------------------------
Chunk ID: models\groq.py::1
Filepath: src\ell\models\groq.py
Content:
from typing import Optional
from ell.configurator import config

try:
    from groq import Groq
    def register(client: Optional[Groq] = None, **client_kwargs):
        if client is None:
            client = Groq(**client_kwargs)
        for model in client.models.list().data:
            config.register_model(model.id, default_client=client, supports_streaming=True)
except ImportError:
    pass
--------------------------------------------------------------------------------
Chunk ID: models\ollama.py::1
Filepath: src\ell\models\ollama.py
Content:
from ell.configurator import config
import openai
import requests
import logging

#XXX: May be deprecated soon because of the new provider framework.
logger = logging.getLogger(__name__)
client = None

def register(base_url):
    """
    Registers Ollama models with the provided base URL.

    This function sets up the Ollama client with the given base URL and
    fetches available models from the Ollama API. It then registers these
    models with the global configuration, allowing them to be used within
    the ell framework.

    Args:
        base_url (str): The base URL of the Ollama API endpoint.

    Note:
        This function updates the global client and configuration.
        It logs any errors encountered during the process.
    """
    global client
    client = openai.Client(base_url=base_url)

    try:
        response = requests.get(f"{base_url}/../api/tags")
        response.raise_for_status()
        models = response.json().get("models", [])

        for model in models:
            config.register_model(model["name"], client)
    except requests.RequestException as e:
        logger.error(f"Failed to fetch models from {base_url}: {e}")
    except Exception as e:
        logger.error(f"An error occurred: {e}")
--------------------------------------------------------------------------------
Chunk ID: models\openai.py::1
Filepath: src\ell\models\openai.py
Content:
"""
This module handles the registration of OpenAI models within the ell framework.

It provides functionality to register various OpenAI models with a given OpenAI client,
making them available for use throughout the system. The module also sets up a default
client behavior for unregistered models.

Key features:
1. Registration of specific OpenAI models with their respective types (system, openai, openai-internal).
2. Utilization of a default OpenAI client for any unregistered models,

The default client behavior ensures that even if a specific model is not explicitly
registered, the system can still attempt to use it with the default OpenAI client.
This fallback mechanism provides flexibility in model usage while maintaining a
structured approach to model registration.

Note: The actual model availability may depend on your OpenAI account's access and the
current offerings from OpenAI.

Additionally, due to the registration of default mdoels, the OpenAI client may be used for
anthropic, cohere, groq, etc. models if their clients are not registered or fail
to register due to an error (lack of API keys, rate limits, etc.)
"""

from ell.configurator import config
import openai

import logging
import colorama

logger = logging.getLogger(__name__)
--------------------------------------------------------------------------------
Chunk ID: models\openai.py::2
Filepath: src\ell\models\openai.py
Content:
def register(client: openai.Client):
    """
    Register OpenAI models with the provided client.

    This function takes an OpenAI client and registers various OpenAI models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (openai.Client): An instance of the OpenAI client to be used
                                for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    #XXX: Deprecation in 0.1.0
    standard_models = [
        'gpt-4-1106-preview',
        'gpt-4-32k-0314',
        'text-embedding-3-large',
        'gpt-4-0125-preview',
        'babbage-002',
        'gpt-4-turbo-preview',
        'gpt-4o',
        'gpt-4o-2024-05-13',
        'gpt-4o-mini-2024-07-18',
        'gpt-4o-mini',
        'gpt-4o-2024-08-06',
        'gpt-3.5-turbo-0301',
        'gpt-3.5-turbo-0613',
        'tts-1',
        'gpt-3.5-turbo',
        'gpt-3.5-turbo-16k',
        'davinci-002',
        'gpt-3.5-turbo-16k-0613',
        'gpt-4-turbo-2024-04-09',
        'gpt-3.5-turbo-0125',
        'gpt-4-turbo',
        'gpt-3.5-turbo-1106',
        'gpt-3.5-turbo-instruct-0914',
        'gpt-3.5-turbo-instruct',
        'gpt-4-0613',
        'gpt-4',
        'gpt-4-0314',
        'gpt-4o-audio-preview',
        'gpt-4o-realtime',
    ]
    for model_id in standard_models:
        config.register_model(model_id, client)

    #XXX: Deprecation in 0.1.0
    config.register_model('o1-preview', client, supports_streaming=False)
    config.register_model('o1-mini', client, supports_streaming=False)

default_client = None
try:
    default_client = openai.Client()
except openai.OpenAIError as e:
    pass

register(default_client)
config.default_client = default_client
--------------------------------------------------------------------------------
Chunk ID: ell\provider.py::1
Filepath: src\ell\provider.py
Content:
from abc import ABC, abstractmethod
from collections import defaultdict
from functools import lru_cache
import inspect
from types import MappingProxyType
from typing import (
    Any,
    Callable,
    Dict,
    FrozenSet,
    List,
    Optional,
    Set,
    Tuple,
    Type,
    TypedDict,
    Union,
)

from pydantic import BaseModel, ConfigDict, Field
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from dataclasses import dataclass
from ell.types.message import LMP
--------------------------------------------------------------------------------
Chunk ID: ell\provider.py::2
Filepath: src\ell\provider.py
Content:
# XXX: Might leave this internal to providers so that the complex code is simpler &
# we can literally jsut call provider.call like any openai fn.
class EllCallParams(BaseModel):
    model: str = Field(..., description="Model identifier")
    messages: List[Message] = Field(..., description="Conversation context")
    client: Any = Field(..., description="API client")
    tools: List[LMP] = Field(default_factory=list, description="Available tools")
    api_params: Dict[str, Any] = Field(
        default_factory=dict, description="API parameters"
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def get_tool_by_name(self, name: str) -> Optional[LMP]:
        """Get a tool by name."""
        return next(
            (tool for tool in (self.tools or [])  if tool.__name__ == name), None
        )


Metadata = Dict[str, Any]
--------------------------------------------------------------------------------
Chunk ID: ell\provider.py::3
Filepath: src\ell\provider.py
Content:
# XXX: Needs a better name.
class Provider(ABC):
    """
    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.
    For example, the OpenAI provider is an API interface to OpenAI's API but also to Ollama and Azure OpenAI.
    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.
    """
    dangerous_disable_validation = False

    ################################
    ### API PARAMETERS #############
    ################################
    @abstractmethod
    def provider_call_function(
        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None
    ) -> Callable[..., Any]:
        """
        Implement this method to return the function that makes the API call to the language model.
        For example, if you're implementing the OpenAI provider, you would return the function that makes the API call to OpenAI's API.
        """
        return NotImplemented

    def disallowed_api_params(self) -> FrozenSet[str]:
        """
        Returns a list of disallowed call params that ell will override.
        """
        return frozenset({"messages", "tools", "model", "stream", "stream_options"})

    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):
        params = _call_params(self.provider_call_function(client, api_params))
        return frozenset(params.keys()) - self.disallowed_api_params()

    ################################
    ### TRANSLATION ###############
    ################################
    @abstractmethod
    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:
        """Converts an ell call to provider call params!"""
        return NotImplemented

    @abstractmethod
    def translate_from_provider(
        self,
        provider_response: Any,
        ell_call: EllCallParams,
        provider_call_params: Dict[str, Any],
        origin_id: Optional[str] = None,
        logger: Optional[Callable[..., None]] = None,
    ) -> Tuple[List[Message], Metadata]:
        """Converts provider responses to universal format. with metadata"""
        return NotImplemented

    ################################
    ### CALL MODEL ################
    ################################
    # Be careful to override this method in your provider.

--------------------------------------------------------------------------------
Chunk ID: ell\provider.py::4
Filepath: src\ell\provider.py
Content:
class Provider(ABC):
    def call(
        self,
        #XXX: In future refactors, we can fully enumerate the args and make ell_call's internal to the _provider implementer interface.
        # This gives us a litellm style interface for free.
        ell_call: EllCallParams,
        origin_id: Optional[str] = None,
        logger: Optional[Any] = None,
    ) -> Tuple[List[Message], Dict[str, Any], Metadata]:
        # Automatic validation of params
        assert (
            not set(ell_call.api_params.keys()).intersection(self.disallowed_api_params()) 
        ), f"Disallowed api parameters: {ell_call.api_params}"

        final_api_call_params = self.translate_to_provider(ell_call)

        call = self.provider_call_function(ell_call.client, final_api_call_params)
        assert self.dangerous_disable_validation or _validate_provider_call_params(final_api_call_params, call)


        provider_resp = call(**final_api_call_params)

        messages, metadata = self.translate_from_provider(
            provider_resp, ell_call, final_api_call_params, origin_id, logger
        )
        assert "choices" not in metadata, "choices should be in the metadata."
        assert self.dangerous_disable_validation or _validate_messages_are_tracked(messages, origin_id)

        return messages, final_api_call_params, metadata
--------------------------------------------------------------------------------
Chunk ID: ell\provider.py::5
Filepath: src\ell\provider.py
Content:
# handhold the the implementer, in production mode we can turn these off for speed.
@lru_cache(maxsize=None)
def _call_params(call: Callable[..., Any]) -> MappingProxyType[str, inspect.Parameter]:
    return inspect.signature(call).parameters


def _validate_provider_call_params(
    api_call_params: Dict[str, Any], call: Callable[..., Any]
):
    provider_call_params = _call_params(call)

    required_params = {
        name: param
        for name, param in provider_call_params.items()
        if param.default == param.empty and param.kind != param.VAR_KEYWORD
    }

    for param_name in required_params:
        assert (
            param_name in api_call_params
        ), f"Provider implementation error: Required parameter '{param_name}' is missing in the converted call parameters converted from ell call."

    for param_name, param_value in api_call_params.items():
        assert (
            param_name in provider_call_params
        ), f"Provider implementation error: Unexpected parameter '{param_name}' in the converted call parameters."

    return True
--------------------------------------------------------------------------------
Chunk ID: ell\provider.py::6
Filepath: src\ell\provider.py
Content:
def _validate_messages_are_tracked(
    messages: List[Message], origin_id: Optional[str] = None
):
    if origin_id is None:
        return

    for message in messages:
        assert isinstance(
            message.text, _lstr
        ), f"Provider implementation error: Message text should be an instance of _lstr, got {type(message.text)}"
        assert (
            origin_id in message.text.__origin_trace__
        ), f"Provider implementation error: Message origin_id {message.text.__origin_trace__} does not match the provided origin_id {origin_id}"
    return True
--------------------------------------------------------------------------------
Chunk ID: providers\__init__.py::1
Filepath: src\ell\providers\__init__.py
Content:
import ell.providers.openai
import ell.providers.groq
import ell.providers.anthropic
import ell.providers.bedrock
# import ell.providers.mistral
# import ell.providers.cohere
# import ell.providers.gemini
# import ell.providers.elevenlabs
# import ell.providers.replicate
# import ell.providers.huggingface
--------------------------------------------------------------------------------
Chunk ID: providers\anthropic.py::1
Filepath: src\ell\providers\anthropic.py
Content:
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent

from ell.types._lstr import _lstr
from ell.types.message import LMP
from ell.configurator import register_provider
from ell.util.serialization import serialize_image
import base64
from io import BytesIO
import json
import requests
from PIL import Image as PILImage

try:
    import anthropic
    from anthropic import Anthropic
    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent
    from anthropic.types.message_create_params import MessageCreateParamsStreaming
    from anthropic._streaming import Stream

    class AnthropicProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            return client.messages.create

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())
            # XXX: Helper, but should be depreicated due to ssot
            assert final_call_params.get("max_tokens") is not None, f"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}}))."

            dirty_msgs = [
                MessageParam(
                    role=cast(Literal["user", "assistant"], message.role), 
                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]
            role_correct_msgs   : List[MessageParam] = []
            for msg in dirty_msgs:
                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):
                    role_correct_msgs.append(msg)
                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])

            system_message = None
            if role_correct_msgs and role_correct_msgs[0]["role"] == "system":
                system_message = role_correct_msgs.pop(0)

            if system_message:
                final_call_params["system"] = system_message["content"][0]["text"]


            final_call_params['stream'] = True
            final_call_params["model"] = ell_call.model
            final_call_params["messages"] = role_correct_msgs

            if ell_call.tools:
                final_call_params["tools"] = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        name=tool.__name__,
                        description=tool.__doc__,
                        input_schema=tool.__ell_params_model__.model_json_schema(),
                    )
                    for tool in ell_call.tools
                ]

            # print(final_call_params)
            return final_call_params

        def translate_from_provider(
            self,
            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            usage = {}
            tracked_results = []
            metadata = {}

            #XXX: Support n > 0

            if provider_call_params.get("stream", False):
                content = []
                current_blocks: Dict[int, Dict[str, Any]] = {}
                message_metadata = {}

                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:
                    for chunk in stream:
                        if chunk.type == "message_start":
                            message_metadata = chunk.message.model_dump()
                            message_metadata.pop("content", None)  # Remove content as we'll build it separately

                        elif chunk.type == "content_block_start":
                            block = chunk.content_block.model_dump()
                            current_blocks[chunk.index] = block
                            if block["type"] == "tool_use":
                                if logger: logger(f" <tool_use: {block['name']}(")
                                block["input"] = "" # force it to be a string, XXX: can implement partially parsed json later.
                        elif chunk.type == "content_block_delta":
                            if chunk.index in current_blocks:
                                block = current_blocks[chunk.index]
                                if (delta := chunk.delta).type == "text_delta":
                                    block["text"] += delta.text
                                    if logger: logger(delta.text)
                                if delta.type == "input_json_delta":
                                    block["input"] += delta.partial_json
                                    if logger: logger(delta.partial_json)

                        elif chunk.type == "content_block_stop":
                            if chunk.index in current_blocks:
                                block = current_blocks.pop(chunk.index)
                                if block["type"] == "text":
                                    content.append(ContentBlock(text=_lstr(block["text"],origin_trace=origin_id)))
                                elif block["type"] == "tool_use":
                                    try:
                                        matching_tool = ell_call.get_tool_by_name(block["name"])
                                        if matching_tool:
                                            content.append(
                                                ContentBlock(
                                                    tool_call=ToolCall(
                                                        tool=matching_tool,
                                                        tool_call_id=_lstr(
                                                            block['id'],origin_trace=origin_id
                                                        ),
                                                        params=json.loads(block['input']) if block['input'] else {},
                                                    )
                                                )
                                            )
                                    except json.JSONDecodeError:
                                        if logger: logger(f" - FAILED TO PARSE JSON")
                                        pass
                                    if logger: logger(f")>")

                        elif chunk.type == "message_delta":
                            message_metadata.update(chunk.delta.model_dump())
                            if chunk.usage:
                                usage.update(chunk.usage.model_dump())

                        elif chunk.type == "message_stop":
                            tracked_results.append(Message(role="assistant", content=content))

                        # print(chunk)
                metadata = message_metadata

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("input_tokens", 0)
            usage["completion_tokens"] = usage.get("output_tokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata

    # XXX: Make a singleton.
    anthropic_provider = AnthropicProvider()
    register_provider(anthropic_provider, anthropic.Anthropic)
    register_provider(anthropic_provider, anthropic.AnthropicBedrock)
    register_provider(anthropic_provider, anthropic.AnthropicVertex)

except ImportError:
    pass
--------------------------------------------------------------------------------
Chunk ID: providers\anthropic.py::2
Filepath: src\ell\providers\anthropic.py
Content:
def serialize_image_for_anthropic(img : ImageContent):
    if img.url:
        # Download the image from the URL
        response = requests.get(img.url)
        response.raise_for_status()  # Raise an exception for bad responses
        pil_image = PILImage.open(BytesIO(response.content))
    elif img.image:
        pil_image = img.image
    else:
        raise ValueError("Image object has neither url nor image data.")
    buffer = BytesIO()
    pil_image.save(buffer, format="PNG")
    base64_image =  base64.b64encode(buffer.getvalue()).decode()
    return dict(
        type="image",
        source=dict(
            type="base64",
            media_type="image/png",
            data=base64_image
        )
    )
--------------------------------------------------------------------------------
Chunk ID: providers\anthropic.py::3
Filepath: src\ell\providers\anthropic.py
Content:
def _content_block_to_anthropic_format(content_block: ContentBlock):
        if (image := content_block.image): return serialize_image_for_anthropic(image)
        elif ((text := content_block.text) is not None): return dict(type="text", text=text)
        elif (parsed := content_block.parsed):
            return dict(type="text", text=json.dumps(parsed.model_dump(), ensure_ascii=False))
        elif (tool_call := content_block.tool_call):
            return dict(
                type="tool_use",
                id=tool_call.tool_call_id,
                name=tool_call.tool.__name__,
                input=tool_call.params.model_dump()
            )
        elif (tool_result := content_block.tool_result):
            return dict(
                type="tool_result",
                tool_use_id=tool_result.tool_call_id,
                content=[_content_block_to_anthropic_format(c) for c in tool_result.result]
            )
        else:
            raise ValueError("Content block is not supported by anthropic")
--------------------------------------------------------------------------------
Chunk ID: providers\bedrock.py::1
Filepath: src\ell\providers\bedrock.py
Content:
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent
from ell.types._lstr import _lstr
import json
from ell.configurator import config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image
from io import BytesIO
import requests
from PIL import Image as PILImage
--------------------------------------------------------------------------------
Chunk ID: providers\bedrock.py::2
Filepath: src\ell\providers\bedrock.py
Content:
try:
    from botocore.client import BaseClient
    from botocore.eventstream import (EventStream)
    class BedrockProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Any, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and api_call_params.get("stream", False):
                api_call_params.pop('stream')
                return client.converse_stream
            else:
                return client.converse

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = {}

            if ell_call.api_params.get('api_params',{}).get('stream', False):
                final_call_params['stream'] = ell_call.api_params.get('api_params',{}).get('stream', False)

            bedrock_converse_messages = [message_to_bedrock_message_format(message) for message in ell_call.messages]

            system_message = None
            if bedrock_converse_messages and bedrock_converse_messages[0]["role"] == "system":
                system_message = bedrock_converse_messages.pop(0)

            if system_message:
                final_call_params["system"] = [{'text':system_message["content"][0]["text"]}]

            final_call_params["modelId"] = ell_call.model
            final_call_params["messages"] = bedrock_converse_messages

            if ell_call.tools:
                tools = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        toolSpec = dict(
                            name=tool.__name__,
                            description=tool.__doc__,
                            inputSchema=dict(
                                json=tool.__ell_params_model__.model_json_schema(),
                            )
                        )
                    )
                    for tool in ell_call.tools
                ]
                final_call_params["toolConfig"] = {'tools':tools}

            return final_call_params

        def translate_from_provider(
                self,
                provider_response: Union[EventStream, Any],
                ell_call: EllCallParams,
                provider_call_params: Dict[str, Any],
                origin_id: Optional[str] = None,
                logger: Optional[Callable[..., None]] = None,
            ) -> Tuple[List[Message], Metadata]:

            usage = {}
            metadata : Metadata = {}

            metadata : Metadata = {}
            tracked_results : List[Message] = []
            did_stream = ell_call.api_params.get("api_params", {}).get('stream')

            if did_stream:
                content = []
                current_block: Optional[Dict[str, Any]] = {}
                message_metadata = {}
                for chunk in provider_response.get('stream'):

                    if "messageStart" in chunk:
                        current_block['content'] = ''
                        pass
                    elif "contentBlockStart" in chunk:
                        pass
                    elif "contentBlockDelta" in chunk:
                        delta = chunk.get("contentBlockDelta", {}).get("delta", {})
                        if "text" in delta:
                            current_block['type'] = 'text'
                            current_block['content'] += delta.get("text")
                            if logger:
                                logger(delta.get("text"))
                        else:
                            pass
                    elif "contentBlockStop" in chunk:
                        if current_block is not None:
                            if current_block["type"] == "text":
                                content.append(ContentBlock(text=_lstr(content=content, origin_trace=origin_id)))

                    elif "messageStop" in chunk:
                        tracked_results.append(Message(role="assistant", content=content))

                    elif "metadata" in chunk:
                        if "usage" in chunk["metadata"]:
                            usage["prompt_tokens"] = chunk["metadata"].get('usage').get("inputTokens", 0)
                            usage["completion_tokens"] = chunk["metadata"].get('usage').get("outputTokens", 0)
                            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']
                            message_metadata["usage"] = usage
                    else:
                        pass


                metadata = message_metadata
            else:
                # Non-streaming response processing (unchanged)
                cbs = []
                for content_block in provider_response.get('output', {}).get('message', {}).get('content', []):
                    if 'text' in content_block:
                        cbs.append(ContentBlock(text=_lstr(content_block.get('text'), origin_trace=origin_id)))
                    elif 'toolUse' in content_block:
                        assert ell_call.tools is not None, "Tools were not provided to the model when calling it and yet bedrock returned a tool use."
                        try:
                            toolUse = content_block['toolUse']
                            matching_tool = ell_call.get_tool_by_name(toolUse["name"])
                            if matching_tool:
                                cbs.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                toolUse['toolUseId'],origin_trace=origin_id
                                            ),
                                            params=toolUse['input'],
                                        )
                                    )
                                )
                        except json.JSONDecodeError:
                            if logger: logger(f" - FAILED TO PARSE JSON")
                            pass
                tracked_results.append(Message(role="assistant", content=cbs))
                if logger:
                    logger(tracked_results[0].text)


                # usage = call_result.response.usage.dict() if call_result.response.get('usage') else {}
                # metadata = call_result.response.model_dump()
                # del metadata["content"]

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("inputTokens", 0)
            usage["completion_tokens"] = usage.get("outputTokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata


    # XXX: Make a singleton.
    register_provider(BedrockProvider(), BaseClient)
except ImportError:
    pass
--------------------------------------------------------------------------------
Chunk ID: providers\bedrock.py::3
Filepath: src\ell\providers\bedrock.py
Content:
def content_block_to_bedrock_format(content_block: ContentBlock) -> Dict[str, Any]:
    if content_block.image:
        img:ImageContent = content_block.image
        if img.url:
            # Download the image from the URL
            response = requests.get(img.url)
            response.raise_for_status()  # Raise an exception for bad responses
            pil_image = PILImage.open(BytesIO(response.content))
        elif img.image:
            pil_image = img.image
        else:
            raise ValueError("Image object has neither url nor image data.")
        buffer = BytesIO()
        pil_image.save(buffer, format="PNG")
        base64_image = buffer.getvalue()
        return {
            "image": {
                "format": "png",
                "source":
                {
                    "bytes": base64_image
                }
            }
        }
    elif content_block.text:
        return {
            "text": content_block.text
        }
    elif content_block.parsed:
        return {
            "type": "text",
            "text": json.dumps(content_block.parsed.model_dump(), ensure_ascii=False)
        }
    elif content_block.tool_call:
        return {
            "toolUse": {
                "toolUseId": content_block.tool_call.tool_call_id,
                "name": content_block.tool_call.tool.__name__,
                "input": content_block.tool_call.params.model_dump()
            }
        }
    elif content_block.tool_result:
        return {
            "toolResult":{
                "toolUseId": content_block.tool_result.tool_call_id,
                "content": [content_block_to_bedrock_format(c) for c in content_block.tool_result.result]
            }
        }
    else:
        raise ValueError("Content block is not supported by bedrock")



def message_to_bedrock_message_format(message: Message) -> Dict[str, Any]:

    converse_message = {
        "role": message.role,
        "content": list(filter(None, [
            content_block_to_bedrock_format(c) for c in message.content
        ]))
    }
    return converse_message
--------------------------------------------------------------------------------
Chunk ID: providers\groq.py::1
Filepath: src\ell\providers\groq.py
Content:
"""
Groq provider.
"""

from ell.providers.openai import OpenAIProvider
from ell.configurator import register_provider


try:
    import groq
    class GroqProvider(OpenAIProvider):
        dangerous_disable_validation = True
        def translate_to_provider(self, *args, **kwargs):
            params = super().translate_to_provider(*args, **kwargs)
            params.pop('stream_options', None)
            return params

        def translate_from_provider(self, *args, **kwargs):
            res, meta = super().translate_from_provider(*args, **kwargs)
            if not meta['usage']:
                meta['usage'] = meta['x_groq']['usage']
            return res, meta
    register_provider(GroqProvider(), groq.Client)
except ImportError:
    pass
--------------------------------------------------------------------------------
Chunk ID: providers\openai.py::1
Filepath: src\ell\providers\openai.py
Content:
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast

from pydantic import BaseModel
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from ell.configurator import _Model, config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image

try:
    # XXX: Could genericize.
    import openai
    from openai._streaming import Stream
    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam

    class OpenAIProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and (isinstance(fmt := api_call_params.get("response_format"), type)) and issubclass(fmt, BaseModel):
                return client.beta.chat.completions.parse
            else:
                return client.chat.completions.create

        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:
            final_call_params = ell_call.api_params.copy()
            final_call_params["model"] = ell_call.model
            # Stream by default for verbose logging.
            final_call_params["stream"] = True
            final_call_params["stream_options"] = {"include_usage": True}

            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.
            if ell_call.tools or final_call_params.get("response_format") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:
                final_call_params.pop("stream", None)
                final_call_params.pop("stream_options", None)
            if ell_call.tools:
                final_call_params.update(
                    tool_choice=final_call_params.get("tool_choice", "auto"),
                    tools=[  
                        dict(
                            type="function",
                            function=dict(
                                name=tool.__name__,
                                description=tool.__doc__,
                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore
                            )
                        ) for tool in ell_call.tools
                    ]
                )
            # messages
            openai_messages : List[ChatCompletionMessageParam] = []
            for message in ell_call.messages:
                if (tool_calls := message.tool_calls):
                    assert message.role == "assistant", "Tool calls must be from the assistant."
                    assert all(t.tool_call_id for t in tool_calls), "Tool calls must have tool call ids."
                    openai_messages.append(dict(
                        tool_calls=[
                            dict(
                                id=cast(str, tool_call.tool_call_id),
                                type="function",
                                function=dict(
                                    name=tool_call.tool.__name__,
                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)
                                )
                            ) for tool_call in tool_calls ],
                        role="assistant",
                        content=None,
                    ))
                elif (tool_results := message.tool_results):
                    for tool_result in tool_results:
                        assert all(cb.type == "text" for cb in tool_result.result), "Tool result does not match expected content blocks."
                        openai_messages.append(dict(
                            role="tool",
                            tool_call_id=tool_result.tool_call_id,
                            content=tool_result.text_only, 
                        ))
                else:
                    openai_messages.append(cast(ChatCompletionMessageParam, dict(
                        role=message.role,
                        content=[_content_block_to_openai_format(c) for c in message.content] 
                             if message.role != "system" 
                             else message.text_only
                    )))

            final_call_params["messages"] = openai_messages

            return final_call_params

        def translate_from_provider(
            self,
            provider_response: Union[
                ChatCompletion, 
                ParsedChatCompletion,
                Stream[ChatCompletionChunk], Any],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            metadata : Metadata = {}
            messages : List[Message] = []
            did_stream = provider_call_params.get("stream", False)


            if did_stream:
                stream = cast(Stream[ChatCompletionChunk], provider_response)
                message_streams = defaultdict(list)
                role : Optional[str] = None
                for chunk in stream:
                    metadata.update(chunk.model_dump(exclude={"choices"}))

                    for chat_compl_chunk in chunk.choices:
                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)
                        delta = chat_compl_chunk.delta
                        role = role or delta.role
                        if  chat_compl_chunk.index == 0 and logger:
                            logger(delta.content, is_refusal=hasattr(delta, "refusal") and delta.refusal)
                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):
                    text = "".join((choice.delta.content or "") for choice in message_stream)
                    messages.append(
                        Message(role=role, 
                                content=_lstr(content=text,origin_trace=origin_id)))
                    #XXX: Support streaming other types.
            else:
                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)
                metadata = chat_completion.model_dump(exclude={"choices"})
                for oai_choice in chat_completion.choices:
                    role = oai_choice.message.role
                    content_blocks = []
                    if (hasattr(message := oai_choice.message, "refusal") and (refusal := message.refusal)):
                        raise ValueError(refusal)
                    if hasattr(message, "parsed"):
                        if (parsed := message.parsed):
                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing
                            if logger: logger(parsed.model_dump_json())
                    else:
                        if (content := message.content):
                            content_blocks.append(
                                ContentBlock(
                                    text=_lstr(content=content,origin_trace=origin_id)))
                            if logger: logger(content)
                        if (tool_calls := message.tool_calls):
                            for tool_call in tool_calls:
                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)
                                assert matching_tool, "Model called tool not found in provided toolset."
                                content_blocks.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                tool_call.id, origin_trace= origin_id),
                                            params=json.loads(tool_call.function.arguments),
                                        )
                                    )
                                )
                                if logger: logger(repr(tool_call))
                    messages.append(Message(role=role, content=content_blocks))
            return messages, metadata


    # xx: singleton needed
    openai_provider = OpenAIProvider()
    register_provider(openai_provider, openai.Client)
except ImportError:
    pass
--------------------------------------------------------------------------------
Chunk ID: providers\openai.py::2
Filepath: src\ell\providers\openai.py
Content:
def _content_block_to_openai_format(content_block: ContentBlock) -> Dict[str, Any]:
    if (image := content_block.image):
        image_url = dict(url=serialize_image(image.image) if image.image else image.url)
        # XXX: Solve per content params better
        if image.detail: image_url["detail"] = image.detail
        return {
            "type": "image_url",
            "image_url": image_url
        }
    elif ((text := content_block.text) is not None): return dict(type="text", text=text)
    elif (parsed := content_block.parsed): return dict(type="text", text=parsed.model_dump_json())
    else:
        raise ValueError(f"Unsupported content block type for openai: {content_block}")
--------------------------------------------------------------------------------
Chunk ID: ell\store.py::1
Filepath: src\ell\store.py
Content:
from abc import ABC, abstractmethod
from contextlib import contextmanager
from datetime import datetime
from typing import Any, Optional, Dict, List, Set, Union
from ell.types._lstr import _lstr
from ell.types import SerializedLMP, Invocation
from ell.types.message import InvocableLM

class BlobStore(ABC):
    @abstractmethod
    def store_blob(self, blob: bytes, blob_id  : str) -> str:
        """Store a blob and return its identifier."""
        pass

    @abstractmethod
    def retrieve_blob(self, blob_id: str) -> bytes:
        """Retrieve a blob by its identifier."""
        pass
--------------------------------------------------------------------------------
Chunk ID: ell\store.py::2
Filepath: src\ell\store.py
Content:
class Store(ABC):
    """
    Abstract base class for serializers. Defines the interface for serializing and deserializing LMPs and invocations.
    """

    def __init__(self, blob_store: Optional[BlobStore] = None):
        self.blob_store = blob_store

    @property
    def has_blob_storage(self) -> bool:
        return self.blob_store is not None

    @abstractmethod
    def write_lmp(self, serialized_lmp: SerializedLMP, uses: Dict[str, Any]) -> Optional[Any]:
        """
        Write an LMP (Language Model Package) to the storage.

        :param serialized_lmp: SerializedLMP object containing all LMP details.
        :param uses: Dictionary of LMPs used by this LMP.
        :return: Optional return value.
        """
        pass

    @abstractmethod
    def write_invocation(self, invocation: Invocation,  consumes: Set[str]) -> Optional[Any]:
        """
        Write an invocation of an LMP to the storage.

        :param invocation: Invocation object containing all invocation details.
        :param results: List of SerializedLStr objects representing the results.
        :param consumes: Set of invocation IDs consumed by this invocation.
        :return: Optional return value.
        """
        pass

    @abstractmethod
    def get_cached_invocations(self, lmp_id :str, state_cache_key :str) -> List[Invocation]:
        """
        Get cached invocations for a given LMP and state cache key.
        """
        pass

    @abstractmethod
    def get_versions_by_fqn(self, fqn :str) -> List[SerializedLMP]:
        """
        Get all versions of an LMP by its fully qualified name.
        """
        pass
--------------------------------------------------------------------------------
Chunk ID: ell\store.py::3
Filepath: src\ell\store.py
Content:
class Store(ABC):


    @contextmanager
    def freeze(self, *lmps: InvocableLM):
        """
        A context manager for caching operations using a particular store.

        Args:
            *lmps: InvocableLM objects to freeze.

        Yields:
            None
        """
        old_cache_values = {}
        try:
            for lmp in lmps:
                old_cache_values[lmp] = getattr(lmp, '__ell_use_cache__', None)
                setattr(lmp, '__ell_use_cache__', self)
            yield
        finally:
            # TODO: Implement cache storage logic here
            for lmp in lmps:
                if lmp in old_cache_values:
                    setattr(lmp, '__ell_use_cache__', old_cache_values[lmp])
                else:
                    delattr(lmp, '__ell_use_cache__')
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::1
Filepath: src\ell\stores\sql.py
Content:
from datetime import datetime, timedelta
import json
import os
from typing import Any, Optional, Dict, List, Set, Union
from pydantic import BaseModel
from sqlmodel import Session, SQLModel, create_engine, select
import ell.store
import cattrs
import numpy as np
from sqlalchemy.sql import text
from ell.types import InvocationTrace, SerializedLMP, Invocation, InvocationContents
from ell.types._lstr import _lstr
from sqlalchemy import or_, func, and_, extract, FromClause
from sqlalchemy.types import TypeDecorator, VARCHAR
from ell.types.studio import SerializedLMPUses, utc_now
from ell.util.serialization import pydantic_ltype_aware_cattr
import gzip
import json
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::2
Filepath: src\ell\stores\sql.py
Content:
class SQLStore(ell.store.Store):
    def __init__(self, db_uri: str, blob_store: Optional[ell.store.BlobStore] = None):
        self.engine = create_engine(db_uri,
                                    json_serializer=lambda obj: json.dumps(pydantic_ltype_aware_cattr.unstructure(obj), 
                                     sort_keys=True, default=repr, ensure_ascii=False))

        SQLModel.metadata.create_all(self.engine)
        self.open_files: Dict[str, Dict[str, Any]] = {}
        super().__init__(blob_store)

    def write_lmp(self, serialized_lmp: SerializedLMP, uses: Dict[str, Any]) -> Optional[Any]:
        with Session(self.engine) as session:
            # Bind the serialized_lmp to the session
            lmp = session.exec(select(SerializedLMP).filter(SerializedLMP.lmp_id == serialized_lmp.lmp_id)).first()

            if lmp:
                # Already added to the DB.
                return lmp
            else:
                session.add(serialized_lmp)

            for use_id in uses:
                used_lmp = session.exec(select(SerializedLMP).where(SerializedLMP.lmp_id == use_id)).first()
                if used_lmp:
                    serialized_lmp.uses.append(used_lmp)

            session.commit()
        return None
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::3
Filepath: src\ell\stores\sql.py
Content:
class SQLStore(ell.store.Store):

    def write_invocation(self, invocation: Invocation, consumes: Set[str]) -> Optional[Any]:
        with Session(self.engine) as session:
            lmp = session.exec(select(SerializedLMP).filter(SerializedLMP.lmp_id == invocation.lmp_id)).first()
            assert lmp is not None, f"LMP with id {invocation.lmp_id} not found. Writing invocation erroneously"

            # Increment num_invocations
            if lmp.num_invocations is None:
                lmp.num_invocations = 1
            else:
                lmp.num_invocations += 1

            # Add the invocation contents
            session.add(invocation.contents)

            # Add the invocation
            session.add(invocation)

            # Now create traces.
            for consumed_id in consumes:
                session.add(InvocationTrace(
                    invocation_consumer_id=invocation.id,
                    invocation_consuming_id=consumed_id
                ))

            session.commit()
            return None
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::4
Filepath: src\ell\stores\sql.py
Content:
class SQLStore(ell.store.Store):

    def get_cached_invocations(self, lmp_id :str, state_cache_key :str) -> List[Invocation]:
        with Session(self.engine) as session:
            return self.get_invocations(session, lmp_filters={"lmp_id": lmp_id}, filters={"state_cache_key": state_cache_key})

    def get_versions_by_fqn(self, fqn :str) -> List[SerializedLMP]:
        with Session(self.engine) as session:
            return self.get_lmps(session, name=fqn)

    ## HELPER METHODS FOR ELL STUDIO! :) 

--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::5
Filepath: src\ell\stores\sql.py
Content:
class SQLStore(ell.store.Store):
    def get_latest_lmps(self, session: Session, skip: int = 0, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Gets all the lmps grouped by unique name with the highest created at
        """
        subquery = (
            select(SerializedLMP.name, func.max(SerializedLMP.created_at).label("max_created_at"))
            .group_by(SerializedLMP.name)
            .subquery()
        )

        filters = {
            "name": subquery.c.name,
            "created_at": subquery.c.max_created_at
        }

        return self.get_lmps(session, skip=skip, limit=limit, subquery=subquery, **filters)
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::6
Filepath: src\ell\stores\sql.py
Content:
class SQLStore(ell.store.Store):


    def get_lmps(self, session: Session, skip: int = 0, limit: int = 10, subquery=None, **filters: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:

        query = select(SerializedLMP)

        if subquery is not None:
            query = query.join(subquery, and_(
                SerializedLMP.name == subquery.c.name,
                SerializedLMP.created_at == subquery.c.max_created_at
            ))

        if filters:
            for key, value in filters.items():
                query = query.where(getattr(SerializedLMP, key) == value)

        query = query.order_by(SerializedLMP.created_at.desc())  # Sort by created_at in descending order
        query = query.offset(skip).limit(limit)
        results = session.exec(query).all()

        return results
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::7
Filepath: src\ell\stores\sql.py
Content:
class SQLStore(ell.store.Store):

    def get_invocations(self, session: Session, lmp_filters: Dict[str, Any], skip: int = 0, limit: int = 10, filters: Optional[Dict[str, Any]] = None, hierarchical: bool = False) -> List[Dict[str, Any]]:

        query = select(Invocation).join(SerializedLMP)

        # Apply LMP filters
        for key, value in lmp_filters.items():
            query = query.where(getattr(SerializedLMP, key) == value)

        # Apply invocation filters
        if filters:
            for key, value in filters.items():
                query = query.where(getattr(Invocation, key) == value)

        # Sort from newest to oldest
        query = query.order_by(Invocation.created_at.desc()).offset(skip).limit(limit)

        invocations = session.exec(query).all()
        return invocations
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::8
Filepath: src\ell\stores\sql.py
Content:
class SQLStore(ell.store.Store):


    def get_traces(self, session: Session):
        query = text("""
        SELECT 
            consumer.lmp_id, 
            trace.*, 
            consumed.lmp_id
        FROM 
            invocation AS consumer
        JOIN 
            invocationtrace AS trace ON consumer.id = trace.invocation_consumer_id
        JOIN 
            invocation AS consumed ON trace.invocation_consuming_id = consumed.id
        """)
        results = session.exec(query).all()

        traces = []
        for (consumer_lmp_id, consumer_invocation_id, consumed_invocation_id, consumed_lmp_id) in results:
            traces.append({
                'consumer': consumer_lmp_id,
                'consumed': consumed_lmp_id
            })

        return traces
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::9
Filepath: src\ell\stores\sql.py
Content:
class SQLStore(ell.store.Store):

    def get_invocations_aggregate(self, session: Session, lmp_filters: Dict[str, Any] = None, filters: Dict[str, Any] = None, days: int = 30) -> Dict[str, Any]:
        # Calculate the start date for the graph data
        start_date = datetime.utcnow() - timedelta(days=days)

        # Base subquery
        base_subquery = (
            select(Invocation.created_at, Invocation.latency_ms, Invocation.prompt_tokens, Invocation.completion_tokens, Invocation.lmp_id)
            .join(SerializedLMP, Invocation.lmp_id == SerializedLMP.lmp_id)
            .filter(Invocation.created_at >= start_date)
        )

        # Apply filters
        if lmp_filters:
            base_subquery = base_subquery.filter(and_(*[getattr(SerializedLMP, k) == v for k, v in lmp_filters.items()]))
        if filters:
            base_subquery = base_subquery.filter(and_(*[getattr(Invocation, k) == v for k, v in filters.items()]))


        data = session.exec(base_subquery).all()

        # Calculate aggregate metrics
        total_invocations = len(data)
        total_tokens = sum(row.prompt_tokens + row.completion_tokens for row in data)
        avg_latency = sum(row.latency_ms for row in data) / total_invocations if total_invocations > 0 else 0
        unique_lmps = len(set(row.lmp_id for row in data))

        # Prepare graph data
        graph_data = []
        for row in data:
            graph_data.append({
                "date": row.created_at,
                "avg_latency": row.latency_ms,
                "tokens": row.prompt_tokens + row.completion_tokens,
                "count": 1
            })

        return {
            "total_invocations": total_invocations,
            "total_tokens": total_tokens,
            "avg_latency": avg_latency,
            "unique_lmps": unique_lmps,
            "graph_data": graph_data
        }
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::10
Filepath: src\ell\stores\sql.py
Content:
class SQLiteStore(SQLStore):
    def __init__(self, db_dir: str):
        assert not db_dir.endswith('.db'), "Create store with a directory not a db."

        os.makedirs(db_dir, exist_ok=True)
        self.db_dir = db_dir
        db_path = os.path.join(db_dir, 'ell.db')
        blob_store = SQLBlobStore(db_dir)
        super().__init__(f'sqlite:///{db_path}', blob_store=blob_store)

class SQLBlobStore(ell.store.BlobStore):
    def __init__(self, db_dir: str):
        self.db_dir = db_dir

    def store_blob(self, blob: bytes, blob_id  : str) -> str:
        file_path = self._get_blob_path(blob_id)
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with gzip.open(file_path, "wb") as f:
            f.write(blob)
        return blob_id

    def retrieve_blob(self, blob_id: str) -> bytes:
        file_path = self._get_blob_path(blob_id)
        with gzip.open(file_path, "rb") as f:
            return f.read()
--------------------------------------------------------------------------------
Chunk ID: stores\sql.py::11
Filepath: src\ell\stores\sql.py
Content:
class SQLBlobStore(ell.store.BlobStore):

    def _get_blob_path(self, id: str, depth: int = 2) -> str:
        assert "-" in id, "Blob id must have a single - in it to split on."
        _type, _id = id.split("-")
        increment = 2
        dirs = [_type] + [_id[i:i+increment] for i in range(0, depth*increment, increment)]
        file_name = _id[depth*increment:]
        return os.path.join(self.db_dir, *dirs, file_name)

class PostgresStore(SQLStore):
    def __init__(self, db_uri: str):
        super().__init__(db_uri)
--------------------------------------------------------------------------------
Chunk ID: studio\__main__.py::1
Filepath: src\ell\studio\__main__.py
Content:
import asyncio
import logging
import socket
import time
import webbrowser
import uvicorn
from argparse import ArgumentParser
from contextlib import closing
from ell.studio.config import Config
from ell.studio.server import create_app
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pathlib import Path
from watchfiles import awatch


logger = logging.getLogger(__file__)


def _socket_is_open(host, port) -> bool:
    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
        return sock.connect_ex((host, port)) == 0


def _setup_logging(level):
    logging.basicConfig(
        format='%(asctime)s %(levelname)-8s] %(message)s',
        level=level,
        datefmt='%Y-%m-%d %H:%M:%S'
    )
--------------------------------------------------------------------------------
Chunk ID: studio\__main__.py::2
Filepath: src\ell\studio\__main__.py
Content:
def main():
    parser = ArgumentParser(description="ell studio")
    parser.add_argument("--storage-dir" , default=None,
                        help="Directory for filesystem serializer storage (default: current directory)")
    parser.add_argument("--pg-connection-string", default=None,
                        help="PostgreSQL connection string (default: None)")
    parser.add_argument("--host", default="127.0.0.1", help="Host to run the server on (default: localhost)")
    parser.add_argument("--port", type=int, default=5555, help="Port to run the server on (default: 5555)")
    parser.add_argument("--dev", action="store_true", help="Run in development mode")
    parser.add_argument("--open", action="store_true", help="Opens the studio web UI in a browser")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enables debug logging for more verbose output")
    args = parser.parse_args()

    _setup_logging(logging.DEBUG if args.verbose else logging.INFO)

    if args.dev:
        assert args.port == 5555, "Port must be 5000 in development mode"

    config = Config.create(storage_dir=args.storage_dir,
                    pg_connection_string=args.pg_connection_string)
    app = create_app(config)

    if not args.dev:
        # In production mode, serve the built React app
        static_dir = Path(__file__).parent / "static"
        # app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")

        @app.get("/{full_path:path}")
        async def serve_react_app(full_path: str):
            file_path = static_dir / full_path
            if file_path.exists() and file_path.is_file():
                return FileResponse(file_path)
            else:
                return FileResponse(static_dir / "index.html")

    # Respect Config.create behavior, which has fallback to env vars.
    db_path = Path(config.storage_dir) if config.storage_dir else None
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: studio\__main__.py::3
Filepath: src\ell\studio\__main__.py
Content:
def main():
    # ... other code

    async def db_watcher(db_path, app):
        last_stat = None

        while True:
            await asyncio.sleep(0.1)  # Fixed interval of 0.1 seconds
            try:
                current_stat = db_path.stat()

                if last_stat is None:
                    logger.info(f"Database file found: {db_path}")
                    await app.notify_clients("database_updated")
                else:
                    # Use a threshold for time comparison to account for filesystem differences
                    time_threshold = 0.1  # 1 second threshold
                    time_changed = abs(current_stat.st_mtime - last_stat.st_mtime) > time_threshold
                    size_changed = current_stat.st_size != last_stat.st_size
                    inode_changed = current_stat.st_ino != last_stat.st_ino

                    if time_changed or size_changed or inode_changed:
                        logger.info(
                            f"Database changed: mtime {time.ctime(last_stat.st_mtime)} -> {time.ctime(current_stat.st_mtime)}, "
                            f"size {last_stat.st_size} -> {current_stat.st_size}, "
                            f"inode {last_stat.st_ino} -> {current_stat.st_ino}"
                        )
                        await app.notify_clients("database_updated")

                last_stat = current_stat
            except FileNotFoundError:
                if last_stat is not None:
                    logger.info(f"Database file deleted: {db_path}")
                    await app.notify_clients("database_updated")
                last_stat = None
                await asyncio.sleep(1)  # Wait a bit longer if the file is missing
            except Exception as e:
                logger.info(f"Error checking database file: {e}")
                await asyncio.sleep(1)  # Wait a bit longer on errors
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: studio\__main__.py::4
Filepath: src\ell\studio\__main__.py
Content:
def main():
    # ... other code

    async def open_browser(host, port):
        while True:
            logger.debug(f"Checking TCP port {port} on {host} for readiness.")
            if _socket_is_open(host, port):
                url = f"http://{host}:{port}"
                logger.debug(f"Port is open, launching {url}.")
                webbrowser.open_new(url)
                return

            logger.debug(f"Port {port} was not open, retrying.")
            await asyncio.sleep(.1)

    # Start the database watcher
    loop = asyncio.new_event_loop()

    config = uvicorn.Config(app=app, host=args.host, port=args.port, loop=loop)
    server = uvicorn.Server(config)
    loop.create_task(server.serve())
    if db_path:
        loop.create_task(db_watcher(db_path, app))
    if args.open:
        loop.create_task(open_browser(args.host, args.port))
    loop.run_forever()

if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
Chunk ID: studio\config.py::1
Filepath: src\ell\studio\config.py
Content:
from functools import lru_cache
import os
from typing import Optional
from pydantic import BaseModel

import logging

logger = logging.getLogger(__name__)


# todo. maybe we default storage dir and other things in the future to a well-known location
# like ~/.ell or something
@lru_cache
def ell_home() -> str:
    return os.path.join(os.path.expanduser("~"), ".ell")


class Config(BaseModel):
    pg_connection_string: Optional[str] = None
    storage_dir: Optional[str] = None

    @classmethod
    def create(
        cls,
        storage_dir: Optional[str] = None,
        pg_connection_string: Optional[str] = None,
    ) -> 'Config':
        pg_connection_string = pg_connection_string or os.getenv("ELL_PG_CONNECTION_STRING")
        storage_dir = storage_dir or os.getenv("ELL_STORAGE_DIR")

        # Enforce that we use either sqlite or postgres, but not both
        if pg_connection_string is not None and storage_dir is not None:
            raise ValueError("Cannot use both sqlite and postgres")

        # For now, fall back to sqlite if no PostgreSQL connection string is provided
        if pg_connection_string is None and storage_dir is None:
            # This intends to honor the default we had set in the CLI
            storage_dir = os.getcwd()

        return cls(pg_connection_string=pg_connection_string, storage_dir=storage_dir)
--------------------------------------------------------------------------------
Chunk ID: studio\connection_manager.py::1
Filepath: src\ell\studio\connection_manager.py
Content:
from fastapi import WebSocket


class ConnectionManager:
    def __init__(self):
        self.active_connections = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            print(f"Broadcasting message to {connection} {message}")
            await connection.send_text(message)
--------------------------------------------------------------------------------
Chunk ID: studio\datamodels.py::1
Filepath: src\ell\studio\datamodels.py
Content:
from datetime import datetime
from typing import List, Optional, Dict, Any
from sqlmodel import SQLModel
from ell.types import SerializedLMPBase, InvocationBase, InvocationContentsBase


class SerializedLMPWithUses(SerializedLMPBase):
    lmp_id : str
    uses: List[SerializedLMPBase]


class InvocationPublic(InvocationBase):
    lmp: SerializedLMPBase
    uses: List["InvocationPublicWithConsumes"]
    contents: InvocationContentsBase

class InvocationPublicWithConsumes(InvocationPublic):
    consumes: List[InvocationPublic]
    consumed_by: List[InvocationPublic]



from pydantic import BaseModel

class GraphDataPoint(BaseModel):
    date: datetime
    count: int
    avg_latency: float
    tokens: int
    # cost: float

class InvocationsAggregate(BaseModel):
    total_invocations: int
    total_tokens: int
    avg_latency: float
    # total_cost: float
    unique_lmps: int
    # successful_invocations: int
    # success_rate: float
    graph_data: List[GraphDataPoint]
--------------------------------------------------------------------------------
Chunk ID: studio\server.py::1
Filepath: src\ell\studio\server.py
Content:
from typing import Optional, Dict, Any

from sqlmodel import Session
from ell.stores.sql import PostgresStore, SQLiteStore
from ell import __version__
from fastapi import FastAPI, Query, HTTPException, Depends, Response, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import logging
import json
from ell.studio.config import Config
from ell.studio.connection_manager import ConnectionManager
from ell.studio.datamodels import InvocationPublicWithConsumes, SerializedLMPWithUses

from ell.types import SerializedLMP
from datetime import datetime, timedelta
from sqlmodel import select


logger = logging.getLogger(__name__)


from ell.studio.datamodels import InvocationsAggregate


def get_serializer(config: Config):
    if config.pg_connection_string:
        return PostgresStore(config.pg_connection_string)
    elif config.storage_dir:
        return SQLiteStore(config.storage_dir)
    else:
        raise ValueError("No storage configuration found")
--------------------------------------------------------------------------------
Chunk ID: studio\server.py::2
Filepath: src\ell\studio\server.py
Content:
def create_app(config:Config):
    serializer = get_serializer(config)

    def get_session():
        with Session(serializer.engine) as session:
            yield session

    app = FastAPI(title="ell Studio", version=__version__)

    # Enable CORS for all origins
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    manager = ConnectionManager()

    @app.websocket("/ws")
    async def websocket_endpoint(websocket: WebSocket):
        await manager.connect(websocket)
        try:
            while True:
                data = await websocket.receive_text()
                # Handle incoming WebSocket messages if needed
        except WebSocketDisconnect:
            manager.disconnect(websocket)


    @app.get("/api/latest/lmps", response_model=list[SerializedLMPWithUses])
    def get_latest_lmps(
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        session: Session = Depends(get_session)
    ):
        lmps = serializer.get_latest_lmps(
            session,
            skip=skip, limit=limit,
            )
        return lmps

    # TOOD: Create a get endpoint to efficient get on the index with /api/lmp/<lmp_id>
    @app.get("/api/lmp/{lmp_id}")
    def get_lmp_by_id(lmp_id: str, session: Session = Depends(get_session)):
        lmp = serializer.get_lmps(session, lmp_id=lmp_id)[0]
        return lmp
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: studio\server.py::3
Filepath: src\ell\studio\server.py
Content:
def create_app(config:Config):
    # ... other code



    @app.get("/api/lmps", response_model=list[SerializedLMPWithUses])
    def get_lmp(
        lmp_id: Optional[str] = Query(None),
        name: Optional[str] = Query(None),
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        session: Session = Depends(get_session)
    ):

        filters : Dict[str, Any] = {}
        if name:
            filters['name'] = name
        if lmp_id:
            filters['lmp_id'] = lmp_id

        lmps = serializer.get_lmps(session, skip=skip, limit=limit, **filters)

        if not lmps:
            raise HTTPException(status_code=404, detail="LMP not found")

        print(lmps[0])
        return lmps



    @app.get("/api/invocation/{invocation_id}", response_model=InvocationPublicWithConsumes)
    def get_invocation(
        invocation_id: str,
        session: Session = Depends(get_session)
    ):
        invocation = serializer.get_invocations(session, lmp_filters=dict(), filters={"id": invocation_id})[0]
        return invocation
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: studio\server.py::4
Filepath: src\ell\studio\server.py
Content:
def create_app(config:Config):
    # ... other code

    @app.get("/api/invocations", response_model=list[InvocationPublicWithConsumes])
    def get_invocations(
        id: Optional[str] = Query(None),
        hierarchical: Optional[bool] = Query(False),
        skip: int = Query(0, ge=0),
        limit: int = Query(100, ge=1, le=100),
        lmp_name: Optional[str] = Query(None),
        lmp_id: Optional[str] = Query(None),
        session: Session = Depends(get_session)
    ):
        lmp_filters = {}
        if lmp_name:
            lmp_filters["name"] = lmp_name
        if lmp_id:
            lmp_filters["lmp_id"] = lmp_id

        invocation_filters = {}
        if id:
            invocation_filters["id"] = id

        invocations = serializer.get_invocations(
            session,
            lmp_filters=lmp_filters,
            filters=invocation_filters,
            skip=skip,
            limit=limit,
            hierarchical=hierarchical
        )
        return invocations
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: studio\server.py::5
Filepath: src\ell\studio\server.py
Content:
def create_app(config:Config):
    # ... other code


    @app.get("/api/traces")
    def get_consumption_graph(
        session: Session = Depends(get_session)
    ):
        traces = serializer.get_traces(session)
        return traces



    @app.get("/api/blob/{blob_id}", response_class=Response)
    def get_blob(
        blob_id: str,
        session: Session = Depends(get_session)
    ):
        if serializer.blob_store is None:
            raise HTTPException(status_code=400, detail="Blob storage is not configured")
        try:
            blob_data = serializer.blob_store.retrieve_blob(blob_id)
            return Response(content=blob_data.decode('utf-8'), media_type="application/json")
        except FileNotFoundError:
            raise HTTPException(status_code=404, detail="Blob not found")
        except Exception as e:
            logger.error(f"Error retrieving blob: {str(e)}")
            raise HTTPException(status_code=500, detail="Internal server error")
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: studio\server.py::6
Filepath: src\ell\studio\server.py
Content:
def create_app(config:Config):
    # ... other code

    @app.get("/api/lmp-history")
    def get_lmp_history(
        days: int = Query(365, ge=1, le=3650),  # Default to 1 year, max 10 years
        session: Session = Depends(get_session)
    ):
        # Calculate the start date
        start_date = datetime.utcnow() - timedelta(days=days)

        # Query to get all LMP creation times within the date range
        query = (
            select(SerializedLMP.created_at)
            .where(SerializedLMP.created_at >= start_date)
            .order_by(SerializedLMP.created_at)
        )

        results = session.exec(query).all()

        # Convert results to a list of dictionaries
        history = [{"date": str(row), "count": 1} for row in results]

        return history
    # ... other code
--------------------------------------------------------------------------------
Chunk ID: studio\server.py::7
Filepath: src\ell\studio\server.py
Content:
def create_app(config:Config):
    # ... other code

    async def notify_clients(entity: str, id: Optional[str] = None):
        message = json.dumps({"entity": entity, "id": id})
        await manager.broadcast(message)

    # Add this method to the app object
    app.notify_clients = notify_clients


    @app.get("/api/invocations/aggregate", response_model=InvocationsAggregate)
    def get_invocations_aggregate(
        lmp_name: Optional[str] = Query(None),
        lmp_id: Optional[str] = Query(None),
        days: int = Query(30, ge=1, le=365),
        session: Session = Depends(get_session)
    ):
        lmp_filters = {}
        if lmp_name:
            lmp_filters["name"] = lmp_name
        if lmp_id:
            lmp_filters["lmp_id"] = lmp_id

        aggregate_data = serializer.get_invocations_aggregate(session, lmp_filters=lmp_filters, days=days)
        return InvocationsAggregate(**aggregate_data)



    return app
--------------------------------------------------------------------------------
Chunk ID: types\__init__.py::1
Filepath: src\ell\types\__init__.py
Content:
"""
The primary types used in ell
"""

from ell.types.message import *
from ell.types.studio import *
from ell.types._lstr import *
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::1
Filepath: src\ell\types\_lstr.py
Content:
"""
LM string that supports logits and keeps track of it'sorigin_trace even after mutation.
"""

import numpy as np
from typing import (
    Optional,
    Set,
    SupportsIndex,
    Union,
    FrozenSet,
    Iterable,
    List,
    Tuple,
    Any,
    Callable,
)
from typing_extensions import override
from pydantic import BaseModel, GetCoreSchemaHandler
from pydantic_core import CoreSchema

from pydantic_core import CoreSchema, core_schema


class _lstr(str):
    """
     A string class that supports logits and keeps track of itsorigin_trace even after mutation.
     This class is designed to be used in prompt engineering libraries where it is essential to associate
     logits with generated text and track the origin of the text.

     The `lstr` class inherits from the built-in `str` class and adds two additional attributes: `logits` and `origin_trace`.
     The `origin_trace` attribute is a frozen set of strings that represents theorigin_trace(s) of the string.

     The class provides various methods for manipulating the string, such as concatenation, slicing, splitting, and joining.
     These methods ensure that the logits andorigin_trace(s) are updated correctly based on the operation performed.

     The `lstr` class is particularly useful in LLM libraries for tracing the flow of prompts through various language model calls.
     By tracking theorigin_trace of each string, it is possible to visualize how outputs from one language model program influence
     the inputs of another, allowing for a detailed analysis of interactions between different large language models. This capability
     is crucial for understanding the propagation of prompts in complex LLM workflows and for building visual graphs that depict these interactions.

     It is important to note that any modification to the string (such as concatenation or replacement) will invalidate the associated logits.
     This is because the logits are specifically tied to the original string content, and any change would require a new computation of logits.
     The logic behind this is detailed elsewhere in this file.

     Example usage:
     ```
     # Create an lstr instance with logits and anorigin_trace
     logits = np.array([1.0, 2.0, 3.0])
    origin_trace = "4e9b7ec9"
     lstr_instance = lstr("Hello", logits,origin_trace)

     # Concatenate two lstr instances
     lstr_instance2 = lstr("World", None, "7f4d2c3a")
     concatenated_lstr = lstr_instance + lstr_instance2

     # Get the logits andorigin_trace of the concatenated lstr
     print(concatenated_lstr.logits)  # Output: None
     print(concatenated_lstr.origin_trace)  # Output: frozenset({'4e9b7ec9', '7f4d2c3a'})

     # Split the concatenated lstr into two parts
     parts = concatenated_lstr.split()
     print(parts)  # Output: [lstr('Hello', None, frozenset({'4e9b7ec9', '7f4d2c3a'})), lstr('World', None, frozenset({'4e9b7ec9', '7f4d2c3a'}))]
     ```
     Attributes:
        origin_trace (FrozenSet[str]): A frozen set of strings representing theorigin_trace(s) of the string.

     Methods:
         __new__: Create a new instance of lstr.
         __repr__: Return a string representation of the lstr instance.
         __add__: Concatenate this lstr instance with another string or lstr instance.
         __mod__: Perform a modulo operation between this lstr instance and another string, lstr, or a tuple of strings and lstrs.
         __mul__: Perform a multiplication operation between this lstr instance and an integer or another lstr.
         __rmul__: Perform a right multiplication operation between an integer or another lstr and this lstr instance.
         __getitem__: Get a slice or index of this lstr instance.
         __getattr__: Get an attribute from this lstr instance.
         join: Join a sequence of strings or lstr instances into a single lstr instance.
         split: Split this lstr instance into a list of lstr instances based on a separator.
         rsplit: Split this lstr instance into a list of lstr instances based on a separator, starting from the right.
         splitlines: Split this lstr instance into a list of lstr instances based on line breaks.
         partition: Partition this lstr instance into three lstr instances based on a separator.
         rpartition: Partition this lstr instance into three lstr instances based on a separator, starting from the right.
    """
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::2
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    def __new__(
        cls,
        content: str,
        logits: Optional[np.ndarray] = None,
        origin_trace: Optional[Union[str, FrozenSet[str]]] = None,
    ):
        """
         Create a new instance of lstr. The `logits` should be a numpy array and `origin_trace` should be a frozen set of strings or a single string.

         Args:
         content (str): The string content of the lstr.
         logits (np.ndarray, optional): The logits associated with this string. Defaults to None.
        origin_trace (Union[str, FrozenSet[str]], optional): Theorigin_trace(s) of this string. Defaults to None.
        """
        instance = super(_lstr, cls).__new__(cls, content)
        # instance._logits = logits
        if isinstance(origin_trace, str):
            instance.__origin_trace__ = frozenset({origin_trace})
        else:
            instance.__origin_trace__ = (
                frozenset(origin_trace) if origin_trace is not None else frozenset()
            )
        return instance

    # _logits: Optional[np.ndarray]
    __origin_trace__: FrozenSet[str]
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::3
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    @classmethod
    def __get_pydantic_core_schema__(
        cls, source_type: Any, handler: GetCoreSchemaHandler
    ) -> CoreSchema:
        def validate_lstr(value):
            if isinstance(value, dict) and value.get("__lstr", False):
                content = value["content"]
                origin_trace = value["__origin_trace__"].split(",")
                return cls(content, origin_trace=origin_trace)
            elif isinstance(value, str):
                return cls(value)
            elif isinstance(value, cls):
                return value
            else:
                raise ValueError(f"Invalid value for lstr: {value}")

        return core_schema.json_or_python_schema(
            json_schema=core_schema.typed_dict_schema(
                {
                    "content": core_schema.typed_dict_field(core_schema.str_schema()),
                    "__origin_trace__": core_schema.typed_dict_field(
                        core_schema.str_schema()
                    ),
                    "__lstr": core_schema.typed_dict_field(core_schema.bool_schema()),
                }
            ),
            python_schema=core_schema.union_schema(
                [
                    core_schema.is_instance_schema(cls),
                    core_schema.no_info_plain_validator_function(validate_lstr),
                ]
            ),
            serialization=core_schema.plain_serializer_function_ser_schema(
                lambda instance: {
                    "content": str(instance),
                    "__origin_trace__": (instance.__origin_trace__),
                    "__lstr": True,
                }
            ),
        )
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::4
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    @property
    def origin_trace(self) -> FrozenSet[str]:
        """
        Get theorigin_trace(s) of this lstr instance.

        Returns:
            FrozenSet[str]: A frozen set of strings representing theorigin_trace(s) of this lstr instance.
        """
        return self.__origin_trace__

    ########################
    ## Overriding methods ##
    ########################
    def __repr__(self) -> str:
        """
        Return a string representation of this lstr instance.

        Returns:
            str: A string representation of this lstr instance, including its content, logits, andorigin_trace(s).
        """
        return super().__repr__()
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::5
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    def __add__(self, other: Union[str, "_lstr"]) -> "_lstr":
        """
        Concatenate this lstr instance with another string or lstr instance.

        Args:
            other (Union[str, "lstr"]): The string or lstr instance to concatenate with this instance.

        Returns:
            lstr: A new lstr instance containing the concatenated content, with theorigin_trace(s) updated accordingly.
        """
        new_content = super(_lstr, self).__add__(other)
        self_origin = self.__origin_trace__

        if isinstance(other, _lstr):
            new_origin = self_origin
            new_origin = new_origin.union(other.__origin_trace__)
        else:
            new_origin = self_origin

        return _lstr(new_content, None, frozenset(new_origin))
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::6
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    def __mod__(
        self, other: Union[str, "_lstr", Tuple[Union[str, "_lstr"], ...]]
    ) -> "_lstr":
        """
        Perform a modulo operation between this lstr instance and another string, lstr, or a tuple of strings and lstrs,
        tracing the operation by logging the operands and the result.

        Args:
            other (Union[str, "lstr", Tuple[Union[str, "lstr"], ...]]): The right operand in the modulo operation.

        Returns:
            lstr: A new lstr instance containing the result of the modulo operation, with theorigin_trace(s) updated accordingly.
        """
        # If 'other' is a tuple, we need to handle each element
        if isinstance(other, tuple):
            result_content = super(_lstr, self).__mod__(tuple(str(o) for o in other))
            new__origin_trace__s = set(self.__origin_trace__)
            for item in other:
                if isinstance(item, _lstr):
                    new__origin_trace__s.update(item.__origin_trace__)
            new__origin_trace__ = frozenset(new__origin_trace__s)
        else:
            result_content = super(_lstr, self).__mod__(other)
            if isinstance(other, _lstr):
                new__origin_trace__ = self.__origin_trace__.union(
                    other.__origin_trace__
                )
            else:
                new__origin_trace__ = self.__origin_trace__

        return _lstr(result_content, None, new__origin_trace__)
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::7
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    def __mul__(self, other: SupportsIndex) -> "_lstr":
        """
        Perform a multiplication operation between this lstr instance and an integer or another lstr,
        tracing the operation by logging the operands and the result.

        Args:
            other (Union[SupportsIndex, "lstr"]): The right operand in the multiplication operation.

        Returns:
            lstr: A new lstr instance containing the result of the multiplication operation, with theorigin_trace(s) updated accordingly.
        """
        if isinstance(other, SupportsIndex):
            result_content = super(_lstr, self).__mul__(other)
            new__origin_trace__ = self.__origin_trace__
        else:
            return NotImplemented

        return _lstr(result_content, None, new__origin_trace__)
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::8
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    def __rmul__(self, other: SupportsIndex) -> "_lstr":
        """
        Perform a right multiplication operation between an integer or another lstr and this lstr instance,
        tracing the operation by logging the operands and the result.

        Args:
            other (Union[SupportsIndex, "lstr"]): The left operand in the multiplication operation.

        Returns:
            lstr: A new lstr instance containing the result of the multiplication operation, with theorigin_trace(s) updated accordingly.
        """
        return self.__mul__(other)  # Multiplication is commutative in this context

--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::9
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    def __getitem__(self, key: Union[SupportsIndex, slice]) -> "_lstr":
        """
        Get a slice or index of this lstr instance.

        Args:
            key (Union[SupportsIndex, slice]): The index or slice to retrieve.

        Returns:
            lstr: A new lstr instance containing the sliced or indexed content, with theorigin_trace(s) preserved.
        """
        result = super(_lstr, self).__getitem__(key)
        # This is a matter of opinon. I believe that when you Index into a language model output, you or divorcing the lodges of the indexed result from their contacts which produce them. Therefore, it is only reasonable to directly index into the lodges without changing the original context, and so any mutation on the string should invalidate the logits.
        # try:
        #     logit_subset = self._logits[key] if self._logits else None
        # except:
        #   logit_subset = None
        logit_subset = None
        return _lstr(result, logit_subset, self.__origin_trace__)
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::10
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    def __getattribute__(self, name: str) -> Union[Callable, Any]:
        """
        Get an attribute from this lstr instance.

        Args:
            name (str): The name of the attribute to retrieve.

        Returns:
            Union[Callable, Any]: The requested attribute, which may be a method or a value.
        """
        # Get the attribute from the superclass (str)
        # First, try to get the attribute from the current class instance

        # Get the attribute using the superclass method
        attr = super().__getattribute__(name)

        # Check if the attribute is a callable and not defined in lstr class itself

        if name == "__class__":
            return type(self)

        if callable(attr) and name not in _lstr.__dict__:
            def wrapped(*args: Any, **kwargs: Any) -> Any:
                result = attr(*args, **kwargs)
                # If the result is a string, return an lstr instance
                if isinstance(result, str):
                    origin_traces = self.__origin_trace__
                    for arg in args:
                        if isinstance(arg, _lstr):
                            origin_traces = origin_traces.union(arg.__origin_trace__)
                    for key, value in kwargs.items():
                        if isinstance(value, _lstr):
                            origin_traces = origin_traces.union(value.__origin_trace__)
                    return _lstr(result, None, origin_traces)

                return result

            return wrapped

        return attr
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::11
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    @override
    def join(self, iterable: Iterable[Union[str, "_lstr"]]) -> "_lstr":
        """
        Join a sequence of strings or lstr instances into a single lstr instance.

        Args:
            iterable (Iterable[Union[str, "lstr"]]): The sequence of strings or lstr instances to join.

        Returns:
            lstr: A new lstr instance containing the joined content, with theorigin_trace(s) updated accordingly.
        """
        new__origin_trace__ = self.__origin_trace__
        parts = []
        for item in iterable:
            if isinstance(item, _lstr):
                new__origin_trace__ = new__origin_trace__.union(item.__origin_trace__)
            parts.append(item)
        new_content = super(_lstr, self).join(parts)

        return _lstr(new_content, None, new__origin_trace__)
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::12
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    @override
    def split(
        self, sep: Optional[Union[str, "_lstr"]] = None, maxsplit: SupportsIndex = -1
    ) -> List["_lstr"]:
        """
        Split this lstr instance into a list of lstr instances based on a separator.

        Args:
            sep (Optional[Union[str, "lstr"]], optional): The separator to split on. Defaults to None.
            maxsplit (SupportsIndex, optional): The maximum number of splits to perform. Defaults to -1.

        Returns:
            List["lstr"]: A list of lstr instances containing the split content, with theorigin_trace(s) preserved.
        """
        return self._split_helper(super(_lstr, self).split, sep, maxsplit)
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::13
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    @override
    def rsplit(
        self, sep: Optional[Union[str, "_lstr"]] = None, maxsplit: SupportsIndex = -1
    ) -> List["_lstr"]:
        """
        Split this lstr instance into a list of lstr instances based on a separator, starting from the right.

        Args:
            sep (Optional[Union[str, "lstr"]], optional): The separator to split on. Defaults to None.
            maxsplit (SupportsIndex, optional): The maximum number of splits to perform. Defaults to -1.

        Returns:
            List["lstr"]: A list of lstr instances containing the split content, with theorigin_trace(s) preserved.
        """
        return self._split_helper(super(_lstr, self).rsplit, sep, maxsplit)
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::14
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    @override
    def splitlines(self, keepends: bool = False) -> List["_lstr"]:
        """
        Split this lstr instance into a list of lstr instances based on line breaks.

        Args:
            keepends (bool, optional): Whether to include the line breaks in the resulting lstr instances. Defaults to False.

        Returns:
            List["lstr"]: A list of lstr instances containing the split content, with theorigin_trace(s) preserved.
        """
        return [
            _lstr(p, None, self.__origin_trace__)
            for p in super(_lstr, self).splitlines(keepends=keepends)
        ]
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::15
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    @override
    def partition(self, sep: Union[str, "_lstr"]) -> Tuple["_lstr", "_lstr", "_lstr"]:
        """
        Partition this lstr instance into three lstr instances based on a separator.

        Args:
            sep (Union[str, "lstr"]): The separator to partition on.

        Returns:
            Tuple["lstr", "lstr", "lstr"]: A tuple of three lstr instances containing the content before the separator, the separator itself, and the content after the separator, with theorigin_trace(s) updated accordingly.
        """
        return self._partition_helper(super(_lstr, self).partition, sep)
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::16
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    @override
    def rpartition(self, sep: Union[str, "_lstr"]) -> Tuple["_lstr", "_lstr", "_lstr"]:
        """
        Partition this lstr instance into three lstr instances based on a separator, starting from the right.

        Args:
            sep (Union[str, "lstr"]): The separator to partition on.

        Returns:
            Tuple["lstr", "lstr", "lstr"]: A tuple of three lstr instances containing the content before the separator, the separator itself, and the content after the separator, with theorigin_trace(s) updated accordingly.
        """
        return self._partition_helper(super(_lstr, self).rpartition, sep)
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::17
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    def _partition_helper(
        self, method, sep: Union[str, "_lstr"]
    ) -> Tuple["_lstr", "_lstr", "_lstr"]:
        """
        Helper method for partitioning this lstr instance based on a separator.

        Args:
            method (Callable): The partitioning method to use (either partition or rpartition).
            sep (Union[str, "lstr"]): The separator to partition on.

        Returns:
            Tuple["lstr", "lstr", "lstr"]: A tuple of three lstr instances containing the content before the separator, the separator itself, and the content after the separator, with theorigin_trace(s) updated accordingly.
        """
        part1, part2, part3 = method(sep)
        new__origin_trace__ = (
            self.__origin_trace__ | sep.__origin_trace__
            if isinstance(sep, _lstr)
            else self.__origin_trace__
        )
        return (
            _lstr(part1, None, new__origin_trace__),
            _lstr(part2, None, new__origin_trace__),
            _lstr(part3, None, new__origin_trace__),
        )
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::18
Filepath: src\ell\types\_lstr.py
Content:
class _lstr(str):

    def _split_helper(
        self,
        method,
        sep: Optional[Union[str, "_lstr"]] = None,
        maxsplit: SupportsIndex = -1,
    ) -> List["_lstr"]:
        """
        Helper method for splitting this lstr instance based on a separator.

        Args:
            method (Callable): The splitting method to use (either split or rsplit).
            sep (Optional[Union[str, "lstr"]], optional): The separator to split on. Defaults to None.
            maxsplit (SupportsIndex, optional): The maximum number of splits to perform. Defaults to -1.

        Returns:
            List["lstr"]: A list of lstr instances containing the split content, with theorigin_trace(s) preserved.
        """
        origin_traces = (
            self.__origin_trace__ | sep.__origin_trace__
            if isinstance(sep, _lstr)
            else self.__origin_trace__
        )
        parts = method(sep, maxsplit)
        return [_lstr(part, None, origin_traces) for part in parts]
--------------------------------------------------------------------------------
Chunk ID: types\_lstr.py::19
Filepath: src\ell\types\_lstr.py
Content:
if __name__ == "__main__":
    import timeit
    import random
    import string

    def generate_random_string(length):
        return "".join(random.choices(string.ascii_letters + string.digits, k=length))

    def test_concatenation():
        s1 = generate_random_string(1000)
        s2 = generate_random_string(1000)

        lstr_time = timeit.timeit(lambda: _lstr(s1) + _lstr(s2), number=10000)
        str_time = timeit.timeit(lambda: s1 + s2, number=10000)

        print(f"Concatenation: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    def test_slicing():
        s = generate_random_string(10000)
        ls = _lstr(s)

        lstr_time = timeit.timeit(lambda: ls[1000:2000], number=10000)
        str_time = timeit.timeit(lambda: s[1000:2000], number=10000)

        print(f"Slicing: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    def test_splitting():
        s = generate_random_string(10000)
        ls = _lstr(s)

        lstr_time = timeit.timeit(lambda: ls.split(), number=1000)
        str_time = timeit.timeit(lambda: s.split(), number=1000)

        print(f"Splitting: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    def test_joining():
        words = [generate_random_string(10) for _ in range(1000)]
        lwords = [_lstr(word) for word in words]

        lstr_time = timeit.timeit(lambda: _lstr(" ").join(lwords), number=1000)
        str_time = timeit.timeit(lambda: " ".join(words), number=1000)

        print(f"Joining: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    print("Running performance tests...")
    test_concatenation()
    test_slicing()
    test_splitting()
    test_joining()

    import cProfile
    import pstats
    from io import StringIO

    def test_add():
        s1 = generate_random_string(1000)
        s2 = generate_random_string(1000)
        ls1 = _lstr(s1, None, "origin1")
        ls2 = _lstr(s2, None, "origin2")

        for _ in range(100000):
            result = ls1 + ls2

    print("\nProfiling __add__ method:")
    profiler = cProfile.Profile()
    profiler.enable()
    test_add()
    profiler.disable()

    s = StringIO()
    ps = pstats.Stats(profiler, stream=s).sort_stats("cumulative")
    ps.print_stats(20)  # Print top 20 lines
    print(s.getvalue())
--------------------------------------------------------------------------------
Chunk ID: types\message.py::1
Filepath: src\ell\types\message.py
Content:
# todo: implement tracing for structured outs. this a v2 feature.
import json
from ell.types._lstr import _lstr
from functools import cached_property
import numpy as np
import base64
from io import BytesIO
from PIL import Image as PILImage

from pydantic import BaseModel, ConfigDict, model_validator, field_serializer
from sqlmodel import Field

from concurrent.futures import ThreadPoolExecutor, as_completed

from typing import Any, Callable, Dict, List, Optional, Union

from ell.util.serialization import serialize_image
_lstr_generic = Union[_lstr, str]
InvocableTool = Callable[..., Union["ToolResult", _lstr_generic, List["ContentBlock"], ]]

# AnyContent represents any type that can be passed to Message.
AnyContent = Union["ContentBlock", str, "ToolCall", "ToolResult", "ImageContent", np.ndarray, PILImage.Image, BaseModel]
--------------------------------------------------------------------------------
Chunk ID: types\message.py::2
Filepath: src\ell\types\message.py
Content:
class ToolResult(BaseModel):
    tool_call_id: _lstr_generic
    result: List["ContentBlock"]

    @property
    def text(self) -> str:
        return _content_to_text(self.result)

    @property
    def text_only(self) -> str:
        return _content_to_text_only(self.result)

    # # XXX: Possibly deprecate
    # def readable_repr(self) -> str:
    #     return f"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"

    def __repr__(self):
        return f"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"
--------------------------------------------------------------------------------
Chunk ID: types\message.py::3
Filepath: src\ell\types\message.py
Content:
class ToolCall(BaseModel):
    tool : InvocableTool
    tool_call_id : Optional[_lstr_generic] = Field(default=None)
    params : BaseModel

    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):
        if not isinstance(params, BaseModel):
            params = tool.__ell_params_model__(**params) #convenience.
        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)

    def __call__(self, **kwargs):
        assert not kwargs, "Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall."

        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
        return self.tool(**self.params.model_dump())

    # XXX: Deprecate in 0.1.0
    def call_and_collect_as_message_block(self):
        raise DeprecationWarning("call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.")

    def call_and_collect_as_content_block(self):
        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)
        return ContentBlock(tool_result=res)

    def call_and_collect_as_message(self):
        return Message(role="user", content=[self.call_and_collect_as_message_block()])

    def __repr__(self):
        return f"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id='{self.tool_call_id}')"
--------------------------------------------------------------------------------
Chunk ID: types\message.py::4
Filepath: src\ell\types\message.py
Content:
class ImageContent(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    image: Optional[PILImage.Image] = Field(default=None)
    url: Optional[str] = Field(default=None)
    detail: Optional[str] = Field(default=None)

    @model_validator(mode='after')
    def check_image_or_url(self):
        if self.image is not None and self.url is not None:
            raise ValueError("Both 'image' and 'url' cannot be set simultaneously.")
        if self.image is None and self.url is None:
            raise ValueError("Either 'image' or 'url' must be set.")
        return self
--------------------------------------------------------------------------------
Chunk ID: types\message.py::5
Filepath: src\ell\types\message.py
Content:
class ImageContent(BaseModel):

    @classmethod
    def coerce(cls, value: Union[str, np.ndarray, PILImage.Image, "ImageContent"]):
        if isinstance(value, cls):
            return value

        if isinstance(value, str):
            if value.startswith('http://') or value.startswith('https://'):
                return cls(url=value)
            try:
                img_data = base64.b64decode(value)
                img = PILImage.open(BytesIO(img_data))
                if img.mode not in ('L', 'RGB', 'RGBA'):
                    return cls(image=img.convert('RGB'))
            except:
                raise ValueError("Invalid base64 string or URL for image")

        if isinstance(value, np.ndarray):
            if value.ndim == 3 and value.shape[2] in (3, 4):
                mode = 'RGB' if value.shape[2] == 3 else 'RGBA'
                return cls(image=PILImage.fromarray(value, mode=mode))
            else:
                raise ValueError(f"Invalid numpy array shape for image: {value.shape}. Expected 3D array with 3 or 4 channels.")

        if isinstance(value, PILImage.Image):
            if value.mode not in ('L', 'RGB', 'RGBA'):
                value = value.convert('RGB')
            return cls(image=value)

        raise ValueError(f"Invalid image type: {type(value)}")

    @field_serializer('image')
    def serialize_image(self, image: Optional[PILImage.Image], _info):
        if image is None:
            return None
        return serialize_image(image)
--------------------------------------------------------------------------------
Chunk ID: types\message.py::6
Filepath: src\ell\types\message.py
Content:
class ContentBlock(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    text: Optional[_lstr_generic] = Field(default=None)
    image: Optional[ImageContent] = Field(default=None)
    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)
    tool_call: Optional[ToolCall] = Field(default=None)
    parsed: Optional[BaseModel] = Field(default=None)
    tool_result: Optional[ToolResult] = Field(default=None)
    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.
    # This breaks us maintaing parity with the openai python client in some sen but so does image.

    def __init__(self, *args, **kwargs):
        if "image" in kwargs and not isinstance(kwargs["image"], ImageContent):
            im = kwargs["image"] = ImageContent.coerce(kwargs["image"])
            # XXX: Backwards compatibility, Deprecate.
            if (d := kwargs.get("image_detail", None)): im.detail = d

        super().__init__(*args, **kwargs)


    @model_validator(mode='after')
    def check_single_non_null(self):
        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]
        if len(non_null_fields) > 1:
            raise ValueError(f"Only one field can be non-null. Found: {', '.join(non_null_fields)}")
        return self

    def __str__(self):
        return repr(self)

    def __repr__(self):
        non_null_fields = [f"{field}={value}" for field, value in self.__dict__.items() if value is not None]
        return f"ContentBlock({', '.join(non_null_fields)})"

    @property
    def type(self):
        if self.text is not None:
            return "text"
        if self.image is not None:
            return "image"
        if self.audio is not None:
            return "audio"
        if self.tool_call is not None:
            return "tool_call"
        if self.parsed is not None:
            return "parsed"
        if self.tool_result is not None:
            return "tool_result"
        return None

    @property
    def content(self):
        return getattr(self, self.type)
--------------------------------------------------------------------------------
Chunk ID: types\message.py::7
Filepath: src\ell\types\message.py
Content:
class ContentBlock(BaseModel):

    @classmethod
    def coerce(cls, content: AnyContent) -> "ContentBlock":
        """
        Coerce various types of content into a ContentBlock.

        This method provides a flexible way to create ContentBlock instances from different types of input.

        Args:
        content: The content to be coerced into a ContentBlock. Can be one of the following types:
        - str: Will be converted to a text ContentBlock.
        - ToolCall: Will be converted to a tool_call ContentBlock.
        - ToolResult: Will be converted to a tool_result ContentBlock.
        - BaseModel: Will be converted to a parsed ContentBlock.
        - ContentBlock: Will be returned as-is.
        - Image: Will be converted to an image ContentBlock.
        - np.ndarray: Will be converted to an image ContentBlock.
        - PILImage.Image: Will be converted to an image ContentBlock.

        Returns:
        ContentBlock: A new ContentBlock instance containing the coerced content.

        Raises:
        ValueError: If the content cannot be coerced into a valid ContentBlock.

        Examples:
        >>> ContentBlock.coerce("Hello, world!")
        ContentBlock(text="Hello, world!")

        >>> tool_call = ToolCall(...)
        >>> ContentBlock.coerce(tool_call)
        ContentBlock(tool_call=tool_call)

        >>> tool_result = ToolResult(...)
        >>> ContentBlock.coerce(tool_result)
        ContentBlock(tool_result=tool_result)

        >>> class MyModel(BaseModel):
        ...     field: str
        >>> model_instance = MyModel(field="value")
        >>> ContentBlock.coerce(model_instance)
        ContentBlock(parsed=model_instance)

        >>> from PIL import Image as PILImage
        >>> img = PILImage.new('RGB', (100, 100))
        >>> ContentBlock.coerce(img)
        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))

        >>> import numpy as np
        >>> arr = np.random.rand(100, 100, 3)
        >>> ContentBlock.coerce(arr)
        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))

        >>> image = Image(url="https://example.com/image.jpg")
        >>> ContentBlock.coerce(image)
        ContentBlock(image=ImageContent(url="https://example.com/image.jpg"))

        Notes:
        - This method is particularly useful when working with heterogeneous content types
          and you want to ensure they are all properly encapsulated in ContentBlock instances.
        - The method performs type checking and appropriate conversions to ensure the resulting
          ContentBlock is valid according to the model's constraints.
        - For image content, Image objects, PIL Image objects, and numpy arrays are supported,
          with automatic conversion to the appropriate format.
        - As a last resort, the method will attempt to create an image from the input before
          raising a ValueError.
        """
        if isinstance(content, ContentBlock):
            return content
        if isinstance(content, str):
            return cls(text=content)
        if isinstance(content, ToolCall):
            return cls(tool_call=content)
        if isinstance(content, ToolResult):
            return cls(tool_result=content)
        if isinstance(content, (ImageContent, np.ndarray, PILImage.Image)):
            return cls(image=ImageContent.coerce(content))
        if isinstance(content, BaseModel):
            return cls(parsed=content)

        raise ValueError(f"Invalid content type: {type(content)}")

    @field_serializer('parsed')
    def serialize_parsed(self, value: Optional[BaseModel], _info):
        if value is None:
            return None
        return value.model_dump(exclude_none=True, exclude_unset=True)
--------------------------------------------------------------------------------
Chunk ID: types\message.py::8
Filepath: src\ell\types\message.py
Content:
def to_content_blocks(
    content: Optional[Union[AnyContent, List[AnyContent]]] = None,
    **content_block_kwargs
) -> List[ContentBlock]:
    """
    Coerce a variety of input types into a list of ContentBlock objects.

    Args:
    content: The content to be coerced. Can be a single item or a list of items.
             Supported types include str, ContentBlock, ToolCall, ToolResult, BaseModel, Image, np.ndarray, and PILImage.Image.
    **content_block_kwargs: Additional keyword arguments to pass to ContentBlock creation if content is None.

    Returns:
    List[ContentBlock]: A list of ContentBlock objects created from the input content.

    Examples:
    >>> coerce_content_list("Hello")
    [ContentBlock(text="Hello")]

    >>> coerce_content_list([ContentBlock(text="Hello"), "World"])
    [ContentBlock(text="Hello"), ContentBlock(text="World")]

    >>> from PIL import Image as PILImage
    >>> pil_image = PILImage.new('RGB', (100, 100))
    >>> coerce_content_list(pil_image)
    [ContentBlock(image=Image(image=<PIL.Image.Image object>))]

    >>> coerce_content_list(Image(url="https://example.com/image.jpg"))
    [ContentBlock(image=Image(url="https://example.com/image.jpg"))]

    >>> coerce_content_list(None, text="Default text")
    [ContentBlock(text="Default text")]
    """
    if content is None:
        return [ContentBlock(**content_block_kwargs)]

    if not isinstance(content, list):
        content = [content]

    return [ContentBlock.model_validate(ContentBlock.coerce(c)) for c in content]
--------------------------------------------------------------------------------
Chunk ID: types\message.py::9
Filepath: src\ell\types\message.py
Content:
class Message(BaseModel):
    role: str
    content: List[ContentBlock]


    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):
        content_blocks = to_content_blocks(content, **content_block_kwargs)

        super().__init__(role=role, content=content_blocks)

    # XXX: This choice of naming is unfortunate, but it is what it is.
    @property
    def text(self) -> str:
        """Returns all text content, replacing non-text content with their representations.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text
            'Hello\\n<PilImage>\\nWorld'
        """
        return _content_to_text(self.content)
--------------------------------------------------------------------------------
Chunk ID: types\message.py::10
Filepath: src\ell\types\message.py
Content:
class Message(BaseModel):

    @property
    def images(self) -> List[ImageContent]:
        """Returns a list of all image content.

        Example:
            >>> from PIL import Image as PILImage
            >>> image1 = Image(url="https://example.com/image.jpg")
            >>> image2 = Image(image=PILImage.new('RGB', (200, 200)))
            >>> message = Message(role="user", content=["Text", image1, "More text", image2])
            >>> len(message.images)
            2
            >>> isinstance(message.images[0], Image)
            True
            >>> message.images[0].url
            'https://example.com/image.jpg'
            >>> isinstance(message.images[1].image, PILImage.Image)
            True
        """
        return [c.image for c in self.content if c.image]
--------------------------------------------------------------------------------
Chunk ID: types\message.py::11
Filepath: src\ell\types\message.py
Content:
class Message(BaseModel):

    @property
    def audios(self) -> List[Union[np.ndarray, List[float]]]:
        """Returns a list of all audio content.

        Example:
            >>> audio1 = np.array([0.1, 0.2, 0.3])
            >>> audio2 = np.array([0.4, 0.5, 0.6])
            >>> message = Message(role="user", content=["Text", audio1, "More text", audio2])
            >>> len(message.audios)
            2
        """
        return [c.audio for c in self.content if c.audio]
--------------------------------------------------------------------------------
Chunk ID: types\message.py::12
Filepath: src\ell\types\message.py
Content:
class Message(BaseModel):

    @property
    def text_only(self) -> str:
        """Returns only the text content, ignoring non-text content.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text_only
            'Hello\\nWorld'
        """
        return _content_to_text_only(self.content)

    @cached_property
    def tool_calls(self) -> List[ToolCall]:
        """Returns a list of all tool calls.

        Example:
            >>> tool_call = ToolCall(tool=lambda x: x, params=BaseModel())
            >>> message = Message(role="user", content=["Text", tool_call])
            >>> len(message.tool_calls)
            1
        """
        return [c.tool_call for c in self.content if c.tool_call is not None]

    @property
    def tool_results(self) -> List[ToolResult]:
        """Returns a list of all tool results.

        Example:
            >>> tool_result = ToolResult(tool_call_id="123", result=[ContentBlock(text="Result")])
            >>> message = Message(role="user", content=["Text", tool_result])
            >>> len(message.tool_results)
            1
        """
        return [c.tool_result for c in self.content if c.tool_result is not None]
--------------------------------------------------------------------------------
Chunk ID: types\message.py::13
Filepath: src\ell\types\message.py
Content:
class Message(BaseModel):

    @property
    def parsed(self) -> Union[BaseModel, List[BaseModel]]:
        """Returns a list of all parsed content.

        Example:
            >>> class CustomModel(BaseModel):
            ...     value: int
            >>> parsed_content = CustomModel(value=42)
            >>> message = Message(role="user", content=["Text", ContentBlock(parsed=parsed_content)])
            >>> len(message.parsed)
            1
        """
        parsed_content = [c.parsed for c in self.content if c.parsed is not None]
        return parsed_content[0] if len(parsed_content) == 1 else parsed_content
--------------------------------------------------------------------------------
Chunk ID: types\message.py::14
Filepath: src\ell\types\message.py
Content:
class Message(BaseModel):

    def call_tools_and_collect_as_message(self, parallel=False, max_workers=None):
        if parallel:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(c.tool_call.call_and_collect_as_content_block) for c in self.content if c.tool_call]
                content = [future.result() for future in as_completed(futures)]
        else:
            content = [c.tool_call.call_and_collect_as_content_block() for c in self.content if c.tool_call]
        return Message(role="user", content=content)
--------------------------------------------------------------------------------
Chunk ID: types\message.py::15
Filepath: src\ell\types\message.py
Content:
# HELPERS 
def system(content: Union[AnyContent, List[AnyContent]]) -> Message:
    """
    Create a system message with the given content.

    Args:
    content (str): The content of the system message.

    Returns:
    Message: A Message object with role set to 'system' and the provided content.
    """
    return Message(role="system", content=content)


def user(content: Union[AnyContent, List[AnyContent]]) -> Message:
    """
    Create a user message with the given content.

    Args:
    content (str): The content of the user message.

    Returns:
    Message: A Message object with role set to 'user' and the provided content.
    """
    return Message(role="user", content=content)


def assistant(content: Union[AnyContent, List[AnyContent]]) -> Message:
    """
    Create an assistant message with the given content.

    Args:
    content (str): The content of the assistant message.

    Returns:
    Message: A Message object with role set to 'assistant' and the provided content.
    """
    return Message(role="assistant", content=content)

#XXX: Make a mixi for these properties.
def _content_to_text_only(content: List[ContentBlock]) -> str:
    return _lstr("\n").join(
            available_text
            for c in content
            if (available_text := (c.tool_result.text_only if c.tool_result else c.text))
        )

# Do we include the .text of a tool result? or its repr as in the current implementaiton?
# What is the user using .text for? I just want to see the result of the tools. text_only should get us the text of the tool results; the tool_call_id is irrelevant.
def _content_to_text(content: List[ContentBlock]) -> str:
    return _lstr("\n").join(
            available_text
            for c in content
            if (available_text :=  c.text or repr(c.content))
        )


# want to enable a use case where the user can actually return a standrd oai chat format
# This is a placehodler will likely come back later for this
LMPParams = Dict[str, Any]
# Well this is disappointing, I wanted to effectively type hint by doign that data sync meta, but eh, at elast we can still reference role or content this way. Probably wil lcan the dict sync meta. TypedDict is the ticket ell oh ell.
MessageOrDict = Union[Message, Dict[str, str]]
# Can support iamge prompts later.
Chat = List[
    Message
]  # [{"role": "system", "content": "prompt"}, {"role": "user", "content": "message"}]
MultiTurnLMP = Callable[..., Chat]
OneTurn = Callable[..., _lstr_generic]
# This is the specific LMP that must accept history as an argument and can take any additional arguments
ChatLMP = Callable[[Chat, Any], Chat]
LMP = Union[OneTurn, MultiTurnLMP, ChatLMP]
InvocableLM = Callable[..., _lstr_generic]
--------------------------------------------------------------------------------
Chunk ID: types\studio.py::1
Filepath: src\ell\types\studio.py
Content:
from datetime import datetime, timezone
import enum
from functools import cached_property

import sqlalchemy.types as types

from ell.types.message import Any, Any, Field, Message, Optional

from sqlmodel import Column, Field, SQLModel
from typing import Optional
from dataclasses import dataclass
from typing import Dict, List, Literal, Union, Any, Optional

from pydantic import BaseModel, field_validator

from datetime import datetime
from typing import Any, List, Optional
from sqlmodel import Field, SQLModel, Relationship, JSON, Column
from sqlalchemy import Index, func

from typing import TypeVar, Any

def utc_now() -> datetime:
    """
    Returns the current UTC timestamp.
    Serializes to ISO-8601.
    """
    return datetime.now(tz=timezone.utc)
--------------------------------------------------------------------------------
Chunk ID: types\studio.py::2
Filepath: src\ell\types\studio.py
Content:
class SerializedLMPUses(SQLModel, table=True):
    """
    Represents the many-to-many relationship between SerializedLMPs.

    This class is used to track which LMPs use or are used by other LMPs.
    """

    lmp_user_id: Optional[str] = Field(default=None, foreign_key="serializedlmp.lmp_id", primary_key=True, index=True)  # ID of the LMP that is being used
    lmp_using_id: Optional[str] = Field(default=None, foreign_key="serializedlmp.lmp_id", primary_key=True, index=True)  # ID of the LMP that is using the other LMP

--------------------------------------------------------------------------------
Chunk ID: types\studio.py::3
Filepath: src\ell\types\studio.py
Content:
class UTCTimestamp(types.TypeDecorator[datetime]):
    cache_ok = True
    impl = types.TIMESTAMP
    def process_result_value(self, value: datetime, dialect:Any):
        return value.replace(tzinfo=timezone.utc)


def UTCTimestampField(index:bool=False, **kwargs:Any):
    return Field(
        sa_column=Column(UTCTimestamp(timezone=True), index=index, **kwargs))


class LMPType(str, enum.Enum):
    LM = "LM"
    TOOL = "TOOL"
    MULTIMODAL = "MULTIMODAL"
    OTHER = "OTHER"
--------------------------------------------------------------------------------
Chunk ID: types\studio.py::4
Filepath: src\ell\types\studio.py
Content:
class SerializedLMPBase(SQLModel):
    lmp_id: Optional[str] = Field(default=None, primary_key=True)
    name: str = Field(index=True)
    source: str
    dependencies: str
    created_at: datetime = UTCTimestampField(index=True, nullable=False)

    lmp_type: LMPType
    api_params: Optional[Dict[str, Any]] = Field(default_factory=dict, sa_column=Column(JSON))
    initial_free_vars: Optional[Dict[str, Any]] = Field(default_factory=dict, sa_column=Column(JSON))
    initial_global_vars: Optional[Dict[str, Any]] = Field(default_factory=dict, sa_column=Column(JSON))
    num_invocations: Optional[int] = Field(default=0)
    commit_message: Optional[str] = Field(default=None)
    version_number: Optional[int] = Field(default=None)
--------------------------------------------------------------------------------
Chunk ID: types\studio.py::5
Filepath: src\ell\types\studio.py
Content:
class SerializedLMP(SerializedLMPBase, table=True):
    invocations: List["Invocation"] = Relationship(back_populates="lmp")
    used_by: Optional[List["SerializedLMP"]] = Relationship(
        back_populates="uses",
        link_model=SerializedLMPUses,
        sa_relationship_kwargs=dict(
            primaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id",
            secondaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id",
        ),
    )
    uses: List["SerializedLMP"] = Relationship(
        back_populates="used_by",
        link_model=SerializedLMPUses,
        sa_relationship_kwargs=dict(
            primaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id",
            secondaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id",
        ),
    )

    class Config:
        table_name = "serializedlmp"
        unique_together = [("version_number", "name")]
--------------------------------------------------------------------------------
Chunk ID: types\studio.py::6
Filepath: src\ell\types\studio.py
Content:
class InvocationTrace(SQLModel, table=True):
    invocation_consumer_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)
    invocation_consuming_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)

# Should be subtyped for differnet kidns of LMPS.
# XXX: Move all ofh te binary data out to a different table.
# XXX: Need a flag that says dont store images.
# XXX: Deprecate the args columns
class InvocationBase(SQLModel):
    id: Optional[str] = Field(default=None, primary_key=True)
    lmp_id: str = Field(foreign_key="serializedlmp.lmp_id", index=True)
    latency_ms: float
    prompt_tokens: Optional[int] = Field(default=None)
    completion_tokens: Optional[int] = Field(default=None)
    state_cache_key: Optional[str] = Field(default=None)
    created_at: datetime = UTCTimestampField(default=func.now(), nullable=False)
    used_by_id: Optional[str] = Field(default=None, foreign_key="invocation.id", index=True)
    # global_vars and free_vars removed from here

--------------------------------------------------------------------------------
Chunk ID: types\studio.py::7
Filepath: src\ell\types\studio.py
Content:
class InvocationContentsBase(SQLModel):
    invocation_id: str = Field(foreign_key="invocation.id", index=True, primary_key=True)
    params: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSON))
    results: Optional[Union[List[Message], Any]] = Field(default=None, sa_column=Column(JSON))
    invocation_api_params: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSON))
    global_vars: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSON))
    free_vars: Optional[Dict[str, Any]] = Field(default=None, sa_column=Column(JSON))
    is_external : bool = Field(default=False)
--------------------------------------------------------------------------------
Chunk ID: types\studio.py::8
Filepath: src\ell\types\studio.py
Content:
class InvocationContentsBase(SQLModel):

    @cached_property
    def should_externalize(self) -> bool:
        import json

        json_fields = [
            self.params,
            self.results,
            self.invocation_api_params,
            self.global_vars,
            self.free_vars
        ]

        total_size = sum(
            len(json.dumps(field, default=(lambda x: json.dumps(x.model_dump(), default=str, ensure_ascii=False)
                                           if isinstance(x, BaseModel) else str(x)), ensure_ascii=False).encode('utf-8'))
            for field in json_fields if field is not None
        )
        # print("total_size", total_size)

        return total_size > 102400  # Precisely 100kb in bytes

class InvocationContents(InvocationContentsBase, table=True):
    invocation: "Invocation" = Relationship(back_populates="contents")
--------------------------------------------------------------------------------
Chunk ID: types\studio.py::9
Filepath: src\ell\types\studio.py
Content:
class Invocation(InvocationBase, table=True):
    lmp: SerializedLMP = Relationship(back_populates="invocations")
    consumed_by: List["Invocation"] = Relationship(
        back_populates="consumes",
        link_model=InvocationTrace,
        sa_relationship_kwargs=dict(
            primaryjoin="Invocation.id==InvocationTrace.invocation_consumer_id",
            secondaryjoin="Invocation.id==InvocationTrace.invocation_consuming_id",
        ),
    )
    consumes: List["Invocation"] = Relationship(
        back_populates="consumed_by",
        link_model=InvocationTrace,
        sa_relationship_kwargs=dict(
            primaryjoin="Invocation.id==InvocationTrace.invocation_consuming_id",
            secondaryjoin="Invocation.id==InvocationTrace.invocation_consumer_id",
        ),
    )
    used_by: Optional["Invocation"] = Relationship(back_populates="uses", sa_relationship_kwargs={"remote_side": "Invocation.id"})
    uses: List["Invocation"] = Relationship(back_populates="used_by")
    contents: InvocationContents = Relationship(back_populates="invocation")

    __table_args__ = (
        Index('ix_invocation_lmp_id_created_at', 'lmp_id', 'created_at'),
        Index('ix_invocation_created_at_latency_ms', 'created_at', 'latency_ms'),
        Index('ix_invocation_created_at_tokens', 'created_at', 'prompt_tokens', 'completion_tokens'),
    )
--------------------------------------------------------------------------------
Chunk ID: util\__init__.py::1
Filepath: src\ell\util\__init__.py
Content:
from .closure import lexically_closured_source
--------------------------------------------------------------------------------
Chunk ID: util\_warnings.py::1
Filepath: src\ell\util\_warnings.py
Content:
from typing import Any, Optional
from colorama import Fore, Style

from ell.configurator import config
import logging
logger = logging.getLogger(__name__)


def _no_api_key_warning(model, client_to_use : Optional[Any], name = None,  long=False, error=False):
    color = Fore.RED if error else Fore.LIGHTYELLOW_EX
    prefix = "ERROR" if error else "WARNING"
    # openai default
    client_to_use_name = client_to_use.__class__.__name__ if (client_to_use) else "OpenAI"
    client_to_use_module = client_to_use.__class__.__module__ if (client_to_use) else "openai"
    lmp_name = f"used by LMP `{name}` " if name else ""
    return f"""{color}{prefix}: No API key found for model `{model}` {lmp_name}using client `{client_to_use_name}`""" + (f""".

To fix this:
* Set your API key in the appropriate environment variable for your chosen provider
* Or, specify a client explicitly in the decorator:
    ```
    import ell
    from {client_to_use_module} import {client_to_use_name}
                                
    @ell.simple(model="{model}", client={client_to_use_name}(api_key=your_api_key))
    def your_lmp_name(...):
        ...
    ```
* Or explicitly specify the client when calling the LMP:

    ```
    your_lmp_name(..., client={client_to_use_name}(api_key=your_api_key))
    ```
""" if long else " at time of definition. Can be okay if custom client specified later! https://docs.ell.so/core_concepts/models_and_api_clients.html ") + f"{Style.RESET_ALL}"
--------------------------------------------------------------------------------
Chunk ID: util\_warnings.py::2
Filepath: src\ell\util\_warnings.py
Content:
def _warnings(model, fn, default_client_from_decorator):

        if not default_client_from_decorator:
            # Check to see if the model is registered and warn the user we're gonna defualt to OpenAI.

            if model not in config.registry:
                logger.warning(f"""{Fore.LIGHTYELLOW_EX}WARNING: Model `{model}` is used by LMP `{fn.__name__}` but no client could be found that supports `{model}`. Defaulting to use the OpenAI client `{config.default_client}` for `{model}`. This is likely because you've spelled the model name incorrectly or are using a newer model from a provider added after this ell version was released. 
                            
* If this is a mistake either specify a client explicitly in the decorator:
```python
import ell
ell.simple(model, client=my_client)
def {fn.__name__}(...):
    ...
```
or explicitly specify the client when the calling the LMP:

```python
ell.simple(model, client=my_client)(...)
```
{Style.RESET_ALL}""")
            elif (client_to_use := config.registry[model].default_client) is None or not client_to_use.api_key:
                logger.warning(_no_api_key_warning(model, fn.__name__, client_to_use, long=False))


def _autocommit_warning():
    if (config.get_client_for("gpt-4o-mini")[0] is None):
        logger.warning(f"{Fore.LIGHTYELLOW_EX}WARNING: Autocommit is enabled but no OpenAI client found for autocommit model 'gpt-4o-mini' (set your OpenAI API key). Commit messages will not be written.{Style.RESET_ALL}")
        return True
    return False
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::1
Filepath: src\ell\util\closure.py
Content:
"""
This should do the following.
# prompt_consts.py
import math
def test():
    return math.sin(10)

# lol3.py
import prompt_consts

X = 7
def xD():
    print(X)
    return prompt_consts.test()

###
Our goal is to use AST & dill to get a full lexical closured source of xD, with the exception of modules that are stored in site-packages. For example.

lexical_extration(xD) returns
#closure.py
import math
def test():
    return math.sin(10)

X = 7 
def xD():
    print(X)
    return test()

"""
import collections
import ast
import hashlib
import itertools
from typing import Any, Dict, Iterable, Optional, Set, Tuple, Callable
import dill
import inspect
import types
from dill.source import getsource
import re
from collections import deque
import black

from ell.util.serialization import is_immutable_variable
from ell.util.should_import import should_import

DELIM = "$$$$$$$$$$$$$$$$$$$$$$$$$"
FORBIDDEN_NAMES = ["ell", "lstr"]
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::2
Filepath: src\ell\util\closure.py
Content:
def lexical_closure(
    func: Any,
    already_closed: Set[int] = None,
    initial_call: bool = False,
    recursion_stack: list = None,
    forced_dependencies: Optional[Dict[str, Any]] = None
) -> Tuple[str, Tuple[str, str], Set[str]]:
    """
    Generate a lexical closure for a given function or callable.

    Args:
        func: The function or callable to process.
        already_closed: Set of already processed function hashes.
        initial_call: Whether this is the initial call to the function.
        recursion_stack: Stack to keep track of the recursion path.

    Returns:
        A tuple containing:
        - The full source code of the closure
        - A tuple of (function source, dependencies source)
        - A set of function hashes that this closure uses
    """
    already_closed = already_closed or set()
    uses = set()
    forced_dependencies = forced_dependencies or {}
    recursion_stack = recursion_stack or []

    if hash(func) in already_closed:
        return "", ("", ""), set()

    recursion_stack.append(getattr(func, '__qualname__', str(func)))

    outer_ell_func = func
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__

    source = getsource(func, lstrip=True)
    already_closed.add(hash(func))

    globals_and_frees = _get_globals_and_frees(func)
    dependencies, imports, modules = _process_dependencies(func, globals_and_frees, already_closed, recursion_stack, uses)
    for k,v in forced_dependencies.items():
        # Todo: dictionary not necessary
        _process_signature_dependency(v, dependencies, already_closed, recursion_stack, uses, k)

    cur_src = _build_initial_source(imports, dependencies, source)

    module_src = _process_modules(modules, cur_src, already_closed, recursion_stack, uses)

    dirty_src = _build_final_source(imports, module_src, dependencies, source)
    dirty_src_without_func = _build_final_source(imports, module_src, dependencies, "")

    CLOSURE_SOURCE[hash(func)] = dirty_src

    dsrc = _clean_src(dirty_src_without_func)

    # Format the sorce and dsrc soruce using Black
    source = _format_source(source)
    dsrc = _format_source(dsrc)

    fn_hash = _generate_function_hash(source, dsrc, func.__qualname__)

    _update_ell_func(outer_ell_func, source, dsrc, globals_and_frees['globals'], globals_and_frees['frees'], fn_hash, uses)

    return (dirty_src, (source, dsrc), ({outer_ell_func} if not initial_call and hasattr(outer_ell_func, "__ell_func__") else uses))
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::3
Filepath: src\ell\util\closure.py
Content:
def _format_source(source: str) -> str:
    """Format the source code using Black."""
    try:
        return black.format_str(source, mode=black.Mode())
    except:
        # If Black formatting fails, return the original source
        return source

def _get_globals_and_frees(func: Callable) -> Dict[str, Dict]:
    """Get global and free variables for a function."""
    globals_dict = collections.OrderedDict(globalvars(func))
    frees_dict = collections.OrderedDict(dill.detect.freevars(func))

    if isinstance(func, type):
        for name, method in collections.OrderedDict(func.__dict__).items():
            if isinstance(method, (types.FunctionType, types.MethodType)):
                globals_dict.update(collections.OrderedDict(dill.detect.globalvars(method)))
                frees_dict.update(collections.OrderedDict(dill.detect.freevars(method)))

    return {'globals': globals_dict, 'frees': frees_dict}
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::4
Filepath: src\ell\util\closure.py
Content:
def _process_dependencies(func, globals_and_frees, already_closed, recursion_stack, uses):
    """Process function dependencies."""
    dependencies = []
    modules = deque()
    imports = []

    if isinstance(func, (types.FunctionType, types.MethodType)):
        _process_default_kwargs(func, dependencies, already_closed, recursion_stack, uses)

    for var_name, var_value in itertools.chain(globals_and_frees['globals'].items(), globals_and_frees['frees'].items()):
        _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack, uses)

    return dependencies, imports, modules
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::5
Filepath: src\ell\util\closure.py
Content:
def _process_default_kwargs(func, dependencies, already_closed, recursion_stack, uses):
    """Process default keyword arguments and annotations of a function."""
    ps = inspect.signature(func).parameters
    for name, param in ps.items():
        if param.default is not inspect.Parameter.empty:
            _process_signature_dependency(param.default, dependencies, already_closed, recursion_stack, uses, name)
        if param.annotation is not inspect.Parameter.empty:
            _process_signature_dependency(param.annotation, dependencies, already_closed, recursion_stack, uses, f"{name}_annotation")
    if func.__annotations__.get('return') is not None:
        _process_signature_dependency(func.__annotations__['return'], dependencies, already_closed, recursion_stack, uses, "return_annotation")
    # XXX: In order to properly analyze this we should walk the AST rather than inspexting the signature; e.g. Field is FieldInfo not Field.
    # I don't care about the actual default at time of execution just the symbols required to statically reproduce the prompt.

--------------------------------------------------------------------------------
Chunk ID: util\closure.py::6
Filepath: src\ell\util\closure.py
Content:
def _process_signature_dependency(val, dependencies, already_closed, recursion_stack, uses, name: Optional[str] = None):
    # Todo: Build general cattr like utility for unstructuring python objects with hooks that keep track of state variables.
    # Todo: break up closure into types and functions.
    # XXX: This is not exhaustive, we should determine should import on all dependencies

    if name not in FORBIDDEN_NAMES:
        try:
            dep = None
            _uses = None
            if isinstance(val, (types.FunctionType, types.MethodType)):
                dep, _, _uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            elif isinstance(val, (list, tuple, set)):
                for item in val:
                    _process_signature_dependency(item, dependencies, already_closed, recursion_stack, uses)
            else:
                val_class = val if isinstance(val, type) else val.__class__
                try:
                    is_builtin = (val_class.__module__ == "builtins" or val_class.__module__ == "__builtins__")
                except:
                    is_builtin = False

                if not is_builtin:
                    if should_import(val_class.__module__):
                        dependencies.append(dill.source.getimport(val_class, alias=val_class.__name__))
                    else:
                        dep, _, _uses = lexical_closure(val_class, already_closed=already_closed, recursion_stack=recursion_stack.copy())

            if dep: dependencies.append(dep)
            if _uses: uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of parameter or annotation {name}", e, recursion_stack)
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::7
Filepath: src\ell\util\closure.py
Content:
def _process_variable(var_name, var_value, dependencies, modules, imports, already_closed, recursion_stack , uses):
    """Process a single variable."""
    try:
        name = inspect.getmodule(var_value).__name__
        if should_import(name):
            imports.append(dill.source.getimport(var_value, alias=var_name))
            return
    except:
        pass

    if isinstance(var_value, (types.FunctionType, type, types.MethodType)):
        _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses)
    elif isinstance(var_value, types.ModuleType):
        _process_module(var_name, var_value, modules, imports, uses)
    elif isinstance(var_value, types.BuiltinFunctionType):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        _process_other_variable(var_name, var_value, dependencies, uses)
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::8
Filepath: src\ell\util\closure.py
Content:
def _process_callable(var_name, var_value, dependencies, already_closed, recursion_stack, uses):
    """Process a callable (function, method, or class)."""
    try:
        module_is_ell = 'ell' in inspect.getmodule(var_value).__name__
    except:
        module_is_ell = False

    if var_name not in FORBIDDEN_NAMES and not module_is_ell:
        try:
            dep, _, _uses = lexical_closure(var_value, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            dependencies.append(dep)
            uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of global or free variable {var_name}", e, recursion_stack)
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::9
Filepath: src\ell\util\closure.py
Content:
def _process_module(var_name, var_value, modules, imports, uses):
    """Process a module."""
    if should_import(var_value.__name__):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        modules.append((var_name, var_value))

def _process_other_variable(var_name, var_value, dependencies, uses):
    """Process variables that are not callables or modules."""
    if isinstance(var_value, str) and '\n' in var_value:
        dependencies.append(f"{var_name} = '''{var_value}'''")
    elif is_immutable_variable(var_value):
        dependencies.append(f"#<BV>\n{var_name} = {repr(var_value)}\n#</BV>")
    else:
        dependencies.append(f"#<BmV>\n{var_name} = <{type(var_value).__name__} object>\n#</BmV>")
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::10
Filepath: src\ell\util\closure.py
Content:
def _build_initial_source(imports, dependencies, source):
    """Build the initial source code."""
    return f"{DELIM}\n" + f"\n{DELIM}\n".join(imports + dependencies + [source]) + f"\n{DELIM}\n"

def _process_modules(modules, cur_src, already_closed, recursion_stack, uses):
    """Process module dependencies."""
    reverse_module_src = deque()
    while modules:
        mname, mval = modules.popleft()
        mdeps = []
        attrs_to_extract = get_referenced_names(cur_src.replace(DELIM, ""), mname)
        for attr in attrs_to_extract:
            _process_module_attribute(mname, mval, attr, mdeps, modules, already_closed, recursion_stack, uses)

        mdeps.insert(0, f"# Extracted from module: {mname}")
        reverse_module_src.appendleft("\n".join(mdeps))

        cur_src = _dereference_module_names(cur_src, mname, attrs_to_extract)

    return list(reverse_module_src)
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::11
Filepath: src\ell\util\closure.py
Content:
def _process_module_attribute(mname, mval, attr, mdeps, modules, already_closed, recursion_stack, uses):
    """Process a single attribute of a module."""
    val = getattr(mval, attr)
    if isinstance(val, (types.FunctionType, type, types.MethodType)):
        try:
            dep, _, dep_uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            mdeps.append(dep)
            uses.update(dep_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of {mname}.{attr}", e, recursion_stack)
    elif isinstance(val, types.ModuleType):
        modules.append((attr, val))
    else:
        mdeps.append(f"{attr} = {repr(val)}")
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::12
Filepath: src\ell\util\closure.py
Content:
def _dereference_module_names(cur_src, mname, attrs_to_extract):
    """Dereference module names in the source code."""
    for attr in attrs_to_extract:
        cur_src = cur_src.replace(f"{mname}.{attr}", attr)
    return cur_src

def _build_final_source(imports, module_src, dependencies, source):
    """Build the final source code."""
    seperated_dependencies = sorted(imports) + sorted(module_src) + sorted(dependencies) + ([source] if source else [])
    seperated_dependencies = list(dict.fromkeys(seperated_dependencies))
    return DELIM + "\n" + f"\n{DELIM}\n".join(seperated_dependencies) + "\n" + DELIM + "\n"

def _generate_function_hash(source, dsrc, qualname):
    """Generate a hash for the function."""
    return "lmp-" + hashlib.md5("\n".join((source, dsrc, qualname)).encode()).hexdigest()
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::13
Filepath: src\ell\util\closure.py
Content:
def _update_ell_func(outer_ell_func, source, dsrc, globals_dict, frees_dict, fn_hash, uses):
    """Update the ell function attributes."""
    formatted_source = _format_source(source)
    formatted_dsrc = _format_source(dsrc)

    if hasattr(outer_ell_func, "__ell_func__"):

        outer_ell_func.__ell_closure__ = (formatted_source, formatted_dsrc, globals_dict, frees_dict)
        outer_ell_func.__ell_hash__ = fn_hash
        outer_ell_func.__ell_uses__ = uses

def _raise_error(message, exception, recursion_stack):
    """Raise an error with detailed information."""
    error_msg = f"{message}. Error: {str(exception)}\n"
    error_msg += f"Recursion stack: {' -> '.join(recursion_stack)}"
    # print(error_msg)
    raise Exception(error_msg)
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::14
Filepath: src\ell\util\closure.py
Content:
def get_referenced_names(code: str, module_name: str):
    """
    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)
    and walks through the tree to find all instances where an attribute of the module is referenced in the code.

    Parameters:
    code (str): The block of code to be parsed.
    module_name (str): The name of the module to look for in the code.

    Returns:
    list: A list of all attributes of the module that are referenced in the code.
    """
    # Remove content between #<BV> and #</BV> tags
    code = re.sub(r'#<BV>\n.*?\n#</BV>', '', code, flags=re.DOTALL)

    # Remove content between #<BmV> and #</BmV> tags
    code = re.sub(r'#<BmV>\n.*?\n#</BmV>', '', code, flags=re.DOTALL)

    tree = ast.parse(code)
    referenced_names = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Attribute):
            if isinstance(node.value, ast.Name) and node.value.id == module_name:
                referenced_names.append(node.attr)

    return referenced_names

CLOSURE_SOURCE: Dict[str, str] = {}
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::15
Filepath: src\ell\util\closure.py
Content:
def lexically_closured_source(func, forced_dependencies: Optional[Dict[str, Any]] = None):
    """
    Generate a lexically closured source for a given function.

    This function takes a callable object (function, method, or class) and generates
    a lexically closured source code. It captures all the dependencies, including
    global variables, free variables, and nested functions, to create a self-contained
    version of the function that can be executed independently.

    Args:
        func (Callable): The function or callable object to process.
        forced_dependencies (Optional[Dict[str, Any]]): A dictionary of additional
            dependencies to include in the closure. Keys are variable names, and
            values are the corresponding objects.

    Returns:
        Tuple[str, Set[Any]]: A tuple containing two elements:
            1. The lexically closured source code as a string.
            2. A set of function objects that this closure uses.

    Raises:
        ValueError: If the input is not a callable object.

    Example:
        def outer(x):
            y = 10
            def inner():
                return x + y
            return inner

        closured_source, uses = lexically_closured_source(outer)
        print(closured_source)
        # Output will include the source for both outer and inner functions,
        # along with any necessary imports and variable definitions.

    Note:
        This function relies on the `lexical_closure` function to perform the
        actual closure generation. It also uses the `__ell_closure__` attribute
        of the function, which is expected to be set by the `lexical_closure` function.
    """
    if not callable(func):
        raise ValueError("Input must be a callable object (function, method, or class).")
    _, fnclosure, uses = lexical_closure(func, initial_call=True, recursion_stack=[], forced_dependencies=forced_dependencies)
    return func.__ell_closure__, uses
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::16
Filepath: src\ell\util\closure.py
Content:
import ast

def _clean_src(dirty_src):
    # Now remove all duplicates and preserve order
    split_by_setion = filter(lambda x: len(x.strip()) > 0, dirty_src.split(DELIM))

    # Now we need to remove all the duplicates
    split_by_setion = list(dict.fromkeys(split_by_setion))

    # Now we need to concat all together
    all_imports = []
    final_src = "\n".join(split_by_setion)
    out_final_src = final_src[:]
    for line in final_src.split("\n"):
        if line.startswith("import") or line.startswith("from"):
            all_imports.append(line)
            out_final_src = out_final_src.replace(line, "")

    all_imports = "\n".join(sorted(all_imports))
    final_src = all_imports + "\n" + out_final_src

    # now replace all "\n\n\n" or longer with "\n\n"
    final_src = re.sub(r"\n{3,}", "\n\n", final_src)

    return final_src
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::17
Filepath: src\ell\util\closure.py
Content:
def is_function_called(func_name, source_code):
    """
    Check if a function is called in the given source code.

    Parameters:
    func_name (str): The name of the function to check.
    source_code (str): The source code to check.

    Returns:
    bool: True if the function is called, False otherwise.
    """
    # Parse the source code into an AST
    tree = ast.parse(source_code)

    # Walk through all the nodes in the AST
    for node in ast.walk(tree):
        # If the node is a function call
        if isinstance(node, ast.Call):
            # If the function being called is the function we're looking for
            if isinstance(node.func, ast.Name) and node.func.id == func_name:
                return True

    # If we've gone through all the nodes and haven't found a call to the function, it's not called
    return False

#!/usr/bin/env python
#
from dill.detect import nestedglobals
import inspect
--------------------------------------------------------------------------------
Chunk ID: util\closure.py::18
Filepath: src\ell\util\closure.py
Content:
def globalvars(func, recurse=True, builtin=False):
    """get objects defined in global scope that are referred to by func

    return a dict of {name:object}"""
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__
    if inspect.ismethod(func): func = func.__func__
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__
    if inspect.isfunction(func):
        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}
        # get references from within closure
        orig_func, func = func, set()
        for obj in orig_func.__closure__ or {}:
            try:
                cell_contents = obj.cell_contents
            except ValueError: # cell is empty
                pass
            else:
                _vars = globalvars(cell_contents, recurse, builtin) or {}
                func.update(_vars) #XXX: (above) be wary of infinte recursion?
                globs.update(_vars)
        # get globals
        globs.update(orig_func.__globals__ or {})
        # get names of references
        if not recurse:
            func.update(orig_func.__code__.co_names)
        else:
            func.update(nestedglobals(orig_func.__code__))
            # find globals for all entries of func
            for key in func.copy(): #XXX: unnecessary...?
                nested_func = globs.get(key)
                if nested_func is orig_func:
                   #func.remove(key) if key in func else None
                    continue  #XXX: globalvars(func, False)?
                func.update(globalvars(nested_func, True, builtin))
    elif inspect.iscode(func):
        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}
       #globs.update(globals())
        if not recurse:
            func = func.co_names # get names
        else:
            orig_func = func.co_name # to stop infinite recursion
            func = set(nestedglobals(func))
            # find globals for all entries of func
            for key in func.copy(): #XXX: unnecessary...?
                if key is orig_func:
                   #func.remove(key) if key in func else None
                    continue  #XXX: globalvars(func, False)?
                nested_func = globs.get(key)
                func.update(globalvars(nested_func, True, builtin))
    # elif inspect.isclass(func):
    # XXX: We need to get lexical closures of all the methods and attributes of the class.\
    # In the future we should exhaustively walk the AST here.
    else:
        return {}
    #NOTE: if name not in __globals__, then we skip it...
    return dict((name,globs[name]) for name in func if name in globs)
--------------------------------------------------------------------------------
Chunk ID: util\closure_util.py::1
Filepath: src\ell\util\closure_util.py
Content:
import ast
import importlib
import os
import black
from dill.detect import nestedglobals

import inspect

import inspect

#!/usr/bin/env python
#
def globalvars(func, recurse=True, builtin=False):
    """get objects defined in global scope that are referred to by func

    return a dict of {name:object}"""
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__
    if inspect.ismethod(func): func = func.__func__
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__
    if inspect.isfunction(func):
        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}
        # get references from within closure
        orig_func, func = func, set()
        for obj in orig_func.__closure__ or {}:
            try:
                cell_contents = obj.cell_contents
            except ValueError: # cell is empty
                pass
            else:
                _vars = globalvars(cell_contents, recurse, builtin) or {}
                func.update(_vars) #XXX: (above) be wary of infinte recursion?
                globs.update(_vars)
        # get globals
        globs.update(orig_func.__globals__ or {})
        # get names of references
        if not recurse:
            func.update(orig_func.__code__.co_names)
        else:
            func.update(nestedglobals(orig_func.__code__))
            # find globals for all entries of func
            for key in func.copy(): #XXX: unnecessary...?
                nested_func = globs.get(key)
                if nested_func is orig_func:
                   #func.remove(key) if key in func else None
                    continue  #XXX: globalvars(func, False)?
                func.update(globalvars(nested_func, True, builtin))
    elif inspect.iscode(func):
        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}
       #globs.update(globals())
        if not recurse:
            func = func.co_names # get names
        else:
            orig_func = func.co_name # to stop infinite recursion
            func = set(nestedglobals(func))
            # find globals for all entries of func
            for key in func.copy(): #XXX: unnecessary...?
                if key is orig_func:
                   #func.remove(key) if key in func else None
                    continue  #XXX: globalvars(func, False)?
                nested_func = globs.get(key)
                func.update(globalvars(nested_func, True, builtin))
    else:
        return {}
    #NOTE: if name not in __globals__, then we skip it...
    return dict((name,globs[name]) for name in func if name in globs)
--------------------------------------------------------------------------------
Chunk ID: util\closure_util.py::2
Filepath: src\ell\util\closure_util.py
Content:
def is_function_called(func_name, source_code):
    """
    Check if a function is called in the given source code.

    Parameters:
    func_name (str): The name of the function to check.
    source_code (str): The source code to check.

    Returns:
    bool: True if the function is called, False otherwise.
    """
    # Parse the source code into an AST
    tree = ast.parse(source_code)

    # Walk through all the nodes in the AST
    for node in ast.walk(tree):
        # If the node is a function call
        if isinstance(node, ast.Call):
            # If the function being called is the function we're looking for
            if isinstance(node.func, ast.Name) and node.func.id == func_name:
                return True

    # If we've gone through all the nodes and haven't found a call to the function, it's not called
    return False
--------------------------------------------------------------------------------
Chunk ID: util\closure_util.py::3
Filepath: src\ell\util\closure_util.py
Content:
def get_referenced_names(code: str, module_name: str):
    """
    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)
    and walks through the tree to find all instances where an attribute of the module is referenced in the code.

    Parameters:
    code (str): The block of code to be parsed.
    module_name (str): The name of the module to look for in the code.

    Returns:
    list: A list of all attributes of the module that are referenced in the code.
    """
    tree = ast.parse(code)
    referenced_names = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Attribute):
            if isinstance(node.value, ast.Name) and node.value.id == module_name:
                referenced_names.append(node.attr)

    return referenced_names
--------------------------------------------------------------------------------
Chunk ID: util\closure_util.py::4
Filepath: src\ell\util\closure_util.py
Content:
def should_import(module_name : str):
    """
    This function checks if a module should be imported based on its origin.
    It returns False if the module is in the local directory or if the module's spec is None.
    Otherwise, it returns True.

    Returns:
    bool: True if the module should be imported, False otherwise.
    """

    # Define the local directory
    DIRECTORY_TO_WATCH = os.environ.get("DIRECTORY_TO_WATCH", os.getcwd())

    # Get the module's spec
    spec = importlib.util.find_spec(module_name)

    if module_name.startswith("ell"):
        return True

    # Return False if the spec is None or if the spec's origin starts with the local directory
    if spec is None or (spec.origin is not None and spec.origin.startswith(DIRECTORY_TO_WATCH)):
        return False

    # Otherwise, return True
    return True


def format_source(source: str) -> str:
    """Format the source code using Black."""
    try:
        return black.format_str(source, mode=black.Mode())
    except:
        # If Black formatting fails, return the original source
        return source
--------------------------------------------------------------------------------
Chunk ID: util\differ.py::1
Filepath: src\ell\util\differ.py
Content:
from ell.configurator import config
from ell.lmp.simple import simple
import difflib

# Todo: update this for single change stuff so that it doesn't summarize small chage but says it specifically.
@simple(config.autocommit_model, temperature=0.2, exempt_from_tracking=True, max_tokens=500)
def write_commit_message_for_diff(old : str, new : str) -> str:
    """You are an expert programmer whose goal is to write commit messages based on diffs.
You will be given two version of source code and their unified diff.
You will be expected to write a commit message that describes the changes between the two versions.

Follow these guidelines:
1. Your commit message should be at most one sentence and highly specific to the changes made. Don't just discuss the functions changed but how they were specifically changed.
2. Your commit message cannot be more than 10 words so use sentence fragments and be concise.
3. The @ell.simple decorator turns a function into a call to a language model program: the function's docstring is the system prompt and the string returned is the user prompt. 
4. It is extremely important that if the system prompt or user prompt changes, your commit message must say what specifically changed, rather than vaguely saying they were updated or changed.
5. It is extremely important that you never refer to a @ell.simple docstring as a docstring: it is a system prompt. 
6. Do NOT say why a change was done, say what specifically changed.
7. Consider all changes ot the program including the globals and free variables

Example response:
'''
Update model temperature and refine system prompt wording:
* Changed temperature from 0.5 to 0.7.
* Updated "with specificity, brevity, and good grammar" to "clearly and concisely" in system prompt.
* The `questions` param was assigned type List[Question]
'''
Response format:
<Short commit message summarizing all the changes with specificity>:
* <Bulleted list of each specific change>.
"""
    clean_program_of_all_bv_tags = lambda program : program.replace("#<BV>", "").replace("#</BV>", "").replace("#<BmV>", "").replace("#</BmV>", "")
    old_clean = clean_program_of_all_bv_tags(old)
    new_clean = clean_program_of_all_bv_tags(new)

    diff = "\n".join(difflib.unified_diff(old_clean.splitlines(), new_clean.splitlines(), lineterm=''))

    return f"""Write a commit message succinctly and specifically describing the changes between these two versions of a program.
Old version:
```
{old_clean}
```

New version:
```
{new_clean}
```

Unified diff:
{diff}
"""
--------------------------------------------------------------------------------
Chunk ID: util\differ.py::2
Filepath: src\ell\util\differ.py
Content:
if __name__ == "__main__":

    from ell.configurator import config
    config.verbose = True

    test_version_1 = '''import ell
import numpy as np

@ell.simple(model="gpt-4o-mini")
def come_up_with_a_premise_for_a_joke_about(topic : str):
    """You are an incredibly funny comedian. Come up with a premise for a joke about topic"""
    return f"come up with a premise for a joke about {topic}"

def get_random_length():
    return int(np.random.beta(2, 5) * 300)

@ell.simple(model="gpt-4o-mini")
def joke(topic : str):
    """You are a funny comedian. You respond in scripts for a standup comedy skit."""
    return f"Act out a full joke. Make your script {get_random_length()} words long. Here's the premise: {come_up_with_a_premise_for_a_joke_about(topic)}"'''

    test_version_2 = '''import ell
import numpy as np

@ell.simple(model="gpt-4o-mini")
def come_up_with_a_premise_for_a_joke_about(topic : str):
    """You are an incredibly funny comedian. Come up with a premise for a joke about topic"""
    return f"come up with a premise for a joke about {topic}"

def get_random_length():
    return int(np.random.beta(2, 5) * 300)

@ell.simple(model="gpt-4o-mini")
def joke(topic : str):
    """You are a funny comedian. You respond in scripts for skits."""
    return f"Act out a full joke. Make your script {get_random_length()} words long. Here's the premise: {come_up_with_a_premise_for_a_joke_about(topic)}"'''

    (write_commit_message_for_diff(test_version_1, test_version_2))
--------------------------------------------------------------------------------
Chunk ID: util\plot_ascii.py::1
Filepath: src\ell\util\plot_ascii.py
Content:
import sys
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import logging
import os

# Load pre-rendered character bitmaps
try:
    package_dir = os.path.dirname(__file__)
    bitmaps_path = os.path.join(package_dir, 'char_bitmaps.npy')
    data = np.load(bitmaps_path, allow_pickle=True).item()
    char_bitmaps = data['char_bitmaps']
    max_char_width = data['max_char_width']
    max_char_height = data['max_char_height']


    ASCII_CHARS = " .:-=+*#%@"
    def plot_ascii(
        image: Image.Image,
        width: int = 100,
        color: bool = True,
    ):
        """
        Convert a PIL Image to ASCII art using pre-rendered character bitmaps and print it to the console with optional coloring.
        """
        num_chars = len(ASCII_CHARS)


        # Adjust the scaling factor to compensate for character aspect ratio
        scale = 0.5  # You can tweak this value based on your terminal's character dimensions
        aspect_ratio = image.height / image.width
        new_width = width
        new_height = int(aspect_ratio * new_width * (max_char_height / max_char_width) * scale)
        image = image.resize((new_width * max_char_width, new_height * max_char_height)).convert('RGB')

        # Convert image to NumPy array
        img_array = np.array(image)

        # Compute brightness using luminance formula
        luminance = 0.2126 * img_array[:, :, 0] + 0.7152 * img_array[:, :, 1] + 0.0722 * img_array[:, :, 2]

        # Normalize brightness to range 0-1
        brightness_normalized = luminance / 255

        if color:
            # Get RGB values for coloring
            r = img_array[:, :, 0]
            g = img_array[:, :, 1]
            b = img_array[:, :, 2]

        # Compute the number of blocks
        y_blocks = new_height
        x_blocks = new_width

        # Reshape brightness_normalized to (y_blocks, max_char_height, x_blocks, max_char_width)
        brightness_blocks = brightness_normalized.reshape(y_blocks, max_char_height, x_blocks, max_char_width)
        brightness_blocks = brightness_blocks.mean(axis=(1, 3))  # Average over each block

        # Normalize again if necessary
        brightness_blocks = brightness_blocks / brightness_blocks.max()

        # Vectorize the selection of ASCII characters
        indices = np.digitize(brightness_blocks, np.linspace(0, 1, num_chars)) - 1
        indices = np.clip(indices, 0, num_chars - 1)
        ascii_chars = np.array(list(ASCII_CHARS))[indices]

        if color:
            # Compute average color for each block
            r_blocks = r.reshape(y_blocks, max_char_height, x_blocks, max_char_width).mean(axis=(1, 3)).astype(int)
            g_blocks = g.reshape(y_blocks, max_char_height, x_blocks, max_char_width).mean(axis=(1, 3)).astype(int)
            b_blocks = b.reshape(y_blocks, max_char_height, x_blocks, max_char_width).mean(axis=(1, 3)).astype(int)

            # Convert RGB to 8-bit color code
            color_codes = 16 + (36 * (r_blocks // 51)) + (6 * (g_blocks // 51)) + (b_blocks // 51)
            color_codes = color_codes.astype(str)

            # Create colored ASCII characters
            colored_ascii = np.char.add(np.char.add("\033[38;5;", color_codes), "m")
            colored_ascii = np.char.add(colored_ascii, np.char.add(ascii_chars, "\033[0m"))

            # Join characters into lines
            ascii_image = ["".join(row) for row in colored_ascii]
        else:
            ascii_image = ["".join(row) for row in ascii_chars]

        # Print the ASCII image
        return ascii_image
except FileNotFoundError:
    def plot_ascii(
        image: Image.Image,
        width: int = 100,
        color: bool = True,
    ):
        return "<image>"
--------------------------------------------------------------------------------
Chunk ID: util\plot_ascii.py::2
Filepath: src\ell\util\plot_ascii.py
Content:
# For packaging .
def render_and_save_char_bitmaps(
    font_path: str = "Courier New.ttf",
    font_size: int = 10,
    output_path: str = "char_bitmaps.npy"
) -> None:
    num_chars = len(ASCII_CHARS)
    char_bitmaps = {}
    max_char_width, max_char_height = 0, 0

    try:
        font = ImageFont.truetype(font_path, font_size)
    except IOError:
        logging.error(f"Font file '{font_path}' not found. Please provide a valid path.")
        sys.exit(1)

    for char in ASCII_CHARS:
        char_image = Image.new('L', (font_size, font_size), color=255)
        draw = ImageDraw.Draw(char_image)
        bbox = font.getbbox(char)
        w = bbox[2] - bbox[0]
        h = bbox[3] - bbox[1]
        draw.text(((font_size - w) / 2, (font_size - h) / 2), char, fill=0, font=font)
        bitmap = np.array(char_image) / 255  # Normalize to 0-1
        char_bitmaps[char] = bitmap
        max_char_width = max(max_char_width, bitmap.shape[1])
        max_char_height = max(max_char_height, bitmap.shape[0])

    # Save the character bitmaps and max dimensions
    data = {
        'char_bitmaps': char_bitmaps,
        'max_char_width': max_char_width,
        'max_char_height': max_char_height
    }
    np.save(output_path, data)
    logging.info(f"Character bitmaps saved to '{output_path}'.")
--------------------------------------------------------------------------------
Chunk ID: util\serialization.py::1
Filepath: src\ell\util\serialization.py
Content:
# Global converter
import base64
import hashlib
from io import BytesIO
import json
import cattrs
import numpy as np
from pydantic import BaseModel
import PIL
from ell.types._lstr import _lstr


pydantic_ltype_aware_cattr = cattrs.Converter()

def serialize_image(img):
    buffer = BytesIO()
    img.save(buffer, format="PNG")
    return "data:image/png;base64," + base64.b64encode(buffer.getvalue()).decode()


# Register hooks for complex types
pydantic_ltype_aware_cattr.register_unstructure_hook(
    np.ndarray,
    lambda arr: {
        "content": serialize_image(PIL.Image.fromarray(arr)),
        "__limage": True
    } if arr.ndim == 3 else (
        {
            "content": base64.b64encode(arr.tobytes()).decode(),
            "dtype": str(arr.dtype),
            "shape": arr.shape,
            "__lndarray": True
        }
    )
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    set,
    lambda s: list(sorted(s))
)
pydantic_ltype_aware_cattr.register_unstructure_hook(
    frozenset,
    lambda s: list(sorted(s))
)


pydantic_ltype_aware_cattr.register_unstructure_hook(
    PIL.Image.Image,
    lambda obj: {
        "content": serialize_image(obj),
        "__limage": True
    }
)

def unstructure_lstr(obj):
    return dict(content=str(obj), **obj.__dict__, __lstr=True)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    _lstr,
    unstructure_lstr
)

pydantic_ltype_aware_cattr.register_unstructure_hook(
    BaseModel,
    lambda obj: obj.model_dump(exclude_none=True, exclude_unset=True)
)
--------------------------------------------------------------------------------
Chunk ID: util\serialization.py::2
Filepath: src\ell\util\serialization.py
Content:
def get_immutable_vars(vars_dict):
    converter = cattrs.Converter()

    def handle_complex_types(obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj]
        elif isinstance(obj, dict):
            return {k: handle_complex_types(v) if not isinstance(v, (int, float, str, bool, type(None))) else v for k, v in obj.items()}
        elif isinstance(obj, (set, frozenset)):
            return list(sorted(handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj))
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return f"<Object of type {type(obj).__name__}>"

    converter.register_unstructure_hook(object, handle_complex_types)
    x = converter.unstructure(vars_dict)
    return x
--------------------------------------------------------------------------------
Chunk ID: util\serialization.py::3
Filepath: src\ell\util\serialization.py
Content:
def compute_state_cache_key(ipstr, fn_closure):
    _global_free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[2]), sort_keys=True, default=repr, ensure_ascii=False)}"
    _free_vars_str = f"{json.dumps(get_immutable_vars(fn_closure[3]), sort_keys=True, default=repr, ensure_ascii=False)}"
    state_cache_key = hashlib.sha256(f"{ipstr}{_global_free_vars_str}{_free_vars_str}".encode('utf-8')).hexdigest()
    return state_cache_key
--------------------------------------------------------------------------------
Chunk ID: util\serialization.py::4
Filepath: src\ell\util\serialization.py
Content:
def prepare_invocation_params(params):
    invocation_params = params

    cleaned_invocation_params = pydantic_ltype_aware_cattr.unstructure(invocation_params)

    # Thisis because we wneed the caching to work on the hash of a cleaned and serialized object.
    jstr = json.dumps(cleaned_invocation_params, sort_keys=True, default=repr, ensure_ascii=False)

    consumes = set()
    import re
    # XXX: Better than registering a hook in cattrs.
    pattern = r'"__origin_trace__":\s*"frozenset\({(.+?)}\)"'

    # Find all matches in the jstr
    matches = re.findall(pattern, jstr)

    # Process each match and add to consumes set
    for match in matches:
        # Remove quotes and spaces, then split by comma
        items = [item.strip().strip("'") for item in match.split(',')]
        consumes.update(items)
    consumes = list(consumes)
    # XXX: Only need to reload because of 'input' caching., we could skip this by making ultimate model caching rather than input hash caching; if prompt same use the same output.. irrespective of version.
    return json.loads(jstr), jstr, consumes
--------------------------------------------------------------------------------
Chunk ID: util\serialization.py::5
Filepath: src\ell\util\serialization.py
Content:
def is_immutable_variable(value):
    """
    Check if a value is immutable.

    This function determines whether the given value is of an immutable type in Python.
    Immutable types are objects whose state cannot be modified after they are created.

    Args:
        value: Any Python object to check for immutability.

    Returns:
        bool: True if the value is immutable, False otherwise.

    Note:
        - This function checks for common immutable types in Python.
        - Custom classes are considered mutable unless they explicitly implement
          immutability (which this function doesn't check for).
        - For some types like tuple, immutability is shallow (i.e., the tuple itself
          is immutable, but its contents might not be).
    """
    immutable_types = (
        int, float, complex, str, bytes,
        tuple, frozenset, type(None),
        bool,  # booleans are immutable
        range,  # range objects are immutable
        slice,  # slice objects are immutable
    )

    if isinstance(value, immutable_types):
        return True

    # Check for immutable instances of mutable types
    if isinstance(value, (tuple, frozenset)):
        return all(is_immutable_variable(item) for item in value)

    return False
--------------------------------------------------------------------------------
Chunk ID: util\should_import.py::1
Filepath: src\ell\util\should_import.py
Content:
import importlib.util
import os
import site
import sys
import sysconfig
from pathlib import Path


def should_import(module_name: str, raise_on_error: bool = False) -> bool:
    """
    Determines whether a module should be imported based on its origin.
    Excludes local modules and standard library modules.

    Args:
        module_name (str): The name of the module to check.

    Returns:
        bool: True if the module should be imported (i.e., it's a third-party module), False otherwise.
    """
    if module_name.startswith("ell"):
        return True
    try:
        try:
            spec = importlib.util.find_spec(module_name)
        except ValueError:
            return False
        if spec is None:
            return False

        origin = spec.origin
        if origin is None:
            return False
        if spec.has_location:
            origin_path = Path(origin).resolve()

            site_packages = list(site.getsitepackages()) + (list(site.getusersitepackages()) if isinstance(site.getusersitepackages(), list) else [site.getusersitepackages()])

            additional_paths = [Path(p).resolve() for p in sys.path if Path(p).resolve() not in map(Path, site_packages)]

            project_root = Path(os.environ.get("ELL_PROJECT_ROOT", os.getcwd())).resolve()

            site_packages_paths = [Path(p).resolve() for p in site_packages]
            stdlib_path = sysconfig.get_paths().get("stdlib")
            if stdlib_path:
                site_packages_paths.append(Path(stdlib_path).resolve())

            additional_paths = [Path(p).resolve() for p in additional_paths]
            local_paths = [project_root]

            cwd = Path.cwd().resolve()
            additional_paths = [path for path in additional_paths if path != cwd]

            for pkg in site_packages_paths:
                if origin_path.is_relative_to(pkg):
                    return True

            for path in additional_paths:
                if origin_path.is_relative_to(path):
                    return False

            for local in local_paths:
                if origin_path.is_relative_to(local):
                    return False

        return True

    except Exception as e:
        if raise_on_error:
            raise e
        return True
--------------------------------------------------------------------------------
Chunk ID: util\verbosity.py::1
Filepath: src\ell\util\verbosity.py
Content:
import sys
import hashlib
import shutil
import textwrap
from typing import Dict, Tuple, List, Any, Optional
from colorama import Fore, Style, init
from ell.types import Message
from ell.configurator import config
import logging
from functools import lru_cache
import threading
from ell.types.message import LMP, ContentBlock, _content_to_text
import requests

from ell.util.plot_ascii import plot_ascii

# Initialize colorama
init(autoreset=True)

# Define colors and styles
ELL_COLORS = {k: v for k, v in vars(Fore).items() if k not in ['RESET', 'BLACK', 'LIGHTBLACK_EX']}
BOLD = Style.BRIGHT
UNDERLINE = '\033[4m'
RESET = Style.RESET_ALL
SYSTEM_COLOR = Fore.CYAN
USER_COLOR = Fore.GREEN
ASSISTANT_COLOR = Fore.YELLOW
PIPE_COLOR = Fore.BLUE

# Set up logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

has_logged_version_statement = False

_version_check_lock = threading.Lock()
_has_logged_version_statement = False
--------------------------------------------------------------------------------
Chunk ID: util\verbosity.py::2
Filepath: src\ell\util\verbosity.py
Content:
def check_version_and_log():
    global _has_logged_version_statement
    if _version_check_lock.acquire(blocking=False):
        try:
            if not _has_logged_version_statement:

                import ell

                try:
                    response = requests.get("https://version.ell.so/ell-ai/pypi", timeout=0.15)
                    if response.status_code == 200:
                        latest_version = response.text.strip()
                        if latest_version != ell.__version__:
                            print(f"{Fore.YELLOW}╔═════════════════════════════════════════════════════════════════╗")
                            print(f"{Fore.YELLOW}║ {Fore.GREEN}A new version of ELL is available: {Fore.CYAN}{latest_version:<29}{Fore.YELLOW}║")
                            print(f"{Fore.YELLOW}║ {Fore.GREEN}You can update by running:{Fore.YELLOW}                                      ║")
                            print(f"{Fore.YELLOW}║ {Fore.CYAN}pip install --upgrade ell-ai{Fore.YELLOW}                                    ║")
                            print(f"{Fore.YELLOW}╚═════════════════════════════════════════════════════════════════╝{Style.RESET_ALL}")
                except requests.RequestException:
                    pass  # Silently handle any network-related errors
                _has_logged_version_statement = True
        finally:
            _version_check_lock.release()
--------------------------------------------------------------------------------
Chunk ID: util\verbosity.py::3
Filepath: src\ell\util\verbosity.py
Content:
@lru_cache(maxsize=128)
def compute_color(invoking_lmp: LMP) -> str:
    """Compute and cache a consistent color for a given LMP."""
    name_hash = hashlib.md5(invoking_lmp.__name__.encode()).hexdigest()
    color_index = int(name_hash, 16) % len(ELL_COLORS)
    return list(ELL_COLORS.values())[color_index]

def format_arg(arg: Any, max_length: int = 8) -> str:
    """Format an argument for display with customizable max length."""
    str_arg = str(arg)
    return f"{Fore.MAGENTA}{str_arg[:max_length]}..{Style.RESET_ALL}" if len(str_arg) > max_length else f"{Fore.MAGENTA}{str_arg}{Style.RESET_ALL}"

def format_kwarg(key: str, value: Any, max_length: int = 8) -> str:
    """Format a keyword argument for display with customizable max length."""
    return f"{Style.DIM}{key}{Style.RESET_ALL}={Fore.MAGENTA}{str(value)[:max_length]}..{Style.RESET_ALL}"

def get_terminal_width() -> int:
    """Get the terminal width, defaulting to 80 if it can't be determined."""
    try:
        return shutil.get_terminal_size((80, 20)).columns
    except Exception:
        logger.warning("Unable to determine terminal size. Defaulting to 80 columns.")
        return 80
--------------------------------------------------------------------------------
Chunk ID: util\verbosity.py::4
Filepath: src\ell\util\verbosity.py
Content:
def wrap_text_with_prefix(message, width: int, prefix: str, subsequent_prefix: str, text_color: str) -> List[str]:
    """Wrap text while preserving the prefix and color for each line."""
    result = []
    for i, content in enumerate(message.content):
        wrapped_lines = []
        if content.image and content.image.image:
            wrapped_lines = plot_ascii(content.image.image, min(80, width - len(prefix)))
        else:
            if content.tool_result:
                contnets_to_wrap = [ContentBlock(text=f"ToolResult(tool_call_id={content.tool_result.tool_call_id}):"), *content.tool_result.result]
            else:
                contnets_to_wrap = [content]

            wrapped_lines = []
            for c in contnets_to_wrap:
                if c.image and c.image.image:
                    block_wrapped_lines = plot_ascii(c.image.image, min(80, width - len(prefix)))
                else:
                    text = _content_to_text([c])
                    paragraphs = text.split('\n')
                    wrapped_paragraphs = [textwrap.wrap(p, width=width - len(prefix)) for p in paragraphs]
                    block_wrapped_lines = [line for paragraph in wrapped_paragraphs for line in paragraph]
                wrapped_lines.extend(block_wrapped_lines)
        if i == 0:
            if wrapped_lines:
                result.append(f"{prefix}{text_color}{wrapped_lines[0]}{RESET}")
            else:
                result.append(f"{prefix}{text_color}{RESET}")
        else:
            result.append(f"{subsequent_prefix}{text_color}{wrapped_lines[0]}{RESET}")
        result.extend([f"{subsequent_prefix}{text_color}{line}{RESET}" for line in wrapped_lines[1:]])
    return result
--------------------------------------------------------------------------------
Chunk ID: util\verbosity.py::5
Filepath: src\ell\util\verbosity.py
Content:
def print_wrapped_messages(messages: List[Message], max_role_length: int, color: str, wrap_width: Optional[int] = None):
    """Print wrapped messages with proper indentation, customizable wrap width, and consistent ASCII piping."""
    terminal_width = get_terminal_width()
    prefix = f"{PIPE_COLOR}│   "
    role_prefix = ' ' * (max_role_length + 2)
    subsequent_prefix = f"{PIPE_COLOR}│   {role_prefix}"
    wrapping_width = wrap_width or (terminal_width - len(prefix))

    for i, message in enumerate(messages):
        role = message.role
        m = message.content[0]


        role_color = SYSTEM_COLOR if role == "system" else USER_COLOR if role == "user" else ASSISTANT_COLOR

        role_line = f"{prefix}{role_color}{role.rjust(max_role_length)}: {RESET}"
        wrapped_lines = wrap_text_with_prefix(message, wrapping_width - len(role_prefix), '', subsequent_prefix, role_color)

        print(f"{role_line}{wrapped_lines[0]}")
        for line in wrapped_lines[1:]:
            print(line)

        if i < len(messages) - 1:
            print(f"{PIPE_COLOR}│{RESET}")
--------------------------------------------------------------------------------
Chunk ID: util\verbosity.py::6
Filepath: src\ell\util\verbosity.py
Content:
def model_usage_logger_pre(
    invoking_lmp: LMP,
    lmp_args: Tuple,
    lmp_kwargs: Dict,
    lmp_hash: str,
    messages: List[Message],
    arg_max_length: int = 8
):
    """Log model usage before execution with customizable argument display length and ASCII box."""
    color =  compute_color(invoking_lmp)
    formatted_args = [format_arg(arg, arg_max_length) for arg in lmp_args]
    formatted_kwargs = [format_kwarg(key, lmp_kwargs[key], arg_max_length) for key in lmp_kwargs]
    formatted_params = ', '.join(formatted_args + formatted_kwargs)

    check_version_and_log()

    terminal_width = get_terminal_width()

    logger.info(f"Invoking LMP: {invoking_lmp.__name__} (hash: {lmp_hash[:8]})")

    print(f"{PIPE_COLOR}╔{'═' * (terminal_width - 2)}╗{RESET}")
    print(f"{PIPE_COLOR}║ {color}{BOLD}{UNDERLINE}{invoking_lmp.__name__}{RESET}{color}({formatted_params}){RESET}")
    print(f"{PIPE_COLOR}╠{'═' * (terminal_width - 2)}╣{RESET}")
    print(f"{PIPE_COLOR}║ {BOLD}Prompt:{RESET}")
    print(f"{PIPE_COLOR}╟{'─' * (terminal_width - 2)}╢{RESET}")

    max_role_length = max(len("assistant"), max(len(message.role) for message in messages))
    print_wrapped_messages(messages, max_role_length, color)
--------------------------------------------------------------------------------
Chunk ID: util\verbosity.py::7
Filepath: src\ell\util\verbosity.py
Content:
def model_usage_logger_post_start(color: str = "", n: int = 1):
    """Log the start of model output with ASCII box."""
    terminal_width = get_terminal_width()
    print(f"{PIPE_COLOR}╟{'─' * (terminal_width - 2)}╢{RESET}")
    print(f"{PIPE_COLOR}║ {BOLD}Output{f'[0 of {n}]' if n > 1 else ''}:{RESET}")
    print(f"{PIPE_COLOR}╟{'─' * (terminal_width - 2)}╢{RESET}")
    print(f"{PIPE_COLOR}│   {ASSISTANT_COLOR}assistant: {RESET}", end='')
    sys.stdout.flush()

from contextlib import contextmanager
--------------------------------------------------------------------------------
Chunk ID: util\verbosity.py::8
Filepath: src\ell\util\verbosity.py
Content:
@contextmanager
def model_usage_logger_post_intermediate( n: int = 1):
    """Context manager to log intermediate model output without wrapping, only indenting if necessary."""
    terminal_width = get_terminal_width()
    prefix = f"{PIPE_COLOR}│   "
    subsequent_prefix = f"{PIPE_COLOR}│   {' ' * (len('assistant: '))}"
    chars_printed = len(subsequent_prefix)

    def log_stream_chunk(stream_chunk: str , is_refusal: bool = False):
        nonlocal chars_printed
        if stream_chunk:
            lines = stream_chunk.split('\n')
            for i, line in enumerate(lines):
                if chars_printed + len(line) > terminal_width - 6:
                    print()
                    if i == 0:
                        print(subsequent_prefix, end='')
                        chars_printed = len(prefix)
                    else:
                        print(subsequent_prefix, end='')
                        chars_printed = len(subsequent_prefix)
                    print(line.lstrip(), end='')
                else:
                    print(line, end='')
                chars_printed += len(line)

                if i < len(lines) - 1:
                    print()
                    print(subsequent_prefix, end='')
                    chars_printed = len(subsequent_prefix)  # Reset for new line
            sys.stdout.flush()

    try:
        yield log_stream_chunk
    finally:
        pass
--------------------------------------------------------------------------------
Chunk ID: util\verbosity.py::9
Filepath: src\ell\util\verbosity.py
Content:
def model_usage_logger_post_end():
    """Log the end of model output with ASCII box closure."""
    terminal_width = get_terminal_width()
    print(f"\n{PIPE_COLOR}╚{'═' * (terminal_width - 2)}╝{RESET}")

def set_log_level(level: str):
    """Set the logging level."""
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f'Invalid log level: {level}')
    logger.setLevel(numeric_level)
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\__init__.py::1
Filepath: x\openai_realtime\src\openai_realtime\__init__.py
Content:
from .client import RealtimeClient
from .api import RealtimeAPI
from .conversation import RealtimeConversation
from .event_handler import RealtimeEventHandler
from .utils import RealtimeUtils

__all__ = [
    "RealtimeClient",
    "RealtimeAPI",
    "RealtimeConversation",
    "RealtimeEventHandler",
    "RealtimeUtils"
]
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\api.py::1
Filepath: x\openai_realtime\src\openai_realtime\api.py
Content:
import asyncio
import json
import websockets
from .event_handler import RealtimeEventHandler
from .utils import RealtimeUtils

class RealtimeAPI(RealtimeEventHandler):
    def __init__(self, url=None, api_key=None, dangerously_allow_api_key_in_browser=False, debug=False):
        super().__init__()
        self.default_url = 'wss://api.openai.com/v1/realtime'
        self.url = url or self.default_url
        self.api_key = api_key
        self.debug = debug
        self.ws = None

    def is_connected(self):
        return self.ws is not None and self.ws.open

    def log(self, *args):
        if self.debug:
            print(*args)
        return True
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\api.py::2
Filepath: x\openai_realtime\src\openai_realtime\api.py
Content:
class RealtimeAPI(RealtimeEventHandler):

    async def connect(self, model='gpt-4o-realtime-preview-2024-10-01'):
        if self.is_connected():
            raise Exception("Already connected")

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'OpenAI-Beta': 'realtime=v1'
        }

        self.ws = await websockets.connect(f"{self.url}?model={model}", extra_headers=headers)

        self.log(f"Connected to {self.url}")

        asyncio.create_task(self._message_handler())

        return True
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\api.py::3
Filepath: x\openai_realtime\src\openai_realtime\api.py
Content:
class RealtimeAPI(RealtimeEventHandler):

    async def _message_handler(self):
        try:
            async for message in self.ws:
                data = json.loads(message)
                self.receive(data['type'], data)
        except websockets.exceptions.ConnectionClosed:
            self.disconnect()
            self.dispatch('close', {'error': True})

    def disconnect(self):
        if self.ws:
            asyncio.create_task(self.ws.close())
            self.ws = None
        return True

    def receive(self, event_name, event):
        self.log("received:", event_name, event)
        self.dispatch(f"server.{event_name}", event)
        self.dispatch("server.*", event)
        return True
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\api.py::4
Filepath: x\openai_realtime\src\openai_realtime\api.py
Content:
class RealtimeAPI(RealtimeEventHandler):

    def send(self, event_name, data=None):
        if not self.is_connected():
            raise Exception("RealtimeAPI is not connected")

        data = data or {}
        if not isinstance(data, dict):
            raise ValueError("data must be a dictionary")

        event = {
            "event_id": RealtimeUtils.generate_id("evt_"),
            "type": event_name,
            **data
        }

        self.dispatch(f"client.{event_name}", event)
        self.dispatch("client.*", event)
        self.log("sent:", event_name, event)

        asyncio.create_task(self.ws.send(json.dumps(event, ensure_ascii=False)))
        return True
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\client.py::1
Filepath: x\openai_realtime\src\openai_realtime\client.py
Content:
import asyncio
import numpy as np
from .event_handler import RealtimeEventHandler
from .api import RealtimeAPI
from .conversation import RealtimeConversation
from .utils import RealtimeUtils
import json

class RealtimeClient(RealtimeEventHandler):
    def __init__(self, url=None, api_key=None, instructions='', dangerously_allow_api_key_in_browser=False, debug=False):
        super().__init__()
        self.default_session_config = {
            'modalities': ['text', 'audio'],
            'instructions': instructions,
            'voice': 'alloy',
            'input_audio_format': 'pcm16',
            'output_audio_format': 'pcm16',
            'input_audio_transcription': None,
            'turn_detection': None,
            'tools': [],
            'tool_choice': 'auto',
            'temperature': 0.8,
            'max_response_output_tokens': 4096,
        }
        self.session_config = {}
        self.transcription_models = [{'model': 'whisper-1'}]
        self.default_server_vad_config = {
            'type': 'server_vad',
            'threshold': 0.5,
            'prefix_padding_ms': 300,
            'silence_duration_ms': 200,
        }
        self.realtime = RealtimeAPI(url, api_key, dangerously_allow_api_key_in_browser, debug)
        self.conversation = RealtimeConversation()
        self._reset_config()
        self._add_api_event_handlers()

    def _reset_config(self):
        self.session_created = False
        self.tools = {}
        self.session_config = self.default_session_config.copy()
        self.input_audio_buffer = np.array([], dtype=np.int16)
        return True
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\client.py::2
Filepath: x\openai_realtime\src\openai_realtime\client.py
Content:
class RealtimeClient(RealtimeEventHandler):

    def _add_api_event_handlers(self):
        self.realtime.on('client.*', lambda event: self.dispatch('realtime.event', {
            'time': RealtimeUtils.generate_id('time_'),
            'source': 'client',
            'event': event
        }))
        self.realtime.on('server.*', lambda event: self.dispatch('realtime.event', {
            'time': RealtimeUtils.generate_id('time_'),
            'source': 'server',
            'event': event
        }))
        self.realtime.on('server.session.created', lambda _: setattr(self, 'session_created', True))

        def handle_conversation_event(event, *args):
            result = self.conversation.process_event(event, *args)
            if result['item']:
                self.dispatch('conversation.updated', result)
            return result

        self.realtime.on('server.response.created', handle_conversation_event)
        self.realtime.on('server.response.output_item.added', handle_conversation_event)
        self.realtime.on('server.response.content_part.added', handle_conversation_event)
        self.realtime.on('server.input_audio_buffer.speech_started', lambda event: (
            handle_conversation_event(event),
            self.dispatch('conversation.interrupted', event)
        ))
        self.realtime.on('server.input_audio_buffer.speech_stopped', lambda event: 
            handle_conversation_event(event, self.input_audio_buffer)
        )
        self.realtime.on('server.conversation.item.created', lambda event: (
            handle_conversation_event(event),
            self.dispatch('conversation.item.appended', {'item': event['item']})
        ))
        self.realtime.on('server.conversation.item.truncated', handle_conversation_event)
        self.realtime.on('server.conversation.item.deleted', handle_conversation_event)
        self.realtime.on('server.conversation.item.input_audio_transcription.completed', handle_conversation_event)
        self.realtime.on('server.response.audio_transcript.delta', handle_conversation_event)
        self.realtime.on('server.response.audio.delta', handle_conversation_event)
        self.realtime.on('server.response.text.delta', handle_conversation_event)
        self.realtime.on('server.response.function_call_arguments.delta', handle_conversation_event)
        def handle_output_item_done( event):
            handle_conversation_event(event)
            item = event.get('item', {})

            if item.get('status') == 'completed':
                self.dispatch('conversation.item.completed', {'item': item})

            formatted = item.get('formatted', {})
            tool = formatted.get('tool') if isinstance(formatted, dict) else None

            if tool:
                asyncio.create_task(self._call_tool(tool))
        self.realtime.on('server.response.output_item.done', handle_output_item_done)
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\client.py::3
Filepath: x\openai_realtime\src\openai_realtime\client.py
Content:
class RealtimeClient(RealtimeEventHandler):



    def is_connected(self):
        return self.realtime.is_connected() and self.session_created

    def reset(self):
        self.disconnect()
        self.clear_event_handlers()
        self.realtime.clear_event_handlers()
        self._reset_config()
        self._add_api_event_handlers()
        return True

    async def connect(self):
        if self.is_connected():
            raise Exception("Already connected, use .disconnect() first")
        await self.realtime.connect()
        self.update_session()
        return True

    async def wait_for_session_created(self):
        if not self.realtime.is_connected():
            raise Exception("Not connected, use .connect() first")
        while not self.session_created:
            await asyncio.sleep(0.001)
        return True

    def disconnect(self):
        self.session_created = False
        self.conversation.clear()
        if self.realtime.is_connected():
            self.realtime.disconnect()

    def get_turn_detection_type(self):
        turn_detection = self.session_config.get('turn_detection')
        if isinstance(turn_detection, dict):
            return turn_detection.get('type')
        return None
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\client.py::4
Filepath: x\openai_realtime\src\openai_realtime\client.py
Content:
class RealtimeClient(RealtimeEventHandler):

    def add_tool(self, definition, handler):
        if not definition.get('name'):
            raise ValueError("Missing tool name in definition")
        name = definition['name']
        if name in self.tools:
            raise ValueError(f"Tool '{name}' already added. Please use .remove_tool('{name}') before trying to add again.")
        if not callable(handler):
            raise ValueError(f"Tool '{name}' handler must be a function")
        self.tools[name] = {'definition': definition, 'handler': handler}
        self.update_session()
        return self.tools[name]
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\client.py::5
Filepath: x\openai_realtime\src\openai_realtime\client.py
Content:
class RealtimeClient(RealtimeEventHandler):

    def remove_tool(self, name):
        if name not in self.tools:
            raise ValueError(f"Tool '{name}' does not exist, cannot be removed.")
        del self.tools[name]
        return True

    def delete_item(self, id):
        self.realtime.send('conversation.item.delete', {'item_id': id})
        return True

    def update_session(self, **kwargs):
        self.session_config.update(kwargs)
        use_tools = [
            {**tool.get('definition', {}), 'type': 'function'}
            for tool in self.tools.values()
        ]
        session = {**self.session_config, 'tools': use_tools}
        if self.realtime.is_connected():
            self.realtime.send('session.update', {'session': session})
        return True
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\client.py::6
Filepath: x\openai_realtime\src\openai_realtime\client.py
Content:
class RealtimeClient(RealtimeEventHandler):

    def send_user_message_content(self, content=None):
        content = content or []
        for c in content:
            if c['type'] == 'input_audio':
                if isinstance(c['audio'], (np.ndarray, bytes)):
                    c['audio'] = RealtimeUtils.array_buffer_to_base64(c['audio'])
        if content:
            self.realtime.send('conversation.item.create', {
                'item': {
                    'type': 'message',
                    'role': 'user',
                    'content': content
                }
            })
        self.create_response()
        return True
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\client.py::7
Filepath: x\openai_realtime\src\openai_realtime\client.py
Content:
class RealtimeClient(RealtimeEventHandler):

    def append_input_audio(self, array_buffer):
        if len(array_buffer) > 0:
            self.realtime.send('input_audio_buffer.append', {
                'audio': RealtimeUtils.array_buffer_to_base64(array_buffer)
            })
            self.input_audio_buffer = RealtimeUtils.merge_int16_arrays(
                self.input_audio_buffer,
                array_buffer
            )
        return True

    def create_response(self):
        if self.get_turn_detection_type() is None and len(self.input_audio_buffer) > 0:
            self.realtime.send('input_audio_buffer.commit')
            self.conversation.queue_input_audio(self.input_audio_buffer)
            self.input_audio_buffer = np.array([], dtype=np.int16)
        self.realtime.send('response.create')
        return True
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\client.py::8
Filepath: x\openai_realtime\src\openai_realtime\client.py
Content:
class RealtimeClient(RealtimeEventHandler):

    def cancel_response(self, id=None, sample_count=0):
        if not id:
            self.realtime.send('response.cancel')
            return {'item': None}
        item = self.conversation.get_item(id)
        if not item:
            raise ValueError(f"Could not find item '{id}'")
        if item['type'] != 'message' or item['role'] != 'assistant':
            raise ValueError("Can only cancel response messages with type 'message' and role 'assistant'")
        self.realtime.send('response.cancel')
        audio_index = next((i for i, c in enumerate(item['content']) if c['type'] == 'audio'), -1)
        if audio_index == -1:
            raise ValueError("Could not find audio on item to cancel")
        self.realtime.send('conversation.item.truncate', {
            'item_id': id,
            'content_index': audio_index,
            'audio_end_ms': int((sample_count / self.conversation.default_frequency) * 1000)
        })
        return {'item': item}
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\client.py::9
Filepath: x\openai_realtime\src\openai_realtime\client.py
Content:
class RealtimeClient(RealtimeEventHandler):

    async def wait_for_next_item(self):
        event = await self.wait_for_next('conversation.item.appended')
        return {'item': event['item']}

    async def wait_for_next_completed_item(self):
        event = await self.wait_for_next('conversation.item.completed')
        return {'item': event['item']}

    async def _call_tool(self, tool):
        try:
            json_arguments = json.loads(tool['arguments'])
            tool_config = self.tools.get(tool['name'])
            if not tool_config:
                raise ValueError(f"Tool '{tool['name']}' has not been added")
            result = await tool_config['handler'](json_arguments)
            self.realtime.send('conversation.item.create', {
                'item': {
                    'type': 'function_call_output',
                    'call_id': tool['call_id'],
                    'output': json.dumps(result, ensure_ascii=False)
                }
            })
        except Exception as e:
            self.realtime.send('conversation.item.create', {
                'item': {
                    'type': 'function_call_output',
                    'call_id': tool['call_id'],
                    'output': json.dumps({'error': str(e)}, ensure_ascii=False)
                }
            })
        self.create_response()
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\conversation.py::1
Filepath: x\openai_realtime\src\openai_realtime\conversation.py
Content:
import numpy as np
import json
from .utils import RealtimeUtils
import copy

class RealtimeConversation:
    def __init__(self):
        self.default_frequency = 24000  # 24,000 Hz
        self.clear()

    def clear(self):
        self.item_lookup = {}
        self.items = []
        self.response_lookup = {}
        self.responses = []
        self.queued_speech_items = {}
        self.queued_transcript_items = {}
        self.queued_input_audio = None
        return True

    def queue_input_audio(self, input_audio):
        self.queued_input_audio = input_audio
        return input_audio

    def process_event(self, event, *args):
        if 'event_id' not in event:
            raise ValueError("Missing 'event_id' on event")
        if 'type' not in event:
            raise ValueError("Missing 'type' on event")

        event_processor = getattr(self, f"_process_{event['type'].replace('.', '_')}", None)
        if not event_processor:
            raise ValueError(f"Missing conversation event processor for '{event['type']}'")

        return event_processor(event, *args)

    def get_item(self, id):
        return self.item_lookup.get(id)

    def get_items(self):
        return self.items.copy()
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\conversation.py::2
Filepath: x\openai_realtime\src\openai_realtime\conversation.py
Content:
class RealtimeConversation:

    def _process_conversation_item_created(self, event):
        item = event['item']
        new_item = copy.deepcopy(item)
        if new_item['id'] not in self.item_lookup:
            self.item_lookup[new_item['id']] = new_item
            self.items.append(new_item)

        new_item['formatted'] = {
            'audio': np.array([], dtype=np.int16),
            'text': '',
            'transcript': ''
        }

        if new_item['type'] == 'message':
            if new_item['role'] == 'user':
                new_item['status'] = 'completed'
                if self.queued_input_audio is not None:
                    new_item['formatted']['audio'] = self.queued_input_audio
                    self.queued_input_audio = None
            else:
                new_item['status'] = 'in_progress'
        elif new_item['type'] == 'function_call':
            new_item['formatted']['tool'] = {
                'type': 'function',
                'name': new_item['name'],
                'call_id': new_item['call_id'],
                'arguments': ''
            }
            new_item['status'] = 'in_progress'
        elif new_item['type'] == 'function_call_output':
            new_item['status'] = 'completed'
            new_item['formatted']['output'] = new_item['output']

        if new_item.get('content'):
            text_content = [c for c in new_item['content'] if c['type'] in ['text', 'input_text']]
            for content in text_content:
                new_item['formatted']['text'] += content['text']

        if new_item['id'] in self.queued_speech_items:
            new_item['formatted']['audio'] = self.queued_speech_items[new_item['id']]['audio']
            del self.queued_speech_items[new_item['id']]

        if new_item['id'] in self.queued_transcript_items:
            new_item['formatted']['transcript'] = self.queued_transcript_items[new_item['id']]['transcript']
            del self.queued_transcript_items[new_item['id']]

        return {'item': new_item, 'delta': None}
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\conversation.py::3
Filepath: x\openai_realtime\src\openai_realtime\conversation.py
Content:
class RealtimeConversation:

    def _process_conversation_item_truncated(self, event):
        item_id, audio_end_ms = event['item_id'], event['audio_end_ms']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"item.truncated: Item '{item_id}' not found")

        end_index = int((audio_end_ms * self.default_frequency) / 1000)
        item['formatted']['transcript'] = ''
        item['formatted']['audio'] = item['formatted']['audio'][:end_index]
        return {'item': item, 'delta': None}

    def _process_conversation_item_deleted(self, event):
        item_id = event['item_id']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"item.deleted: Item '{item_id}' not found")

        del self.item_lookup[item['id']]
        self.items = [i for i in self.items if i['id'] != item['id']]
        return {'item': item, 'delta': None}
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\conversation.py::4
Filepath: x\openai_realtime\src\openai_realtime\conversation.py
Content:
class RealtimeConversation:

    def _process_conversation_item_input_audio_transcription_completed(self, event):
        item_id, content_index, transcript = event['item_id'], event['content_index'], event['transcript']
        item = self.item_lookup.get(item_id)
        formatted_transcript = transcript or ' '

        if not item:
            self.queued_transcript_items[item_id] = {'transcript': formatted_transcript}
            return {'item': None, 'delta': None}

        item['content'][content_index]['transcript'] = transcript
        item['formatted']['transcript'] = formatted_transcript
        return {'item': item, 'delta': {'transcript': transcript}}

    def _process_input_audio_buffer_speech_started(self, event):
        item_id, audio_start_ms = event['item_id'], event['audio_start_ms']
        self.queued_speech_items[item_id] = {'audio_start_ms': audio_start_ms}
        return {'item': None, 'delta': None}
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\conversation.py::5
Filepath: x\openai_realtime\src\openai_realtime\conversation.py
Content:
class RealtimeConversation:

    def _process_input_audio_buffer_speech_stopped(self, event, input_audio_buffer):
        item_id, audio_end_ms = event['item_id'], event['audio_end_ms']
        speech = self.queued_speech_items[item_id]
        speech['audio_end_ms'] = audio_end_ms
        if input_audio_buffer is not None:
            start_index = int((speech['audio_start_ms'] * self.default_frequency) / 1000)
            end_index = int((speech['audio_end_ms'] * self.default_frequency) / 1000)
            speech['audio'] = input_audio_buffer[start_index:end_index]
        return {'item': None, 'delta': None}
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\conversation.py::6
Filepath: x\openai_realtime\src\openai_realtime\conversation.py
Content:
class RealtimeConversation:

    def _process_response_created(self, event):
        response = event['response']
        if response['id'] not in self.response_lookup:
            self.response_lookup[response['id']] = response
            self.responses.append(response)
        return {'item': None, 'delta': None}

    def _process_response_output_item_added(self, event):
        response_id, item = event['response_id'], event['item']
        response = self.response_lookup.get(response_id)
        if not response:
            raise ValueError(f"response.output_item.added: Response '{response_id}' not found")
        response['output'].append(item['id'])
        return {'item': None, 'delta': None}

    def _process_response_output_item_done(self, event):
        item = event['item']
        if not item:
            raise ValueError("response.output_item.done: Missing 'item'")
        found_item = self.item_lookup.get(item['id'])
        if not found_item:
            raise ValueError(f"response.output_item.done: Item '{item['id']}' not found")
        found_item['status'] = item['status']
        return {'item': found_item, 'delta': None}

    def _process_response_content_part_added(self, event):
        item_id, part = event['item_id'], event['part']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.content_part.added: Item '{item_id}' not found")
        item['content'].append(part)
        return {'item': item, 'delta': None}
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\conversation.py::7
Filepath: x\openai_realtime\src\openai_realtime\conversation.py
Content:
class RealtimeConversation:

    def _process_response_audio_transcript_delta(self, event):
        item_id, content_index, delta = event['item_id'], event['content_index'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.audio_transcript.delta: Item '{item_id}' not found")
        item['content'][content_index]['transcript'] += delta
        item['formatted']['transcript'] += delta
        return {'item': item, 'delta': {'transcript': delta}}
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\conversation.py::8
Filepath: x\openai_realtime\src\openai_realtime\conversation.py
Content:
class RealtimeConversation:

    def _process_response_audio_delta(self, event):
        item_id, content_index, delta = event['item_id'], event['content_index'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.audio.delta: Item '{item_id}' not found")
        array_buffer = RealtimeUtils.base64_to_array_buffer(delta)
        append_values = np.frombuffer(array_buffer, dtype=np.int16)
        item['formatted']['audio'] = np.concatenate([item['formatted']['audio'], append_values])
        return {'item': item, 'delta': {'audio': append_values}}
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\conversation.py::9
Filepath: x\openai_realtime\src\openai_realtime\conversation.py
Content:
class RealtimeConversation:

    def _process_response_text_delta(self, event):
        item_id, content_index, delta = event['item_id'], event['content_index'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.text.delta: Item '{item_id}' not found")
        item['content'][content_index]['text'] += delta
        item['formatted']['text'] += delta
        return {'item': item, 'delta': {'text': delta}}

    def _process_response_function_call_arguments_delta(self, event):
        item_id, delta = event['item_id'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.function_call_arguments.delta: Item '{item_id}' not found")
        item['arguments'] += delta
        item['formatted']['tool']['arguments'] += delta
        return {'item': item, 'delta': {'arguments': delta}}
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\event_handler.py::1
Filepath: x\openai_realtime\src\openai_realtime\event_handler.py
Content:
import asyncio
from typing import Callable, Dict, List, Any

class RealtimeEventHandler:
    def __init__(self):
        self.event_handlers: Dict[str, List[Callable]] = {}
        self.next_event_handlers: Dict[str, List[Callable]] = {}

    def clear_event_handlers(self):
        self.event_handlers.clear()
        self.next_event_handlers.clear()
        return True

    def on(self, event_name: str, callback: Callable = None):
        def decorator(func):
            if event_name not in self.event_handlers:
                self.event_handlers[event_name] = []
            self.event_handlers[event_name].append(func)
            return func

        if callback is None:
            return decorator
        else:
            return decorator(callback)

    def on_next(self, event_name: str, callback: Callable):
        if event_name not in self.next_event_handlers:
            self.next_event_handlers[event_name] = []
        self.next_event_handlers[event_name].append(callback)

    def off(self, event_name: str, callback: Callable = None):
        if event_name in self.event_handlers:
            if callback:
                self.event_handlers[event_name].remove(callback)
            else:
                del self.event_handlers[event_name]
        return True

    def off_next(self, event_name: str, callback: Callable = None):
        if event_name in self.next_event_handlers:
            if callback:
                self.next_event_handlers[event_name].remove(callback)
            else:
                del self.next_event_handlers[event_name]
        return True

    async def wait_for_next(self, event_name: str, timeout: float = None):
        next_event = None
        def set_next_event(event):
            nonlocal next_event
            next_event = event

        self.on_next(event_name, set_next_event)

        start_time = asyncio.get_event_loop().time()
        while not next_event:
            if timeout and asyncio.get_event_loop().time() - start_time > timeout:
                return None
            await asyncio.sleep(0.001)

        return next_event

    def dispatch(self, event_name: str, event: Any):
        handlers = self.event_handlers.get(event_name, []).copy()
        for handler in handlers:
            handler(event)

        next_handlers = self.next_event_handlers.pop(event_name, [])
        for next_handler in next_handlers:
            next_handler(event)

        return True
--------------------------------------------------------------------------------
Chunk ID: openai_realtime\utils.py::1
Filepath: x\openai_realtime\src\openai_realtime\utils.py
Content:
import base64
import numpy as np

class RealtimeUtils:
    @staticmethod
    def float_to_16bit_pcm(float32_array):
        int16_array = (np.clip(float32_array, -1, 1) * 32767).astype(np.int16)
        return int16_array.tobytes()

    @staticmethod
    def base64_to_array_buffer(base64_string):
        return base64.b64decode(base64_string)

    @staticmethod
    def array_buffer_to_base64(array_buffer):
        if isinstance(array_buffer, np.ndarray):
            if array_buffer.dtype == np.float32:
                array_buffer = RealtimeUtils.float_to_16bit_pcm(array_buffer)
            elif array_buffer.dtype == np.int16:
                array_buffer = array_buffer.tobytes()
        return base64.b64encode(array_buffer).decode('utf-8')

    @staticmethod
    def merge_int16_arrays(left, right):
        if isinstance(left, bytes):
            left = np.frombuffer(left, dtype=np.int16)
        if isinstance(right, bytes):
            right = np.frombuffer(right, dtype=np.int16)
        if not isinstance(left, np.ndarray) or not isinstance(right, np.ndarray):
            raise ValueError("Both items must be numpy arrays or bytes objects")
        return np.concatenate((left, right))

    @staticmethod
    def generate_id(prefix, length=21):
        import random
        chars = '123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz'
        return prefix + ''.join(random.choice(chars) for _ in range(length - len(prefix)))
--------------------------------------------------------------------------------
