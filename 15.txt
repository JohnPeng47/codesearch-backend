_____ Graph _____

###### Cluster Eval ######
Score: 4.333333333333333
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: 0.1.0\cem.py::2


       # Gym environment

# Define the Policy Network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state):
        logits = self.fc(state)
        return logits

    def get_action(self, state):
        logits = self.forward(state)
        action_probs = torch.softmax(logits, dim=-1)
        action = torch.multinomial(action_probs, num_samples=1)
        return action.squeeze(-1)

# Function to create multiple environments
def make_env(env_name, seed):
    def _init():
        env = gym.make(env_name)
        return env
    return _init
====================================================================
-> Chunk: 0.1.0\cem.py::6


# Main execution code
if __name__ == '__main__':
    # Initialize environments
    env_fns = [make_env(ENV_NAME, SEED + i) for i in range(NUM_ENVIRONMENTS)]
    envs = AsyncVectorEnv(env_fns)

    # Get environment details
    dummy_env = gym.make(ENV_NAME)
    state_dim = dummy_env.observation_space.shape[0]
    action_dim = dummy_env.action_space.n
    dummy_env.close()

    # Initialize policy network and optimizer
    policy = PolicyNetwork(state_dim, action_dim)
    optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    # Training Loop
    for iteration in range(1, NUM_ITERATIONS + 1):
        try:
            # Step 1: Collect Trajectories
            trajectories = collect_trajectories(envs, policy, TRAJECTORIES_PER_ITER, MAX_STEPS)
        except Exception as e:
            print(f"Error during trajectory collection at iteration {iteration}: {e}")
            break

        # Step 2: Select Elite Trajectories
        elite_trajectories = select_elite(trajectories, ELITE_PERCENT)

        if len(elite_trajectories) == 0:
            print(f"Iteration {iteration}: No elite trajectories found. Skipping update.")
            continue

        # Step 3: Create Training Data
        states, actions = create_training_data(elite_trajectories)

        if states is None or actions is None:
            print(f"Iteration {iteration}: No training data available. Skipping update.")
            continue

        # Step 4: Behavioral Cloning (Policy Update)
        dataset_size = states.size(0)
        indices = np.arange(dataset_size)
        np.random.shuffle(indices)

        for start in range(0, dataset_size, BATCH_SIZE):
            end = start + BATCH_SIZE
            batch_indices = indices[start:end]
            batch_states = states[batch_indices]
            batch_actions = actions[batch_indices]

            optimizer.zero_grad()
            logits = policy(batch_states)
            loss = criterion(logits, batch_actions)
            loss.backward()
            optimizer.step()

        # Step 5: Evaluate Current Policy
        avg_reward = np.mean([traj['reward'] for traj in elite_trajectories])
        print(f"Iteration {iteration}: Elite Trajectories: {len(elite_trajectories)}, Average Reward: {avg_reward:.2f}")

    # Close environments
    envs.close()

    # Testing the Trained Policy
    def test_policy(policy, env_name=ENV_NAME, episodes=5, max_steps=500):
        env = gym.make(env_name)
        total_rewards = []
        for episode in range(episodes):
            obs, _ = env.reset()
            done = False
            episode_reward = 0
            for _ in range(max_steps):
                obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)
                with torch.no_grad():
                    action = policy.get_action(obs_tensor).item()
                obs, reward, done, info, _ = env.step(action)
                episode_reward += reward
                if done:
                    break
            total_rewards.append(episode_reward)
            print(f"Test Episode {episode + 1}: Reward: {episode_reward}")
        env.close()
        print(f"Average Test Reward over {episodes} episodes: {np.mean(total_rewards):.2f}")

    # Run the test
    test_policy(policy)
====================================================================
-> Chunk: 0.1.0\cpbo.py::1


import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import namedtuple
from torch.utils.data import DataLoader, TensorDataset

# Define a simple policy network
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)  # Output action probabilities
        )

    def forward(self, x):
        return self.network(x)
====================================================================
-> Chunk: 0.1.0\cpbo.py::6


# Main CBPO algorithm
def CBPO(env_name='CartPole-v1', num_epochs=10, num_episodes_per_epoch=100, gamma=0.99, 
         batch_size=64, learning_rate=1e-3, device='cpu'):

    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    policy = PolicyNetwork(state_dim, action_dim).to(device)
    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")

        # 1. Collect trajectories
        trajectories = collect_trajectories(env, policy, num_episodes_per_epoch, device)

        # 2. Create labeled dataset
        states, actions, labels = create_labeled_dataset(trajectories, gamma, device)

        # 3. Create DataLoader
        dataset = TensorDataset(states, actions, labels)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        # 4. Behavioral Cloning Update
        behavioral_cloning_update(policy, optimizer, dataloader, device)

        # 5. Evaluate current policy
        avg_reward = evaluate_policy(env, policy, device)
        print(f"Average Reward: {avg_reward}")

        # Early stopping if solved
        if avg_reward >= env.spec.reward_threshold:
            print(f"Environment solved in {epoch+1} epochs!")
            break

    env.close()
    return policy
====================================================================


###### Cluster Eval ######
Score: 4.333333333333333
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: ell\configurator.py::1


from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)
====================================================================
-> Chunk: ell\configurator.py::2


class Config(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    registry: Dict[str, _Model] = Field(default_factory=dict, description="A dictionary mapping model names to their configurations.")
    verbose: bool = Field(default=False, description="If True, enables verbose logging.")
    wrapped_logging: bool = Field(default=True, description="If True, enables wrapped logging for better readability.")
    override_wrapped_logging_width: Optional[int] = Field(default=None, description="If set, overrides the default width for wrapped logging.")
    store: Optional[Store] = Field(default=None, description="An optional Store instance for persistence.")
    autocommit: bool = Field(default=False, description="If True, enables automatic committing of changes to the store.")
    lazy_versioning: bool = Field(default=True, description="If True, enables lazy versioning for improved performance.")
    default_api_params: Dict[str, Any] = Field(default_factory=dict, description="Default parameters for language models.")
    default_client: Optional[openai.Client] = Field(default=None, description="The default OpenAI client used when a specific model client is not found.")
    autocommit_model: str = Field(default="gpt-4o-mini", description="When set, changes the default autocommit model from GPT 4o mini.")
    providers: Dict[Type, Provider] = Field(default_factory=dict, description="A dictionary mapping client types to provider classes.")
    def __init__(self, **data):
        super().__init__(**data)
        self._lock = threading.Lock()
        self._local = threading.local()
====================================================================
-> Chunk: ell\configurator.py::3


class Config(BaseModel):


    def register_model(
        self, 
        name: str,
        default_client: Optional[Union[openai.Client, Any]] = None,
        supports_streaming: Optional[bool] = None
    ) -> None:
        """
        Register a model with its configuration.
        """
        with self._lock:
            # XXX: Will be deprecated in 0.1.0
            self.registry[name] = _Model(
                name=name,
                default_client=default_client,
                supports_streaming=supports_streaming
            )
====================================================================
-> Chunk: ell\configurator.py::4


class Config(BaseModel):



    @contextmanager
    def model_registry_override(self, overrides: Dict[str, _Model]):
        """
        Temporarily override the model registry with new model configurations.

        :param overrides: A dictionary of model names to ModelConfig instances to override.
        :type overrides: Dict[str, ModelConfig]
        """
        if not hasattr(self._local, 'stack'):
            self._local.stack = []

        with self._lock:
            current_registry = self._local.stack[-1] if self._local.stack else self.registry
            new_registry = current_registry.copy()
            new_registry.update(overrides)

        self._local.stack.append(new_registry)
        try:
            yield
        finally:
            self._local.stack.pop()
====================================================================
-> Chunk: ell\configurator.py::5


class Config(BaseModel):

    def get_client_for(self, model_name: str) -> Tuple[Optional[openai.Client], bool]:
        """
        Get the OpenAI client for a specific model name.

        :param model_name: The name of the model to get the client for.
        :type model_name: str
        :return: The OpenAI client for the specified model, or None if not found, and a fallback flag.
        :rtype: Tuple[Optional[openai.Client], bool]
        """
        current_registry = self._local.stack[-1] if hasattr(self._local, 'stack') and self._local.stack else self.registry
        model_config = current_registry.get(model_name)
        fallback = False
        if not model_config:
            warning_message = f"Warning: A default provider for model '{model_name}' could not be found. Falling back to default OpenAI client from environment variables."
            if self.verbose:
                from colorama import Fore, Style
                _config_logger.warning(f"{Fore.LIGHTYELLOW_EX}{warning_message}{Style.RESET_ALL}")
            else:
                _config_logger.debug(warning_message)
            client = self.default_client
            fallback = True
        else:
            client = model_config.default_client
        return client, fallback
====================================================================
-> Chunk: ell\configurator.py::6


class Config(BaseModel):

    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:
        """
        Register a provider class for a specific client type.

        :param provider_class: The provider class to register.
        :type provider_class: Type[Provider]
        """
        assert isinstance(client_type, type), "client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))"
        with self._lock:
            self.providers[client_type] = provider
====================================================================
-> Chunk: ell\configurator.py::7


class Config(BaseModel):

    def get_provider_for(self, client: Union[Type[Any], Any]) -> Optional[Provider]:
        """
        Get the provider instance for a specific client instance.

        :param client: The client instance to get the provider for.
        :type client: Any
        :return: The provider instance for the specified client, or None if not found.
        :rtype: Optional[Provider]
        """

        client_type = type(client) if not isinstance(client, type) else client
        for provider_type, provider in self.providers.items():
            if issubclass(client_type, provider_type) or client_type == provider_type:
                return provider
        return None

# Single* instance
# XXX: Make a singleton
config = Config()
====================================================================
-> Chunk: lmp\_track.py::3


def _serialize_lmp(func):
    # Serialize deptjh first all fo the used lmps.
    for f in func.__ell_uses__:
        _serialize_lmp(f)

    if getattr(func, "_has_serialized_lmp", False):
        return
    func._has_serialized_lmp = False
    fn_closure = func.__ell_closure__
    lmp_type = func.__ell_type__
    name = func.__qualname__
    api_params = getattr(func, "__ell_api_params__", None)

    lmps = config.store.get_versions_by_fqn(fqn=name)
    version = 0
    already_in_store = any(lmp.lmp_id == func.__ell_hash__ for lmp in lmps)

    if not already_in_store:
        commit = None
        if lmps:
            latest_lmp = max(lmps, key=lambda x: x.created_at)
            version = latest_lmp.version_number + 1
            if config.autocommit:
                # XXX: Move this out to autocommit itself.
                if not _autocommit_warning():
                    from ell.util.differ import write_commit_message_for_diff
                    commit = str(write_commit_message_for_diff(
                    f"{latest_lmp.dependencies}\n\n{latest_lmp.source}", 
                        f"{fn_closure[1]}\n\n{fn_closure[0]}")[0])

        serialized_lmp = SerializedLMP(
            lmp_id=func.__ell_hash__,
            name=name,
            created_at=utc_now(),
            source=fn_closure[0],
            dependencies=fn_closure[1],
            commit_message=commit,
            initial_global_vars=get_immutable_vars(fn_closure[2]),
            initial_free_vars=get_immutable_vars(fn_closure[3]),
            lmp_type=lmp_type,
            api_params=api_params if api_params else None,
            version_number=version,
        )
        config.store.write_lmp(serialized_lmp, [f.__ell_hash__ for f in func.__ell_uses__])
    func._has_serialized_lmp = True
====================================================================
-> Chunk: studio\__main__.py::4


def main():
    # ... other code

    async def open_browser(host, port):
        while True:
            logger.debug(f"Checking TCP port {port} on {host} for readiness.")
            if _socket_is_open(host, port):
                url = f"http://{host}:{port}"
                logger.debug(f"Port is open, launching {url}.")
                webbrowser.open_new(url)
                return

            logger.debug(f"Port {port} was not open, retrying.")
            await asyncio.sleep(.1)

    # Start the database watcher
    loop = asyncio.new_event_loop()

    config = uvicorn.Config(app=app, host=args.host, port=args.port, loop=loop)
    server = uvicorn.Server(config)
    loop.create_task(server.serve())
    if db_path:
        loop.create_task(db_watcher(db_path, app))
    if args.open:
        loop.create_task(open_browser(args.host, args.port))
    loop.run_forever()

if __name__ == "__main__":
    main()
====================================================================
-> Chunk: studio\config.py::1


from functools import lru_cache
import os
from typing import Optional
from pydantic import BaseModel

import logging

logger = logging.getLogger(__name__)


# todo. maybe we default storage dir and other things in the future to a well-known location
# like ~/.ell or something
@lru_cache
def ell_home() -> str:
    return os.path.join(os.path.expanduser("~"), ".ell")


class Config(BaseModel):
    pg_connection_string: Optional[str] = None
    storage_dir: Optional[str] = None

    @classmethod
    def create(
        cls,
        storage_dir: Optional[str] = None,
        pg_connection_string: Optional[str] = None,
    ) -> 'Config':
        pg_connection_string = pg_connection_string or os.getenv("ELL_PG_CONNECTION_STRING")
        storage_dir = storage_dir or os.getenv("ELL_STORAGE_DIR")

        # Enforce that we use either sqlite or postgres, but not both
        if pg_connection_string is not None and storage_dir is not None:
            raise ValueError("Cannot use both sqlite and postgres")

        # For now, fall back to sqlite if no PostgreSQL connection string is provided
        if pg_connection_string is None and storage_dir is None:
            # This intends to honor the default we had set in the CLI
            storage_dir = os.getcwd()

        return cls(pg_connection_string=pg_connection_string, storage_dir=storage_dir)
====================================================================
-> Chunk: types\studio.py::5


class SerializedLMP(SerializedLMPBase, table=True):
    invocations: List["Invocation"] = Relationship(back_populates="lmp")
    used_by: Optional[List["SerializedLMP"]] = Relationship(
        back_populates="uses",
        link_model=SerializedLMPUses,
        sa_relationship_kwargs=dict(
            primaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id",
            secondaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id",
        ),
    )
    uses: List["SerializedLMP"] = Relationship(
        back_populates="used_by",
        link_model=SerializedLMPUses,
        sa_relationship_kwargs=dict(
            primaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_using_id",
            secondaryjoin="SerializedLMP.lmp_id==SerializedLMPUses.lmp_user_id",
        ),
    )

    class Config:
        table_name = "serializedlmp"
        unique_together = [("version_number", "name")]
====================================================================


###### Cluster Eval ######
Score: 4.333333333333333
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: ell\configurator.py::8


def init(
    store: Optional[Union[Store, str]] = None,
    verbose: bool = False,
    autocommit: bool = True,
    lazy_versioning: bool = True,
    default_api_params: Optional[Dict[str, Any]] = None,
    default_client: Optional[Any] = None,
    autocommit_model: str = "gpt-4o-mini"
) -> None:
    """
    Initialize the ELL configuration with various settings.

    :param verbose: Set verbosity of ELL operations.
    :type verbose: bool
    :param store: Set the store for ELL. Can be a Store instance or a string path for SQLiteStore.
    :type store: Union[Store, str], optional
    :param autocommit: Set autocommit for the store operations.
    :type autocommit: bool
    :param lazy_versioning: Enable or disable lazy versioning.
    :type lazy_versioning: bool
    :param default_api_params: Set default parameters for language models.
    :type default_api_params: Dict[str, Any], optional
    :param default_openai_client: Set the default OpenAI client.
    :type default_openai_client: openai.Client, optional
    :param autocommit_model: Set the model used for autocommitting.
    :type autocommit_model: str
    """
    # XXX: prevent double init
    config.verbose = verbose
    config.lazy_versioning = lazy_versioning

    if isinstance(store, str):
        from ell.stores.sql import SQLiteStore
        config.store = SQLiteStore(store)
    else:
        config.store = store
    config.autocommit = autocommit or config.autocommit

    if default_api_params is not None:
        config.default_api_params.update(default_api_params)

    if default_client is not None:
        config.default_client = default_client

    if autocommit_model is not None:
        config.autocommit_model = autocommit_model
====================================================================
-> Chunk: stores\sql.py::10


class SQLiteStore(SQLStore):
    def __init__(self, db_dir: str):
        assert not db_dir.endswith('.db'), "Create store with a directory not a db."

        os.makedirs(db_dir, exist_ok=True)
        self.db_dir = db_dir
        db_path = os.path.join(db_dir, 'ell.db')
        blob_store = SQLBlobStore(db_dir)
        super().__init__(f'sqlite:///{db_path}', blob_store=blob_store)

class SQLBlobStore(ell.store.BlobStore):
    def __init__(self, db_dir: str):
        self.db_dir = db_dir

    def store_blob(self, blob: bytes, blob_id  : str) -> str:
        file_path = self._get_blob_path(blob_id)
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with gzip.open(file_path, "wb") as f:
            f.write(blob)
        return blob_id

    def retrieve_blob(self, blob_id: str) -> bytes:
        file_path = self._get_blob_path(blob_id)
        with gzip.open(file_path, "rb") as f:
            return f.read()
====================================================================
-> Chunk: stores\sql.py::11


class SQLBlobStore(ell.store.BlobStore):

    def _get_blob_path(self, id: str, depth: int = 2) -> str:
        assert "-" in id, "Blob id must have a single - in it to split on."
        _type, _id = id.split("-")
        increment = 2
        dirs = [_type] + [_id[i:i+increment] for i in range(0, depth*increment, increment)]
        file_name = _id[depth*increment:]
        return os.path.join(self.db_dir, *dirs, file_name)

class PostgresStore(SQLStore):
    def __init__(self, db_uri: str):
        super().__init__(db_uri)
====================================================================
-> Chunk: studio\server.py::1


from typing import Optional, Dict, Any

from sqlmodel import Session
from ell.stores.sql import PostgresStore, SQLiteStore
from ell import __version__
from fastapi import FastAPI, Query, HTTPException, Depends, Response, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import logging
import json
from ell.studio.config import Config
from ell.studio.connection_manager import ConnectionManager
from ell.studio.datamodels import InvocationPublicWithConsumes, SerializedLMPWithUses

from ell.types import SerializedLMP
from datetime import datetime, timedelta
from sqlmodel import select


logger = logging.getLogger(__name__)


from ell.studio.datamodels import InvocationsAggregate


def get_serializer(config: Config):
    if config.pg_connection_string:
        return PostgresStore(config.pg_connection_string)
    elif config.storage_dir:
        return SQLiteStore(config.storage_dir)
    else:
        raise ValueError("No storage configuration found")
====================================================================


###### Cluster Eval ######
Score: 4.333333333333333
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: lmp\complex.py::4


def _get_messages(prompt_ret: Union[str, list[MessageOrDict]], prompt: LMP) -> list[Message]:
    """
    Helper function to convert the output of an LMP into a list of Messages.
    """
    if isinstance(prompt_ret, str):
        has_system_prompt = prompt.__doc__ is not None and prompt.__doc__.strip() != ""
        messages =     [Message(role="system", content=[ContentBlock(text=_lstr(prompt.__doc__ ) )])] if has_system_prompt else []
        return messages + [
            Message(role="user", content=[ContentBlock(text=prompt_ret)])
        ]
    else:
        assert isinstance(
            prompt_ret, list
        ), "Need to pass a list of Messages to the language model"
        return prompt_ret
====================================================================
-> Chunk: lmp\tool.py::2


def tool(*, exempt_from_tracking: bool = False, **tool_kwargs):
    def tool_decorator(fn: Callable[..., Any]) -> InvocableTool:
        _under_fn = fn

        @wraps(fn)
        def wrapper(
            *fn_args,
            _invocation_origin: str = None,
            _tool_call_id: str = None,
            **fn_kwargs
        ):
            #XXX: Post release, we need to wrap all tool arguments in type primitives for tracking I guess or change that tool makes the tool function inoperable.
            #XXX: Most people are not going to manually try and call the tool without a type primitive and if they do it will most likely be wrapped with l strs.

            if config.verbose and not exempt_from_tracking:
                pass
                # tool_usage_logger_pre(fn, fn_args, fn_kwargs, name, color)

            result = fn(*fn_args, **fn_kwargs)

            _invocation_api_params = dict(tool_kwargs=tool_kwargs)

            # Here you might want to add logic for tracking the tool usage
            # Similar to how it's done in the lm decorator # Use _invocation_origin

            if isinstance(result, str) and _invocation_origin:
                result = _lstr(result,origin_trace=_invocation_origin)

            #XXX: This _tool_call_id thing is a hack. Tracking should happen via params in the api
            # So if you call wiuth a _tool_callId
            if _tool_call_id:
                # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
                try:
                    if isinstance(result, ContentBlock):
                        content_results = [result]
                    elif isinstance(result, list) and all(isinstance(c, ContentBlock) for c in result):
                        content_results = result
                    else:
                        content_results = [ContentBlock(text=_lstr(json.dumps(result, ensure_ascii=False),origin_trace=_invocation_origin))]
                except TypeError as e:
                    raise TypeError(f"Failed to convert tool use result to ContentBlock: {e}. Tools must return json serializable objects. or a list of ContentBlocks.")
                # XXX: Need to support images and other content types somehow. We should look for images inside of the the result and then go from there.
                # try:
                #     content_results = coerce_content_list(result)
                # except ValueError as e:

                # TODO: poolymorphic validation here is important (cant have tool_call or formatted_response in the result)
                # XXX: Should we put this coercion here or in the tool call/result area.
                for c in content_results:
                    assert not c.tool_call, "Tool call in tool result"
                    # assert not c.formatted_response, "Formatted response in tool result"
                    if c.parsed:
                        # Warning: Formatted response in tool result will be converted to text
                        # TODO: Logging needs to produce not print.
                        print(f"Warning: Formatted response in tool result will be converted to text. Original: {c.parsed}")
                        c.text = _lstr(c.parsed.model_dump_json(),origin_trace=_invocation_origin)
                        c.parsed = None
                    assert not c.audio, "Audio in tool result"
                return ToolResult(tool_call_id=_tool_call_id, result=content_results), _invocation_api_params, {}
            else:
                return result, _invocation_api_params, {}
        # ... other code
    # ... other code
====================================================================
-> Chunk: providers\anthropic.py::1


from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent

from ell.types._lstr import _lstr
from ell.types.message import LMP
from ell.configurator import register_provider
from ell.util.serialization import serialize_image
import base64
from io import BytesIO
import json
import requests
from PIL import Image as PILImage

try:
    import anthropic
    from anthropic import Anthropic
    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent
    from anthropic.types.message_create_params import MessageCreateParamsStreaming
    from anthropic._streaming import Stream

    class AnthropicProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            return client.messages.create

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())
            # XXX: Helper, but should be depreicated due to ssot
            assert final_call_params.get("max_tokens") is not None, f"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}}))."

            dirty_msgs = [
                MessageParam(
                    role=cast(Literal["user", "assistant"], message.role), 
                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]
            role_correct_msgs   : List[MessageParam] = []
            for msg in dirty_msgs:
                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):
                    role_correct_msgs.append(msg)
                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])

            system_message = None
            if role_correct_msgs and role_correct_msgs[0]["role"] == "system":
                system_message = role_correct_msgs.pop(0)

            if system_message:
                final_call_params["system"] = system_message["content"][0]["text"]


            final_call_params['stream'] = True
            final_call_params["model"] = ell_call.model
            final_call_params["messages"] = role_correct_msgs

            if ell_call.tools:
                final_call_params["tools"] = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        name=tool.__name__,
                        description=tool.__doc__,
                        input_schema=tool.__ell_params_model__.model_json_schema(),
                    )
                    for tool in ell_call.tools
                ]

            # print(final_call_params)
            return final_call_params

        def translate_from_provider(
            self,
            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            usage = {}
            tracked_results = []
            metadata = {}

            #XXX: Support n > 0

            if provider_call_params.get("stream", False):
                content = []
                current_blocks: Dict[int, Dict[str, Any]] = {}
                message_metadata = {}

                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:
                    for chunk in stream:
                        if chunk.type == "message_start":
                            message_metadata = chunk.message.model_dump()
                            message_metadata.pop("content", None)  # Remove content as we'll build it separately

                        elif chunk.type == "content_block_start":
                            block = chunk.content_block.model_dump()
                            current_blocks[chunk.index] = block
                            if block["type"] == "tool_use":
                                if logger: logger(f" <tool_use: {block['name']}(")
                                block["input"] = "" # force it to be a string, XXX: can implement partially parsed json later.
                        elif chunk.type == "content_block_delta":
                            if chunk.index in current_blocks:
                                block = current_blocks[chunk.index]
                                if (delta := chunk.delta).type == "text_delta":
                                    block["text"] += delta.text
                                    if logger: logger(delta.text)
                                if delta.type == "input_json_delta":
                                    block["input"] += delta.partial_json
                                    if logger: logger(delta.partial_json)

                        elif chunk.type == "content_block_stop":
                            if chunk.index in current_blocks:
                                block = current_blocks.pop(chunk.index)
                                if block["type"] == "text":
                                    content.append(ContentBlock(text=_lstr(block["text"],origin_trace=origin_id)))
                                elif block["type"] == "tool_use":
                                    try:
                                        matching_tool = ell_call.get_tool_by_name(block["name"])
                                        if matching_tool:
                                            content.append(
                                                ContentBlock(
                                                    tool_call=ToolCall(
                                                        tool=matching_tool,
                                                        tool_call_id=_lstr(
                                                            block['id'],origin_trace=origin_id
                                                        ),
                                                        params=json.loads(block['input']) if block['input'] else {},
                                                    )
                                                )
                                            )
                                    except json.JSONDecodeError:
                                        if logger: logger(f" - FAILED TO PARSE JSON")
                                        pass
                                    if logger: logger(f")>")

                        elif chunk.type == "message_delta":
                            message_metadata.update(chunk.delta.model_dump())
                            if chunk.usage:
                                usage.update(chunk.usage.model_dump())

                        elif chunk.type == "message_stop":
                            tracked_results.append(Message(role="assistant", content=content))

                        # print(chunk)
                metadata = message_metadata

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("input_tokens", 0)
            usage["completion_tokens"] = usage.get("output_tokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata

    # XXX: Make a singleton.
    anthropic_provider = AnthropicProvider()
    register_provider(anthropic_provider, anthropic.Anthropic)
    register_provider(anthropic_provider, anthropic.AnthropicBedrock)
    register_provider(anthropic_provider, anthropic.AnthropicVertex)

except ImportError:
    pass
====================================================================
-> Chunk: providers\bedrock.py::2


try:
    from botocore.client import BaseClient
    from botocore.eventstream import (EventStream)
    class BedrockProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Any, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and api_call_params.get("stream", False):
                api_call_params.pop('stream')
                return client.converse_stream
            else:
                return client.converse

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = {}

            if ell_call.api_params.get('api_params',{}).get('stream', False):
                final_call_params['stream'] = ell_call.api_params.get('api_params',{}).get('stream', False)

            bedrock_converse_messages = [message_to_bedrock_message_format(message) for message in ell_call.messages]

            system_message = None
            if bedrock_converse_messages and bedrock_converse_messages[0]["role"] == "system":
                system_message = bedrock_converse_messages.pop(0)

            if system_message:
                final_call_params["system"] = [{'text':system_message["content"][0]["text"]}]

            final_call_params["modelId"] = ell_call.model
            final_call_params["messages"] = bedrock_converse_messages

            if ell_call.tools:
                tools = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        toolSpec = dict(
                            name=tool.__name__,
                            description=tool.__doc__,
                            inputSchema=dict(
                                json=tool.__ell_params_model__.model_json_schema(),
                            )
                        )
                    )
                    for tool in ell_call.tools
                ]
                final_call_params["toolConfig"] = {'tools':tools}

            return final_call_params

        def translate_from_provider(
                self,
                provider_response: Union[EventStream, Any],
                ell_call: EllCallParams,
                provider_call_params: Dict[str, Any],
                origin_id: Optional[str] = None,
                logger: Optional[Callable[..., None]] = None,
            ) -> Tuple[List[Message], Metadata]:

            usage = {}
            metadata : Metadata = {}

            metadata : Metadata = {}
            tracked_results : List[Message] = []
            did_stream = ell_call.api_params.get("api_params", {}).get('stream')

            if did_stream:
                content = []
                current_block: Optional[Dict[str, Any]] = {}
                message_metadata = {}
                for chunk in provider_response.get('stream'):

                    if "messageStart" in chunk:
                        current_block['content'] = ''
                        pass
                    elif "contentBlockStart" in chunk:
                        pass
                    elif "contentBlockDelta" in chunk:
                        delta = chunk.get("contentBlockDelta", {}).get("delta", {})
                        if "text" in delta:
                            current_block['type'] = 'text'
                            current_block['content'] += delta.get("text")
                            if logger:
                                logger(delta.get("text"))
                        else:
                            pass
                    elif "contentBlockStop" in chunk:
                        if current_block is not None:
                            if current_block["type"] == "text":
                                content.append(ContentBlock(text=_lstr(content=content, origin_trace=origin_id)))

                    elif "messageStop" in chunk:
                        tracked_results.append(Message(role="assistant", content=content))

                    elif "metadata" in chunk:
                        if "usage" in chunk["metadata"]:
                            usage["prompt_tokens"] = chunk["metadata"].get('usage').get("inputTokens", 0)
                            usage["completion_tokens"] = chunk["metadata"].get('usage').get("outputTokens", 0)
                            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']
                            message_metadata["usage"] = usage
                    else:
                        pass


                metadata = message_metadata
            else:
                # Non-streaming response processing (unchanged)
                cbs = []
                for content_block in provider_response.get('output', {}).get('message', {}).get('content', []):
                    if 'text' in content_block:
                        cbs.append(ContentBlock(text=_lstr(content_block.get('text'), origin_trace=origin_id)))
                    elif 'toolUse' in content_block:
                        assert ell_call.tools is not None, "Tools were not provided to the model when calling it and yet bedrock returned a tool use."
                        try:
                            toolUse = content_block['toolUse']
                            matching_tool = ell_call.get_tool_by_name(toolUse["name"])
                            if matching_tool:
                                cbs.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                toolUse['toolUseId'],origin_trace=origin_id
                                            ),
                                            params=toolUse['input'],
                                        )
                                    )
                                )
                        except json.JSONDecodeError:
                            if logger: logger(f" - FAILED TO PARSE JSON")
                            pass
                tracked_results.append(Message(role="assistant", content=cbs))
                if logger:
                    logger(tracked_results[0].text)


                # usage = call_result.response.usage.dict() if call_result.response.get('usage') else {}
                # metadata = call_result.response.model_dump()
                # del metadata["content"]

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("inputTokens", 0)
            usage["completion_tokens"] = usage.get("outputTokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata


    # XXX: Make a singleton.
    register_provider(BedrockProvider(), BaseClient)
except ImportError:
    pass
====================================================================
-> Chunk: providers\openai.py::1


from abc import ABC, abstractmethod
from collections import defaultdict
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast

from pydantic import BaseModel
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from ell.configurator import _Model, config, register_provider
from ell.types.message import LMP
from ell.util.serialization import serialize_image

try:
    # XXX: Could genericize.
    import openai
    from openai._streaming import Stream
    from openai.types.chat import ChatCompletion, ParsedChatCompletion, ChatCompletionChunk, ChatCompletionMessageParam

    class OpenAIProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : openai.Client, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            if api_call_params and (isinstance(fmt := api_call_params.get("response_format"), type)) and issubclass(fmt, BaseModel):
                return client.beta.chat.completions.parse
            else:
                return client.chat.completions.create

        def translate_to_provider(self, ell_call : EllCallParams) -> Dict[str, Any]:
            final_call_params = ell_call.api_params.copy()
            final_call_params["model"] = ell_call.model
            # Stream by default for verbose logging.
            final_call_params["stream"] = True
            final_call_params["stream_options"] = {"include_usage": True}

            # XXX: Deprecation of config.registry.supports_streaming when streaming is implemented.
            if ell_call.tools or final_call_params.get("response_format") or (regisered_model := config.registry.get(ell_call.model, None)) and regisered_model.supports_streaming is False:
                final_call_params.pop("stream", None)
                final_call_params.pop("stream_options", None)
            if ell_call.tools:
                final_call_params.update(
                    tool_choice=final_call_params.get("tool_choice", "auto"),
                    tools=[  
                        dict(
                            type="function",
                            function=dict(
                                name=tool.__name__,
                                description=tool.__doc__,
                                parameters=tool.__ell_params_model__.model_json_schema(),  #type: ignore
                            )
                        ) for tool in ell_call.tools
                    ]
                )
            # messages
            openai_messages : List[ChatCompletionMessageParam] = []
            for message in ell_call.messages:
                if (tool_calls := message.tool_calls):
                    assert message.role == "assistant", "Tool calls must be from the assistant."
                    assert all(t.tool_call_id for t in tool_calls), "Tool calls must have tool call ids."
                    openai_messages.append(dict(
                        tool_calls=[
                            dict(
                                id=cast(str, tool_call.tool_call_id),
                                type="function",
                                function=dict(
                                    name=tool_call.tool.__name__,
                                    arguments=json.dumps(tool_call.params.model_dump(), ensure_ascii=False)
                                )
                            ) for tool_call in tool_calls ],
                        role="assistant",
                        content=None,
                    ))
                elif (tool_results := message.tool_results):
                    for tool_result in tool_results:
                        assert all(cb.type == "text" for cb in tool_result.result), "Tool result does not match expected content blocks."
                        openai_messages.append(dict(
                            role="tool",
                            tool_call_id=tool_result.tool_call_id,
                            content=tool_result.text_only, 
                        ))
                else:
                    openai_messages.append(cast(ChatCompletionMessageParam, dict(
                        role=message.role,
                        content=[_content_block_to_openai_format(c) for c in message.content] 
                             if message.role != "system" 
                             else message.text_only
                    )))

            final_call_params["messages"] = openai_messages

            return final_call_params

        def translate_from_provider(
            self,
            provider_response: Union[
                ChatCompletion, 
                ParsedChatCompletion,
                Stream[ChatCompletionChunk], Any],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            metadata : Metadata = {}
            messages : List[Message] = []
            did_stream = provider_call_params.get("stream", False)


            if did_stream:
                stream = cast(Stream[ChatCompletionChunk], provider_response)
                message_streams = defaultdict(list)
                role : Optional[str] = None
                for chunk in stream:
                    metadata.update(chunk.model_dump(exclude={"choices"}))

                    for chat_compl_chunk in chunk.choices:
                        message_streams[chat_compl_chunk.index].append(chat_compl_chunk)
                        delta = chat_compl_chunk.delta
                        role = role or delta.role
                        if  chat_compl_chunk.index == 0 and logger:
                            logger(delta.content, is_refusal=hasattr(delta, "refusal") and delta.refusal)
                for _, message_stream in sorted(message_streams.items(), key=lambda x: x[0]):
                    text = "".join((choice.delta.content or "") for choice in message_stream)
                    messages.append(
                        Message(role=role, 
                                content=_lstr(content=text,origin_trace=origin_id)))
                    #XXX: Support streaming other types.
            else:
                chat_completion = cast(Union[ChatCompletion, ParsedChatCompletion], provider_response)
                metadata = chat_completion.model_dump(exclude={"choices"})
                for oai_choice in chat_completion.choices:
                    role = oai_choice.message.role
                    content_blocks = []
                    if (hasattr(message := oai_choice.message, "refusal") and (refusal := message.refusal)):
                        raise ValueError(refusal)
                    if hasattr(message, "parsed"):
                        if (parsed := message.parsed):
                            content_blocks.append(ContentBlock(parsed=parsed)) #XXX: Origin tracing
                            if logger: logger(parsed.model_dump_json())
                    else:
                        if (content := message.content):
                            content_blocks.append(
                                ContentBlock(
                                    text=_lstr(content=content,origin_trace=origin_id)))
                            if logger: logger(content)
                        if (tool_calls := message.tool_calls):
                            for tool_call in tool_calls:
                                matching_tool = ell_call.get_tool_by_name(tool_call.function.name)
                                assert matching_tool, "Model called tool not found in provided toolset."
                                content_blocks.append(
                                    ContentBlock(
                                        tool_call=ToolCall(
                                            tool=matching_tool,
                                            tool_call_id=_lstr(
                                                tool_call.id, origin_trace= origin_id),
                                            params=json.loads(tool_call.function.arguments),
                                        )
                                    )
                                )
                                if logger: logger(repr(tool_call))
                    messages.append(Message(role=role, content=content_blocks))
            return messages, metadata


    # xx: singleton needed
    openai_provider = OpenAIProvider()
    register_provider(openai_provider, openai.Client)
except ImportError:
    pass
====================================================================
-> Chunk: types\_lstr.py::1


"""
LM string that supports logits and keeps track of it'sorigin_trace even after mutation.
"""

import numpy as np
from typing import (
    Optional,
    Set,
    SupportsIndex,
    Union,
    FrozenSet,
    Iterable,
    List,
    Tuple,
    Any,
    Callable,
)
from typing_extensions import override
from pydantic import BaseModel, GetCoreSchemaHandler
from pydantic_core import CoreSchema

from pydantic_core import CoreSchema, core_schema


class _lstr(str):
    """
     A string class that supports logits and keeps track of itsorigin_trace even after mutation.
     This class is designed to be used in prompt engineering libraries where it is essential to associate
     logits with generated text and track the origin of the text.

     The `lstr` class inherits from the built-in `str` class and adds two additional attributes: `logits` and `origin_trace`.
     The `origin_trace` attribute is a frozen set of strings that represents theorigin_trace(s) of the string.

     The class provides various methods for manipulating the string, such as concatenation, slicing, splitting, and joining.
     These methods ensure that the logits andorigin_trace(s) are updated correctly based on the operation performed.

     The `lstr` class is particularly useful in LLM libraries for tracing the flow of prompts through various language model calls.
     By tracking theorigin_trace of each string, it is possible to visualize how outputs from one language model program influence
     the inputs of another, allowing for a detailed analysis of interactions between different large language models. This capability
     is crucial for understanding the propagation of prompts in complex LLM workflows and for building visual graphs that depict these interactions.

     It is important to note that any modification to the string (such as concatenation or replacement) will invalidate the associated logits.
     This is because the logits are specifically tied to the original string content, and any change would require a new computation of logits.
     The logic behind this is detailed elsewhere in this file.

     Example usage:
     ```
     # Create an lstr instance with logits and anorigin_trace
     logits = np.array([1.0, 2.0, 3.0])
    origin_trace = "4e9b7ec9"
     lstr_instance = lstr("Hello", logits,origin_trace)

     # Concatenate two lstr instances
     lstr_instance2 = lstr("World", None, "7f4d2c3a")
     concatenated_lstr = lstr_instance + lstr_instance2

     # Get the logits andorigin_trace of the concatenated lstr
     print(concatenated_lstr.logits)  # Output: None
     print(concatenated_lstr.origin_trace)  # Output: frozenset({'4e9b7ec9', '7f4d2c3a'})

     # Split the concatenated lstr into two parts
     parts = concatenated_lstr.split()
     print(parts)  # Output: [lstr('Hello', None, frozenset({'4e9b7ec9', '7f4d2c3a'})), lstr('World', None, frozenset({'4e9b7ec9', '7f4d2c3a'}))]
     ```
     Attributes:
        origin_trace (FrozenSet[str]): A frozen set of strings representing theorigin_trace(s) of the string.

     Methods:
         __new__: Create a new instance of lstr.
         __repr__: Return a string representation of the lstr instance.
         __add__: Concatenate this lstr instance with another string or lstr instance.
         __mod__: Perform a modulo operation between this lstr instance and another string, lstr, or a tuple of strings and lstrs.
         __mul__: Perform a multiplication operation between this lstr instance and an integer or another lstr.
         __rmul__: Perform a right multiplication operation between an integer or another lstr and this lstr instance.
         __getitem__: Get a slice or index of this lstr instance.
         __getattr__: Get an attribute from this lstr instance.
         join: Join a sequence of strings or lstr instances into a single lstr instance.
         split: Split this lstr instance into a list of lstr instances based on a separator.
         rsplit: Split this lstr instance into a list of lstr instances based on a separator, starting from the right.
         splitlines: Split this lstr instance into a list of lstr instances based on line breaks.
         partition: Partition this lstr instance into three lstr instances based on a separator.
         rpartition: Partition this lstr instance into three lstr instances based on a separator, starting from the right.
    """
====================================================================
-> Chunk: types\_lstr.py::2


class _lstr(str):

    def __new__(
        cls,
        content: str,
        logits: Optional[np.ndarray] = None,
        origin_trace: Optional[Union[str, FrozenSet[str]]] = None,
    ):
        """
         Create a new instance of lstr. The `logits` should be a numpy array and `origin_trace` should be a frozen set of strings or a single string.

         Args:
         content (str): The string content of the lstr.
         logits (np.ndarray, optional): The logits associated with this string. Defaults to None.
        origin_trace (Union[str, FrozenSet[str]], optional): Theorigin_trace(s) of this string. Defaults to None.
        """
        instance = super(_lstr, cls).__new__(cls, content)
        # instance._logits = logits
        if isinstance(origin_trace, str):
            instance.__origin_trace__ = frozenset({origin_trace})
        else:
            instance.__origin_trace__ = (
                frozenset(origin_trace) if origin_trace is not None else frozenset()
            )
        return instance

    # _logits: Optional[np.ndarray]
    __origin_trace__: FrozenSet[str]
====================================================================
-> Chunk: types\_lstr.py::3


class _lstr(str):

    @classmethod
    def __get_pydantic_core_schema__(
        cls, source_type: Any, handler: GetCoreSchemaHandler
    ) -> CoreSchema:
        def validate_lstr(value):
            if isinstance(value, dict) and value.get("__lstr", False):
                content = value["content"]
                origin_trace = value["__origin_trace__"].split(",")
                return cls(content, origin_trace=origin_trace)
            elif isinstance(value, str):
                return cls(value)
            elif isinstance(value, cls):
                return value
            else:
                raise ValueError(f"Invalid value for lstr: {value}")

        return core_schema.json_or_python_schema(
            json_schema=core_schema.typed_dict_schema(
                {
                    "content": core_schema.typed_dict_field(core_schema.str_schema()),
                    "__origin_trace__": core_schema.typed_dict_field(
                        core_schema.str_schema()
                    ),
                    "__lstr": core_schema.typed_dict_field(core_schema.bool_schema()),
                }
            ),
            python_schema=core_schema.union_schema(
                [
                    core_schema.is_instance_schema(cls),
                    core_schema.no_info_plain_validator_function(validate_lstr),
                ]
            ),
            serialization=core_schema.plain_serializer_function_ser_schema(
                lambda instance: {
                    "content": str(instance),
                    "__origin_trace__": (instance.__origin_trace__),
                    "__lstr": True,
                }
            ),
        )
====================================================================
-> Chunk: types\_lstr.py::4


class _lstr(str):

    @property
    def origin_trace(self) -> FrozenSet[str]:
        """
        Get theorigin_trace(s) of this lstr instance.

        Returns:
            FrozenSet[str]: A frozen set of strings representing theorigin_trace(s) of this lstr instance.
        """
        return self.__origin_trace__

    ########################
    ## Overriding methods ##
    ########################
    def __repr__(self) -> str:
        """
        Return a string representation of this lstr instance.

        Returns:
            str: A string representation of this lstr instance, including its content, logits, andorigin_trace(s).
        """
        return super().__repr__()
====================================================================
-> Chunk: types\_lstr.py::5


class _lstr(str):

    def __add__(self, other: Union[str, "_lstr"]) -> "_lstr":
        """
        Concatenate this lstr instance with another string or lstr instance.

        Args:
            other (Union[str, "lstr"]): The string or lstr instance to concatenate with this instance.

        Returns:
            lstr: A new lstr instance containing the concatenated content, with theorigin_trace(s) updated accordingly.
        """
        new_content = super(_lstr, self).__add__(other)
        self_origin = self.__origin_trace__

        if isinstance(other, _lstr):
            new_origin = self_origin
            new_origin = new_origin.union(other.__origin_trace__)
        else:
            new_origin = self_origin

        return _lstr(new_content, None, frozenset(new_origin))
====================================================================
-> Chunk: types\_lstr.py::6


class _lstr(str):

    def __mod__(
        self, other: Union[str, "_lstr", Tuple[Union[str, "_lstr"], ...]]
    ) -> "_lstr":
        """
        Perform a modulo operation between this lstr instance and another string, lstr, or a tuple of strings and lstrs,
        tracing the operation by logging the operands and the result.

        Args:
            other (Union[str, "lstr", Tuple[Union[str, "lstr"], ...]]): The right operand in the modulo operation.

        Returns:
            lstr: A new lstr instance containing the result of the modulo operation, with theorigin_trace(s) updated accordingly.
        """
        # If 'other' is a tuple, we need to handle each element
        if isinstance(other, tuple):
            result_content = super(_lstr, self).__mod__(tuple(str(o) for o in other))
            new__origin_trace__s = set(self.__origin_trace__)
            for item in other:
                if isinstance(item, _lstr):
                    new__origin_trace__s.update(item.__origin_trace__)
            new__origin_trace__ = frozenset(new__origin_trace__s)
        else:
            result_content = super(_lstr, self).__mod__(other)
            if isinstance(other, _lstr):
                new__origin_trace__ = self.__origin_trace__.union(
                    other.__origin_trace__
                )
            else:
                new__origin_trace__ = self.__origin_trace__

        return _lstr(result_content, None, new__origin_trace__)
====================================================================
-> Chunk: types\_lstr.py::7


class _lstr(str):

    def __mul__(self, other: SupportsIndex) -> "_lstr":
        """
        Perform a multiplication operation between this lstr instance and an integer or another lstr,
        tracing the operation by logging the operands and the result.

        Args:
            other (Union[SupportsIndex, "lstr"]): The right operand in the multiplication operation.

        Returns:
            lstr: A new lstr instance containing the result of the multiplication operation, with theorigin_trace(s) updated accordingly.
        """
        if isinstance(other, SupportsIndex):
            result_content = super(_lstr, self).__mul__(other)
            new__origin_trace__ = self.__origin_trace__
        else:
            return NotImplemented

        return _lstr(result_content, None, new__origin_trace__)
====================================================================
-> Chunk: types\_lstr.py::8


class _lstr(str):

    def __rmul__(self, other: SupportsIndex) -> "_lstr":
        """
        Perform a right multiplication operation between an integer or another lstr and this lstr instance,
        tracing the operation by logging the operands and the result.

        Args:
            other (Union[SupportsIndex, "lstr"]): The left operand in the multiplication operation.

        Returns:
            lstr: A new lstr instance containing the result of the multiplication operation, with theorigin_trace(s) updated accordingly.
        """
        return self.__mul__(other)  # Multiplication is commutative in this context
====================================================================
-> Chunk: types\_lstr.py::9


class _lstr(str):

    def __getitem__(self, key: Union[SupportsIndex, slice]) -> "_lstr":
        """
        Get a slice or index of this lstr instance.

        Args:
            key (Union[SupportsIndex, slice]): The index or slice to retrieve.

        Returns:
            lstr: A new lstr instance containing the sliced or indexed content, with theorigin_trace(s) preserved.
        """
        result = super(_lstr, self).__getitem__(key)
        # This is a matter of opinon. I believe that when you Index into a language model output, you or divorcing the lodges of the indexed result from their contacts which produce them. Therefore, it is only reasonable to directly index into the lodges without changing the original context, and so any mutation on the string should invalidate the logits.
        # try:
        #     logit_subset = self._logits[key] if self._logits else None
        # except:
        #   logit_subset = None
        logit_subset = None
        return _lstr(result, logit_subset, self.__origin_trace__)
====================================================================
-> Chunk: types\_lstr.py::10


class _lstr(str):

    def __getattribute__(self, name: str) -> Union[Callable, Any]:
        """
        Get an attribute from this lstr instance.

        Args:
            name (str): The name of the attribute to retrieve.

        Returns:
            Union[Callable, Any]: The requested attribute, which may be a method or a value.
        """
        # Get the attribute from the superclass (str)
        # First, try to get the attribute from the current class instance

        # Get the attribute using the superclass method
        attr = super().__getattribute__(name)

        # Check if the attribute is a callable and not defined in lstr class itself

        if name == "__class__":
            return type(self)

        if callable(attr) and name not in _lstr.__dict__:
            def wrapped(*args: Any, **kwargs: Any) -> Any:
                result = attr(*args, **kwargs)
                # If the result is a string, return an lstr instance
                if isinstance(result, str):
                    origin_traces = self.__origin_trace__
                    for arg in args:
                        if isinstance(arg, _lstr):
                            origin_traces = origin_traces.union(arg.__origin_trace__)
                    for key, value in kwargs.items():
                        if isinstance(value, _lstr):
                            origin_traces = origin_traces.union(value.__origin_trace__)
                    return _lstr(result, None, origin_traces)

                return result

            return wrapped

        return attr
====================================================================
-> Chunk: types\_lstr.py::11


class _lstr(str):

    @override
    def join(self, iterable: Iterable[Union[str, "_lstr"]]) -> "_lstr":
        """
        Join a sequence of strings or lstr instances into a single lstr instance.

        Args:
            iterable (Iterable[Union[str, "lstr"]]): The sequence of strings or lstr instances to join.

        Returns:
            lstr: A new lstr instance containing the joined content, with theorigin_trace(s) updated accordingly.
        """
        new__origin_trace__ = self.__origin_trace__
        parts = []
        for item in iterable:
            if isinstance(item, _lstr):
                new__origin_trace__ = new__origin_trace__.union(item.__origin_trace__)
            parts.append(item)
        new_content = super(_lstr, self).join(parts)

        return _lstr(new_content, None, new__origin_trace__)
====================================================================
-> Chunk: types\_lstr.py::12


class _lstr(str):

    @override
    def split(
        self, sep: Optional[Union[str, "_lstr"]] = None, maxsplit: SupportsIndex = -1
    ) -> List["_lstr"]:
        """
        Split this lstr instance into a list of lstr instances based on a separator.

        Args:
            sep (Optional[Union[str, "lstr"]], optional): The separator to split on. Defaults to None.
            maxsplit (SupportsIndex, optional): The maximum number of splits to perform. Defaults to -1.

        Returns:
            List["lstr"]: A list of lstr instances containing the split content, with theorigin_trace(s) preserved.
        """
        return self._split_helper(super(_lstr, self).split, sep, maxsplit)
====================================================================
-> Chunk: types\_lstr.py::13


class _lstr(str):

    @override
    def rsplit(
        self, sep: Optional[Union[str, "_lstr"]] = None, maxsplit: SupportsIndex = -1
    ) -> List["_lstr"]:
        """
        Split this lstr instance into a list of lstr instances based on a separator, starting from the right.

        Args:
            sep (Optional[Union[str, "lstr"]], optional): The separator to split on. Defaults to None.
            maxsplit (SupportsIndex, optional): The maximum number of splits to perform. Defaults to -1.

        Returns:
            List["lstr"]: A list of lstr instances containing the split content, with theorigin_trace(s) preserved.
        """
        return self._split_helper(super(_lstr, self).rsplit, sep, maxsplit)
====================================================================
-> Chunk: types\_lstr.py::14


class _lstr(str):

    @override
    def splitlines(self, keepends: bool = False) -> List["_lstr"]:
        """
        Split this lstr instance into a list of lstr instances based on line breaks.

        Args:
            keepends (bool, optional): Whether to include the line breaks in the resulting lstr instances. Defaults to False.

        Returns:
            List["lstr"]: A list of lstr instances containing the split content, with theorigin_trace(s) preserved.
        """
        return [
            _lstr(p, None, self.__origin_trace__)
            for p in super(_lstr, self).splitlines(keepends=keepends)
        ]
====================================================================
-> Chunk: types\_lstr.py::15


class _lstr(str):

    @override
    def partition(self, sep: Union[str, "_lstr"]) -> Tuple["_lstr", "_lstr", "_lstr"]:
        """
        Partition this lstr instance into three lstr instances based on a separator.

        Args:
            sep (Union[str, "lstr"]): The separator to partition on.

        Returns:
            Tuple["lstr", "lstr", "lstr"]: A tuple of three lstr instances containing the content before the separator, the separator itself, and the content after the separator, with theorigin_trace(s) updated accordingly.
        """
        return self._partition_helper(super(_lstr, self).partition, sep)
====================================================================
-> Chunk: types\_lstr.py::16


class _lstr(str):

    @override
    def rpartition(self, sep: Union[str, "_lstr"]) -> Tuple["_lstr", "_lstr", "_lstr"]:
        """
        Partition this lstr instance into three lstr instances based on a separator, starting from the right.

        Args:
            sep (Union[str, "lstr"]): The separator to partition on.

        Returns:
            Tuple["lstr", "lstr", "lstr"]: A tuple of three lstr instances containing the content before the separator, the separator itself, and the content after the separator, with theorigin_trace(s) updated accordingly.
        """
        return self._partition_helper(super(_lstr, self).rpartition, sep)
====================================================================
-> Chunk: types\_lstr.py::17


class _lstr(str):

    def _partition_helper(
        self, method, sep: Union[str, "_lstr"]
    ) -> Tuple["_lstr", "_lstr", "_lstr"]:
        """
        Helper method for partitioning this lstr instance based on a separator.

        Args:
            method (Callable): The partitioning method to use (either partition or rpartition).
            sep (Union[str, "lstr"]): The separator to partition on.

        Returns:
            Tuple["lstr", "lstr", "lstr"]: A tuple of three lstr instances containing the content before the separator, the separator itself, and the content after the separator, with theorigin_trace(s) updated accordingly.
        """
        part1, part2, part3 = method(sep)
        new__origin_trace__ = (
            self.__origin_trace__ | sep.__origin_trace__
            if isinstance(sep, _lstr)
            else self.__origin_trace__
        )
        return (
            _lstr(part1, None, new__origin_trace__),
            _lstr(part2, None, new__origin_trace__),
            _lstr(part3, None, new__origin_trace__),
        )
====================================================================
-> Chunk: types\_lstr.py::18


class _lstr(str):

    def _split_helper(
        self,
        method,
        sep: Optional[Union[str, "_lstr"]] = None,
        maxsplit: SupportsIndex = -1,
    ) -> List["_lstr"]:
        """
        Helper method for splitting this lstr instance based on a separator.

        Args:
            method (Callable): The splitting method to use (either split or rsplit).
            sep (Optional[Union[str, "lstr"]], optional): The separator to split on. Defaults to None.
            maxsplit (SupportsIndex, optional): The maximum number of splits to perform. Defaults to -1.

        Returns:
            List["lstr"]: A list of lstr instances containing the split content, with theorigin_trace(s) preserved.
        """
        origin_traces = (
            self.__origin_trace__ | sep.__origin_trace__
            if isinstance(sep, _lstr)
            else self.__origin_trace__
        )
        parts = method(sep, maxsplit)
        return [_lstr(part, None, origin_traces) for part in parts]
====================================================================
-> Chunk: types\_lstr.py::19


if __name__ == "__main__":
    import timeit
    import random
    import string

    def generate_random_string(length):
        return "".join(random.choices(string.ascii_letters + string.digits, k=length))

    def test_concatenation():
        s1 = generate_random_string(1000)
        s2 = generate_random_string(1000)

        lstr_time = timeit.timeit(lambda: _lstr(s1) + _lstr(s2), number=10000)
        str_time = timeit.timeit(lambda: s1 + s2, number=10000)

        print(f"Concatenation: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    def test_slicing():
        s = generate_random_string(10000)
        ls = _lstr(s)

        lstr_time = timeit.timeit(lambda: ls[1000:2000], number=10000)
        str_time = timeit.timeit(lambda: s[1000:2000], number=10000)

        print(f"Slicing: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    def test_splitting():
        s = generate_random_string(10000)
        ls = _lstr(s)

        lstr_time = timeit.timeit(lambda: ls.split(), number=1000)
        str_time = timeit.timeit(lambda: s.split(), number=1000)

        print(f"Splitting: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    def test_joining():
        words = [generate_random_string(10) for _ in range(1000)]
        lwords = [_lstr(word) for word in words]

        lstr_time = timeit.timeit(lambda: _lstr(" ").join(lwords), number=1000)
        str_time = timeit.timeit(lambda: " ".join(words), number=1000)

        print(f"Joining: lstr: {lstr_time:.6f}s, str: {str_time:.6f}s")

    print("Running performance tests...")
    test_concatenation()
    test_slicing()
    test_splitting()
    test_joining()

    import cProfile
    import pstats
    from io import StringIO

    def test_add():
        s1 = generate_random_string(1000)
        s2 = generate_random_string(1000)
        ls1 = _lstr(s1, None, "origin1")
        ls2 = _lstr(s2, None, "origin2")

        for _ in range(100000):
            result = ls1 + ls2

    print("\nProfiling __add__ method:")
    profiler = cProfile.Profile()
    profiler.enable()
    test_add()
    profiler.disable()

    s = StringIO()
    ps = pstats.Stats(profiler, stream=s).sort_stats("cumulative")
    ps.print_stats(20)  # Print top 20 lines
    print(s.getvalue())
====================================================================
-> Chunk: types\message.py::2


class ToolResult(BaseModel):
    tool_call_id: _lstr_generic
    result: List["ContentBlock"]

    @property
    def text(self) -> str:
        return _content_to_text(self.result)

    @property
    def text_only(self) -> str:
        return _content_to_text_only(self.result)

    # # XXX: Possibly deprecate
    # def readable_repr(self) -> str:
    #     return f"ToolResult(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"

    def __repr__(self):
        return f"{self.__class__.__name__}(tool_call_id={self.tool_call_id}, result={_content_to_text(self.result)})"
====================================================================
-> Chunk: types\message.py::3


class ToolCall(BaseModel):
    tool : InvocableTool
    tool_call_id : Optional[_lstr_generic] = Field(default=None)
    params : BaseModel

    def __init__(self, tool, params : Union[BaseModel, Dict[str, Any]],  tool_call_id=None):
        if not isinstance(params, BaseModel):
            params = tool.__ell_params_model__(**params) #convenience.
        super().__init__(tool=tool, tool_call_id=tool_call_id, params=params)

    def __call__(self, **kwargs):
        assert not kwargs, "Unexpected arguments provided. Calling a tool uses the params provided in the ToolCall."

        # XXX: TODO: MOVE TRACKING CODE TO _TRACK AND OUT OF HERE AND API.
        return self.tool(**self.params.model_dump())

    # XXX: Deprecate in 0.1.0
    def call_and_collect_as_message_block(self):
        raise DeprecationWarning("call_and_collect_as_message_block is deprecated. Use collect_as_content_block instead.")

    def call_and_collect_as_content_block(self):
        res = self.tool(**self.params.model_dump(), _tool_call_id=self.tool_call_id)
        return ContentBlock(tool_result=res)

    def call_and_collect_as_message(self):
        return Message(role="user", content=[self.call_and_collect_as_message_block()])

    def __repr__(self):
        return f"{self.__class__.__name__}({self.tool.__name__}({self.params}), tool_call_id='{self.tool_call_id}')"
====================================================================
-> Chunk: types\message.py::6


class ContentBlock(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)

    text: Optional[_lstr_generic] = Field(default=None)
    image: Optional[ImageContent] = Field(default=None)
    audio: Optional[Union[np.ndarray, List[float]]] = Field(default=None)
    tool_call: Optional[ToolCall] = Field(default=None)
    parsed: Optional[BaseModel] = Field(default=None)
    tool_result: Optional[ToolResult] = Field(default=None)
    # TODO: Add a JSON type? This would be nice for response_format. This is different than resposne_format = model. Or we could be opinionated and automatically parse the json response. That might be nice.
    # This breaks us maintaing parity with the openai python client in some sen but so does image.

    def __init__(self, *args, **kwargs):
        if "image" in kwargs and not isinstance(kwargs["image"], ImageContent):
            im = kwargs["image"] = ImageContent.coerce(kwargs["image"])
            # XXX: Backwards compatibility, Deprecate.
            if (d := kwargs.get("image_detail", None)): im.detail = d

        super().__init__(*args, **kwargs)


    @model_validator(mode='after')
    def check_single_non_null(self):
        non_null_fields = [field for field, value in self.__dict__.items() if value is not None]
        if len(non_null_fields) > 1:
            raise ValueError(f"Only one field can be non-null. Found: {', '.join(non_null_fields)}")
        return self

    def __str__(self):
        return repr(self)

    def __repr__(self):
        non_null_fields = [f"{field}={value}" for field, value in self.__dict__.items() if value is not None]
        return f"ContentBlock({', '.join(non_null_fields)})"

    @property
    def type(self):
        if self.text is not None:
            return "text"
        if self.image is not None:
            return "image"
        if self.audio is not None:
            return "audio"
        if self.tool_call is not None:
            return "tool_call"
        if self.parsed is not None:
            return "parsed"
        if self.tool_result is not None:
            return "tool_result"
        return None

    @property
    def content(self):
        return getattr(self, self.type)
====================================================================
-> Chunk: types\message.py::7


class ContentBlock(BaseModel):

    @classmethod
    def coerce(cls, content: AnyContent) -> "ContentBlock":
        """
        Coerce various types of content into a ContentBlock.

        This method provides a flexible way to create ContentBlock instances from different types of input.

        Args:
        content: The content to be coerced into a ContentBlock. Can be one of the following types:
        - str: Will be converted to a text ContentBlock.
        - ToolCall: Will be converted to a tool_call ContentBlock.
        - ToolResult: Will be converted to a tool_result ContentBlock.
        - BaseModel: Will be converted to a parsed ContentBlock.
        - ContentBlock: Will be returned as-is.
        - Image: Will be converted to an image ContentBlock.
        - np.ndarray: Will be converted to an image ContentBlock.
        - PILImage.Image: Will be converted to an image ContentBlock.

        Returns:
        ContentBlock: A new ContentBlock instance containing the coerced content.

        Raises:
        ValueError: If the content cannot be coerced into a valid ContentBlock.

        Examples:
        >>> ContentBlock.coerce("Hello, world!")
        ContentBlock(text="Hello, world!")

        >>> tool_call = ToolCall(...)
        >>> ContentBlock.coerce(tool_call)
        ContentBlock(tool_call=tool_call)

        >>> tool_result = ToolResult(...)
        >>> ContentBlock.coerce(tool_result)
        ContentBlock(tool_result=tool_result)

        >>> class MyModel(BaseModel):
        ...     field: str
        >>> model_instance = MyModel(field="value")
        >>> ContentBlock.coerce(model_instance)
        ContentBlock(parsed=model_instance)

        >>> from PIL import Image as PILImage
        >>> img = PILImage.new('RGB', (100, 100))
        >>> ContentBlock.coerce(img)
        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))

        >>> import numpy as np
        >>> arr = np.random.rand(100, 100, 3)
        >>> ContentBlock.coerce(arr)
        ContentBlock(image=ImageContent(image=<PIL.Image.Image object>))

        >>> image = Image(url="https://example.com/image.jpg")
        >>> ContentBlock.coerce(image)
        ContentBlock(image=ImageContent(url="https://example.com/image.jpg"))

        Notes:
        - This method is particularly useful when working with heterogeneous content types
          and you want to ensure they are all properly encapsulated in ContentBlock instances.
        - The method performs type checking and appropriate conversions to ensure the resulting
          ContentBlock is valid according to the model's constraints.
        - For image content, Image objects, PIL Image objects, and numpy arrays are supported,
          with automatic conversion to the appropriate format.
        - As a last resort, the method will attempt to create an image from the input before
          raising a ValueError.
        """
        if isinstance(content, ContentBlock):
            return content
        if isinstance(content, str):
            return cls(text=content)
        if isinstance(content, ToolCall):
            return cls(tool_call=content)
        if isinstance(content, ToolResult):
            return cls(tool_result=content)
        if isinstance(content, (ImageContent, np.ndarray, PILImage.Image)):
            return cls(image=ImageContent.coerce(content))
        if isinstance(content, BaseModel):
            return cls(parsed=content)

        raise ValueError(f"Invalid content type: {type(content)}")

    @field_serializer('parsed')
    def serialize_parsed(self, value: Optional[BaseModel], _info):
        if value is None:
            return None
        return value.model_dump(exclude_none=True, exclude_unset=True)
====================================================================
-> Chunk: types\message.py::8


def to_content_blocks(
    content: Optional[Union[AnyContent, List[AnyContent]]] = None,
    **content_block_kwargs
) -> List[ContentBlock]:
    """
    Coerce a variety of input types into a list of ContentBlock objects.

    Args:
    content: The content to be coerced. Can be a single item or a list of items.
             Supported types include str, ContentBlock, ToolCall, ToolResult, BaseModel, Image, np.ndarray, and PILImage.Image.
    **content_block_kwargs: Additional keyword arguments to pass to ContentBlock creation if content is None.

    Returns:
    List[ContentBlock]: A list of ContentBlock objects created from the input content.

    Examples:
    >>> coerce_content_list("Hello")
    [ContentBlock(text="Hello")]

    >>> coerce_content_list([ContentBlock(text="Hello"), "World"])
    [ContentBlock(text="Hello"), ContentBlock(text="World")]

    >>> from PIL import Image as PILImage
    >>> pil_image = PILImage.new('RGB', (100, 100))
    >>> coerce_content_list(pil_image)
    [ContentBlock(image=Image(image=<PIL.Image.Image object>))]

    >>> coerce_content_list(Image(url="https://example.com/image.jpg"))
    [ContentBlock(image=Image(url="https://example.com/image.jpg"))]

    >>> coerce_content_list(None, text="Default text")
    [ContentBlock(text="Default text")]
    """
    if content is None:
        return [ContentBlock(**content_block_kwargs)]

    if not isinstance(content, list):
        content = [content]

    return [ContentBlock.model_validate(ContentBlock.coerce(c)) for c in content]
====================================================================
-> Chunk: types\message.py::9


class Message(BaseModel):
    role: str
    content: List[ContentBlock]


    def __init__(self, role: str, content: Union[AnyContent, List[AnyContent], None] = None, **content_block_kwargs):
        content_blocks = to_content_blocks(content, **content_block_kwargs)

        super().__init__(role=role, content=content_blocks)

    # XXX: This choice of naming is unfortunate, but it is what it is.
    @property
    def text(self) -> str:
        """Returns all text content, replacing non-text content with their representations.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text
            'Hello\\n<PilImage>\\nWorld'
        """
        return _content_to_text(self.content)
====================================================================
-> Chunk: types\message.py::10


class Message(BaseModel):

    @property
    def images(self) -> List[ImageContent]:
        """Returns a list of all image content.

        Example:
            >>> from PIL import Image as PILImage
            >>> image1 = Image(url="https://example.com/image.jpg")
            >>> image2 = Image(image=PILImage.new('RGB', (200, 200)))
            >>> message = Message(role="user", content=["Text", image1, "More text", image2])
            >>> len(message.images)
            2
            >>> isinstance(message.images[0], Image)
            True
            >>> message.images[0].url
            'https://example.com/image.jpg'
            >>> isinstance(message.images[1].image, PILImage.Image)
            True
        """
        return [c.image for c in self.content if c.image]
====================================================================
-> Chunk: types\message.py::11


class Message(BaseModel):

    @property
    def audios(self) -> List[Union[np.ndarray, List[float]]]:
        """Returns a list of all audio content.

        Example:
            >>> audio1 = np.array([0.1, 0.2, 0.3])
            >>> audio2 = np.array([0.4, 0.5, 0.6])
            >>> message = Message(role="user", content=["Text", audio1, "More text", audio2])
            >>> len(message.audios)
            2
        """
        return [c.audio for c in self.content if c.audio]
====================================================================
-> Chunk: types\message.py::12


class Message(BaseModel):

    @property
    def text_only(self) -> str:
        """Returns only the text content, ignoring non-text content.

        Example:
            >>> message = Message(role="user", content=["Hello", PILImage.new('RGB', (100, 100)), "World"])
            >>> message.text_only
            'Hello\\nWorld'
        """
        return _content_to_text_only(self.content)

    @cached_property
    def tool_calls(self) -> List[ToolCall]:
        """Returns a list of all tool calls.

        Example:
            >>> tool_call = ToolCall(tool=lambda x: x, params=BaseModel())
            >>> message = Message(role="user", content=["Text", tool_call])
            >>> len(message.tool_calls)
            1
        """
        return [c.tool_call for c in self.content if c.tool_call is not None]

    @property
    def tool_results(self) -> List[ToolResult]:
        """Returns a list of all tool results.

        Example:
            >>> tool_result = ToolResult(tool_call_id="123", result=[ContentBlock(text="Result")])
            >>> message = Message(role="user", content=["Text", tool_result])
            >>> len(message.tool_results)
            1
        """
        return [c.tool_result for c in self.content if c.tool_result is not None]
====================================================================
-> Chunk: types\message.py::13


class Message(BaseModel):

    @property
    def parsed(self) -> Union[BaseModel, List[BaseModel]]:
        """Returns a list of all parsed content.

        Example:
            >>> class CustomModel(BaseModel):
            ...     value: int
            >>> parsed_content = CustomModel(value=42)
            >>> message = Message(role="user", content=["Text", ContentBlock(parsed=parsed_content)])
            >>> len(message.parsed)
            1
        """
        parsed_content = [c.parsed for c in self.content if c.parsed is not None]
        return parsed_content[0] if len(parsed_content) == 1 else parsed_content
====================================================================
-> Chunk: types\message.py::14


class Message(BaseModel):

    def call_tools_and_collect_as_message(self, parallel=False, max_workers=None):
        if parallel:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(c.tool_call.call_and_collect_as_content_block) for c in self.content if c.tool_call]
                content = [future.result() for future in as_completed(futures)]
        else:
            content = [c.tool_call.call_and_collect_as_content_block() for c in self.content if c.tool_call]
        return Message(role="user", content=content)
====================================================================
-> Chunk: types\message.py::15


# HELPERS 
def system(content: Union[AnyContent, List[AnyContent]]) -> Message:
    """
    Create a system message with the given content.

    Args:
    content (str): The content of the system message.

    Returns:
    Message: A Message object with role set to 'system' and the provided content.
    """
    return Message(role="system", content=content)


def user(content: Union[AnyContent, List[AnyContent]]) -> Message:
    """
    Create a user message with the given content.

    Args:
    content (str): The content of the user message.

    Returns:
    Message: A Message object with role set to 'user' and the provided content.
    """
    return Message(role="user", content=content)


def assistant(content: Union[AnyContent, List[AnyContent]]) -> Message:
    """
    Create an assistant message with the given content.

    Args:
    content (str): The content of the assistant message.

    Returns:
    Message: A Message object with role set to 'assistant' and the provided content.
    """
    return Message(role="assistant", content=content)

#XXX: Make a mixi for these properties.
def _content_to_text_only(content: List[ContentBlock]) -> str:
    return _lstr("\n").join(
            available_text
            for c in content
            if (available_text := (c.tool_result.text_only if c.tool_result else c.text))
        )

# Do we include the .text of a tool result? or its repr as in the current implementaiton?
# What is the user using .text for? I just want to see the result of the tools. text_only should get us the text of the tool results; the tool_call_id is irrelevant.
def _content_to_text(content: List[ContentBlock]) -> str:
    return _lstr("\n").join(
            available_text
            for c in content
            if (available_text :=  c.text or repr(c.content))
        )


# want to enable a use case where the user can actually return a standrd oai chat format
# This is a placehodler will likely come back later for this
LMPParams = Dict[str, Any]
# Well this is disappointing, I wanted to effectively type hint by doign that data sync meta, but eh, at elast we can still reference role or content this way. Probably wil lcan the dict sync meta. TypedDict is the ticket ell oh ell.
MessageOrDict = Union[Message, Dict[str, str]]
# Can support iamge prompts later.
Chat = List[
    Message
]  # [{"role": "system", "content": "prompt"}, {"role": "user", "content": "message"}]
MultiTurnLMP = Callable[..., Chat]
OneTurn = Callable[..., _lstr_generic]
# This is the specific LMP that must accept history as an argument and can take any additional arguments
ChatLMP = Callable[[Chat, Any], Chat]
LMP = Union[OneTurn, MultiTurnLMP, ChatLMP]
InvocableLM = Callable[..., _lstr_generic]
====================================================================
-> Chunk: util\verbosity.py::4


def wrap_text_with_prefix(message, width: int, prefix: str, subsequent_prefix: str, text_color: str) -> List[str]:
    """Wrap text while preserving the prefix and color for each line."""
    result = []
    for i, content in enumerate(message.content):
        wrapped_lines = []
        if content.image and content.image.image:
            wrapped_lines = plot_ascii(content.image.image, min(80, width - len(prefix)))
        else:
            if content.tool_result:
                contnets_to_wrap = [ContentBlock(text=f"ToolResult(tool_call_id={content.tool_result.tool_call_id}):"), *content.tool_result.result]
            else:
                contnets_to_wrap = [content]

            wrapped_lines = []
            for c in contnets_to_wrap:
                if c.image and c.image.image:
                    block_wrapped_lines = plot_ascii(c.image.image, min(80, width - len(prefix)))
                else:
                    text = _content_to_text([c])
                    paragraphs = text.split('\n')
                    wrapped_paragraphs = [textwrap.wrap(p, width=width - len(prefix)) for p in paragraphs]
                    block_wrapped_lines = [line for paragraph in wrapped_paragraphs for line in paragraph]
                wrapped_lines.extend(block_wrapped_lines)
        if i == 0:
            if wrapped_lines:
                result.append(f"{prefix}{text_color}{wrapped_lines[0]}{RESET}")
            else:
                result.append(f"{prefix}{text_color}{RESET}")
        else:
            result.append(f"{subsequent_prefix}{text_color}{wrapped_lines[0]}{RESET}")
        result.extend([f"{subsequent_prefix}{text_color}{line}{RESET}" for line in wrapped_lines[1:]])
    return result
====================================================================


###### Cluster Eval ######
Score: 3.3333333333333335
Cluster name: Graph Cluster
Graph Cluster:

-> Chunk: openai_realtime\api.py::1


import asyncio
import json
import websockets
from .event_handler import RealtimeEventHandler
from .utils import RealtimeUtils

class RealtimeAPI(RealtimeEventHandler):
    def __init__(self, url=None, api_key=None, dangerously_allow_api_key_in_browser=False, debug=False):
        super().__init__()
        self.default_url = 'wss://api.openai.com/v1/realtime'
        self.url = url or self.default_url
        self.api_key = api_key
        self.debug = debug
        self.ws = None

    def is_connected(self):
        return self.ws is not None and self.ws.open

    def log(self, *args):
        if self.debug:
            print(*args)
        return True
====================================================================
-> Chunk: openai_realtime\api.py::2


class RealtimeAPI(RealtimeEventHandler):

    async def connect(self, model='gpt-4o-realtime-preview-2024-10-01'):
        if self.is_connected():
            raise Exception("Already connected")

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'OpenAI-Beta': 'realtime=v1'
        }

        self.ws = await websockets.connect(f"{self.url}?model={model}", extra_headers=headers)

        self.log(f"Connected to {self.url}")

        asyncio.create_task(self._message_handler())

        return True
====================================================================
-> Chunk: openai_realtime\api.py::3


class RealtimeAPI(RealtimeEventHandler):

    async def _message_handler(self):
        try:
            async for message in self.ws:
                data = json.loads(message)
                self.receive(data['type'], data)
        except websockets.exceptions.ConnectionClosed:
            self.disconnect()
            self.dispatch('close', {'error': True})

    def disconnect(self):
        if self.ws:
            asyncio.create_task(self.ws.close())
            self.ws = None
        return True

    def receive(self, event_name, event):
        self.log("received:", event_name, event)
        self.dispatch(f"server.{event_name}", event)
        self.dispatch("server.*", event)
        return True
====================================================================
-> Chunk: openai_realtime\api.py::4


class RealtimeAPI(RealtimeEventHandler):

    def send(self, event_name, data=None):
        if not self.is_connected():
            raise Exception("RealtimeAPI is not connected")

        data = data or {}
        if not isinstance(data, dict):
            raise ValueError("data must be a dictionary")

        event = {
            "event_id": RealtimeUtils.generate_id("evt_"),
            "type": event_name,
            **data
        }

        self.dispatch(f"client.{event_name}", event)
        self.dispatch("client.*", event)
        self.log("sent:", event_name, event)

        asyncio.create_task(self.ws.send(json.dumps(event, ensure_ascii=False)))
        return True
====================================================================
-> Chunk: openai_realtime\client.py::1


import asyncio
import numpy as np
from .event_handler import RealtimeEventHandler
from .api import RealtimeAPI
from .conversation import RealtimeConversation
from .utils import RealtimeUtils
import json

class RealtimeClient(RealtimeEventHandler):
    def __init__(self, url=None, api_key=None, instructions='', dangerously_allow_api_key_in_browser=False, debug=False):
        super().__init__()
        self.default_session_config = {
            'modalities': ['text', 'audio'],
            'instructions': instructions,
            'voice': 'alloy',
            'input_audio_format': 'pcm16',
            'output_audio_format': 'pcm16',
            'input_audio_transcription': None,
            'turn_detection': None,
            'tools': [],
            'tool_choice': 'auto',
            'temperature': 0.8,
            'max_response_output_tokens': 4096,
        }
        self.session_config = {}
        self.transcription_models = [{'model': 'whisper-1'}]
        self.default_server_vad_config = {
            'type': 'server_vad',
            'threshold': 0.5,
            'prefix_padding_ms': 300,
            'silence_duration_ms': 200,
        }
        self.realtime = RealtimeAPI(url, api_key, dangerously_allow_api_key_in_browser, debug)
        self.conversation = RealtimeConversation()
        self._reset_config()
        self._add_api_event_handlers()

    def _reset_config(self):
        self.session_created = False
        self.tools = {}
        self.session_config = self.default_session_config.copy()
        self.input_audio_buffer = np.array([], dtype=np.int16)
        return True
====================================================================
-> Chunk: openai_realtime\conversation.py::1


import numpy as np
import json
from .utils import RealtimeUtils
import copy

class RealtimeConversation:
    def __init__(self):
        self.default_frequency = 24000  # 24,000 Hz
        self.clear()

    def clear(self):
        self.item_lookup = {}
        self.items = []
        self.response_lookup = {}
        self.responses = []
        self.queued_speech_items = {}
        self.queued_transcript_items = {}
        self.queued_input_audio = None
        return True

    def queue_input_audio(self, input_audio):
        self.queued_input_audio = input_audio
        return input_audio

    def process_event(self, event, *args):
        if 'event_id' not in event:
            raise ValueError("Missing 'event_id' on event")
        if 'type' not in event:
            raise ValueError("Missing 'type' on event")

        event_processor = getattr(self, f"_process_{event['type'].replace('.', '_')}", None)
        if not event_processor:
            raise ValueError(f"Missing conversation event processor for '{event['type']}'")

        return event_processor(event, *args)

    def get_item(self, id):
        return self.item_lookup.get(id)

    def get_items(self):
        return self.items.copy()
====================================================================
-> Chunk: openai_realtime\conversation.py::2


class RealtimeConversation:

    def _process_conversation_item_created(self, event):
        item = event['item']
        new_item = copy.deepcopy(item)
        if new_item['id'] not in self.item_lookup:
            self.item_lookup[new_item['id']] = new_item
            self.items.append(new_item)

        new_item['formatted'] = {
            'audio': np.array([], dtype=np.int16),
            'text': '',
            'transcript': ''
        }

        if new_item['type'] == 'message':
            if new_item['role'] == 'user':
                new_item['status'] = 'completed'
                if self.queued_input_audio is not None:
                    new_item['formatted']['audio'] = self.queued_input_audio
                    self.queued_input_audio = None
            else:
                new_item['status'] = 'in_progress'
        elif new_item['type'] == 'function_call':
            new_item['formatted']['tool'] = {
                'type': 'function',
                'name': new_item['name'],
                'call_id': new_item['call_id'],
                'arguments': ''
            }
            new_item['status'] = 'in_progress'
        elif new_item['type'] == 'function_call_output':
            new_item['status'] = 'completed'
            new_item['formatted']['output'] = new_item['output']

        if new_item.get('content'):
            text_content = [c for c in new_item['content'] if c['type'] in ['text', 'input_text']]
            for content in text_content:
                new_item['formatted']['text'] += content['text']

        if new_item['id'] in self.queued_speech_items:
            new_item['formatted']['audio'] = self.queued_speech_items[new_item['id']]['audio']
            del self.queued_speech_items[new_item['id']]

        if new_item['id'] in self.queued_transcript_items:
            new_item['formatted']['transcript'] = self.queued_transcript_items[new_item['id']]['transcript']
            del self.queued_transcript_items[new_item['id']]

        return {'item': new_item, 'delta': None}
====================================================================
-> Chunk: openai_realtime\conversation.py::3


class RealtimeConversation:

    def _process_conversation_item_truncated(self, event):
        item_id, audio_end_ms = event['item_id'], event['audio_end_ms']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"item.truncated: Item '{item_id}' not found")

        end_index = int((audio_end_ms * self.default_frequency) / 1000)
        item['formatted']['transcript'] = ''
        item['formatted']['audio'] = item['formatted']['audio'][:end_index]
        return {'item': item, 'delta': None}

    def _process_conversation_item_deleted(self, event):
        item_id = event['item_id']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"item.deleted: Item '{item_id}' not found")

        del self.item_lookup[item['id']]
        self.items = [i for i in self.items if i['id'] != item['id']]
        return {'item': item, 'delta': None}
====================================================================
-> Chunk: openai_realtime\conversation.py::4


class RealtimeConversation:

    def _process_conversation_item_input_audio_transcription_completed(self, event):
        item_id, content_index, transcript = event['item_id'], event['content_index'], event['transcript']
        item = self.item_lookup.get(item_id)
        formatted_transcript = transcript or ' '

        if not item:
            self.queued_transcript_items[item_id] = {'transcript': formatted_transcript}
            return {'item': None, 'delta': None}

        item['content'][content_index]['transcript'] = transcript
        item['formatted']['transcript'] = formatted_transcript
        return {'item': item, 'delta': {'transcript': transcript}}

    def _process_input_audio_buffer_speech_started(self, event):
        item_id, audio_start_ms = event['item_id'], event['audio_start_ms']
        self.queued_speech_items[item_id] = {'audio_start_ms': audio_start_ms}
        return {'item': None, 'delta': None}
====================================================================
-> Chunk: openai_realtime\conversation.py::5


class RealtimeConversation:

    def _process_input_audio_buffer_speech_stopped(self, event, input_audio_buffer):
        item_id, audio_end_ms = event['item_id'], event['audio_end_ms']
        speech = self.queued_speech_items[item_id]
        speech['audio_end_ms'] = audio_end_ms
        if input_audio_buffer is not None:
            start_index = int((speech['audio_start_ms'] * self.default_frequency) / 1000)
            end_index = int((speech['audio_end_ms'] * self.default_frequency) / 1000)
            speech['audio'] = input_audio_buffer[start_index:end_index]
        return {'item': None, 'delta': None}
====================================================================
-> Chunk: openai_realtime\conversation.py::6


class RealtimeConversation:

    def _process_response_created(self, event):
        response = event['response']
        if response['id'] not in self.response_lookup:
            self.response_lookup[response['id']] = response
            self.responses.append(response)
        return {'item': None, 'delta': None}

    def _process_response_output_item_added(self, event):
        response_id, item = event['response_id'], event['item']
        response = self.response_lookup.get(response_id)
        if not response:
            raise ValueError(f"response.output_item.added: Response '{response_id}' not found")
        response['output'].append(item['id'])
        return {'item': None, 'delta': None}

    def _process_response_output_item_done(self, event):
        item = event['item']
        if not item:
            raise ValueError("response.output_item.done: Missing 'item'")
        found_item = self.item_lookup.get(item['id'])
        if not found_item:
            raise ValueError(f"response.output_item.done: Item '{item['id']}' not found")
        found_item['status'] = item['status']
        return {'item': found_item, 'delta': None}

    def _process_response_content_part_added(self, event):
        item_id, part = event['item_id'], event['part']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.content_part.added: Item '{item_id}' not found")
        item['content'].append(part)
        return {'item': item, 'delta': None}
====================================================================
-> Chunk: openai_realtime\conversation.py::7


class RealtimeConversation:

    def _process_response_audio_transcript_delta(self, event):
        item_id, content_index, delta = event['item_id'], event['content_index'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.audio_transcript.delta: Item '{item_id}' not found")
        item['content'][content_index]['transcript'] += delta
        item['formatted']['transcript'] += delta
        return {'item': item, 'delta': {'transcript': delta}}
====================================================================
-> Chunk: openai_realtime\conversation.py::8


class RealtimeConversation:

    def _process_response_audio_delta(self, event):
        item_id, content_index, delta = event['item_id'], event['content_index'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.audio.delta: Item '{item_id}' not found")
        array_buffer = RealtimeUtils.base64_to_array_buffer(delta)
        append_values = np.frombuffer(array_buffer, dtype=np.int16)
        item['formatted']['audio'] = np.concatenate([item['formatted']['audio'], append_values])
        return {'item': item, 'delta': {'audio': append_values}}
====================================================================
-> Chunk: openai_realtime\conversation.py::9


class RealtimeConversation:

    def _process_response_text_delta(self, event):
        item_id, content_index, delta = event['item_id'], event['content_index'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.text.delta: Item '{item_id}' not found")
        item['content'][content_index]['text'] += delta
        item['formatted']['text'] += delta
        return {'item': item, 'delta': {'text': delta}}

    def _process_response_function_call_arguments_delta(self, event):
        item_id, delta = event['item_id'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.function_call_arguments.delta: Item '{item_id}' not found")
        item['arguments'] += delta
        item['formatted']['tool']['arguments'] += delta
        return {'item': item, 'delta': {'arguments': delta}}
====================================================================


Total coherence score: 4.133333333333333
