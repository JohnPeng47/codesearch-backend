_____ random _____

###### Cluster Eval ######
Score: 4.0
Cluster name: Random 2
Random 2:

-> Chunk: util\serialization.py::2


def get_immutable_vars(vars_dict):
    converter = cattrs.Converter()

    def handle_complex_types(obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj]
        elif isinstance(obj, dict):
            return {k: handle_complex_types(v) if not isinstance(v, (int, float, str, bool, type(None))) else v for k, v in obj.items()}
        elif isinstance(obj, (set, frozenset)):
            return list(sorted(handle_complex_types(item) if not isinstance(item, (int, float, str, bool, type(None))) else item for item in obj))
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return f"<Object of type {type(obj).__name__}>"

    converter.register_unstructure_hook(object, handle_complex_types)
    x = converter.unstructure(vars_dict)
    return x
====================================================================
-> Chunk: openai_realtime\api.py::2


class RealtimeAPI(RealtimeEventHandler):

    async def connect(self, model='gpt-4o-realtime-preview-2024-10-01'):
        if self.is_connected():
            raise Exception("Already connected")

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'OpenAI-Beta': 'realtime=v1'
        }

        self.ws = await websockets.connect(f"{self.url}?model={model}", extra_headers=headers)

        self.log(f"Connected to {self.url}")

        asyncio.create_task(self._message_handler())

        return True
====================================================================
-> Chunk: types\studio.py::6


class InvocationTrace(SQLModel, table=True):
    invocation_consumer_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)
    invocation_consuming_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)

# Should be subtyped for differnet kidns of LMPS.
# XXX: Move all ofh te binary data out to a different table.
# XXX: Need a flag that says dont store images.
# XXX: Deprecate the args columns
class InvocationBase(SQLModel):
    id: Optional[str] = Field(default=None, primary_key=True)
    lmp_id: str = Field(foreign_key="serializedlmp.lmp_id", index=True)
    latency_ms: float
    prompt_tokens: Optional[int] = Field(default=None)
    completion_tokens: Optional[int] = Field(default=None)
    state_cache_key: Optional[str] = Field(default=None)
    created_at: datetime = UTCTimestampField(default=func.now(), nullable=False)
    used_by_id: Optional[str] = Field(default=None, foreign_key="invocation.id", index=True)
    # global_vars and free_vars removed from here
====================================================================
-> Chunk: ell\__version__.py::1


try:
    from importlib.metadata import version
except ImportError:
    from importlib_metadata import version

__version__ = version("ell-ai")
====================================================================
-> Chunk: util\closure.py::6


def _process_signature_dependency(val, dependencies, already_closed, recursion_stack, uses, name: Optional[str] = None):
    # Todo: Build general cattr like utility for unstructuring python objects with hooks that keep track of state variables.
    # Todo: break up closure into types and functions.
    # XXX: This is not exhaustive, we should determine should import on all dependencies

    if name not in FORBIDDEN_NAMES:
        try:
            dep = None
            _uses = None
            if isinstance(val, (types.FunctionType, types.MethodType)):
                dep, _, _uses = lexical_closure(val, already_closed=already_closed, recursion_stack=recursion_stack.copy())
            elif isinstance(val, (list, tuple, set)):
                for item in val:
                    _process_signature_dependency(item, dependencies, already_closed, recursion_stack, uses)
            else:
                val_class = val if isinstance(val, type) else val.__class__
                try:
                    is_builtin = (val_class.__module__ == "builtins" or val_class.__module__ == "__builtins__")
                except:
                    is_builtin = False

                if not is_builtin:
                    if should_import(val_class.__module__):
                        dependencies.append(dill.source.getimport(val_class, alias=val_class.__name__))
                    else:
                        dep, _, _uses = lexical_closure(val_class, already_closed=already_closed, recursion_stack=recursion_stack.copy())

            if dep: dependencies.append(dep)
            if _uses: uses.update(_uses)
        except Exception as e:
            _raise_error(f"Failed to capture the lexical closure of parameter or annotation {name}", e, recursion_stack)
====================================================================


###### Cluster Eval ######
Score: 3.0
Cluster name: Random 5
Random 5:

-> Chunk: models\bedrock.py::1


from typing import Any
from ell.configurator import config
import logging

logger = logging.getLogger(__name__)


def register(client: Any):
    """
    Register Bedrock models with the provided client.

    This function takes an boto3 client and registers various Bedrock models
    with the global configuration. It allows the system to use these models
    for different AI tasks.

    Args:
        client (boto3.client): An instance of the bedrock client to be used
                                        for model registration.

    Note:
        The function doesn't return anything but updates the global
        configuration with the registered models.
    """
    model_data = [
        ('anthropic.claude-3-opus-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-sonnet-20240229-v1:0', 'bedrock'),
        ('anthropic.claude-3-haiku-20240307-v1:0', 'bedrock'),
        ('anthropic.claude-3-5-sonnet-20240620-v1:0', 'bedrock'),

        ('mistral.mistral-7b-instruct-v0:2', 'bedrock'),
        ('mistral.mixtral-8x7b-instruct-v0:1', 'bedrock'),
        ('mistral.mistral-large-2402-v1:0', 'bedrock'),
        ('mistral.mistral-small-2402-v1:0', 'bedrock'),


        ('ai21.jamba-instruct-v1:0','bedrock'),
        ('ai21.j2-ultra-v1', 'bedrock'),
        ('ai21.j2-mid-v1', 'bedrock'),

        ('amazon.titan-embed-text-v1', 'bedrock'),
        ('amazon.titan-text-lite-v1', 'bedrock'),
        ('amazon.titan-text-express-v1', 'bedrock'),
        ('amazon.titan-image-generator-v2:0', 'bedrock'),
        ('amazon.titan-image-generator-v1', 'bedrock'),

        ('cohere.command-r-plus-v1:0', 'bedrock'),
        ('cohere.command-r-v1:0', 'bedrock'),
        ('cohere.embed-english-v3', 'bedrock'),
        ('cohere.embed-multilingual-v3', 'bedrock'),
        ('cohere.command-text-v14', 'bedrock'),

        ('meta.llama3-8b-instruct-v1:0', 'bedrock'),
        ('meta.llama3-70b-instruct-v1:0', 'bedrock'),
        ('meta.llama2-13b-chat-v1', 'bedrock'),
        ('meta.llama2-70b-chat-v1', 'bedrock'),
        ('meta.llama2-13b-v1', 'bedrock'),

    ]

    for model_id, owned_by in model_data:
        config.register_model(name=model_id, default_client=client, supports_streaming=True)

default_client = None
try:

    import boto3
    default_client = boto3.client('bedrock-runtime')
except Exception as e:
    pass

register(default_client)
====================================================================
-> Chunk: util\verbosity.py::6


def model_usage_logger_pre(
    invoking_lmp: LMP,
    lmp_args: Tuple,
    lmp_kwargs: Dict,
    lmp_hash: str,
    messages: List[Message],
    arg_max_length: int = 8
):
    """Log model usage before execution with customizable argument display length and ASCII box."""
    color =  compute_color(invoking_lmp)
    formatted_args = [format_arg(arg, arg_max_length) for arg in lmp_args]
    formatted_kwargs = [format_kwarg(key, lmp_kwargs[key], arg_max_length) for key in lmp_kwargs]
    formatted_params = ', '.join(formatted_args + formatted_kwargs)

    check_version_and_log()

    terminal_width = get_terminal_width()

    logger.info(f"Invoking LMP: {invoking_lmp.__name__} (hash: {lmp_hash[:8]})")

    print(f"{PIPE_COLOR}╔{'═' * (terminal_width - 2)}╗{RESET}")
    print(f"{PIPE_COLOR}║ {color}{BOLD}{UNDERLINE}{invoking_lmp.__name__}{RESET}{color}({formatted_params}){RESET}")
    print(f"{PIPE_COLOR}╠{'═' * (terminal_width - 2)}╣{RESET}")
    print(f"{PIPE_COLOR}║ {BOLD}Prompt:{RESET}")
    print(f"{PIPE_COLOR}╟{'─' * (terminal_width - 2)}╢{RESET}")

    max_role_length = max(len("assistant"), max(len(message.role) for message in messages))
    print_wrapped_messages(messages, max_role_length, color)
====================================================================
-> Chunk: util\closure_util.py::3


def get_referenced_names(code: str, module_name: str):
    """
    This function takes a block of code and a module name as input. It parses the code into an Abstract Syntax Tree (AST)
    and walks through the tree to find all instances where an attribute of the module is referenced in the code.

    Parameters:
    code (str): The block of code to be parsed.
    module_name (str): The name of the module to look for in the code.

    Returns:
    list: A list of all attributes of the module that are referenced in the code.
    """
    tree = ast.parse(code)
    referenced_names = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Attribute):
            if isinstance(node.value, ast.Name) and node.value.id == module_name:
                referenced_names.append(node.attr)

    return referenced_names
====================================================================
-> Chunk: util\verbosity.py::2


def check_version_and_log():
    global _has_logged_version_statement
    if _version_check_lock.acquire(blocking=False):
        try:
            if not _has_logged_version_statement:

                import ell

                try:
                    response = requests.get("https://version.ell.so/ell-ai/pypi", timeout=0.15)
                    if response.status_code == 200:
                        latest_version = response.text.strip()
                        if latest_version != ell.__version__:
                            print(f"{Fore.YELLOW}╔═════════════════════════════════════════════════════════════════╗")
                            print(f"{Fore.YELLOW}║ {Fore.GREEN}A new version of ELL is available: {Fore.CYAN}{latest_version:<29}{Fore.YELLOW}║")
                            print(f"{Fore.YELLOW}║ {Fore.GREEN}You can update by running:{Fore.YELLOW}                                      ║")
                            print(f"{Fore.YELLOW}║ {Fore.CYAN}pip install --upgrade ell-ai{Fore.YELLOW}                                    ║")
                            print(f"{Fore.YELLOW}╚═════════════════════════════════════════════════════════════════╝{Style.RESET_ALL}")
                except requests.RequestException:
                    pass  # Silently handle any network-related errors
                _has_logged_version_statement = True
        finally:
            _version_check_lock.release()
====================================================================
-> Chunk: types\studio.py::6


class InvocationTrace(SQLModel, table=True):
    invocation_consumer_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)
    invocation_consuming_id: str = Field(foreign_key="invocation.id", primary_key=True, index=True)

# Should be subtyped for differnet kidns of LMPS.
# XXX: Move all ofh te binary data out to a different table.
# XXX: Need a flag that says dont store images.
# XXX: Deprecate the args columns
class InvocationBase(SQLModel):
    id: Optional[str] = Field(default=None, primary_key=True)
    lmp_id: str = Field(foreign_key="serializedlmp.lmp_id", index=True)
    latency_ms: float
    prompt_tokens: Optional[int] = Field(default=None)
    completion_tokens: Optional[int] = Field(default=None)
    state_cache_key: Optional[str] = Field(default=None)
    created_at: datetime = UTCTimestampField(default=func.now(), nullable=False)
    used_by_id: Optional[str] = Field(default=None, foreign_key="invocation.id", index=True)
    # global_vars and free_vars removed from here
====================================================================


###### Cluster Eval ######
Score: 3.0
Cluster name: Random 6
Random 6:

-> Chunk: types\_lstr.py::8


class _lstr(str):

    def __rmul__(self, other: SupportsIndex) -> "_lstr":
        """
        Perform a right multiplication operation between an integer or another lstr and this lstr instance,
        tracing the operation by logging the operands and the result.

        Args:
            other (Union[SupportsIndex, "lstr"]): The left operand in the multiplication operation.

        Returns:
            lstr: A new lstr instance containing the result of the multiplication operation, with theorigin_trace(s) updated accordingly.
        """
        return self.__mul__(other)  # Multiplication is commutative in this context
====================================================================
-> Chunk: ell\provider.py::2


# XXX: Might leave this internal to providers so that the complex code is simpler &
# we can literally jsut call provider.call like any openai fn.
class EllCallParams(BaseModel):
    model: str = Field(..., description="Model identifier")
    messages: List[Message] = Field(..., description="Conversation context")
    client: Any = Field(..., description="API client")
    tools: List[LMP] = Field(default_factory=list, description="Available tools")
    api_params: Dict[str, Any] = Field(
        default_factory=dict, description="API parameters"
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def get_tool_by_name(self, name: str) -> Optional[LMP]:
        """Get a tool by name."""
        return next(
            (tool for tool in (self.tools or [])  if tool.__name__ == name), None
        )


Metadata = Dict[str, Any]
====================================================================
-> Chunk: types\_lstr.py::15


class _lstr(str):

    @override
    def partition(self, sep: Union[str, "_lstr"]) -> Tuple["_lstr", "_lstr", "_lstr"]:
        """
        Partition this lstr instance into three lstr instances based on a separator.

        Args:
            sep (Union[str, "lstr"]): The separator to partition on.

        Returns:
            Tuple["lstr", "lstr", "lstr"]: A tuple of three lstr instances containing the content before the separator, the separator itself, and the content after the separator, with theorigin_trace(s) updated accordingly.
        """
        return self._partition_helper(super(_lstr, self).partition, sep)
====================================================================
-> Chunk: util\closure.py::5


def _process_default_kwargs(func, dependencies, already_closed, recursion_stack, uses):
    """Process default keyword arguments and annotations of a function."""
    ps = inspect.signature(func).parameters
    for name, param in ps.items():
        if param.default is not inspect.Parameter.empty:
            _process_signature_dependency(param.default, dependencies, already_closed, recursion_stack, uses, name)
        if param.annotation is not inspect.Parameter.empty:
            _process_signature_dependency(param.annotation, dependencies, already_closed, recursion_stack, uses, f"{name}_annotation")
    if func.__annotations__.get('return') is not None:
        _process_signature_dependency(func.__annotations__['return'], dependencies, already_closed, recursion_stack, uses, "return_annotation")
    # XXX: In order to properly analyze this we should walk the AST rather than inspexting the signature; e.g. Field is FieldInfo not Field.
    # I don't care about the actual default at time of execution just the symbols required to statically reproduce the prompt.
====================================================================
-> Chunk: ell\store.py::1


from abc import ABC, abstractmethod
from contextlib import contextmanager
from datetime import datetime
from typing import Any, Optional, Dict, List, Set, Union
from ell.types._lstr import _lstr
from ell.types import SerializedLMP, Invocation
from ell.types.message import InvocableLM

class BlobStore(ABC):
    @abstractmethod
    def store_blob(self, blob: bytes, blob_id  : str) -> str:
        """Store a blob and return its identifier."""
        pass

    @abstractmethod
    def retrieve_blob(self, blob_id: str) -> bytes:
        """Retrieve a blob by its identifier."""
        pass
====================================================================


###### Cluster Eval ######
Score: 2.6666666666666665
Cluster name: Random 4
Random 4:

-> Chunk: ell\provider.py::5


# handhold the the implementer, in production mode we can turn these off for speed.
@lru_cache(maxsize=None)
def _call_params(call: Callable[..., Any]) -> MappingProxyType[str, inspect.Parameter]:
    return inspect.signature(call).parameters


def _validate_provider_call_params(
    api_call_params: Dict[str, Any], call: Callable[..., Any]
):
    provider_call_params = _call_params(call)

    required_params = {
        name: param
        for name, param in provider_call_params.items()
        if param.default == param.empty and param.kind != param.VAR_KEYWORD
    }

    for param_name in required_params:
        assert (
            param_name in api_call_params
        ), f"Provider implementation error: Required parameter '{param_name}' is missing in the converted call parameters converted from ell call."

    for param_name, param_value in api_call_params.items():
        assert (
            param_name in provider_call_params
        ), f"Provider implementation error: Unexpected parameter '{param_name}' in the converted call parameters."

    return True
====================================================================
-> Chunk: types\message.py::11


class Message(BaseModel):

    @property
    def audios(self) -> List[Union[np.ndarray, List[float]]]:
        """Returns a list of all audio content.

        Example:
            >>> audio1 = np.array([0.1, 0.2, 0.3])
            >>> audio2 = np.array([0.4, 0.5, 0.6])
            >>> message = Message(role="user", content=["Text", audio1, "More text", audio2])
            >>> len(message.audios)
            2
        """
        return [c.audio for c in self.content if c.audio]
====================================================================
-> Chunk: util\closure.py::9


def _process_module(var_name, var_value, modules, imports, uses):
    """Process a module."""
    if should_import(var_value.__name__):
        imports.append(dill.source.getimport(var_value, alias=var_name))
    else:
        modules.append((var_name, var_value))

def _process_other_variable(var_name, var_value, dependencies, uses):
    """Process variables that are not callables or modules."""
    if isinstance(var_value, str) and '\n' in var_value:
        dependencies.append(f"{var_name} = '''{var_value}'''")
    elif is_immutable_variable(var_value):
        dependencies.append(f"#<BV>\n{var_name} = {repr(var_value)}\n#</BV>")
    else:
        dependencies.append(f"#<BmV>\n{var_name} = <{type(var_value).__name__} object>\n#</BmV>")
====================================================================
-> Chunk: 0.1.0\cem.py::5


def select_elite(trajectories, percentile=ELITE_PERCENT):
    rewards = [traj['reward'] for traj in trajectories]
    if not rewards:
        return []
    reward_threshold = np.percentile(rewards, 100 - percentile)
    elite_trajectories = [traj for traj in trajectories if traj['reward'] >= reward_threshold]
    return elite_trajectories

# Function to create training dataset from elite trajectories
def create_training_data(elite_trajectories):
    states = []
    actions = []
    for traj in elite_trajectories:
        states.extend(traj['states'])
        actions.extend(traj['actions'])
    if not states or not actions:
        return None, None
    # Convert lists to NumPy arrays first for efficiency
    states = np.array(states, dtype=np.float32)
    actions = np.array(actions, dtype=np.int64)
    # Convert to PyTorch tensors
    states = torch.from_numpy(states)
    actions = torch.from_numpy(actions)
    return states, actions
====================================================================
-> Chunk: util\__init__.py::1


from .closure import lexically_closured_source
====================================================================


###### Cluster Eval ######
Score: 2.6666666666666665
Cluster name: Random 7
Random 7:

-> Chunk: openai_realtime\conversation.py::7


class RealtimeConversation:

    def _process_response_audio_transcript_delta(self, event):
        item_id, content_index, delta = event['item_id'], event['content_index'], event['delta']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.audio_transcript.delta: Item '{item_id}' not found")
        item['content'][content_index]['transcript'] += delta
        item['formatted']['transcript'] += delta
        return {'item': item, 'delta': {'transcript': delta}}
====================================================================
-> Chunk: 0.1.0\autostreamprevention.py::1


import openai
import os

# Define the function to stream the response
def stream_openai_response(prompt):
    try:
        # Make the API call
        response = openai.chat.completions.create(
            model="o1-mini",  # Specify the model
            messages=[{"role": "user", "content": prompt}],
            stream=True  # Enable streaming
        )

        # Stream the response
        for chunk in response:
            if chunk.choices[0].delta.get("content"):
                print(chunk.choices[0].delta.content, end="", flush=True)

        print()  # Print a newline at the end

    except Exception as e:
        print(f"An error occurred: {e}")

# Example usage
prompt = "Tell me a short joke."
stream_openai_response(prompt)
====================================================================
-> Chunk: openai_realtime\client.py::3


class RealtimeClient(RealtimeEventHandler):



    def is_connected(self):
        return self.realtime.is_connected() and self.session_created

    def reset(self):
        self.disconnect()
        self.clear_event_handlers()
        self.realtime.clear_event_handlers()
        self._reset_config()
        self._add_api_event_handlers()
        return True

    async def connect(self):
        if self.is_connected():
            raise Exception("Already connected, use .disconnect() first")
        await self.realtime.connect()
        self.update_session()
        return True

    async def wait_for_session_created(self):
        if not self.realtime.is_connected():
            raise Exception("Not connected, use .connect() first")
        while not self.session_created:
            await asyncio.sleep(0.001)
        return True

    def disconnect(self):
        self.session_created = False
        self.conversation.clear()
        if self.realtime.is_connected():
            self.realtime.disconnect()

    def get_turn_detection_type(self):
        turn_detection = self.session_config.get('turn_detection')
        if isinstance(turn_detection, dict):
            return turn_detection.get('type')
        return None
====================================================================
-> Chunk: openai_realtime\conversation.py::6


class RealtimeConversation:

    def _process_response_created(self, event):
        response = event['response']
        if response['id'] not in self.response_lookup:
            self.response_lookup[response['id']] = response
            self.responses.append(response)
        return {'item': None, 'delta': None}

    def _process_response_output_item_added(self, event):
        response_id, item = event['response_id'], event['item']
        response = self.response_lookup.get(response_id)
        if not response:
            raise ValueError(f"response.output_item.added: Response '{response_id}' not found")
        response['output'].append(item['id'])
        return {'item': None, 'delta': None}

    def _process_response_output_item_done(self, event):
        item = event['item']
        if not item:
            raise ValueError("response.output_item.done: Missing 'item'")
        found_item = self.item_lookup.get(item['id'])
        if not found_item:
            raise ValueError(f"response.output_item.done: Item '{item['id']}' not found")
        found_item['status'] = item['status']
        return {'item': found_item, 'delta': None}

    def _process_response_content_part_added(self, event):
        item_id, part = event['item_id'], event['part']
        item = self.item_lookup.get(item_id)
        if not item:
            raise ValueError(f"response.content_part.added: Item '{item_id}' not found")
        item['content'].append(part)
        return {'item': item, 'delta': None}
====================================================================
-> Chunk: lmp\complex.py::4


def _get_messages(prompt_ret: Union[str, list[MessageOrDict]], prompt: LMP) -> list[Message]:
    """
    Helper function to convert the output of an LMP into a list of Messages.
    """
    if isinstance(prompt_ret, str):
        has_system_prompt = prompt.__doc__ is not None and prompt.__doc__.strip() != ""
        messages =     [Message(role="system", content=[ContentBlock(text=_lstr(prompt.__doc__ ) )])] if has_system_prompt else []
        return messages + [
            Message(role="user", content=[ContentBlock(text=prompt_ret)])
        ]
    else:
        assert isinstance(
            prompt_ret, list
        ), "Need to pass a list of Messages to the language model"
        return prompt_ret
====================================================================


###### Cluster Eval ######
Score: 2.6666666666666665
Cluster name: Random 9
Random 9:

-> Chunk: ell\provider.py::1


from abc import ABC, abstractmethod
from collections import defaultdict
from functools import lru_cache
import inspect
from types import MappingProxyType
from typing import (
    Any,
    Callable,
    Dict,
    FrozenSet,
    List,
    Optional,
    Set,
    Tuple,
    Type,
    TypedDict,
    Union,
)

from pydantic import BaseModel, ConfigDict, Field
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from dataclasses import dataclass
from ell.types.message import LMP
====================================================================
-> Chunk: openai_realtime\client.py::4


class RealtimeClient(RealtimeEventHandler):

    def add_tool(self, definition, handler):
        if not definition.get('name'):
            raise ValueError("Missing tool name in definition")
        name = definition['name']
        if name in self.tools:
            raise ValueError(f"Tool '{name}' already added. Please use .remove_tool('{name}') before trying to add again.")
        if not callable(handler):
            raise ValueError(f"Tool '{name}' handler must be a function")
        self.tools[name] = {'definition': definition, 'handler': handler}
        self.update_session()
        return self.tools[name]
====================================================================
-> Chunk: studio\__main__.py::2


def main():
    parser = ArgumentParser(description="ell studio")
    parser.add_argument("--storage-dir" , default=None,
                        help="Directory for filesystem serializer storage (default: current directory)")
    parser.add_argument("--pg-connection-string", default=None,
                        help="PostgreSQL connection string (default: None)")
    parser.add_argument("--host", default="127.0.0.1", help="Host to run the server on (default: localhost)")
    parser.add_argument("--port", type=int, default=5555, help="Port to run the server on (default: 5555)")
    parser.add_argument("--dev", action="store_true", help="Run in development mode")
    parser.add_argument("--open", action="store_true", help="Opens the studio web UI in a browser")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enables debug logging for more verbose output")
    args = parser.parse_args()

    _setup_logging(logging.DEBUG if args.verbose else logging.INFO)

    if args.dev:
        assert args.port == 5555, "Port must be 5000 in development mode"

    config = Config.create(storage_dir=args.storage_dir,
                    pg_connection_string=args.pg_connection_string)
    app = create_app(config)

    if not args.dev:
        # In production mode, serve the built React app
        static_dir = Path(__file__).parent / "static"
        # app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")

        @app.get("/{full_path:path}")
        async def serve_react_app(full_path: str):
            file_path = static_dir / full_path
            if file_path.exists() and file_path.is_file():
                return FileResponse(file_path)
            else:
                return FileResponse(static_dir / "index.html")

    # Respect Config.create behavior, which has fallback to env vars.
    db_path = Path(config.storage_dir) if config.storage_dir else None
    # ... other code
====================================================================
-> Chunk: ell\configurator.py::4


class Config(BaseModel):



    @contextmanager
    def model_registry_override(self, overrides: Dict[str, _Model]):
        """
        Temporarily override the model registry with new model configurations.

        :param overrides: A dictionary of model names to ModelConfig instances to override.
        :type overrides: Dict[str, ModelConfig]
        """
        if not hasattr(self._local, 'stack'):
            self._local.stack = []

        with self._lock:
            current_registry = self._local.stack[-1] if self._local.stack else self.registry
            new_registry = current_registry.copy()
            new_registry.update(overrides)

        self._local.stack.append(new_registry)
        try:
            yield
        finally:
            self._local.stack.pop()
====================================================================
-> Chunk: ell\configurator.py::1


from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)
====================================================================


###### Cluster Eval ######
Score: 2.3333333333333335
Cluster name: Random 8
Random 8:

-> Chunk: util\verbosity.py::1


import sys
import hashlib
import shutil
import textwrap
from typing import Dict, Tuple, List, Any, Optional
from colorama import Fore, Style, init
from ell.types import Message
from ell.configurator import config
import logging
from functools import lru_cache
import threading
from ell.types.message import LMP, ContentBlock, _content_to_text
import requests

from ell.util.plot_ascii import plot_ascii

# Initialize colorama
init(autoreset=True)

# Define colors and styles
ELL_COLORS = {k: v for k, v in vars(Fore).items() if k not in ['RESET', 'BLACK', 'LIGHTBLACK_EX']}
BOLD = Style.BRIGHT
UNDERLINE = '\033[4m'
RESET = Style.RESET_ALL
SYSTEM_COLOR = Fore.CYAN
USER_COLOR = Fore.GREEN
ASSISTANT_COLOR = Fore.YELLOW
PIPE_COLOR = Fore.BLUE

# Set up logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

has_logged_version_statement = False

_version_check_lock = threading.Lock()
_has_logged_version_statement = False
====================================================================
-> Chunk: types\_lstr.py::9


class _lstr(str):

    def __getitem__(self, key: Union[SupportsIndex, slice]) -> "_lstr":
        """
        Get a slice or index of this lstr instance.

        Args:
            key (Union[SupportsIndex, slice]): The index or slice to retrieve.

        Returns:
            lstr: A new lstr instance containing the sliced or indexed content, with theorigin_trace(s) preserved.
        """
        result = super(_lstr, self).__getitem__(key)
        # This is a matter of opinon. I believe that when you Index into a language model output, you or divorcing the lodges of the indexed result from their contacts which produce them. Therefore, it is only reasonable to directly index into the lodges without changing the original context, and so any mutation on the string should invalidate the logits.
        # try:
        #     logit_subset = self._logits[key] if self._logits else None
        # except:
        #   logit_subset = None
        logit_subset = None
        return _lstr(result, logit_subset, self.__origin_trace__)
====================================================================
-> Chunk: studio\__main__.py::3


def main():
    # ... other code

    async def db_watcher(db_path, app):
        last_stat = None

        while True:
            await asyncio.sleep(0.1)  # Fixed interval of 0.1 seconds
            try:
                current_stat = db_path.stat()

                if last_stat is None:
                    logger.info(f"Database file found: {db_path}")
                    await app.notify_clients("database_updated")
                else:
                    # Use a threshold for time comparison to account for filesystem differences
                    time_threshold = 0.1  # 1 second threshold
                    time_changed = abs(current_stat.st_mtime - last_stat.st_mtime) > time_threshold
                    size_changed = current_stat.st_size != last_stat.st_size
                    inode_changed = current_stat.st_ino != last_stat.st_ino

                    if time_changed or size_changed or inode_changed:
                        logger.info(
                            f"Database changed: mtime {time.ctime(last_stat.st_mtime)} -> {time.ctime(current_stat.st_mtime)}, "
                            f"size {last_stat.st_size} -> {current_stat.st_size}, "
                            f"inode {last_stat.st_ino} -> {current_stat.st_ino}"
                        )
                        await app.notify_clients("database_updated")

                last_stat = current_stat
            except FileNotFoundError:
                if last_stat is not None:
                    logger.info(f"Database file deleted: {db_path}")
                    await app.notify_clients("database_updated")
                last_stat = None
                await asyncio.sleep(1)  # Wait a bit longer if the file is missing
            except Exception as e:
                logger.info(f"Error checking database file: {e}")
                await asyncio.sleep(1)  # Wait a bit longer on errors
    # ... other code
====================================================================
-> Chunk: ell\store.py::1


from abc import ABC, abstractmethod
from contextlib import contextmanager
from datetime import datetime
from typing import Any, Optional, Dict, List, Set, Union
from ell.types._lstr import _lstr
from ell.types import SerializedLMP, Invocation
from ell.types.message import InvocableLM

class BlobStore(ABC):
    @abstractmethod
    def store_blob(self, blob: bytes, blob_id  : str) -> str:
        """Store a blob and return its identifier."""
        pass

    @abstractmethod
    def retrieve_blob(self, blob_id: str) -> bytes:
        """Retrieve a blob by its identifier."""
        pass
====================================================================
-> Chunk: lmp\complex.py::3


def complex(model: str, client: Optional[Any] = None, tools: Optional[List[Callable]] = None, exempt_from_tracking=False, post_callback: Optional[Callable] = None, **api_params):
    def parameterized_lm_decorator(
        prompt: LMP,
    ) -> Callable[..., Union[List[Message], Message]]:
        # ... other code



        model_call.__ell_api_params__ = default_api_params_from_decorator #type: ignore
        model_call.__ell_func__ = prompt #type: ignore
        model_call.__ell_type__ = LMPType.LM #type: ignore
        model_call.__ell_exempt_from_tracking = exempt_from_tracking #type: ignore


        if exempt_from_tracking:
            return model_call
        else:
            # XXX: Analyze decorators with AST instead.
            return _track(model_call, forced_dependencies=dict(tools=tools, response_format=api_params.get("response_format", {})))
    return parameterized_lm_decorator
====================================================================


###### Cluster Eval ######
Score: 2.0
Cluster name: Random 0
Random 0:

-> Chunk: util\closure.py::18


def globalvars(func, recurse=True, builtin=False):
    """get objects defined in global scope that are referred to by func

    return a dict of {name:object}"""
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__
    if inspect.ismethod(func): func = func.__func__
    while hasattr(func, "__ell_func__"):
        func = func.__ell_func__
    if inspect.isfunction(func):
        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}
        # get references from within closure
        orig_func, func = func, set()
        for obj in orig_func.__closure__ or {}:
            try:
                cell_contents = obj.cell_contents
            except ValueError: # cell is empty
                pass
            else:
                _vars = globalvars(cell_contents, recurse, builtin) or {}
                func.update(_vars) #XXX: (above) be wary of infinte recursion?
                globs.update(_vars)
        # get globals
        globs.update(orig_func.__globals__ or {})
        # get names of references
        if not recurse:
            func.update(orig_func.__code__.co_names)
        else:
            func.update(nestedglobals(orig_func.__code__))
            # find globals for all entries of func
            for key in func.copy(): #XXX: unnecessary...?
                nested_func = globs.get(key)
                if nested_func is orig_func:
                   #func.remove(key) if key in func else None
                    continue  #XXX: globalvars(func, False)?
                func.update(globalvars(nested_func, True, builtin))
    elif inspect.iscode(func):
        globs = vars(inspect.getmodule(sum)).copy() if builtin else {}
       #globs.update(globals())
        if not recurse:
            func = func.co_names # get names
        else:
            orig_func = func.co_name # to stop infinite recursion
            func = set(nestedglobals(func))
            # find globals for all entries of func
            for key in func.copy(): #XXX: unnecessary...?
                if key is orig_func:
                   #func.remove(key) if key in func else None
                    continue  #XXX: globalvars(func, False)?
                nested_func = globs.get(key)
                func.update(globalvars(nested_func, True, builtin))
    # elif inspect.isclass(func):
    # XXX: We need to get lexical closures of all the methods and attributes of the class.\
    # In the future we should exhaustively walk the AST here.
    else:
        return {}
    #NOTE: if name not in __globals__, then we skip it...
    return dict((name,globs[name]) for name in func if name in globs)
====================================================================
-> Chunk: ell\configurator.py::6


class Config(BaseModel):

    def register_provider(self, provider: Provider, client_type: Type[Any]) -> None:
        """
        Register a provider class for a specific client type.

        :param provider_class: The provider class to register.
        :type provider_class: Type[Provider]
        """
        assert isinstance(client_type, type), "client_type must be a type (e.g. openai.Client), not an an instance (myclient := openai.Client()))"
        with self._lock:
            self.providers[client_type] = provider
====================================================================
-> Chunk: 0.1.0\cem.py::5


def select_elite(trajectories, percentile=ELITE_PERCENT):
    rewards = [traj['reward'] for traj in trajectories]
    if not rewards:
        return []
    reward_threshold = np.percentile(rewards, 100 - percentile)
    elite_trajectories = [traj for traj in trajectories if traj['reward'] >= reward_threshold]
    return elite_trajectories

# Function to create training dataset from elite trajectories
def create_training_data(elite_trajectories):
    states = []
    actions = []
    for traj in elite_trajectories:
        states.extend(traj['states'])
        actions.extend(traj['actions'])
    if not states or not actions:
        return None, None
    # Convert lists to NumPy arrays first for efficiency
    states = np.array(states, dtype=np.float32)
    actions = np.array(actions, dtype=np.int64)
    # Convert to PyTorch tensors
    states = torch.from_numpy(states)
    actions = torch.from_numpy(actions)
    return states, actions
====================================================================
-> Chunk: openai_realtime\api.py::2


class RealtimeAPI(RealtimeEventHandler):

    async def connect(self, model='gpt-4o-realtime-preview-2024-10-01'):
        if self.is_connected():
            raise Exception("Already connected")

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'OpenAI-Beta': 'realtime=v1'
        }

        self.ws = await websockets.connect(f"{self.url}?model={model}", extra_headers=headers)

        self.log(f"Connected to {self.url}")

        asyncio.create_task(self._message_handler())

        return True
====================================================================
-> Chunk: providers\openai.py::2


def _content_block_to_openai_format(content_block: ContentBlock) -> Dict[str, Any]:
    if (image := content_block.image):
        image_url = dict(url=serialize_image(image.image) if image.image else image.url)
        # XXX: Solve per content params better
        if image.detail: image_url["detail"] = image.detail
        return {
            "type": "image_url",
            "image_url": image_url
        }
    elif ((text := content_block.text) is not None): return dict(type="text", text=text)
    elif (parsed := content_block.parsed): return dict(type="text", text=parsed.model_dump_json())
    else:
        raise ValueError(f"Unsupported content block type for openai: {content_block}")
====================================================================


###### Cluster Eval ######
Score: 2.0
Cluster name: Random 1
Random 1:

-> Chunk: providers\anthropic.py::1


from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union, cast
from ell.provider import  EllCallParams, Metadata, Provider
from ell.types import Message, ContentBlock, ToolCall, ImageContent

from ell.types._lstr import _lstr
from ell.types.message import LMP
from ell.configurator import register_provider
from ell.util.serialization import serialize_image
import base64
from io import BytesIO
import json
import requests
from PIL import Image as PILImage

try:
    import anthropic
    from anthropic import Anthropic
    from anthropic.types import Message as AnthropicMessage, MessageParam, RawMessageStreamEvent
    from anthropic.types.message_create_params import MessageCreateParamsStreaming
    from anthropic._streaming import Stream

    class AnthropicProvider(Provider):
        dangerous_disable_validation = True

        def provider_call_function(self, client : Anthropic, api_call_params : Optional[Dict[str, Any]] = None) -> Callable[..., Any]:
            return client.messages.create

        def translate_to_provider(self, ell_call : EllCallParams):
            final_call_params = cast(MessageCreateParamsStreaming, ell_call.api_params.copy())
            # XXX: Helper, but should be depreicated due to ssot
            assert final_call_params.get("max_tokens") is not None, f"max_tokens is required for anthropic calls, pass it to the @ell.simple/complex decorator, e.g. @ell.simple(..., max_tokens=your_max_tokens) or pass it to the model directly as a parameter when calling your LMP: your_lmp(..., api_params=({{'max_tokens': your_max_tokens}}))."

            dirty_msgs = [
                MessageParam(
                    role=cast(Literal["user", "assistant"], message.role), 
                    content=[_content_block_to_anthropic_format(c) for c in message.content]) for message in ell_call.messages]
            role_correct_msgs   : List[MessageParam] = []
            for msg in dirty_msgs:
                if (not len(role_correct_msgs) or role_correct_msgs[-1]['role'] != msg['role']):
                    role_correct_msgs.append(msg)
                else: cast(List, role_correct_msgs[-1]['content']).extend(msg['content'])

            system_message = None
            if role_correct_msgs and role_correct_msgs[0]["role"] == "system":
                system_message = role_correct_msgs.pop(0)

            if system_message:
                final_call_params["system"] = system_message["content"][0]["text"]


            final_call_params['stream'] = True
            final_call_params["model"] = ell_call.model
            final_call_params["messages"] = role_correct_msgs

            if ell_call.tools:
                final_call_params["tools"] = [
                    #XXX: Cleaner with LMP's as a class.
                    dict(
                        name=tool.__name__,
                        description=tool.__doc__,
                        input_schema=tool.__ell_params_model__.model_json_schema(),
                    )
                    for tool in ell_call.tools
                ]

            # print(final_call_params)
            return final_call_params

        def translate_from_provider(
            self,
            provider_response : Union[Stream[RawMessageStreamEvent], AnthropicMessage],
            ell_call: EllCallParams,
            provider_call_params: Dict[str, Any],
            origin_id: Optional[str] = None,
            logger: Optional[Callable[..., None]] = None,
        ) -> Tuple[List[Message], Metadata]:

            usage = {}
            tracked_results = []
            metadata = {}

            #XXX: Support n > 0

            if provider_call_params.get("stream", False):
                content = []
                current_blocks: Dict[int, Dict[str, Any]] = {}
                message_metadata = {}

                with cast(Stream[RawMessageStreamEvent], provider_response) as stream:
                    for chunk in stream:
                        if chunk.type == "message_start":
                            message_metadata = chunk.message.model_dump()
                            message_metadata.pop("content", None)  # Remove content as we'll build it separately

                        elif chunk.type == "content_block_start":
                            block = chunk.content_block.model_dump()
                            current_blocks[chunk.index] = block
                            if block["type"] == "tool_use":
                                if logger: logger(f" <tool_use: {block['name']}(")
                                block["input"] = "" # force it to be a string, XXX: can implement partially parsed json later.
                        elif chunk.type == "content_block_delta":
                            if chunk.index in current_blocks:
                                block = current_blocks[chunk.index]
                                if (delta := chunk.delta).type == "text_delta":
                                    block["text"] += delta.text
                                    if logger: logger(delta.text)
                                if delta.type == "input_json_delta":
                                    block["input"] += delta.partial_json
                                    if logger: logger(delta.partial_json)

                        elif chunk.type == "content_block_stop":
                            if chunk.index in current_blocks:
                                block = current_blocks.pop(chunk.index)
                                if block["type"] == "text":
                                    content.append(ContentBlock(text=_lstr(block["text"],origin_trace=origin_id)))
                                elif block["type"] == "tool_use":
                                    try:
                                        matching_tool = ell_call.get_tool_by_name(block["name"])
                                        if matching_tool:
                                            content.append(
                                                ContentBlock(
                                                    tool_call=ToolCall(
                                                        tool=matching_tool,
                                                        tool_call_id=_lstr(
                                                            block['id'],origin_trace=origin_id
                                                        ),
                                                        params=json.loads(block['input']) if block['input'] else {},
                                                    )
                                                )
                                            )
                                    except json.JSONDecodeError:
                                        if logger: logger(f" - FAILED TO PARSE JSON")
                                        pass
                                    if logger: logger(f")>")

                        elif chunk.type == "message_delta":
                            message_metadata.update(chunk.delta.model_dump())
                            if chunk.usage:
                                usage.update(chunk.usage.model_dump())

                        elif chunk.type == "message_stop":
                            tracked_results.append(Message(role="assistant", content=content))

                        # print(chunk)
                metadata = message_metadata

            # process metadata for ell
            # XXX: Unify an ell metadata format for ell studio.
            usage["prompt_tokens"] = usage.get("input_tokens", 0)
            usage["completion_tokens"] = usage.get("output_tokens", 0)
            usage["total_tokens"] = usage['prompt_tokens'] + usage['completion_tokens']

            metadata["usage"] = usage
            return tracked_results, metadata

    # XXX: Make a singleton.
    anthropic_provider = AnthropicProvider()
    register_provider(anthropic_provider, anthropic.Anthropic)
    register_provider(anthropic_provider, anthropic.AnthropicBedrock)
    register_provider(anthropic_provider, anthropic.AnthropicVertex)

except ImportError:
    pass
====================================================================
-> Chunk: ell\provider.py::3


# XXX: Needs a better name.
class Provider(ABC):
    """
    Abstract base class for all providers. Providers are API interfaces to language models, not necessarily API providers.
    For example, the OpenAI provider is an API interface to OpenAI's API but also to Ollama and Azure OpenAI.
    In Ell. We hate abstractions. The only reason this exists is to force implementers to implement their own provider correctly -_-.
    """
    dangerous_disable_validation = False

    ################################
    ### API PARAMETERS #############
    ################################
    @abstractmethod
    def provider_call_function(
        self, client: Any, api_call_params: Optional[Dict[str, Any]] = None
    ) -> Callable[..., Any]:
        """
        Implement this method to return the function that makes the API call to the language model.
        For example, if you're implementing the OpenAI provider, you would return the function that makes the API call to OpenAI's API.
        """
        return NotImplemented

    def disallowed_api_params(self) -> FrozenSet[str]:
        """
        Returns a list of disallowed call params that ell will override.
        """
        return frozenset({"messages", "tools", "model", "stream", "stream_options"})

    def available_api_params(self, client: Any, api_params: Optional[Dict[str, Any]] = None):
        params = _call_params(self.provider_call_function(client, api_params))
        return frozenset(params.keys()) - self.disallowed_api_params()

    ################################
    ### TRANSLATION ###############
    ################################
    @abstractmethod
    def translate_to_provider(self, ell_call: EllCallParams) -> Dict[str, Any]:
        """Converts an ell call to provider call params!"""
        return NotImplemented

    @abstractmethod
    def translate_from_provider(
        self,
        provider_response: Any,
        ell_call: EllCallParams,
        provider_call_params: Dict[str, Any],
        origin_id: Optional[str] = None,
        logger: Optional[Callable[..., None]] = None,
    ) -> Tuple[List[Message], Metadata]:
        """Converts provider responses to universal format. with metadata"""
        return NotImplemented

    ################################
    ### CALL MODEL ################
    ################################
    # Be careful to override this method in your provider.
====================================================================
-> Chunk: lmp\_track.py::3


def _serialize_lmp(func):
    # Serialize deptjh first all fo the used lmps.
    for f in func.__ell_uses__:
        _serialize_lmp(f)

    if getattr(func, "_has_serialized_lmp", False):
        return
    func._has_serialized_lmp = False
    fn_closure = func.__ell_closure__
    lmp_type = func.__ell_type__
    name = func.__qualname__
    api_params = getattr(func, "__ell_api_params__", None)

    lmps = config.store.get_versions_by_fqn(fqn=name)
    version = 0
    already_in_store = any(lmp.lmp_id == func.__ell_hash__ for lmp in lmps)

    if not already_in_store:
        commit = None
        if lmps:
            latest_lmp = max(lmps, key=lambda x: x.created_at)
            version = latest_lmp.version_number + 1
            if config.autocommit:
                # XXX: Move this out to autocommit itself.
                if not _autocommit_warning():
                    from ell.util.differ import write_commit_message_for_diff
                    commit = str(write_commit_message_for_diff(
                    f"{latest_lmp.dependencies}\n\n{latest_lmp.source}", 
                        f"{fn_closure[1]}\n\n{fn_closure[0]}")[0])

        serialized_lmp = SerializedLMP(
            lmp_id=func.__ell_hash__,
            name=name,
            created_at=utc_now(),
            source=fn_closure[0],
            dependencies=fn_closure[1],
            commit_message=commit,
            initial_global_vars=get_immutable_vars(fn_closure[2]),
            initial_free_vars=get_immutable_vars(fn_closure[3]),
            lmp_type=lmp_type,
            api_params=api_params if api_params else None,
            version_number=version,
        )
        config.store.write_lmp(serialized_lmp, [f.__ell_hash__ for f in func.__ell_uses__])
    func._has_serialized_lmp = True
====================================================================
-> Chunk: openai_realtime\api.py::1


import asyncio
import json
import websockets
from .event_handler import RealtimeEventHandler
from .utils import RealtimeUtils

class RealtimeAPI(RealtimeEventHandler):
    def __init__(self, url=None, api_key=None, dangerously_allow_api_key_in_browser=False, debug=False):
        super().__init__()
        self.default_url = 'wss://api.openai.com/v1/realtime'
        self.url = url or self.default_url
        self.api_key = api_key
        self.debug = debug
        self.ws = None

    def is_connected(self):
        return self.ws is not None and self.ws.open

    def log(self, *args):
        if self.debug:
            print(*args)
        return True
====================================================================
-> Chunk: ell\configurator.py::4


class Config(BaseModel):



    @contextmanager
    def model_registry_override(self, overrides: Dict[str, _Model]):
        """
        Temporarily override the model registry with new model configurations.

        :param overrides: A dictionary of model names to ModelConfig instances to override.
        :type overrides: Dict[str, ModelConfig]
        """
        if not hasattr(self._local, 'stack'):
            self._local.stack = []

        with self._lock:
            current_registry = self._local.stack[-1] if self._local.stack else self.registry
            new_registry = current_registry.copy()
            new_registry.update(overrides)

        self._local.stack.append(new_registry)
        try:
            yield
        finally:
            self._local.stack.pop()
====================================================================


###### Cluster Eval ######
Score: 2.0
Cluster name: Random 3
Random 3:

-> Chunk: types\_lstr.py::9


class _lstr(str):

    def __getitem__(self, key: Union[SupportsIndex, slice]) -> "_lstr":
        """
        Get a slice or index of this lstr instance.

        Args:
            key (Union[SupportsIndex, slice]): The index or slice to retrieve.

        Returns:
            lstr: A new lstr instance containing the sliced or indexed content, with theorigin_trace(s) preserved.
        """
        result = super(_lstr, self).__getitem__(key)
        # This is a matter of opinon. I believe that when you Index into a language model output, you or divorcing the lodges of the indexed result from their contacts which produce them. Therefore, it is only reasonable to directly index into the lodges without changing the original context, and so any mutation on the string should invalidate the logits.
        # try:
        #     logit_subset = self._logits[key] if self._logits else None
        # except:
        #   logit_subset = None
        logit_subset = None
        return _lstr(result, logit_subset, self.__origin_trace__)
====================================================================
-> Chunk: 0.1.0\context_versioning.py::1


import inspect
import ast
from contextlib import contextmanager

@contextmanager
def context():
    # Get the current frame
    frame = inspect.currentframe()
    try:
        # Get the caller's frame
        caller_frame = frame.f_back.f_back
        # Get the filename and line number where the context manager is called
        filename = caller_frame.f_code.co_filename
        lineno = caller_frame.f_lineno

        # Read the source code from the file
        with open(filename, 'r') as f:
            source = f.read()

        # Parse the source code into an AST
        parsed = ast.parse(source, filename)
        # print(source)
        # Find the 'with' statement at the given line number
        class WithVisitor(ast.NodeVisitor):
            def __init__(self, target_lineno):
                self.target_lineno = target_lineno
                self.with_node = None

            def visit_With(self, node):
                if node.lineno <= self.target_lineno <= node.end_lineno:
                    self.with_node = node
                self.generic_visit(node)

        visitor = WithVisitor(lineno)
        visitor.visit(parsed)

        # print(parsed, source)
        if visitor.with_node:
            # Extract the source code of the block inside 'with'
            start = visitor.with_node.body[0].lineno
            end = visitor.with_node.body[-1].end_lineno
            block_source = '\n'.join(source.splitlines()[start-1:end])
            print("Source code inside 'with' block:")
            print(block_source)
        else:
            print("Could not find the 'with' block.")

        # Yield control to the block inside 'with'
        yield
    finally:
        # Any cleanup can be done here
        pass

from context_versioning import context
# Example usage
if __name__ == "__main__":
    with context():
        x = 10
        y = x * 2
        print(y)
====================================================================
-> Chunk: 0.1.0\cem.py::6


# Main execution code
if __name__ == '__main__':
    # Initialize environments
    env_fns = [make_env(ENV_NAME, SEED + i) for i in range(NUM_ENVIRONMENTS)]
    envs = AsyncVectorEnv(env_fns)

    # Get environment details
    dummy_env = gym.make(ENV_NAME)
    state_dim = dummy_env.observation_space.shape[0]
    action_dim = dummy_env.action_space.n
    dummy_env.close()

    # Initialize policy network and optimizer
    policy = PolicyNetwork(state_dim, action_dim)
    optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    # Training Loop
    for iteration in range(1, NUM_ITERATIONS + 1):
        try:
            # Step 1: Collect Trajectories
            trajectories = collect_trajectories(envs, policy, TRAJECTORIES_PER_ITER, MAX_STEPS)
        except Exception as e:
            print(f"Error during trajectory collection at iteration {iteration}: {e}")
            break

        # Step 2: Select Elite Trajectories
        elite_trajectories = select_elite(trajectories, ELITE_PERCENT)

        if len(elite_trajectories) == 0:
            print(f"Iteration {iteration}: No elite trajectories found. Skipping update.")
            continue

        # Step 3: Create Training Data
        states, actions = create_training_data(elite_trajectories)

        if states is None or actions is None:
            print(f"Iteration {iteration}: No training data available. Skipping update.")
            continue

        # Step 4: Behavioral Cloning (Policy Update)
        dataset_size = states.size(0)
        indices = np.arange(dataset_size)
        np.random.shuffle(indices)

        for start in range(0, dataset_size, BATCH_SIZE):
            end = start + BATCH_SIZE
            batch_indices = indices[start:end]
            batch_states = states[batch_indices]
            batch_actions = actions[batch_indices]

            optimizer.zero_grad()
            logits = policy(batch_states)
            loss = criterion(logits, batch_actions)
            loss.backward()
            optimizer.step()

        # Step 5: Evaluate Current Policy
        avg_reward = np.mean([traj['reward'] for traj in elite_trajectories])
        print(f"Iteration {iteration}: Elite Trajectories: {len(elite_trajectories)}, Average Reward: {avg_reward:.2f}")

    # Close environments
    envs.close()

    # Testing the Trained Policy
    def test_policy(policy, env_name=ENV_NAME, episodes=5, max_steps=500):
        env = gym.make(env_name)
        total_rewards = []
        for episode in range(episodes):
            obs, _ = env.reset()
            done = False
            episode_reward = 0
            for _ in range(max_steps):
                obs_tensor = torch.from_numpy(obs).float().unsqueeze(0)
                with torch.no_grad():
                    action = policy.get_action(obs_tensor).item()
                obs, reward, done, info, _ = env.step(action)
                episode_reward += reward
                if done:
                    break
            total_rewards.append(episode_reward)
            print(f"Test Episode {episode + 1}: Reward: {episode_reward}")
        env.close()
        print(f"Average Test Reward over {episodes} episodes: {np.mean(total_rewards):.2f}")

    # Run the test
    test_policy(policy)
====================================================================
-> Chunk: ell\configurator.py::1


from functools import lru_cache, wraps
from typing import Dict, Any, Optional, Tuple, Union, Type
import openai
import logging
from contextlib import contextmanager
import threading
from pydantic import BaseModel, ConfigDict, Field
from ell.store import Store
from ell.provider import Provider
from dataclasses import dataclass, field

_config_logger = logging.getLogger(__name__)

@dataclass(frozen=True)
class _Model:
    name: str
    default_client: Optional[Union[openai.Client, Any]] = None
    #XXX: Deprecation in 0.1.0
    #XXX: We will depreciate this when streaming is implemented. 
    # Currently we stream by default for the verbose renderer,
    # but in the future we will not support streaming by default 
    # and stream=True must be passed which will then make API providers the
    # single source of truth for whether or not a model supports an api parameter.
    # This makes our implementation extremely light, only requiring us to provide
    # a list of model names in registration.
    supports_streaming : Optional[bool] = field(default=None)
====================================================================
-> Chunk: ell\provider.py::1


from abc import ABC, abstractmethod
from collections import defaultdict
from functools import lru_cache
import inspect
from types import MappingProxyType
from typing import (
    Any,
    Callable,
    Dict,
    FrozenSet,
    List,
    Optional,
    Set,
    Tuple,
    Type,
    TypedDict,
    Union,
)

from pydantic import BaseModel, ConfigDict, Field
from ell.types import Message, ContentBlock, ToolCall
from ell.types._lstr import _lstr
import json
from dataclasses import dataclass
from ell.types.message import LMP
====================================================================


Total coherence score: 2.6333333333333333
